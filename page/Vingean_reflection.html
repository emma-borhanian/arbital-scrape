<!DOCTYPE html><html><head><meta charset="utf-8"><title>Vingean reflection</title><link rel="stylesheet" type="text/css" href="../common.css"><link rel="stylesheet" type="text/css" href="../page-style.css"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  SVG: {EqnChunk: 50, EqnChunkFactor: 1.5, EqChunkDelay: 10, useFontCache: false, linebreaks: {automatic: true}},
  tex2jax: {
    inlineMath: [['$~$', '$~$']],
    displayMath: [['$$~$', '$~$$']],
    processEscapes: true,
    preview: 'none',
  },
  showProcessingMessages: false,
  messageStyle: 'none',
  // http://docs.mathjax.org/en/latest/config-files.html#the-tex-ams-svg-configuration-file
  jax: ["input/TeX","output/SVG", "output/PreviewHTML"],
  extensions: ["tex2jax.js","MathMenu.js","MathZoom.js", "fast-preview.js", "AssistiveMML.js", "a11y/accessibility-menu.js"],
  TeX: { extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"] }
});</script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_SVG" async></script><script type="text/javascript" src="../arbital-demo-bundle.js"></script><script type="text/javascript">window.addEventListener('DOMContentLoaded', e=>window.loadAllDemos())
</script></head><body><header><h1 class="title">Vingean reflection</h1><div class="page-info"><p class="metadata-link"><a href="../metadata/Vingean_reflection.json.html">Vingean_reflection.json</a></p><p class="arbital-url"><a href="https://arbital.com/p/Vingean_reflection">https://arbital.com/p/Vingean_reflection</a></p><p class="creator">by
 <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a> Dec 18 2015 
updated
 Jun 21 2016</p></div><p class="clickbait">The problem of thinking about your future self when it's smarter than you.</p><nav class="breadcrumbs"><nav class="breadcrumb" aria-label="Breadcrumb"><ul><li><a href="../index.html">Index</a></li><li>Vingean reflection</li></ul></nav><nav class="breadcrumb" aria-label="Breadcrumb"><ul><li><a href="ai_alignment.html">AI alignment</a></li><li>â€¦</li></ul></nav></nav></header><hr><main><p>[summary:  <a href="Vinge_principle.html">Vinge&#39;s Principle</a> implies that when an agent is designing another agent (or modifying its own code), it needs to approve the other agent's design without knowing the other agent's exact future actions.  <a href="Vingean_reflection.html">Vingean reflection</a> is reasoning about cognitive systems, especially cognitive systems very similar to yourself (including your actual self), under the constraint that you can't predict the exact future outputs.</p>
<p>In <a href="tiling_agents.html">Tiling agents theory</a>, this appears as the rule that we should talk about our successor's actions only inside of quantifiers.</p>
<p>"Vingean reflection" may be a much more general issue in the design of advanced cognitive systems than it might appear at first glance.  An agent reasoning about the consequences of <em>its current code</em>, or considering what will happen if it <em>spends another minute thinking,</em> can be viewed as doing Vingean reflection.   Vingean reflection can also be seen as the study of how a given agent <em>wants</em> thinking to occur in cognitive computations, which may be importantly different from how the agent <em>currently</em> thinks.]</p>
<p><a href="Vinge_principle.html">Vinge&#39;s Principle</a> implies that when an agent is designing another agent (or modifying its own code), it needs to approve the other agent's design without knowing the other agent's exact future actions.</p>
<p>Deep Blue's programmers decided to run Deep Blue, <em>without</em> knowing Deep Blue's exact moves against Kasparov or how Kasparov would reply to each move, and without being able to visualize the exact real-outcome instead.  Instead, by reasoning about the way Deep Blue was searching through game trees, they arrived at a well-justified but abstract belief that Deep Blue was 'trying to win' (rather than trying to lose) and reasoning effectively to that end.</p>
<p><a href="Vingean_reflection.html">Vingean reflection</a> is reasoning about cognitive systems, especially cognitive systems very similar to yourself (including your actual self), under the constraint that you can't predict the exact future outputs.  We need to make predictions about the consequence of operating an agent in an environment via reasoning on some more abstract level, somehow.</p>
<p>In <a href="tiling_agents.html">Tiling agents theory</a>, this appears as the rule that we should talk about our successor's actions only inside of quantifiers.</p>
<p>"Vingean reflection" may be a much more general issue in the design of advanced cognitive systems than it might appear at first glance.  An agent reasoning about the consequences of <em>its current code</em>, or considering what will happen if it <em>spends another minute thinking,</em> can be viewed as doing Vingean reflection.  A reflective, self-modeling chess-player would not choose to spend another minute thinking, if it thought that its further thoughts would be trying to lose rather than win the game - but it can't predict its own exact thoughts in advance.</p>
<p>Vingean reflection can also be seen as the study of how a given agent <em>wants</em> thinking to occur in cognitive computations, which may be importantly different from how the agent <em>currently</em> thinks.  If these two coincide, we say the agent is <a href="reflective_stability.html">reflectively stable</a>.</p>
<p><a href="tiling_agents.html">Tiling agents theory</a> is presently the main line of research trying to slowly get started on formalizing Vingean reflection and reflective stability.</p>
<p>Further reading:</p>
<ul>
<li><a href="http://intelligence.org/files/VingeanReflection.pdf">http://intelligence.org/files/VingeanReflection.pdf</a></li>
<li><a href="http://intelligence.org/files/TilingAgentsDraft.pdf">http://intelligence.org/files/TilingAgentsDraft.pdf</a></li>
</ul></main><hr><footer><p class="tagged"><h2>Tagged</h2><span class="page-comma-list"><a class="page-link" href="../page/stub_meta_tag.html">Stub</a>,
 <a class="page-link" href="../page/Vingean_uncertainty.html">Vingean uncertainty</a></span></p><p class="likes"><h2>Likes</h2><span class="page-comma-list"><a class="page-link" href="../page/AlexRay.html">Alex Ray</a>,
 <a class="page-link" href="../page/StevenDee.html">Steven Dee</a></span></p><p class="reverse-related"><h2>Reverse Related</h2><ul class="page-list"><li><a class="page-link" href="../page/stub_meta_tag.html">Stub</a> <q>This page only gives a very brief overview of the topic. If you're able to, please help expand or improve it!</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/Vingean_uncertainty.html">Vingean uncertainty</a> <q>You can't predict the exact actions of an agent smarter than you - so is there anything you _can_ say about them?</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li></ul></p><p class="children"><h2>Children</h2><ul class="page-tree"><li><a class="page-link" href="../page/reflective_consistency.html">Reflective consistency</a> <q>A decision system is reflectively consistent if it can approve of itself, or approve the construction of similar decision systems (as well as perhaps approving other decision systems too).</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/reflective_stability.html">Reflective stability</a> <q>Wanting to think the way you currently think, building other agents and self-modifications that think the same way.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a><ul class="page-tree"><li><a class="page-link" href="../page/preference_stability.html">Consequentialist preferences are reflectively stable by default</a> <q>Gandhi wouldn't take a pill that made him want to kill people, because he knows in that case more people will be murdered.  A paperclip maximizer doesn't want to stop maximizing paperclips.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/otherizer.html">Other-izing (wanted: new optimization idiom)</a> <q>Maximization isn't possible for bounded agents, and satisficing doesn't seem like enough.  What other kind of 'izing' might be good for realistic, bounded agents?</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/reflective_degree_of_freedom.html">Reflectively consistent degree of freedom</a> <q>When an instrumentally efficient, self-modifying AI can be like X or like X' in such a way that X wants to be X and X' wants to be X', that's a reflectively consistent degree of freedom.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a><ul class="page-tree"><li><a class="page-link" href="../page/humean_free_boundary.html">Humean degree of freedom</a> <q>A concept includes 'Humean degrees of freedom' when the intuitive borders of the human version of that concept depend on our values, making that concept less natural for AIs to learn.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/value_laden.html">Value-laden</a> <q>Cure cancer, but avoid any bad side effects?  Categorizing &quot;bad side effects&quot; requires knowing what's &quot;bad&quot;.  If an agent needs to load complex human goals to evaluate something, it's &quot;value-laden&quot;.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li></ul></li></ul></li><li><a class="page-link" href="../page/tiling_agents.html">Tiling agents theory</a> <q>The theory of self-modifying agents that build successors that are very similar to themselves, like repeating tiles on a tesselated plane.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/Vinge_principle.html">Vinge's Principle</a> <q>An agent building another agent must usually approve its design without knowing the agent's exact policy choices.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li></ul></p></footer></body></html>