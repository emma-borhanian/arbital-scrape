<!DOCTYPE html><html><head><meta charset="utf-8"><title>Faithful simulation</title><link rel="stylesheet" type="text/css" href="../common.css"><link rel="stylesheet" type="text/css" href="../page-style.css"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  SVG: {EqnChunk: 50, EqnChunkFactor: 1.5, EqChunkDelay: 10, useFontCache: false, linebreaks: {automatic: true}},
  tex2jax: {
    inlineMath: [['$~$', '$~$']],
    displayMath: [['$$~$', '$~$$']],
    processEscapes: true,
    preview: 'none',
  },
  showProcessingMessages: false,
  messageStyle: 'none',
  // http://docs.mathjax.org/en/latest/config-files.html#the-tex-ams-svg-configuration-file
  jax: ["input/TeX","output/SVG", "output/PreviewHTML"],
  extensions: ["tex2jax.js","MathMenu.js","MathZoom.js", "fast-preview.js", "AssistiveMML.js", "a11y/accessibility-menu.js"],
  TeX: { extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"] }
});</script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_SVG" async></script><script type="text/javascript" src="../arbital-demo-bundle.js"></script><script type="text/javascript">window.addEventListener('DOMContentLoaded', e=>window.loadAllDemos())
</script></head><body><header><h1 class="title">Faithful simulation</h1><div class="page-info"><p class="metadata-link"><a href="../metadata/faithful_simulation.json.html">faithful_simulation.json</a></p><p class="arbital-url"><a href="https://arbital.com/p/faithful_simulation">https://arbital.com/p/faithful_simulation</a></p><p class="creator">by
 <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a> Apr 14 2016 
updated
 Apr 14 2016</p></div><p class="clickbait">How would you identify, to a Task AGI (aka Genie), the problem of scanning a human brain, and then running a sufficiently accurate simulation of it for the simulation to not be crazy or psychotic?</p><nav class="breadcrumbs"><nav class="breadcrumb" aria-label="Breadcrumb"><ul><li><a href="../index.html">Index</a></li><li>Faithful simulation</li></ul></nav><nav class="breadcrumb" aria-label="Breadcrumb"><ul><li><a href="ai_alignment.html">AI alignment</a></li><li><a href="AGI_typology.html">Strategic AGI typology</a></li><li><a href="task_agi.html">Task-directed AGI</a></li><li>…</li></ul></nav><nav class="breadcrumb" aria-label="Breadcrumb"><ul><li><a href="ai_alignment.html">AI alignment</a></li><li><a href="task_agi.html">Task-directed AGI</a></li><li>…</li></ul></nav></nav></header><hr><main><p>The safe simulation problem is to start with some dynamical physical process $~$D$~$ which would, if run long enough in some specified environment, produce some trustworthy information of great value, and to compute some <em>adequate</em> simulation $~$S_D$~$ of $~$D$~$ faster than the physical process could have run.  In this context, the term "adequate" is <a href="value_laden.html">value-laden</a> - it means that whatever we would use $~$D$~$ for, using $~$S_D$~$ instead produces within epsilon of the expected <a href="value_alignment_value.html">value</a> we could have gotten from using the real $~$D.$~$  In more concrete terms, for example, we might want to tell a Task AGI "upload this human and run them as a simulation", and we don't want some tiny systematic skew in how the Task AGI models serotonin to turn the human into a psychopath, which is a <em>bad</em> (value-destroying) simulation fault.  Perfect simulation will be out of the question; the brain is almost certainly a chaotic system and hence we can't hope to produce <em>exactly</em> the same result as a biological brain.  The question, then, is what kind not-exactly-the-same-result the simulation is allowed to produce.</p>
<p>As with "<a href="low_impact.html">low impact</a>" hopefully being lower-<a href="Kolmogorov_complexity.html">complexity</a> than "<a href="value_laden.html">low bad impact</a>", we might hope to get an <em>adequate</em> simulation via some notion of <em>faithful</em> simulation, which rules out bumps in serotonin that turn the upload into a psychopath, while possibly also ruling out any number of other changes we <em>wouldn't</em> see as important; with this notion of "faithfulness" still being permissive enough to allow the simulation to take place at a level above individual quarks.  On whatever computing power is available - possibly nanocomputers, if the brain was scanned via molecular nanotechnology - the upload must be runnable fast enough to <a href="pivotal.html">make the simulation task worthwhile</a>.</p>
<p>Since the main use for the notion of "faithful simulation" currently appears to be <a href="safe_plan_identification.html">identifying</a> a safe plan for uploading one or more humans as a <a href="pivotal.html">pivotal act</a>, we might also consider this problem in conjunction with the special case of wanting to avoid <a href="mindcrime.html">mindcrime</a>.  In other words, we'd like a criterion of faithful simulation which the AGI can compute <em>without</em> it needing to observe millions of hypothetical simulated brains for ten seconds apiece, which could constitute creating millions of people and killing them ten seconds later.  We'd much prefer, e.g., a criterion of faithful simulation of individual neurons and synapses between them up to the level of, say, two interacting cortical columns, such that we could be confident that in aggregate the faithful simulation of the neurons would correspond to the faithful simulation of whole human brains.  This way the AGI would not need to think about or simulate whole brains in order to verify that an uploading procedure would produce a faithful simulation, and mindcrime could be avoided.</p>
<p>Note that the notion of a "functional property" of the brain - seeing the neurons as computing something important, and not wanting to disturb the computation - is still value-laden.  It involves regarding the brain as a means to a computational end, and what we see as the important computational end is value-laden, given that chaos guarantees the input-output relation won't be <em>exactly</em> the same.  The brain can equally be seen as implicitly computing, say, the parity of the number of synapse activations; it's just that we don't see this functional property as a valuable one that we want to preserve.</p>
<p>To the extent that some notion of function might be invoked in a notion of faithful, permitted speedups, we should hope that rather than needing the AGI to understand the high-level functional properties of the brain <em>and which details we thought were too important to simplify,</em> it might be enough to understand a 'functional' model of individual neurons and synapses, with the resulting transform of the uploaded brain still allowing for a pivotal speedup <em>and</em> knowably-faithful simulation of the larger brain.</p>
<p>At the same time, strictly local measures of faithfulness seem problematic if they can conceal <em>systematic</em> larger divergences.  We might think that any perturbation of a simulated neuron which has as little effect as adding one phonon is "within thermal uncertainty" and therefore unimportant, but if all of these perturbations are pointing in the same direction relative to some larger functional property, the difference might be very significant.  Similarly if all simulated synapses released slightly more serotonin, rather than releasing slightly more or less serotonin in no particular systematic pattern.</p></main><hr><section class="comments"><h2>Comments</h2><div class="comment"><p><a class="page-link" href="../page/PaulChristiano.html">Paul Christiano</a></p><p><p>One natural standard: it should be hard to distinguish an adequate model from the system-to-be-modeled, based on input/output behavior alone.</p>
<p>How hard? Ideally we'd have an "equally competent" modeler and distinguisher, and ask the modeler to try to fool the distinguisher. This is a popular approach to generative modeling, and something I've talked about in the context of AI control (as has Jessica).</p>
<p>This definition runs into many subtleties, but I think it is a natural starting point for a discussion. In particular, we are already way beyond concerns like "the brain is almost certainly a chaotic system and hence we can't hope to produce exactly the same result as a biological brain."</p>
<p>The key property we want from the distinguisher is that it can learn to detect relevant differences between the model and the real system. This seems like it might be the kind of problem that I would classify as "probably easy if the agent is powerful and the difference is really important" and you would classify as "way too hard to count on."</p>
<p>You could also ask the model to output various intermediate results or to simulate requested measurements on the simulated brain, and give this extra information to the distinguisher. (Though I don't think this would really help.)</p></p><div class="comment"><p><a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></p><p><blockquote>
  <p>The key property we want from the distinguisher is that it can learn to detect relevant differences between the model and the real system. This seems like it might be the kind of problem that I would classify as "probably easy if the agent is powerful and the difference is really important" and you would classify as "way too hard to count on."</p>
</blockquote>
<p><em>Counting on</em> things before you've found a solution to them isn't very <a href="AI_safety_mindset.html">mindset</a>, but I do consider this a promising approach.  Definitely, the generative-adversarial approach in modern neural networks causes me to hope that this is the sort of thing that actually works in practice.  So I might not be as pessimistic as you think?  I still think in general that one does not go about taking things for granted, but the notion of faithful simulation seems like one that could prove to have a tractable core after hammering on it for a bit, and it also seems very possible that if you're reasonably smart and you can't detect any expected differences in the behavior of neural columns then the corresponding human simulation is faithful.</p>
<p>My current thoughts on possible failure modes:</p>
<ol>
<li>"No differences you know about" might mix up the map and the territority in some obscurely fatal way that leads to the equivalent of the AI deliberately managing to 'not know' about inconvenient divergences.</li>
<li>If we use a limited AI and don't let it run thousands of simulations of people that it can compare to thousands of brains in vats, then in practice its column-level tests won't detect cumulative neural-level differences that lead to an 80% probability of schizophrenia.</li>
<li>The adversarial approach as written won't work because it will turn out that it's <em>always</em> possible for an equally smart adversary to tell the difference, especially for simulations that can be computed at a worthwhile speedup.  Which means this test won't meaningfully discriminate in the region of intuitively faithful vs. nonfaithful simulations.  (This strikes me as the sort of issue that's repairable, but perhaps not trivially so.)</li>
</ol></p></div><div class="comment"><p><a class="page-link" href="../page/PaulChristiano.html">Paul Christiano</a></p><p><p>Methodologically, I am trying to understand what approaches may or may not work and what the key difficulties are. I am trying to anticipate what problems are hard or easy in order to understand what approaches may or may not work. I wouldn't describe this as "taking things for granted," I think we are probably miscommunicating.</p>
<blockquote>
  <p>it's always possible for an equally smart adversary to tell the difference</p>
</blockquote>
<p>This is a big problem, I think that it's the more real version of "perfect simulation will be out of the question." Note that this is only a concern for some processes (e.g. if the simulation output is one bit, then you don't have this problem).</p>
<p>(Note that in practice generative adversarial models are extremely finicky to train, at least partly for this reason.)</p>
<p>I think the other big problem is the complementary one, that even an equally smart adversary can't reliably distinguish a crappy simulation from a good simulation (where a dumb example is that no distinguisher can detect a steganographically encoded message even though that implies the simulation was poor).</p></p></div></div></section><footer><p class="tagged"><h2>Tagged</h2><span class="page-comma-list"><a class="page-link" href="../page/taskagi_open_problems.html">Open subproblems in aligning a Task-based AGI</a>,
 <a class="page-link" href="../page/work_in_progress_meta_tag.html">Work in progress</a></span></p><p class="likes"><h2>Likes</h2><span class="page-comma-list"><a class="page-link" href="../page/EricRogstad.html">Eric Rogstad</a></span></p><p class="reverse-related"><h2>Reverse Related</h2><ul class="page-list"><li><a class="page-link" href="../page/taskagi_open_problems.html">Open subproblems in aligning a Task-based AGI</a> <q>Open research problems, especially ones we can model today, in building an AGI that can &quot;paint all cars pink&quot; without turning its future light cone into pink-painted cars.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/work_in_progress_meta_tag.html">Work in progress</a> <q>This page is being actively worked on by an editor. Check with them before making major changes.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li></ul></p></footer></body></html>