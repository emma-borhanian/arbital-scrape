<!DOCTYPE html><html><head><meta charset="utf-8"><title>Decision theory</title><link rel="stylesheet" type="text/css" href="../common.css"><link rel="stylesheet" type="text/css" href="../page-style.css"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  SVG: {EqnChunk: 50, EqnChunkFactor: 1.5, EqChunkDelay: 10, useFontCache: false, linebreaks: {automatic: true}},
  tex2jax: {
    inlineMath: [['$~$', '$~$']],
    displayMath: [['$$~$', '$~$$']],
    processEscapes: true,
    preview: 'none',
  },
  showProcessingMessages: false,
  messageStyle: 'none',
  // http://docs.mathjax.org/en/latest/config-files.html#the-tex-ams-svg-configuration-file
  jax: ["input/TeX","output/SVG", "output/PreviewHTML"],
  extensions: ["tex2jax.js","MathMenu.js","MathZoom.js", "fast-preview.js", "AssistiveMML.js", "a11y/accessibility-menu.js"],
  TeX: { extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"] }
});</script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_SVG" async></script><script type="text/javascript" src="../arbital-demo-bundle.js"></script><script type="text/javascript">window.addEventListener('DOMContentLoaded', e=>window.loadAllDemos())
</script></head><body><header><h1 class="title">Decision theory</h1><div class="page-info"><p class="metadata-link"><a href="../metadata/decision_theory.json.html">decision_theory.json</a></p><p class="arbital-url"><a href="https://arbital.com/p/decision_theory">https://arbital.com/p/decision_theory</a></p><p class="creator">by
 <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a> Dec 2 2015</p></div><p class="clickbait">The mathematical study of ideal decisionmaking</p><nav class="breadcrumbs"><nav class="breadcrumb" aria-label="Breadcrumb"><ul><li><a href="../index.html">Index</a></li><li>Decision theory</li></ul></nav></nav></header><hr><main><p>[todo: currently here as a placeholder for concepts like 'expected utility'.]</p></main><hr><section class="comments"><h2>Comments</h2><div class="comment"><p><a class="page-link" href="../page/SilasBarta.html">Silas Barta</a></p><p><p>How about having both this and a Utility article be parents/prerequisites of <a href="expected_utility_formalism.html">Expected utility formalism</a>; then, you could have</p>
<p>Utility -&gt; Marginal Utility -&gt; <a href="supply_and_demand.html">Supply and Demand</a></p>
<p>(Or maybe have utility and marginal utility be the same?)</p></p></div><div class="comment"><p><a class="page-link" href="../page/EricBruylant.html">Eric Bruylant</a></p><p><p>Note to self (or others): Add link from <a href="https://wiki.lesswrong.com/wiki/Decision_theory">https://wiki.lesswrong.com/wiki/Decision_theory</a> when this exists.</p></p></div></section><footer><p class="tagged"><h2>Tagged</h2><span class="page-comma-list"><a class="page-link" href="../page/stub_meta_tag.html">Stub</a></span></p><p class="related"><h2>Related</h2><ul class="page-list"><li><a class="page-link" href="../page/Indirect_decision_theory.html">Indirect decision theory</a> <q>In which I argue that understanding decision theory can be delegated to AI.

### Indirect normativitâ€¦</q> - <a class="page-link" href="../page/PaulChristiano.html">Paul Christiano</a></li></ul></p><p class="reverse-related"><h2>Reverse Related</h2><ul class="page-list"><li><a class="page-link" href="../page/stub_meta_tag.html">Stub</a> <q>This page only gives a very brief overview of the topic. If you're able to, please help expand or improve it!</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li></ul></p><p class="children"><h2>Children</h2><ul class="page-tree"><li><a class="page-link" href="../page/causal_dt.html">Causal decision theories</a> <q>On CDT, to choose rationally, you should imagine the world where your physical act changes, then imagine running that world forward in time.  (Therefore, it's irrational to vote in elections.)</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/evidential_dt.html">Evidential decision theories</a> <q>Theories which hold that the principle of rational choice is &quot;Choose the act that would be the best news, if somebody told you that you'd chosen that act.&quot;</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/expected_utility_formalism.html">Expected utility formalism</a> <q>Expected utility is the central idea in the quantitative implementation of consequentialism</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a><ul class="page-tree"><li><a class="page-link" href="../page/coherence_theorems.html">Coherence theorems</a> <q>A 'coherence theorem' shows that something bad happens to an agent if its decisions can't be viewed as 'coherent' in some sense. E.g., an inconsistent preference ordering leads to going in circles.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/intro_utility_coherence.html">Coherent decisions imply consistent utilities</a> <q>Why do we all use the 'expected utility' formalism?  Because any behavior that can't be viewed from that perspective, must be qualitatively self-defeating (in various mathy ways).</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/expected_utility.html">Expected utility</a> <q>Scoring actions based on the average score of their probable consequences.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/expected_utility_agent.html">Expected utility agent</a> <q>If you're not some kind of expected utility agent, you're going in circles.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/utility_function.html">Utility function</a> <q>The only coherent way of wanting things is to assign consistent relative scores to outcomes.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li></ul></li><li><a class="page-link" href="../page/logical_dt.html">Logical decision theories</a> <q>Root page for topics on logical decision theory, with multiple intros for different audiences.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a><ul class="page-tree"><li><a class="page-link" href="../page/5kv.html">An Introduction to Logical Decision Theory for Everyone Else</a> <q>So like what the heck is 'logical decision theory' in terms a normal person can understand?</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/fair_problem_class.html">Fair problem class</a> <q>A problem is 'fair' (according to logical decision theory) when only the results matter and not how we get there.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/ldt_guide.html">Guide to Logical Decision Theory</a> <q>The entry point for learning about logical decision theory.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/ldt_intro_phil.html">Introduction to Logical Decision Theory for Analytic Philosophers</a> <q>Why &quot;choose as if controlling the logical output of your decision algorithm&quot; is the most appealing candidate for the principle of rational choice.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/ldt_intro_compsci.html">Introduction to Logical Decision Theory for Computer Scientists</a> <q>'Logical decision theory' from a math/programming standpoint, including how two agents with mutual knowledge of each other's code can cooperate on the Prisoner's Dilemma.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/ldt_intro_econ.html">Introduction to Logical Decision Theory for Economists</a> <q>An introduction to 'logical decision theory' and its implications for the Ultimatum Game, voting in elections, bargaining problems, and more.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/newcomblike.html">Newcomblike decision problems</a> <q>Decision problems in which your choice correlates with something other than its physical consequences (say, because somebody has predicted you very well) can do weird things to some decision theories.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a><ul class="page-tree"><li><a class="page-link" href="../page/rationality_of_voting.html">'Rationality' of voting in elections</a> <q>&quot;A single vote is very unlikely to swing the election, so your vote is unlikely to have an effect&quot; versus &quot;Many people similar to you are making a similar decision about whether to vote.&quot;</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/pd_tournament_99ldt_1cdt.html">99LDT x 1CDT oneshot PD tournament as arguable counterexample to LDT doing better than CDT</a> <q>Arguendo, if 99 LDT agents and 1 CDT agent are facing off in a one-shot Prisoner's Dilemma tournament, the CDT agent does better on a problem that CDT considers 'fair'.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/absentminded_driver.html">Absent-Minded Driver dilemma</a> <q>A road contains two identical intersections.  An absent-minded driver wants to turn right at the second intersection.  &quot;With what probability should the driver turn right?&quot; argue decision theorists.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/death_in_damascus.html">Death in Damascus</a> <q>Death tells you that It is coming for you tomorrow.  You can stay in Damascus or flee to Aleppo.  Whichever decision you actually make is the wrong one.  This gives some decision theories trouble.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/newcombs_problem.html">Newcomb's Problem</a> <q>There are two boxes in front of you, Box A and Box B.  You can take both boxes, or only Box B.  Box A contains $1000.  Box B contains $1,000,000 if and only if Omega predicted you'd take only Box B.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/parfits_hitchhiker.html">Parfit's Hitchhiker</a> <q>You are dying in the desert.  A truck-driver who is very good at reading faces finds you, and offers to drive you into the city if you promise to pay $1,000 on arrival.  You are a selfish rationalist.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/prisoners_dilemma.html">Prisoner's Dilemma</a> <q>You and an accomplice have been arrested.  Both of you must decide, in isolation, whether to testify against the other prisoner--which subtracts one year from your sentence, and adds two to theirs.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a><ul class="page-tree"><li><a class="page-link" href="../page/true_prisoners_dilemma.html">True Prisoner's Dilemma</a> <q>A scenario that would reproduce the ideal payoff matrix of the Prisoner's Dilemma about human beings who care about their public reputation and each other.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li></ul></li><li><a class="page-link" href="../page/toxoplasmosis_dilemma.html">Toxoplasmosis dilemma</a> <q>A parasitic infection, carried by cats, may make humans enjoy petting cats more.  A kitten, now in front of you, isn't infected.  But if you *want* to pet it, you may already be infected.  Do you?</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/transparent_newcombs_problem.html">Transparent Newcomb's Problem</a> <q>Omega has left behind a transparent Box A containing $1000, and a transparent Box B containing $1,000,000 or nothing.  Box B is full iff Omega thinks you one-box on seeing a full Box B.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/ultimatum_game.html">Ultimatum Game</a> <q>A Proposer decides how to split $10 between themselves and the Responder.  The Responder can take what is offered, or refuse, in which case both parties get nothing.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li></ul></li><li><a class="page-link" href="../page/omega_troll.html">Omega (alien philosopher-troll)</a> <q>The entity that sets up all those trolley problems.  An alien philosopher/troll imbued with unlimited powers, excellent predictive ability, and very odd motives.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/updateless_dt.html">Updateless decision theories</a> <q>Decision theories that maximize their policies (mappings from sense inputs to actions), rather than using their sense inputs to update their beliefs and then selecting actions.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li></ul></li><li><a class="page-link" href="../page/modal_combat.html">Modal combat</a> <q>Modal combat</q> - <a class="page-link" href="../page/JaimeSevillaMolina.html">Jaime Sevilla Molina</a></li></ul></p></footer></body></html>