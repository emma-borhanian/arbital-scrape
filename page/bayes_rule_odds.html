<!DOCTYPE html><html><head><meta charset="utf-8"><title>Bayes' rule: Odds form</title><link rel="stylesheet" type="text/css" href="../common.css"><link rel="stylesheet" type="text/css" href="../page-style.css"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  SVG: {EqnChunk: 50, EqnChunkFactor: 1.5, EqChunkDelay: 10, useFontCache: false, linebreaks: {automatic: true}},
  tex2jax: {
    inlineMath: [['$~$', '$~$']],
    displayMath: [['$$~$', '$~$$']],
    processEscapes: true,
    preview: 'none',
  },
  showProcessingMessages: false,
  messageStyle: 'none',
  // http://docs.mathjax.org/en/latest/config-files.html#the-tex-ams-svg-configuration-file
  jax: ["input/TeX","output/SVG", "output/PreviewHTML"],
  extensions: ["tex2jax.js","MathMenu.js","MathZoom.js", "fast-preview.js", "AssistiveMML.js", "a11y/accessibility-menu.js"],
  TeX: { extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"] }
});</script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_SVG" async></script><script type="text/javascript" src="../arbital-demo-bundle.js"></script><script type="text/javascript">window.addEventListener('DOMContentLoaded', e=>window.loadAllDemos())
</script></head><body><header><h1 class="title">Bayes' rule: Odds form</h1><div class="page-info"><p class="metadata-link"><a href="../metadata/bayes_rule_odds.json.html">bayes_rule_odds.json</a></p><p class="arbital-url"><a href="https://arbital.com/p/bayes_rule_odds">https://arbital.com/p/bayes_rule_odds</a></p><p class="creator">by
 <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a> Feb 8 2016 
updated
 Oct 13 2016</p></div><p class="clickbait">The simplest and most easily understandable form of Bayes' rule uses relative odds.</p><nav class="breadcrumbs"><nav class="breadcrumb" aria-label="Breadcrumb"><ul><li><a href="../index.html">Index</a></li><li>Bayes' rule: Odds form</li></ul></nav><nav class="breadcrumb" aria-label="Breadcrumb"><ul><li><a href="math.html">Mathematics</a></li><li><a href="probability_theory.html">Probability theory</a></li><li><a href="bayes_reasoning.html">Bayesian reasoning</a></li><li><a href="bayes_rule.html">Bayes' rule</a></li><li>…</li></ul></nav><nav class="breadcrumb" aria-label="Breadcrumb"><ul><li><a href="rationality.html">Rationality</a></li><li><a href="probability_theory.html">Probability theory</a></li><li><a href="bayes_reasoning.html">Bayesian reasoning</a></li><li><a href="bayes_rule.html">Bayes' rule</a></li><li>…</li></ul></nav></nav></header><hr><main><p>[summary:  A form of <a href="bayes_rule.html">Bayes&#39; rule</a> that uses relative <a href="odds.html">odds</a>.</p>
<p>Suppose we're trying to solve a mysterious murder, and we <a href="prior_probability.html">start out</a> thinking the odds of Professor Plum vs. Miss Scarlet committing the murder are 1 : 2, that is, Scarlet is twice as likely as Plum to have committed the murder.  We then observe that the victim was bludgeoned with a lead pipe.  If we think that Plum, <em>if</em> he commits a murder, is around 60% likely to use a lead pipe, and that Scarlet, <em>if</em> she commits a murder, would be around 6% likely to us a lead pipe, this implies <a href="relative_likelihood.html">relative likelihoods</a> of 10 : 1 for Plum vs. Scarlet using the pipe.</p>
<p>The <a href="posterior_probability.html">posterior</a> odds for Plum vs. Scarlet, after observing the victim to have been murdered by a pipe, are $~$(1 : 2) \times (10 : 1) = (10 : 2) = (5 : 1)$~$.  We now think Plum is around five times as likely as Scarlet to have committed the murder.]</p>
<p>One of the more convenient forms of <a href="bayes_rule.html">Bayes&#39; rule</a> uses <a href="odds.html">relative odds</a>. Bayes' rule says that, when you observe a piece of evidence $~$e,$~$ your <a href="posterior_probability.html">posterior</a> odds $~$\mathbb O(\boldsymbol H \mid e)$~$ for your hypothesis [-vector] $~$\boldsymbol H$~$ given $~$e$~$ is just your <a href="prior_probability.html">prior</a> odds $~$\mathbb O(\boldsymbol H)$~$ on $~$\boldsymbol H$~$ times the <a href="likelihood_function.html">Likelihood function</a> $~$\mathcal L_e(\boldsymbol H).$~$</p>
<p>For example, suppose we're trying to solve a mysterious murder, and we start out thinking the odds of Professor Plum vs. Miss Scarlet committing the murder are 1 : 2, that is, Scarlet is twice as likely as Plum to have committed the murder <a href="prior_probability.html">a priori</a>.  We then observe that the victim was bludgeoned with a lead pipe.  If we think that Plum, <em>if</em> he commits a murder, is around 60% likely to use a lead pipe, and that Scarlet, <em>if</em> she commits a murder, would be around 6% likely to us a lead pipe, this implies <a href="relative_likelihood.html">relative likelihoods</a> of 10 : 1 for Plum vs. Scarlet using the pipe.  The <a href="posterior_probability.html">posterior</a> odds for Plum vs. Scarlet, after observing the victim to have been murdered by a pipe, are $~$(1 : 2) \times (10 : 1) = (10 : 2) = (5 : 1)$~$.  We now think Plum is around five times as likely as Scarlet to have committed the murder.</p>
<h1 id="oddsfunctions">Odds functions</h1>
<p>Let $~$\boldsymbol H$~$ denote a [-vector] of hypotheses. An odds function $~$\mathbb O$~$ is a function that maps $~$\boldsymbol H$~$ to a set of <a href="odds.html">Odds</a>. For example, if $~$\boldsymbol H = (H_1, H_2, H_3),$~$ then $~$\mathbb O(\boldsymbol H)$~$ might be $~$(6 : 2 : 1),$~$ which says that $~$H_1$~$ is 3x as likely as $~$H_2$~$ and 6x as likely as $~$H_3.$~$ An odds function captures our <em>relative</em> probabilities between the hypotheses in $~$\boldsymbol H;$~$ for example, (6 : 2 : 1) odds are the same as (18 : 6 : 3) odds. We don't need to know the absolute probabilities of the $~$H_i$~$ in order to know the relative odds. All we require is that the relative odds are proportional to the absolute probabilities:
$$~$\mathbb O(\boldsymbol H) \propto \mathbb P(\boldsymbol H).$~$$</p>
<p>In the example with the death of Mr. Boddy, suppose $~$H_1$~$ denotes the proposition "Reverend Green murdered Mr. Boddy", $~$H_2$~$ denotes "Mrs. White did it", and $~$H_3$~$ denotes "Colonel Mustard did it". Let $~$\boldsymbol H$~$ be the vector $~$(H_1, H_2, H_3).$~$ If these propositions respectively have <a href="prior_probability.html">prior</a> probabilities of 80%, 8%, and 4% (the remaining 8% being reserved for other hypotheses), then $~$\mathbb O(\boldsymbol H) = (80 : 8 : 4) = (20 : 2 : 1)$~$ represents our <em>relative</em> credences about the murder suspects &mdash; that Reverend Green is 10 times as likely to be the murderer as Miss White, who is twice as likely to be the murderer as Colonel Mustard.</p>
<h1 id="likelihoodfunctions">Likelihood functions</h1>
<p>Suppose we discover that the victim was murdered by wrench.  Suppose we think that Reverend Green, Mrs. White, and Colonel Mustard, <em>if</em> they murdered someone, would respectively be 60%, 90%, and 30% likely to use a wrench.  Letting $~$e_w$~$ denote the observation "The victim was murdered by wrench," we would have $~$\mathbb P(e_w\mid \boldsymbol H) = (0.6, 0.9, 0.3).$~$ This gives us a <a href="likelihood_function.html">Likelihood function</a> defined as $~$\mathcal L_{e_w}(\boldsymbol H) = P(e_w \mid \boldsymbol H).$~$</p>
<h1 id="bayesruleoddsform">Bayes' rule, odds form</h1>
<p>Let $~$\mathbb O(\boldsymbol H\mid e)$~$ denote the <a href="posterior_probability.html">posterior</a> odds of the hypotheses $~$\boldsymbol H$~$ after observing evidence $~$e.$~$  <a href="bayes_rule_proof.html">Bayes&#39; rule</a> then states:</p>
<p>$$~$\mathbb O(\boldsymbol H) \times \mathcal L_{e}(\boldsymbol H) = \mathbb O(\boldsymbol H\mid e)$~$$</p>
<p>This says that we can multiply the relative prior credence $~$\mathbb O(\boldsymbol H)$~$ by the likelihood $~$\mathcal L_{e}(\boldsymbol H)$~$ to arrive at the relative posterior credence $~$\mathbb O(\boldsymbol H\mid e).$~$ Because odds are invariant under multiplication by a positive constant, it wouldn't make any difference if the <em>likelihood</em> function was scaled up or down by a constant, because that would only have the effect of multiplying the final odds by a constant, which does not affect them. Thus, only the <a href="relative_likelihood.html">relative likelihoods</a> are necessary to perform the calculation; the absolute likelihoods are unnecessary. Therefore, when performing the calculation, we can simplify $~$\mathcal L_e(\boldsymbol H) = (0.6, 0.9, 0.3)$~$ to the relative likelihoods $~$(2 : 3 : 1).$~$</p>
<p>In our example, this makes the calculation quite easy. The prior odds for Green vs White vs Mustard were $~$(20 : 2 : 1).$~$ The relative likelihoods were $~$(0.6 : 0.9 : 0.3)$~$ = $~$(2 : 3 : 1).$~$ Thus, the relative posterior odds after observing $~$e_w$~$ = Mr. Boddy was killed by wrench are $~$(20 : 2 : 1) \times (2 : 3 : 1) = (40 : 6 : 1).$~$ Given the evidence, Reverend Green is 40 times as likely as Colonel Mustard to be the killer, and 20/3 times as likely as Mrs. White.</p>
<p>Bayes' rule states that this <em>relative</em> proportioning of odds among these three suspects will be correct, regardless of how our remaining 8% probability mass is assigned to all other suspects and possibilities, or indeed, how much probability mass we assigned to other suspects to begin with. For a proof, see <a href="bayes_rule_proof.html">Proof of Bayes&#39; rule</a>.</p>
<h1 id="visualization">Visualization</h1>
<p><a href="bayes_frequency_diagram.html">Frequency diagrams</a>, <a href="bayes_waterfall_diagram.html">waterfall diagrams</a>, and <a href="bayes_rule_proportional.html">spotlight diagrams</a> may be helpful for explaining or visualizing the odds form of Bayes' rule.</p></main><hr><section class="comments"><h2>Comments</h2><div class="comment"><p><a class="page-link" href="../page/EmileKroeger.html">Emile Kroeger</a></p><p><p>This page asks me if I learnt the concept of "Odds ratio" - but nowhere in the page does it actually explicitly <em>talk</em> about odds ratios, only about odds.</p></p></div></section><footer><p class="tagged"><h2>Tagged</h2><span class="page-comma-list"><a class="page-link" href="../page/b_class_meta_tag.html">B-Class</a></span></p><p class="all-creators"><h2>All Creators</h2><span class="page-comma-list"><a class="page-link" href="../page/AlexeiAndreev.html">Alexei Andreev</a>,
 <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a>,
 <a class="page-link" href="../page/NateSoares.html">Nate Soares</a></span></p><p class="likes"><h2>Likes</h2><span class="page-comma-list"><a class="page-link" href="../page/AndrewMcKnight.html">Andrew McKnight</a>,
 <a class="page-link" href="../page/CamSpiers.html">Cam Spiers</a>,
 <a class="page-link" href="../page/EranVax.html">Eran Vax</a>,
 <a class="page-link" href="../page/IanPitchford.html">Ian Pitchford</a>,
 <a class="page-link" href="../page/NadeemMohsin.html">Nadeem Mohsin</a>,
 <a class="page-link" href="../page/NateSoares.html">Nate Soares</a>,
 <a class="page-link" href="../page/RonnyFernandez.html">Ronny Fernandez</a>,
 <a class="page-link" href="../page/SzymonWilczyski.html">Szymon Wilczyński</a></span></p><p class="reverse-related"><h2>Reverse Related</h2><ul class="page-list"><li><a class="page-link" href="../page/b_class_meta_tag.html">B-Class</a> <q>This page is mostly complete and without major problems, but has not had detailed feedback from the target audience and reviewers.</q> - <a class="page-link" href="../page/EricBruylant.html">Eric Bruylant</a></li></ul></p><p class="children"><h2>Children</h2><ul class="page-tree"><li><a class="page-link" href="../page/bayes_rule_odds_intro.html">Introduction to Bayes' rule: Odds form</a> <q>Bayes' rule is simple, if you think in terms of relative odds.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li></ul></p></footer></body></html>