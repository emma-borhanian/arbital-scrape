<!DOCTYPE html><html><head><meta charset="utf-8"><title>Glossary (Value Alignment Theory)</title><link rel="stylesheet" type="text/css" href="../common.css"><link rel="stylesheet" type="text/css" href="../page-style.css"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  SVG: {EqnChunk: 50, EqnChunkFactor: 1.5, EqChunkDelay: 10, useFontCache: false, linebreaks: {automatic: true}},
  tex2jax: {
    inlineMath: [['$~$', '$~$']],
    displayMath: [['$$~$', '$~$$']],
    processEscapes: true,
    preview: 'none',
  },
  showProcessingMessages: false,
  messageStyle: 'none',
  // http://docs.mathjax.org/en/latest/config-files.html#the-tex-ams-svg-configuration-file
  jax: ["input/TeX","output/SVG", "output/PreviewHTML"],
  extensions: ["tex2jax.js","MathMenu.js","MathZoom.js", "fast-preview.js", "AssistiveMML.js", "a11y/accessibility-menu.js"],
  TeX: { extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"] }
});</script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_SVG" async></script><script type="text/javascript" src="../arbital-demo-bundle.js"></script><script type="text/javascript">window.addEventListener('DOMContentLoaded', e=>window.loadAllDemos())
</script></head><body><header><h1 class="title">Glossary (Value Alignment Theory)</h1><div class="page-info"><p class="metadata-link"><a href="../metadata/value_alignment_glossary.json.html">value_alignment_glossary.json</a></p><p class="arbital-url"><a href="https://arbital.com/p/value_alignment_glossary">https://arbital.com/p/value_alignment_glossary</a></p><p class="creator">by
 <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a> Jul 2 2015 
updated
 Dec 17 2015</p></div><p class="clickbait">Words that have a special meaning in the context of creating nice AIs.</p><nav class="breadcrumbs"><nav class="breadcrumb" aria-label="Breadcrumb"><ul><li><a href="../index.html">Index</a></li><li>Glossary (Value Alignment Theory)</li></ul></nav><nav class="breadcrumb" aria-label="Breadcrumb"><ul><li><a href="ai_alignment.html">AI alignment</a></li><li>â€¦</li></ul></nav></nav></header><hr><main><p>The parent page for the definitions of words that are given a special or unusual meaning inside <a href="ai_alignment.html">value alignment theory</a>.</p>
<p>If a word or phrase could be mistaken for ordinary English (e.g. 'value' or 'utility function'), then <a href="make_glossary_pages.html">you should create a glossary page</a> indicating its special meaning inside VAT, and the first use of that word in any potentially confusing context should be linked here.  While other special phrases (like Value Alignment Theory) should also be linked to their concept pages for understandability, they do not need glossary definitions apart from their existing concept pages.  However, an overloaded word like <a href="value_alignment_value.html">&#39;value&#39;</a> needs its own brief (or lengthy) page that can be quickly consulted by somebody wondering what that word is taken to mean in the context of VAT.</p>
<p>See also <a href="5b.html">Linguistic conventions in value alignment</a>.</p></main><hr><footer><p class="all-creators"><h2>All Creators</h2><span class="page-comma-list"><a class="page-link" href="../page/AlexeiAndreev.html">Alexei Andreev</a>,
 <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></span></p><p class="likes"><h2>Likes</h2><span class="page-comma-list"><a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></span></p><p class="related"><h2>Related</h2><ul class="page-list"><li><a class="page-link" href="../page/hypercomputer.html">Hypercomputer</a> <q>Some formalisms demand computers larger than the limit of all finite computers</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/relative_ability.html">Infrahuman, par-human, superhuman, efficient, optimal</a> <q>A categorization of AI ability levels relative to human, with some gotchas in the ordering.  E.g., in simple domains where humans can play optimally, optimal play is not superhuman.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/instrumental.html">Instrumental</a> <q>What is &quot;instrumental&quot; in the context of Value Alignment Theory?</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/pivotal.html">Pivotal event</a> <q>Which types of AIs, if they work, can do things that drastically change the nature of the further game?</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/value_alignment_programmer.html">Programmer</a> <q>Who is building these advanced agents?</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/value_alignment_utility.html">Utility</a> <q>What is &quot;utility&quot; in the context of Value Alignment Theory?</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/value_alignment_value.html">Value</a> <q>The word 'value' in the phrase 'value alignment' is a metasyntactic variable that indicates the speaker's future goals for intelligent life.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li></ul></p><p class="children"><h2>Children</h2><ul class="page-tree"><li><a class="page-link" href="../page/ai_concept.html">'Concept'</a> <q>In the context of Artificial Intelligence, a 'concept' is a category, something that identifies thingies as being inside or outside the concept.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/cognitive_domain.html">Cognitive domain</a> <q>An allegedly compact unit of knowledge, such that ideas inside the unit interact mainly with each other and less with ideas in other domains.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a><ul class="page-tree"><li><a class="page-link" href="../page/domain_distance.html">Distances between cognitive domains</a> <q>Often in AI alignment we want to ask, &quot;How close is 'being able to do X' to 'being able to do Y'?&quot;</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li></ul></li><li><a class="page-link" href="../page/FAI.html">Friendly AI</a> <q>Old terminology for an AI whose preferences have been successfully aligned with idealized human values.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li></ul></p></footer></body></html>