<!DOCTYPE html><html><head><meta charset="utf-8"><title>Existential risk</title><link rel="stylesheet" type="text/css" href="../common.css"><link rel="stylesheet" type="text/css" href="../page-style.css"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  SVG: {EqnChunk: 50, EqnChunkFactor: 1.5, EqChunkDelay: 10, useFontCache: false, linebreaks: {automatic: true}},
  tex2jax: {
    inlineMath: [['$~$', '$~$']],
    displayMath: [['$$~$', '$~$$']],
    processEscapes: true,
    preview: 'none',
  },
  showProcessingMessages: false,
  messageStyle: 'none',
  // http://docs.mathjax.org/en/latest/config-files.html#the-tex-ams-svg-configuration-file
  jax: ["input/TeX","output/SVG", "output/PreviewHTML"],
  extensions: ["tex2jax.js","MathMenu.js","MathZoom.js", "fast-preview.js", "AssistiveMML.js", "a11y/accessibility-menu.js"],
  TeX: { extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"] }
});</script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_SVG" async></script><script type="text/javascript" src="../arbital-demo-bundle.js"></script><script type="text/javascript">window.addEventListener('DOMContentLoaded', e=>window.loadAllDemos())
</script></head><body><header><h1 class="title">Existential risk</h1><div class="page-info"><p class="metadata-link"><a href="../metadata/x_risk.json.html">x_risk.json</a></p><p class="arbital-url"><a href="https://arbital.com/p/x_risk">https://arbital.com/p/x_risk</a></p><p class="creator">by
 <a class="page-link" href="../page/EricRogstad.html">Eric Rogstad</a> Dec 23 2016 
updated
 Jan 2 2017</p></div><nav class="breadcrumbs"><nav class="breadcrumb" aria-label="Breadcrumb"><ul><li><a href="../index.html">Index</a></li><li>Existential risk</li></ul></nav><nav class="breadcrumb" aria-label="Breadcrumb"><ul><li><a href="ea.html">Effective altruism</a></li><li>…</li></ul></nav></nav></header><hr><main><blockquote>
  <p>Because of accelerating technological progress, humankind may be rapidly approaching a critical phase in its career. In addition to well-known threats such as nuclear holocaust, the prospects of radically transforming technologies like nanotech systems and machine intelligence present us with unprecedented opportunities and risks. Our future, and whether we will have a future at all, may well be determined by how we deal with these challenges.</p>
  <p>…</p>
  <p>Some of these threats are relatively well known while others, including some of the gravest, have gone almost unrecognized. Existential risks have a cluster of features that make ordinary risk management ineffective.</p>
</blockquote>
<p>-- <a href="http://www.nickbostrom.com/existential/risks.html">Nick Bostrom</a></p>
<p>See also: <a href="https://en.wikipedia.org/wiki/Global_catastrophic_risk#Global_catastrophic_vs_existential">Existential risks</a> on Wikipedia</p></main><hr><footer><p class="all-creators"><h2>All Creators</h2><span class="page-comma-list"><a class="page-link" href="../page/BenPace.html">Ben Pace</a>,
 <a class="page-link" href="../page/EricRogstad.html">Eric Rogstad</a></span></p><p class="related"><h2>Related</h2><ul class="page-list"><li><a class="page-link" href="../page/off_earth_efficiently_mitigates_xrisk.html">A permanent, self-sustaining off-Earth colony would be a much more effective mitigation of x-risk than even an equally well funded system of disaster shelters on Earth.</a> <q>See also the less precise claim: Establishing a permanent off-Earth colony would be a useful way to …</q> - <a class="page-link" href="../page/EricRogstad.html">Eric Rogstad</a></li><li><a class="page-link" href="../page/consciousness_research_important.html">Consciousness research is critically important</a> <q>See: Principia Qualia: blueprint for a new cause area, consciousness research with an eye toward et…</q> - <a class="page-link" href="../page/EricRogstad.html">Eric Rogstad</a></li><li><a class="page-link" href="../page/off_earth_mitigates_xrisk.html">Establishing a permanent off-Earth colony would be a useful way to mitigate x-risk</a> <q>- Posed by [purplepeople](http://effective-altruism.com/user/purplepeople/) on the [EA Forum](http:/…</q> - <a class="page-link" href="../page/EricRogstad.html">Eric Rogstad</a></li><li><a class="page-link" href="../page/6xk.html">Ethics research should proceed in parallel to value alignment research</a> - <a class="page-link" href="../page/EricRogstad.html">Eric Rogstad</a></li><li><a class="page-link" href="../page/off_earth_warm_scarf.html">For mitigating AI x-risk, an off-Earth colony would be about as useful as a warm scarf</a> <q>H/T to Eliezer Yudkowsky for [&quot;warm scarf&quot;](https://www.facebook.com/robert.wiblin/posts/75711126783…</q> - <a class="page-link" href="../page/EricRogstad.html">Eric Rogstad</a></li></ul></p></footer></body></html>