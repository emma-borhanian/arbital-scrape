<!DOCTYPE html><html><head><meta charset="utf-8"><title>Two independent events: Square visualization</title><link rel="stylesheet" type="text/css" href="../common.css"><link rel="stylesheet" type="text/css" href="../page-style.css"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  SVG: {EqnChunk: 50, EqnChunkFactor: 1.5, EqChunkDelay: 10, useFontCache: false, linebreaks: {automatic: true}},
  tex2jax: {
    inlineMath: [['$~$', '$~$']],
    displayMath: [['$$~$', '$~$$']],
    processEscapes: true,
    preview: 'none',
  },
  showProcessingMessages: false,
  messageStyle: 'none',
  // http://docs.mathjax.org/en/latest/config-files.html#the-tex-ams-svg-configuration-file
  jax: ["input/TeX","output/SVG", "output/PreviewHTML"],
  extensions: ["tex2jax.js","MathMenu.js","MathZoom.js", "fast-preview.js", "AssistiveMML.js", "a11y/accessibility-menu.js"],
  TeX: { extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"] }
});</script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_SVG" async></script><script type="text/javascript" src="../arbital-demo-bundle.js"></script><script type="text/javascript">window.addEventListener('DOMContentLoaded', e=>window.loadAllDemos())
</script></head><body><header><h1 class="title">Two independent events: Square visualization</h1><div class="page-info"><p class="metadata-link"><a href="../metadata/4cl.json.html">4cl.json</a></p><p class="arbital-url"><a href="https://arbital.com/p/4cl">https://arbital.com/p/4cl</a></p><p class="creator">by
 <a class="page-link" href="../page/TsviBT.html">Tsvi BT</a> Jun 15 2016 
updated
 Jun 16 2016</p></div><nav class="breadcrumbs"><nav class="breadcrumb" aria-label="Breadcrumb"><ul><li><a href="../index.html">Index</a></li><li>Two independent events: Square visualization</li></ul></nav><nav class="breadcrumb" aria-label="Breadcrumb"><ul><li><a href="math.html">Mathematics</a></li><li><a href="probability_theory.html">Probability theory</a></li><li><a href="two_independent_events.html">Two independent events</a></li><li>…</li></ul></nav><nav class="breadcrumb" aria-label="Breadcrumb"><ul><li><a href="rationality.html">Rationality</a></li><li><a href="probability_theory.html">Probability theory</a></li><li><a href="two_independent_events.html">Two independent events</a></li><li>…</li></ul></nav></nav></header><hr><main><p>$$~$
\newcommand{\true}{\text{True}}
\newcommand{\false}{\text{False}}
\newcommand{\bP}{\mathbb{P}} 
$~$$</p>
<p>[summary: 
$$~$
\newcommand{\true}{\text{True}}
\newcommand{\false}{\text{False}}
\newcommand{\bP}{\mathbb{P}}
$~$$</p>
<p>Say $~$A$~$ and $~$B$~$ are independent [event_probability events], so $~$\bP(A, B) =&nbsp;\bP(A)\bP(B).$~$ Then we can draw their joint probability distribution using the using the <a href="496.html">square visualization</a> of probabilities:</p>
<p><img src="http://i.imgur.com/0off1db.png" width="312" height="272"></p>
<p>]</p>
<p>This is what independence looks like, using the <a href="496.html">square visualization</a> of probabilities:</p>
<p><img src="http://i.imgur.com/0off1db.png" width="390" height="338"></p>
<p>We can see that the [event_probability events] $~$A$~$ and $~$B$~$ don't interact; we say that $~$A$~$ and $~$B$~$ are <em>independent</em>. Whether we look at the whole square, or just the red part of
the square where $~$A$~$ is true, the probability of $~$B$~$ stays the same. In other words, $~$\bP(B \mid A) = \bP(B)$~$. That's what we mean by independence: the
probability of $~$B$~$ doesn't change if you condition on $~$A$~$.</p>
<p>Our square of probabilities can be generated by multiplying together the probability of $~$A$~$ and the probability of $~$B$~$:</p>
<p><img src="http://i.imgur.com/pjwcoTn.png" width="640" height="275"></p>
<p>This picture demonstrates another way to define what it means for $~$A$~$ and $~$B$~$ to be independent:</p>
<p>$$~$\bP(A, B) =&nbsp;\bP(A)\bP(B)\ .$~$$</p>
<h2 id="intermsoffactoringajointdistribution">In terms of factoring a joint distribution</h2>
<p>Let's contrast independence with non-independence. Here's a picture of two ordinary, non-independent events $~$A$~$ and $~$B$~$:</p>
<p><img src="http://i.imgur.com/6ZHSR0l.png" width="529" height="327"></p>
<p>(If the meaning of this picture isn't clear, take a look at <a href="496.html">Square visualization of probabilities on two events</a>.)</p>
<p>We have the red blocks for $~$\bP(A)$~$ and the blue blocks for $~$\bP(\neg A)$~$ lined up in columns. This means we've [factoring_probability factored] our
probability distribution using $~$A$~$ as the first factor: </p>
<p>$$~$\bP(A,B) = \bP(A) \bP(B \mid A)\ .$~$$</p>
<p>We could just as well have factored by $~$B$~$ first: $~$\bP(A,B) = \bP(B) \bP( A \mid B)\ .$~$ Then we'd draw a picture like this:</p>
<p><img src="http://i.imgur.com/O0RNzxw.png" width="390" height="390"></p>
<p>Now, here again is the picture of <a href="two_independent_events.html">two independent events</a> $~$A$~$ and $~$B$~$:</p>
<p><img src="http://i.imgur.com/0off1db.png" width="390" height="338"></p>
<p>In this picture, there's red and blue lined-up columns for $~$\bP(A)$~$ and $~$\bP(\neg A)$~$, and there's <em>also</em>  dark and light lined-up rows for $~$\bP(B)$~$ and
$~$\bP(\neg B)$~$. It looks like we somehow [factoring_probability factored] our probability distribution $~$\bP$~$ using both $~$A$~$ and 
$~$B$~$ as the first factor. </p>
<p>In fact, this is exactly what happened: since $~$A$~$ and $~$B$~$ are <a href="two_independent_events.html">independent</a>, we have that $~$\bP(B \mid A) = \bP(B)$~$. So the diagram
above is actually factored according to $~$A$~$ first: $~$\bP(A,B) = \bP(A) \bP(B \mid A)$~$. It's just that $~$\bP(B \mid A)= \bP(B) = \bP(B \mid \neg A)$~$, since $~$B$~$
is independent from $~$A$~$. So we don't need to have different ratios of dark to light (a.k.a. conditional probabilities of $~$B$~$) in the left and right columns:</p>
<p><img src="http://i.imgur.com/Nfiuz3d.png" width="618" height="420"></p>
<p>In this visualization, we can see what happens to the probability of $~$B$~$ when you condition on $~$A$~$ or on $~$\neg A$~$: it doesn't change at all. The ratio of
[the area where $~$B$~$ happens] to [the whole area], is the same as the ratio $~$\bP(B \mid A)$~$ where we only look at the area where $~$A$~$ happens, which is the
same as the ratio $~$\bP(B \mid \neg A)$~$ where we only look at the area where $~$\neg A$~$ happens. The fact that the probability of $~$B$~$ doesn't change when we
condition on $~$A$~$ is exactly what we mean when we say that $~$A$~$ and $~$B$~$ are independent.</p>
<p>The square diagram above is <em>also</em> factored according to $~$B$~$ first, using $~$\bP(A,B) = \bP(B) \bP(A \mid B)$~$. The red / blue ratios are the same in both rows
because $~$\bP(A \mid B) = \bP(A) = \bP(A \mid \neg B)$~$, since $~$A$~$ and $~$B$~$ are independent:</p>
<p><img src="http://i.imgur.com/DfDljOL.png" width="636" height="468"></p>
<p>We couldn't do any of this stuff if the columns and rows didn't both line up. (Which is good, because then we'd have proved the false statement that any two
events are independent!)</p>
<h2 id="intermsofmultiplyingmarginalprobabilities">In terms of multiplying marginal probabilities</h2>
<p>Another way to say that $~$A$~$ and $~$B$~$ are independent variables %note:We're using  the [event_variable_equivalence equivalence] between [event_probability
events] and [binary_random_variable binary variables].% is that for any truth values $~$t_A,t_B \in \{\true, \false\},$~$</p>
<p>$$~$\bP(A = t_A, B= t_B) =&nbsp;\bP(A = t_A)\bP(B = t_B)\ .$~$$</p>
<p>So the <a href="joint_probability.html">joint probabilities</a> for $~$A$~$ and $~$B$~$ are computed by separately getting the probability of $~$A$~$ and the probability of $~$B$~$, and then
multiplying the two probabilities together. For example, say we want to compute the probability $~$\bP(A, \neg B) = \bP(A = \true, B = \false)$~$. We start with
the [marginal_probability marginal probability] of $~$A$~$:</p>
<p><img src="http://i.imgur.com/ZnxqSMo.png" width="250" height="300"></p>
<p>and the probability of $~$\neg B$~$:</p>
<p><img src="http://i.imgur.com/txRlJyE.png" width="335" height="240"></p>
<p>and then we multiply them:</p>
<p><img src="http://i.imgur.com/GOOnTuF.png" width="440" height="390"></p>
<p>We can get all the joint probabilities this way. So we can visualize the whole joint distribution as the thing that you get when you multiply two independent
probability distributions together. We just overlay the two distributions: </p>
<p><img src="http://i.imgur.com/X4FSciB.png" width="532" height="816"></p>
<p>To be a little more mathematically elegant, we'd use the [topological_product topological product of two spaces] shown earlier to draw the joint distribution
as a product of the distributions of $~$A$~$ and $~$B$~$: </p>
<p><img src="http://i.imgur.com/pjwcoTn.png" width="640" height="275"></p></main><hr><footer><p class="all-creators"><h2>All Creators</h2><span class="page-comma-list"><a class="page-link" href="../page/EricRogstad.html">Eric Rogstad</a>,
 <a class="page-link" href="../page/JaimeSevillaMolina.html">Jaime Sevilla Molina</a>,
 <a class="page-link" href="../page/TsviBT.html">Tsvi BT</a></span></p><p class="likes"><h2>Likes</h2><span class="page-comma-list"><a class="page-link" href="../page/EricBruylant.html">Eric Bruylant</a>,
 <a class="page-link" href="../page/MarkChimes.html">Mark Chimes</a>,
 <a class="page-link" href="../page/NateSoares.html">Nate Soares</a></span></p></footer></body></html>