<!DOCTYPE html><html><head><meta charset="utf-8"><title>Methodology of foreseeable difficulties</title><link rel="stylesheet" type="text/css" href="../common.css"><link rel="stylesheet" type="text/css" href="../page-style.css"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  SVG: {EqnChunk: 50, EqnChunkFactor: 1.5, EqChunkDelay: 10, useFontCache: false, linebreaks: {automatic: true}},
  tex2jax: {
    inlineMath: [['$~$', '$~$']],
    displayMath: [['$$~$', '$~$$']],
    processEscapes: true,
    preview: 'none',
  },
  showProcessingMessages: false,
  messageStyle: 'none',
  // http://docs.mathjax.org/en/latest/config-files.html#the-tex-ams-svg-configuration-file
  jax: ["input/TeX","output/SVG", "output/PreviewHTML"],
  extensions: ["tex2jax.js","MathMenu.js","MathZoom.js", "fast-preview.js", "AssistiveMML.js", "a11y/accessibility-menu.js"],
  TeX: { extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"] }
});</script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_SVG" async></script><script type="text/javascript" src="../arbital-demo-bundle.js"></script><script type="text/javascript">window.addEventListener('DOMContentLoaded', e=>window.loadAllDemos())
</script></head><body><header><h1 class="title">Methodology of foreseeable difficulties</h1><div class="page-info"><p class="metadata-link"><a href="../metadata/foreseeable_difficulties.json.html">foreseeable_difficulties.json</a></p><p class="arbital-url"><a href="https://arbital.com/p/foreseeable_difficulties">https://arbital.com/p/foreseeable_difficulties</a></p><p class="creator">by
 <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a> Jun 9 2015 
updated
 Nov 23 2016</p></div><p class="clickbait">Building a nice AI is likely to be hard enough, and contain enough gotchas that won't show up in the AI's early days, that we need to foresee problems coming in advance.</p><nav class="breadcrumbs"><nav class="breadcrumb" aria-label="Breadcrumb"><ul><li><a href="../index.html">Index</a></li><li>Methodology of foreseeable difficulties</li></ul></nav><nav class="breadcrumb" aria-label="Breadcrumb"><ul><li><a href="ai_alignment.html">AI alignment</a></li><li><a href="advanced_safety.html">Advanced safety</a></li><li>…</li></ul></nav></nav></header><hr><main><p>Much of the current literature about value alignment centers on purported reasons to expect that certain problems will require solution, or be difficult, or be more difficult than some people seem to expect.  The subject of this page's approval rating is this practice, considered as a policy or methodology.</p>
<p>The basic motivation behind trying to foresee difficulties is the large number of predicted <a href="context_disaster.html">Context Change</a> problems where an AI seems to behave nicely up until it reaches some threshold level of cognitive ability and then it behaves less nicely.  In some cases the problems are generated without the AI having formed that intention in advance, meaning that even transparency of the AI's thought processes during its earlier state can't save us.  This means we have to see problems of this type in advance.</p>
<p>(The fact that Context Change problems of this type can be <em>hard</em> to see in advance, or that we might conceivably fail to see one, doesn't mean we can skip this duty of analysis.  Not trying to foresee them means relying on observation, and it seems <em>predictable</em> that trying to eyeball the AI and rejecting theory <em>definitely</em> doesn't catch important classes of problem.)</p>
<p>[todo: # Examples]</p>
<p>[todo: …most of value alignment theory, so try to pick 3 cases that illustrate the point in different ways.  Pick from Context Change?]</p>
<h1 id="arguments">Arguments</h1>
<p>For:  it's sometimes possible to strongly foresee a difficulty coming in a case where you've observed naive respondents to seem to think that no difficulty exists, and in cases where the development trajectory of the agent seems to imply a potential <a href="context_disaster.html">Treacherous Turn</a>.  If there's even one real Treacherous Turn out of all the cases that have been argued, then the point carries that past a certain point, you have to see the bullet coming before it actually hits you.  The theoretical analysis suggests really strongly that blindly forging ahead 'experimentally' will be fatal.  Someone with such a strong commitment to experimentalism that they want to ignore this theoretical analysis… it's not clear what we can say to them, except maybe to appeal to the normative principle of not predictably destroying the world in cases where it seems like we could have done better.</p>
<p>Against:  no real arguments against in the actual literature, but it would be surprising if somebody didn't claim that the foreseeable difficulties program was too pessimistic, or inevitably ungrounded from reality and productive only of bad ideas even when refuted, etcetera.</p>
<p>Primary reply: look, dammit, people actually are way too optimistic about FAI, we have them on the record, [todo: find 3 prestigious examples] and it's hard to see how humanity could avoid walking directly into the whirling razor blades without better foresight of difficulty.  One potential strategy is enough academic respect and consensus on enough really obvious foreseeable difficulties that the people claiming it will all be easy are actually asked to explain why the foreseeable difficulty consensus is wrong, and if they can't explain that well, they lose respect.</p>
<p>Will interact with the arguments on <a href="108.html">empiricism vs. theorism is a false dichotomy</a>.</p></main><hr><section class="comments"><h2>Comments</h2><div class="comment"><p><a class="page-link" href="../page/PaulChristiano.html">Paul Christiano</a></p><p><p>Often complaints are with the particular problems which purportedly will require novel solutions or be very difficult, rather than with this methodology in the abstract. These objections often go with "we'll cross that bridge when we come to it," but it's not a fully general version of "we'll cross that bridge when we come to it."</p>
<p>For example, often these problems cannot be or have not been modeled in any explicit way. For example, no one has precisely or clearly articulated a possible capability state in which our lack of understanding of self-referential reasoning leads to catastrophic trouble. If we had such a capability state in mind, it would serve both as an argument that the problem may be critical, and moreover as a way of determining what exactly the problem is.</p></p></div></section><footer><p class="tagged"><h2>Tagged</h2><span class="page-comma-list"><a class="page-link" href="../page/work_in_progress_meta_tag.html">Work in progress</a></span></p><p class="all-creators"><h2>All Creators</h2><span class="page-comma-list"><a class="page-link" href="../page/AlexeiAndreev.html">Alexei Andreev</a>,
 <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a>,
 <a class="page-link" href="../page/MatthewGraves.html">Matthew Graves</a></span></p><p class="likes"><h2>Likes</h2><span class="page-comma-list"><a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></span></p><p class="related"><h2>Related</h2><ul class="page-list"><li><a class="page-link" href="../page/goodharts_curse.html">Goodhart's Curse</a> <q>The Optimizer's Curse meets Goodhart's Law.  For example, if our values are V, and an AI's utility function U is a proxy for V, optimizing for high U seeks out 'errors'--that is, high values of U - V.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li></ul></p><p class="reverse-related"><h2>Reverse Related</h2><ul class="page-list"><li><a class="page-link" href="../page/work_in_progress_meta_tag.html">Work in progress</a> <q>This page is being actively worked on by an editor. Check with them before making major changes.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li></ul></p></footer></body></html>