<!DOCTYPE html><html><head><meta charset="utf-8"><title>&quot;Presumably the advantage of...&quot;</title><link rel="stylesheet" type="text/css" href="../common.css"><link rel="stylesheet" type="text/css" href="../page-style.css"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  SVG: {EqnChunk: 50, EqnChunkFactor: 1.5, EqChunkDelay: 10, useFontCache: false, linebreaks: {automatic: true}},
  tex2jax: {
    inlineMath: [['$~$', '$~$']],
    displayMath: [['$$~$', '$~$$']],
    processEscapes: true,
    preview: 'none',
  },
  showProcessingMessages: false,
  messageStyle: 'none',
  // http://docs.mathjax.org/en/latest/config-files.html#the-tex-ams-svg-configuration-file
  jax: ["input/TeX","output/SVG", "output/PreviewHTML"],
  extensions: ["tex2jax.js","MathMenu.js","MathZoom.js", "fast-preview.js", "AssistiveMML.js", "a11y/accessibility-menu.js"],
  TeX: { extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"] }
});</script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_SVG" async></script><script type="text/javascript" src="../arbital-demo-bundle.js"></script><script type="text/javascript">window.addEventListener('DOMContentLoaded', e=>window.loadAllDemos())
</script></head><body><header><h1 class="title">&quot;Presumably the advantage of...&quot;</h1><div class="page-info"><p class="metadata-link"><a href="../metadata/2nd.json.html">2nd.json</a></p><p class="arbital-url"><a href="https://arbital.com/p/2nd">https://arbital.com/p/2nd</a></p><p class="creator">by
 <a class="page-link" href="../page/PaulChristiano.html">Paul Christiano</a> Mar 16 2016</p></div><nav class="breadcrumbs"><nav class="breadcrumb" aria-label="Breadcrumb"><ul><li><a href="../index.html">Index</a></li><li>&quot;Presumably the advantage of...&quot;</li></ul></nav><nav class="breadcrumb" aria-label="Breadcrumb"><ul><li><a href="ai_alignment.html">AI alignment</a></li><li><a href="AGI_typology.html">Strategic AGI typology</a></li><li><a href="task_agi.html">Task-directed AGI</a></li><li><a href="taskagi_open_problems.html">Open subproblems in aligning a Task-based AGI</a></li><li>…</li></ul></nav><nav class="breadcrumb" aria-label="Breadcrumb"><ul><li><a href="ai_alignment.html">AI alignment</a></li><li><a href="task_agi.html">Task-directed AGI</a></li><li><a href="taskagi_open_problems.html">Open subproblems in aligning a Task-based AGI</a></li><li>…</li></ul></nav></nav></header><hr><main><blockquote class="comment-context">The problem of conservatism is an extension of the supervised learning problem in which, given labeled examples, we try to generate further cases that are almost certainly positive examples of a concept, rather than demanding that we label all possible further examples correctly\.  Another way of looking at it is that, given labeled training data, we don't just want to learn a simple concept that fits the labeled data, <mark>we want to learn a simple small concept that fits the data \- one that, subject to the constraint of labeling the training data correctly, predicts as few other positive examples as possible\.</mark></blockquote>
<p>Presumably the advantage of this approach---rather than simply learning to imitate the human burrito-making process or even human burritos, is that it might be easier to do. Is that right?</p>
<p>I think that's a valid goal, but I'm not sure how well "conservative generalizations" actually address the problem. Certainly it still leaves you at a significant disadvantage relative to a non-conservative agent, and it seems more natural to first consider direct approaches to making imitation effective (like bootstrapping + <a href="https://medium.com/ai-control/mimicry-maximization-and-meeting-halfway-c149dd23fc17">meeting halfway</a>).</p>
<p>Of course all of these approaches still involve a lot of extra work, so maybe the difference is are expectations about how different research angles will work out.</p></main><hr><footer></footer></body></html>