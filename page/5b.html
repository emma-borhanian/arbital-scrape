<!DOCTYPE html><html><head><meta charset="utf-8"><title>Linguistic conventions in value alignment</title><link rel="stylesheet" type="text/css" href="../common.css"><link rel="stylesheet" type="text/css" href="../page-style.css"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  SVG: {EqnChunk: 50, EqnChunkFactor: 1.5, EqChunkDelay: 10, useFontCache: false, linebreaks: {automatic: true}},
  tex2jax: {
    inlineMath: [['$~$', '$~$']],
    displayMath: [['$$~$', '$~$$']],
    processEscapes: true,
    preview: 'none',
  },
  showProcessingMessages: false,
  messageStyle: 'none',
  // http://docs.mathjax.org/en/latest/config-files.html#the-tex-ams-svg-configuration-file
  jax: ["input/TeX","output/SVG", "output/PreviewHTML"],
  extensions: ["tex2jax.js","MathMenu.js","MathZoom.js", "fast-preview.js", "AssistiveMML.js", "a11y/accessibility-menu.js"],
  TeX: { extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"] }
});</script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_SVG" async></script><script type="text/javascript" src="../arbital-demo-bundle.js"></script><script type="text/javascript">window.addEventListener('DOMContentLoaded', e=>window.loadAllDemos())
</script></head><body><header><h1 class="title">Linguistic conventions in value alignment</h1><div class="page-info"><p class="metadata-link"><a href="../metadata/5b.json.html">5b.json</a></p><p class="arbital-url"><a href="https://arbital.com/p/5b">https://arbital.com/p/5b</a></p><p class="creator">by
 <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a> Apr 27 2015 
updated
 Dec 17 2015</p></div><p class="clickbait">How and why to use precise language and words with special meaning when talking about value alignment.</p><nav class="breadcrumbs"><nav class="breadcrumb" aria-label="Breadcrumb"><ul><li><a href="../index.html">Index</a></li><li>Linguistic conventions in value alignment</li></ul></nav><nav class="breadcrumb" aria-label="Breadcrumb"><ul><li><a href="ai_alignment.html">AI alignment</a></li><li>â€¦</li></ul></nav></nav></header><hr><main><p>A central page to list the language conventions in <a href="ai_alignment.html">value alignment theory</a>.  See also <a href="value_alignment_glossary.html">Glossary (Value Alignment Theory)</a>.</p>
<h1 id="languagedealingwithwantsdesiresutilitypreferenceandvalue">Language dealing with wants, desires, utility, preference, and value.</h1>
<p>We need a language rich enough to distinguish at least the following as different <a href="intension_extension.html">intensional concepts</a>, even if their <a href="intension_extension.html">extensions</a> end up being identical:</p>
<ul>
<li>A.  What the programmers explicitly, verbally said they wanted to achieve by building the AI.</li>
<li>B.  What the programmers wordlessly, intuitively meant; the actual criterion they would use for rating the desirability of outcomes, if they could actually look at those outcomes and assign ratings.</li>
<li>C.  What programmers <em>should</em> want from the AI (from within some view on normativity, shouldness, or rightness).</li>
<li>D.  The AI's explicitly represented cognitive preferences, if any.</li>
<li>E.  The property that running the AI tends to produce in the world; the property that the AI behaves in such fashion as to bring about.</li>
</ul>
<p>So far, the following reserved terms have been advocated for the subject of value alignment:</p>
<ul>
<li><a href="value_alignment_value.html"><strong>Value</strong></a> and <strong>valuable</strong> to refer to C.  On views which identify C with B, it thereby refers to B.</li>
<li><strong>Optimization target</strong> to mean only E.  We can also say, e.g., that natural selection has an 'optimization target' of inclusive genetic fitness.  'Optimization target' is meant to be an exceedingly general term that can talk about irrational agents and nonagents.</li>
<li><a href="value_alignment_utility.html"><strong>Utility</strong></a> to mean a Von Neumann-Morgenstern utility function, reserved to talk about agents that behave like some bounded analogue of expected utility optimizers.  Utility is explicitly not assumed to be normative.  E.g., if speaking of a paperclip maximizer, we will say that an outcome has higher utility iff it contains more paperclips.  Thus 'utility' is reserved to refer to D or E.</li>
<li><strong>Desire</strong> to mean anthropomorphic human-style desires, referring to A or B rather than C, D, or E.  ('Wants' are general over humans and AIs.)</li>
<li><strong>Preference</strong> and <strong>prefer</strong> to be general terms that can be used for both humans and AIs.  'Preference' refers to B or D rather than A, C, or E.  It means 'what the agent explicitly and cognitively wants' rather than 'what the agent should want' or 'what the agent mistakenly thinks it wants' or 'what the agent's behavior tends to optimize'.  Someone can be said to prefer their extrapolated volition to be implemented rather than their current desires, but if so they must explicitly, cognitively prefer that, or accept it in an explicit choice between options.</li>
<li><a href="preference_framework.html"><strong>Preference framework</strong></a> to be an even more general term that can refer to e.g. meta-utility functions that change based on observations, or to meta-preferences about how one's own preferences should be extrapolated.  A 'preference framework' should refer to constructs more coherent than the human mass of desires and ad-hoc reflections, but not as strictly restricted as a VNM utility function.  Stuart Armstrong's [ utility indifference] framework for [ value learning] is an example of a preference framework that is not a vanilla/ordinary utility function.</li>
<li><strong>Goal</strong> remains a generic, unreserved term that could refer to any of A-E, and also particular things an agent wants to get done for [ instrumental] reasons.</li>
<li><strong><a href="intended_goal.html">Intended goal</a></strong> to refer to B only.</li>
<li><strong>Want</strong> remains a generic, unreserved term that could refer to humans or other agents, or terminal or instrumental goals.</li>
</ul>
<p>'Terminal' and 'instrumental' have their standard contrasting meanings.</p></main><hr><footer><p class="tagged"><h2>Tagged</h2><span class="page-comma-list"><a class="page-link" href="../page/definition_meta_tag.html">Definition</a></span></p><p class="all-creators"><h2>All Creators</h2><span class="page-comma-list"><a class="page-link" href="../page/AlexeiAndreev.html">Alexei Andreev</a>,
 <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></span></p><p class="likes"><h2>Likes</h2><span class="page-comma-list"><a class="page-link" href="../page/RokResnik.html">Rok Resnik</a></span></p><p class="reverse-related"><h2>Reverse Related</h2><ul class="page-list"><li><a class="page-link" href="../page/definition_meta_tag.html">Definition</a> <q>Meta tag used to mark pages that strictly define a particular term or phrase.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li></ul></p><p class="children"><h2>Children</h2><ul class="page-tree"><li><a class="page-link" href="../page/value_alignment_utility.html">Utility</a> <q>What is &quot;utility&quot; in the context of Value Alignment Theory?</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li></ul></p></footer></body></html>