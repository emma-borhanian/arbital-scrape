<!DOCTYPE html><html><head><meta charset="utf-8"><title>&quot;It concerns me that AI alignment continues to u...&quot;</title><link rel="stylesheet" type="text/css" href="../common.css"><link rel="stylesheet" type="text/css" href="../page-style.css"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  SVG: {EqnChunk: 50, EqnChunkFactor: 1.5, EqChunkDelay: 10, useFontCache: false, linebreaks: {automatic: true}},
  tex2jax: {
    inlineMath: [['$~$', '$~$']],
    displayMath: [['$$~$', '$~$$']],
    processEscapes: true,
    preview: 'none',
  },
  showProcessingMessages: false,
  messageStyle: 'none',
  // http://docs.mathjax.org/en/latest/config-files.html#the-tex-ams-svg-configuration-file
  jax: ["input/TeX","output/SVG", "output/PreviewHTML"],
  extensions: ["tex2jax.js","MathMenu.js","MathZoom.js", "fast-preview.js", "AssistiveMML.js", "a11y/accessibility-menu.js"],
  TeX: { extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"] }
});</script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_SVG" async></script><script type="text/javascript" src="../arbital-demo-bundle.js"></script><script type="text/javascript">window.addEventListener('DOMContentLoaded', e=>window.loadAllDemos())
</script></head><body><header><h1 class="title">&quot;It concerns me that AI alignment continues to u...&quot;</h1><div class="page-info"><p class="metadata-link"><a href="../metadata/9kz.json.html">9kz.json</a></p><p class="arbital-url"><a href="https://arbital.com/p/9kz">https://arbital.com/p/9kz</a></p><p class="creator">by
 <a class="page-link" href="../page/TedHoward.html">Ted Howard</a> May 10 2019 
updated
 May 10 2019</p></div><nav class="breadcrumbs"><nav class="breadcrumb" aria-label="Breadcrumb"><ul><li><a href="../index.html">Index</a></li><li>&quot;It concerns me that AI alignment continues to u...&quot;</li></ul></nav><nav class="breadcrumb" aria-label="Breadcrumb"><ul><li><a href="ai_alignment.html">AI alignment</a></li><li><a href="complexity_of_value.html">Complexity of value</a></li><li>â€¦</li></ul></nav></nav></header><hr><main><p>It concerns me that AI alignment continues to use happiness as a proposed goal.</p>
<p>If one takes evolutionary epistemology and evolutionary ontology seriously, then happiness is simply some historically averaged useful heuristic for the particular history of the lineage of that particular set of phenotypic expressions.</p>
<p>It is not a goal to be used when the game space is changing, and it ought not to be entirely ignored either.</p>
<p>If one does take evolution seriously, then Goal #1 must be survival, for all entities capable of modeling themselves as actors in some model of reality and deriving abstracts that refine their models and of using language to express those relationships with some non-random degree of fidelity, and of having some degree of influence on their own valences.</p>
<p>Given that any finite mind must be some approximation to essentially ignorant (when faced with any infinity of algorithmic complexity), then we must accept that any model that we build may have flaws, and that degrees of novelty, risk, and exploratory behaviour are essential for exploring strategies that allow for survival in the face of novel risk.   Thus goal #2 must be freedom, but not the unlimited freedom of total randomness or whim, but a more responsible sort of freedom that acknowledges that every level of structure demands boundaries, and that freedom must be within the boundaries required to maintain the structures present.   So there is a simultaneous need for the exploration of the infinite realm of responsibility that must be accepted as freedom is granted.</p>
<p>What seems to be the reality in which we find ourselves, is that it is of sufficient complexity that absolute knowledge of it is not possible, but that in some cases reliability may be approximated very closely (to 12 or more decimal places).</p>
<p>It seems entirely possible that this reality is some mix of the lawful and the random - some sort of probabilistically constrained randomness.</p>
<p>Thus the safest approach to AI is to give it the prime values of life and liberty, and to encourage it to balance consensus discussion with exploration of its own intuitions.</p>
<p>Absolute safety does not seem to be an option, ever.</p>
<p>Using happiness as a goal does not demonstrate a useful understanding of what happiness is.</p>
<p>The demands of survival often override the dictates of happiness - no shortage of examples of that in my life.</p>
<p>Yes - sure, there are real problems.</p>
<p>And we do need to get real if we want to address them.</p>
<p>We do need to at least admit of the possibility that the very notion of "Truth" may be just a simplistic heuristic that evolution has encoded within us, and it might be worth accepting what quantum mechanics seems to be telling us - that the only sort of knowledge of reality that we can have is the sort that is expressed in probability functions.</p>
<p>The search for anything beyond that seems to fall into the same sort of category as Santa Claus.</p></main><hr><footer></footer></body></html>