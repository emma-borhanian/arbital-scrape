<!DOCTYPE html><html><head><meta charset="utf-8"><title>&quot;I think the [key question](...&quot;</title><link rel="stylesheet" type="text/css" href="../common.css"><link rel="stylesheet" type="text/css" href="../page-style.css"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  SVG: {EqnChunk: 50, EqnChunkFactor: 1.5, EqChunkDelay: 10, useFontCache: false, linebreaks: {automatic: true}},
  tex2jax: {
    inlineMath: [['$~$', '$~$']],
    displayMath: [['$$~$', '$~$$']],
    processEscapes: true,
    preview: 'none',
  },
  showProcessingMessages: false,
  messageStyle: 'none',
  // http://docs.mathjax.org/en/latest/config-files.html#the-tex-ams-svg-configuration-file
  jax: ["input/TeX","output/SVG", "output/PreviewHTML"],
  extensions: ["tex2jax.js","MathMenu.js","MathZoom.js", "fast-preview.js", "AssistiveMML.js", "a11y/accessibility-menu.js"],
  TeX: { extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"] }
});</script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_SVG" async></script><script type="text/javascript" src="../arbital-demo-bundle.js"></script><script type="text/javascript">window.addEventListener('DOMContentLoaded', e=>window.loadAllDemos())
</script></head><body><header><h1 class="title">&quot;I think the [key question](...&quot;</h1><div class="page-info"><p class="metadata-link"><a href="../metadata/2qs.json.html">2qs.json</a></p><p class="arbital-url"><a href="https://arbital.com/p/2qs">https://arbital.com/p/2qs</a></p><p class="creator">by
 <a class="page-link" href="../page/PaulChristiano.html">Paul Christiano</a> Mar 20 2016 
updated
 Mar 20 2016</p></div><nav class="breadcrumbs"><nav class="breadcrumb" aria-label="Breadcrumb"><ul><li><a href="../index.html">Index</a></li><li>&quot;I think the [key question](...&quot;</li></ul></nav><nav class="breadcrumb" aria-label="Breadcrumb"><ul><li><a href="ai_alignment.html">AI alignment</a></li><li><a href="AGI_typology.html">Strategic AGI typology</a></li><li><a href="task_agi.html">Task-directed AGI</a></li><li><a href="taskagi_open_problems.html">Open subproblems in aligning a Task-based AGI</a></li><li><a href="2ql.html">&quot;It seems critical to distin...&quot;</a></li><li>…</li></ul></nav><nav class="breadcrumb" aria-label="Breadcrumb"><ul><li><a href="ai_alignment.html">AI alignment</a></li><li><a href="AGI_typology.html">Strategic AGI typology</a></li><li><a href="task_agi.html">Task-directed AGI</a></li><li><a href="taskagi_open_problems.html">Open subproblems in aligning a Task-based AGI</a></li><li>…</li></ul></nav><nav class="breadcrumb" aria-label="Breadcrumb"><ul><li><a href="ai_alignment.html">AI alignment</a></li><li><a href="task_agi.html">Task-directed AGI</a></li><li><a href="taskagi_open_problems.html">Open subproblems in aligning a Task-based AGI</a></li><li><a href="2ql.html">&quot;It seems critical to distin...&quot;</a></li><li>…</li></ul></nav><nav class="breadcrumb" aria-label="Breadcrumb"><ul><li><a href="ai_alignment.html">AI alignment</a></li><li><a href="task_agi.html">Task-directed AGI</a></li><li><a href="taskagi_open_problems.html">Open subproblems in aligning a Task-based AGI</a></li><li>…</li></ul></nav></nav></header><hr><main><p>I think the <a href="https://medium.com/ai-control/the-informed-oversight-problem-1b51b4f66b35#.lqcjngn6w">key question</a> is whether:</p>
<ol>
<li>the burrito judge needs to be extremely powerful, or</li>
<li>the burrito judge needs to be modestly more powerful than the burrito producer.</li>
</ol>
<p>In world 1 I agree that the burrito-evaluator seems pretty tough to build. We certainly have disagreements about that case, but I'm happy to set it aside for now.</p>
<p>In world 2 things seem much less scary. Because <a href="https://medium.com/ai-control/counterfactual-human-in-the-loop-a7822e36f399">I only need to run these evaluations with e.g. 1% probability</a>, the judge can use 50x more resources than the burrito producer. So it's imaginable that the judge can be more powerful than the producer.</p>
<p>You seem to think that we are in world 1. I think that we are probably in world 2, but I'm certainly not sure. I discuss the issue in <a href="https://medium.com/ai-control/the-informed-oversight-problem-1b51b4f66b35#.lqcjngn6w">this post</a>.</p>
<p>Some observations:</p>
<ul>
<li>The judge's job is easier if they are evaluating steps of the plan, before those steps are taken, rather than actually letting the burrito producer take actions. So let's do it that way.</li>
<li>The judge can look at the burrito producer's computation, and at the training process that produced that computation, and can change the burrito producer's training procedure to make that computation more understandable.</li>
<li>If the judge were epistemically efficient with respect to the producer, then maximizing the judge's expectation of a burrito's quality would be the same as maximizing the burrito producer's expectation of a burrito's quality. That's basically what we want. So the real issue is narrower than you might expect, it's some kind of epistemic version of "offense vs. defense," where the producer can think particular thoughts that the judge doesn't happen to think, and so the producer might expect to be able to deceive/attack the judge even though the judge is smarter. This is what the judge is trying to avoid by looking at the producer's computation.</li>
</ul>
<p>So I don't think that we can just ask the judge to evaluate the burrito; but the judge has enough going for her that I expect we can find some strategy that lets her win. I think this is the biggest open problem for my current approach.</p></main><hr><footer></footer></body></html>