<!DOCTYPE html><html><head><meta charset="utf-8"><title>Research directions in AI control</title><link rel="stylesheet" type="text/css" href="../common.css"><link rel="stylesheet" type="text/css" href="../page-style.css"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  SVG: {EqnChunk: 50, EqnChunkFactor: 1.5, EqChunkDelay: 10, useFontCache: false, linebreaks: {automatic: true}},
  tex2jax: {
    inlineMath: [['$~$', '$~$']],
    displayMath: [['$$~$', '$~$$']],
    processEscapes: true,
    preview: 'none',
  },
  showProcessingMessages: false,
  messageStyle: 'none',
  // http://docs.mathjax.org/en/latest/config-files.html#the-tex-ams-svg-configuration-file
  jax: ["input/TeX","output/SVG", "output/PreviewHTML"],
  extensions: ["tex2jax.js","MathMenu.js","MathZoom.js", "fast-preview.js", "AssistiveMML.js", "a11y/accessibility-menu.js"],
  TeX: { extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"] }
});</script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_SVG" async></script><script type="text/javascript" src="../arbital-demo-bundle.js"></script><script type="text/javascript">window.addEventListener('DOMContentLoaded', e=>window.loadAllDemos())
</script></head><body><header><h1 class="title">Research directions in AI control</h1><div class="page-info"><p class="metadata-link"><a href="../metadata/research_directions_ai_control.json.html">research_directions_ai_control.json</a></p><p class="arbital-url"><a href="https://arbital.com/p/research_directions_ai_control">https://arbital.com/p/research_directions_ai_control</a></p><p class="creator">by
 <a class="page-link" href="../page/PaulChristiano.html">Paul Christiano</a> Feb 3 2016 
updated
 Mar 4 2016</p></div><nav class="breadcrumbs"><nav class="breadcrumb" aria-label="Breadcrumb"><ul><li><a href="../index.html">Index</a></li><li>Research directions in AI control</li></ul></nav><nav class="breadcrumb" aria-label="Breadcrumb"><ul><li><a href="active_learning_powerful_predictors.html">Active learning for opaque, powerful predictors</a></li><li>…</li></ul></nav></nav></header><hr><main><p>What research would best advance our understanding of AI control?</p>
<p>I’ve been thinking about this question a lot over the last few weeks. This post lays out my best guesses.</p>
<h3 id="mycurrenttakeonaicontrol">My current take on AI control</h3>
<p>I want to&nbsp;<a href="possible_stance_ai_control_research.html">focus on existing AI techniques</a>, minimizing speculation about future developments. As a special case, I would like to&nbsp;<a href="unsupervised_learning_ai_control.html">use minimal assumptions about unsupervised learning</a>, instead relying on supervised and<a href="reinforcement_learning_linguistic_convention.html?title=reinforcement-learning-and-linguistic-convention">reinforcement learning</a>. My goal is to find&nbsp;<a href="Scalable_ai_control.html?title=scalable-ai-control">scalable</a>&nbsp;approaches to AI control that can be applied to existing AI systems.</p>
<p>For now, I think that&nbsp;<a href="act_based_agents.html">act-based approaches</a>&nbsp;look significantly more promising than goal-directed approaches. (Note that both categories are&nbsp;<a href="learn_policies_goals.html?title=learn-policies-or-goals">consistent with using value learning</a>.) I think that many apparent problems are distinctive to goal-directed approaches&nbsp;<a href="abstract_approval_direction.html">and can be temporarily set aside</a>. But a more direct motivation is that the goal-directed approach seems to require speculative future developments in AI, whereas we can&nbsp;<a href="concrete_approval_directed_agents.html">take a stab</a>&nbsp;at the act-based approach now (though obviously much more work is needed).</p>
<p>In light of those views, I find the following research directions most attractive:</p>
<h3 id="fourpromisingdirections">Four promising directions</h3>
<ul>
<li><a href="elaborations_apprenticeship_learning.htmlelaborations_apprenticeship_learning">Elaborating on apprenticeship learning</a>.<br />
Imitating human behavior seems especially promising as a scalable approach to AI control, but there are many outstanding problems.</li>
<li><a href="efficient_feedback.html">Efficiently using human feedback</a>.<br />
The limited availability of human feedback may be a serious bottleneck for realistic approaches to AI control.</li>
<li><a href="human_arguments_ai_control.htmlhuman_arguments_ai_control">Explaining human judgments and disagreements</a>.<br />
My preferred approach to AI control requires humans to understand AIs’ plans and beliefs. We don’t know how to solve the analogous problem for humans.</li>
<li><a href="reward_engineering.html?title=reward-engineering">Designing feedback mechanisms for reinforcement learning</a>.<br />
A grab bag of problems, united by a need for proxies of hard-to-optimize, implicit objectives.</li>
</ul>
<p>I will probably be doing work in one or more of these directions soon. I am also interested in talking with anyone who is considering looking into these or similar questions.</p>
<p>I’d love to find considerations that would change my view — whether arguments against these projects, or more promising alternatives. But these are my current best guesses, and I consider them good enough that the right next step is to work on them.</p>
<p>(This research was supported as part of the&nbsp;<a href="http://futureoflife.org/"><em>Future of Life Institute</em></a> FLI-RFP-AI1 program, grant #2015–143898.)</p></main><hr><footer></footer></body></html>