<!DOCTYPE html><html><head><meta charset="utf-8"><title>Compressing multiple messages</title><link rel="stylesheet" type="text/css" href="../common.css"><link rel="stylesheet" type="text/css" href="../page-style.css"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  SVG: {EqnChunk: 50, EqnChunkFactor: 1.5, EqChunkDelay: 10, useFontCache: false, linebreaks: {automatic: true}},
  tex2jax: {
    inlineMath: [['$~$', '$~$']],
    displayMath: [['$$~$', '$~$$']],
    processEscapes: true,
    preview: 'none',
  },
  showProcessingMessages: false,
  messageStyle: 'none',
  // http://docs.mathjax.org/en/latest/config-files.html#the-tex-ams-svg-configuration-file
  jax: ["input/TeX","output/SVG", "output/PreviewHTML"],
  extensions: ["tex2jax.js","MathMenu.js","MathZoom.js", "fast-preview.js", "AssistiveMML.js", "a11y/accessibility-menu.js"],
  TeX: { extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"] }
});</script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_SVG" async></script><script type="text/javascript" src="../arbital-demo-bundle.js"></script><script type="text/javascript">window.addEventListener('DOMContentLoaded', e=>window.loadAllDemos())
</script></head><body><header><h1 class="title">Compressing multiple messages</h1><div class="page-info"><p class="metadata-link"><a href="../metadata/multiple_compression.json.html">multiple_compression.json</a></p><p class="arbital-url"><a href="https://arbital.com/p/multiple_compression">https://arbital.com/p/multiple_compression</a></p><p class="creator">by
 <a class="page-link" href="../page/NateSoares.html">Nate Soares</a> Jun 2 2016 
updated
 Jun 3 2016</p></div><nav class="breadcrumbs"><nav class="breadcrumb" aria-label="Breadcrumb"><ul><li><a href="../index.html">Index</a></li><li>Compressing multiple messages</li></ul></nav><nav class="breadcrumb" aria-label="Breadcrumb"><ul><li><a href="math.html">Mathematics</a></li><li><a href="information_theory.html">Information theory</a></li><li>â€¦</li></ul></nav></nav></header><hr><main><p>How many <a href="data_bit.html">bits of data</a> does it take to encode an [3v9 $~$n$~$-message]? Naively, the answer is $~$\lceil \log_2(n) \rceil$~$ ([n_message_bit_length why?]): For example, it takes 5 bits to encode a 21-message, because 4 bits are only enough to encode 16 different messages, but 5 bits are enough to encode 32. The use of the <a href="mathematics_ceiling.html">Ceiling</a> function implies an inefficiency: 2 bits are required to encode a 3-message, but 2 bits are enough to distinguish between four different possibilities. One of those possibilities is being wasted. That inefficiency can be reduced by encoding multiple $~$n$~$-messages at the same time. For example, while an individual 3-message requires 2 bits to encode, a series of 10 3-messages requires at most 16 bits to encode: $~$3^{10} &lt; 2^{16}.$~$</p>
<p>Why is it that encoding ten 3-messages together (using bits) is cheaper than encoding ten 3-messages separately? Naively, there are three different factors that allow the combined encoding to be shorter than the sum of the separate encodings: The messages could have different likelihoods ([expected_compression allowing the combined message to be compressed in expectation]); the messages could be dependent on each other ([compressing_dependent_messages meaning they can be compressed]); and the mismatch between bits and 3-messages gets washed out as we put more three-messages together (see <a href="bits_in_a_trit.html">How many bits to a trit?</a>).</p>
<p>In fact, the first two factors are equivalent: 10 3-messages are equivalent to one $~$3^{10}$~$ message, and in general, [n<em>k</em>messages $~$n$~$ $~$k$~$-messages are equivalent to one $~$n^k$~$-message]. If the individual n-messages are dependent on each other, then different $~$n^k$~$ messages have different likelihoods: For example, if message 3 never follows message 2, then in the combined message, "32" never appears as a substring.</p>
<p>Thus, there are two different ways that an encoding of $~$k$~$ $~$n$~$-messages can be shorter than $~$k$~$ times the encoding of an $~$n$~$-message: The various combined messages can have different likelihoods, and the efficiency of the coding might increase. To study the effect of different likelihoods on the encoding length in isolation, we can [assume_maximum_efficiency assume that the codings are maximally efficient] and see how much additional [compression] the different likelihoods get us. To study code efficiency in isolation, we can [assume_equal_likelihood_messages assume each message is equally likely] and see how much additional compression we get as we put more $~$n$~$-messages together. In practice, real compression involves using both techniques at once.</p></main><hr><section class="comments"><h2>Comments</h2><div class="comment"><p><a class="page-link" href="../page/EricRogstad.html">Eric Rogstad</a></p><p><blockquote class="comment-context">Why is it that encoding ten 3\-messages together \(using bits\) is cheaper than encoding ten 3\-messages separately? Naively, there are three different factors that allow the combined encoding to be shorter than the sum of the separate encodings: The messages could have different likelihoods \(allowing the combined message to be compressed in expectation\); the messages could be dependent on each other \(meaning they can be compressed\); and the mismatch between bits and 3\-messages gets washed out as we put more three\-messages together \(<mark>see How many bits is a trit?\)</mark>\.</blockquote>
<p>Did you mean this to link to <a href="bits_in_a_trit.html">How many bits to a trit?</a>?</p></p></div><div class="comment"><p><a class="page-link" href="../page/EricRogstad.html">Eric Rogstad</a></p><p><blockquote class="comment-context">In fact, the first two factors are equivalent: 10 3\-messages are equivalent to one $~$3^{10}$~$ message, and in general, \$\$n\$ \$k\$\-messages are equivalent to one \$n^k\$\-message\. If the individual n\-messages are dependent on each other, then different $~$n^k$~$ messages have different likelihoods: For example, if message 3 never follows message 2, then in the combined message, "32" never appears as a substring\.</blockquote>
<p>Is what follows the colon meant to be justification for what precedes it? I'm not following.</p></p></div></section><footer><p class="tagged"><h2>Tagged</h2><span class="page-comma-list"><a class="page-link" href="../page/work_in_progress_meta_tag.html">Work in progress</a></span></p><p class="all-creators"><h2>All Creators</h2><span class="page-comma-list"><a class="page-link" href="../page/EricRogstad.html">Eric Rogstad</a>,
 <a class="page-link" href="../page/NateSoares.html">Nate Soares</a></span></p><p class="likes"><h2>Likes</h2><span class="page-comma-list"><a class="page-link" href="../page/EricRogstad.html">Eric Rogstad</a></span></p><p class="reverse-related"><h2>Reverse Related</h2><ul class="page-list"><li><a class="page-link" href="../page/work_in_progress_meta_tag.html">Work in progress</a> <q>This page is being actively worked on by an editor. Check with them before making major changes.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li></ul></p></footer></body></html>