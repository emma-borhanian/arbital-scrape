<!DOCTYPE html><html><head><meta charset="utf-8"><title>Value alignment problem</title><link rel="stylesheet" type="text/css" href="../common.css"><link rel="stylesheet" type="text/css" href="../page-style.css"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  SVG: {EqnChunk: 50, EqnChunkFactor: 1.5, EqChunkDelay: 10, useFontCache: false, linebreaks: {automatic: true}},
  tex2jax: {
    inlineMath: [['$~$', '$~$']],
    displayMath: [['$$~$', '$~$$']],
    processEscapes: true,
    preview: 'none',
  },
  showProcessingMessages: false,
  messageStyle: 'none',
  // http://docs.mathjax.org/en/latest/config-files.html#the-tex-ams-svg-configuration-file
  jax: ["input/TeX","output/SVG", "output/PreviewHTML"],
  extensions: ["tex2jax.js","MathMenu.js","MathZoom.js", "fast-preview.js", "AssistiveMML.js", "a11y/accessibility-menu.js"],
  TeX: { extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"] }
});</script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_SVG" async></script><script type="text/javascript" src="../arbital-demo-bundle.js"></script><script type="text/javascript">window.addEventListener('DOMContentLoaded', e=>window.loadAllDemos())
</script></head><body><header><h1 class="title">Value alignment problem</h1><div class="page-info"><p class="metadata-link"><a href="../metadata/value_alignment_problem.json.html">value_alignment_problem.json</a></p><p class="arbital-url"><a href="https://arbital.com/p/value_alignment_problem">https://arbital.com/p/value_alignment_problem</a></p><p class="creator">by
 <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a> May 15 2015 
updated
 Feb 2 2017</p></div><p class="clickbait">You want to build an advanced AI with the right values... but how?</p><nav class="breadcrumbs"><nav class="breadcrumb" aria-label="Breadcrumb"><ul><li><a href="../index.html">Index</a></li><li>Value alignment problem</li></ul></nav><nav class="breadcrumb" aria-label="Breadcrumb"><ul><li><a href="ai_alignment.html">AI alignment</a></li><li>â€¦</li></ul></nav></nav></header><hr><main><p>Disambiguation:  For the research subject that includes the entire edifice of how and why to produce good AIs, see <a href="ai_alignment.html">AI alignment</a>.</p>
<p>[summary: The 'value alignment problem' is to produce <a href="sufficiently_advanced_ai.html">sufficiently advanced machine intelligences</a> that <em>want</em> to do <a href="beneficial.html">beneficial</a> things and not do harmful things.  The largest-looming subproblem is <a href="value_identification.html">&#39;value identification&#39; or &#39;value learning&#39;</a> (sometimes considered synonymous with value alignment) but this also includes subproblems like <a href="corrigibility.html">Corrigibility</a>, that is, AI values such that it doesn't <em>want</em> to interfere with you correcting what you see as an error in its code.]</p>
<p>The 'value alignment problem' is to produce <a href="sufficiently_advanced_ai.html">sufficiently advanced machine intelligences</a> that <em>want</em> to do <a href="beneficial.html">beneficial</a> things and not do harmful things.  The largest-looming subproblem is <a href="value_identification.html">&#39;value identification&#39; or &#39;value learning&#39;</a> (sometimes considered synonymous with value alignment) but this also includes subproblems like <a href="corrigibility.html">Corrigibility</a>, that is, AI values such that it doesn't <em>want</em> to interfere with you correcting what you see as an error in its code.</p></main><hr><footer><p class="tagged"><h2>Tagged</h2><span class="page-comma-list"><a class="page-link" href="../page/stub_meta_tag.html">Stub</a></span></p><p class="all-creators"><h2>All Creators</h2><span class="page-comma-list"><a class="page-link" href="../page/AlexeiAndreev.html">Alexei Andreev</a>,
 <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></span></p><p class="likes"><h2>Likes</h2><span class="page-comma-list"><a class="page-link" href="../page/AlexeiAndreev.html">Alexei Andreev</a>,
 <a class="page-link" href="../page/BenWest.html">Ben West</a>,
 <a class="page-link" href="../page/JeremyPerret.html">Jeremy Perret</a>,
 <a class="page-link" href="../page/RonnyFernandez.html">Ronny Fernandez</a>,
 <a class="page-link" href="../page/TomBrown.html">Tom Brown</a></span></p><p class="reverse-related"><h2>Reverse Related</h2><ul class="page-list"><li><a class="page-link" href="../page/stub_meta_tag.html">Stub</a> <q>This page only gives a very brief overview of the topic. If you're able to, please help expand or improve it!</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li></ul></p><p class="children"><h2>Children</h2><ul class="page-tree"><li><a class="page-link" href="../page/preference_framework.html">Preference framework</a> <q>What's the thing an agent uses to compare its preferences?</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a><ul class="page-tree"><li><a class="page-link" href="../page/attainable_optimum.html">Attainable optimum</a> <q>The 'attainable optimum' of an agent's preferences is the best that agent can actually do given its finite intelligence and resources (as opposed to the global maximum of those preferences).</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/meta_utility.html">Meta-utility function</a> <q>Preference frameworks built out of simple utility functions, but where, e.g., the 'correct' utility function for a possible world depends on whether a button is pressed.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/moral_uncertainty.html">Moral uncertainty</a> <q>A meta-utility function in which the utility function as usually considered, takes on different values in different possible worlds, potentially distinguishable by evidence.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a><ul class="page-tree"><li><a class="page-link" href="../page/ideal_target.html">Ideal target</a> <q>The 'ideal target' of a meta-utility function is the value the ground-level utility function would take on if the agent updated on all possible evidence; the 'true' utilities under moral uncertainty.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li></ul></li></ul></li><li><a class="page-link" href="../page/total_alignment.html">Total alignment</a> <q>We say that an advanced AI is &quot;totally aligned&quot; when it knows *exactly* which outcomes and plans are beneficial, with no further user input.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li></ul></p></footer></body></html>