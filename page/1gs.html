<!DOCTYPE html><html><head><meta charset="utf-8"><title>&quot;Re: simulating a hostile su...&quot;</title><link rel="stylesheet" type="text/css" href="../common.css"><link rel="stylesheet" type="text/css" href="../page-style.css"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  SVG: {EqnChunk: 50, EqnChunkFactor: 1.5, EqChunkDelay: 10, useFontCache: false, linebreaks: {automatic: true}},
  tex2jax: {
    inlineMath: [['$~$', '$~$']],
    displayMath: [['$$~$', '$~$$']],
    processEscapes: true,
    preview: 'none',
  },
  showProcessingMessages: false,
  messageStyle: 'none',
  // http://docs.mathjax.org/en/latest/config-files.html#the-tex-ams-svg-configuration-file
  jax: ["input/TeX","output/SVG", "output/PreviewHTML"],
  extensions: ["tex2jax.js","MathMenu.js","MathZoom.js", "fast-preview.js", "AssistiveMML.js", "a11y/accessibility-menu.js"],
  TeX: { extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"] }
});</script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_SVG" async></script><script type="text/javascript" src="../arbital-demo-bundle.js"></script><script type="text/javascript">window.addEventListener('DOMContentLoaded', e=>window.loadAllDemos())
</script></head><body><header><h1 class="title">&quot;Re: simulating a hostile su...&quot;</h1><div class="page-info"><p class="metadata-link"><a href="../metadata/1gs.json.html">1gs.json</a></p><p class="arbital-url"><a href="https://arbital.com/p/1gs">https://arbital.com/p/1gs</a></p><p class="creator">by
 <a class="page-link" href="../page/PaulChristiano.html">Paul Christiano</a> Dec 29 2015</p></div><nav class="breadcrumbs"><nav class="breadcrumb" aria-label="Breadcrumb"><ul><li><a href="../index.html">Index</a></li><li>&quot;Re: simulating a hostile su...&quot;</li></ul></nav><nav class="breadcrumb" aria-label="Breadcrumb"><ul><li><a href="ai_alignment.html">AI alignment</a></li><li><a href="distant_SIs.html">Modeling distant superintelligences</a></li><li>â€¦</li></ul></nav></nav></header><hr><main><p>Re: simulating a hostile superintelligence:</p>
<p>I find this concern really unconcerning. </p>
<p>Some points:</p>
<ul>
<li>This is only really a problem if our own AI development, on Earth, is going so slowly that "having your AI speculate about what aliens might do" is not only the most effective way to develop a powerful AI, it is <em>way</em> more effective than what we were doing anyway. But it looks like "do AI development super slowly" is already a dead end for a bunch of other reasons, so we don't really need to talk about this particular bizarre reason. I guess you aren't yet convinced that this is a dead end, but I do hope to convince you at some point.</li>
<li>At the point where such massive amounts of internal computing power are being deployed, it seems implausible that an AI system won't be thinking about how to think. At that point, the concern is not about the internal robustness of our system, but instead about the whether the AI is well-calibrated about its own internal robustness. The latter problem seems like one that we essentially have to solve anyway).</li>
</ul>
<p>I think that there is a higher burden of proof for advancing concerns that AI researchers will dismiss out of hand as crazy, and that we should probably only do it for concerns that are way more solid than this one. Otherwise (1) it will become impossible to advance real concerns that sound crazy, if a pattern is established that crazy-sounding concerns actually are crazy, (2) people interested in AI safety will be roundly dismissed as crazy.</p></main><hr><section class="comments"><h2>Comments</h2><div class="comment"><p><a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></p><p><p>The concern is for when you have a preference-limited AI that already contains enough computing power and has enough potential intelligence to be extremely dangerous, and it contains something that's smaller than itself but unlimited and hostile.  Like, your genie has a lot of cognitive power but, by design of its preferences, it doesn't do more than a fraction of what it could; if that's a primary scenario you're optimizing for, then having your genie thinking deeply about possible hostile superintelligences seems potentially worrisome.  In fact, it seems like a case of, "If you try to channel cognitive resources <em>this</em> way, but you ignore <em>this</em> problem, of course the AI just blows up anyway."</p>
<p>I agree that like a large subset of potential killer problems, this would not be high on my list of things to explain to people who were already having trouble "taking things seriously", just like I'd be trying to phrase everything in terms of scenarios with no nanotechnology even though I think the physics argument for nanotechnology is straightforward.</p></p></div></section><footer></footer></body></html>