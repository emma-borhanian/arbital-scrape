<!DOCTYPE html><html><head><meta charset="utf-8"><title>Valley of Dangerous Complacency</title><link rel="stylesheet" type="text/css" href="../common.css"><link rel="stylesheet" type="text/css" href="../page-style.css"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  SVG: {EqnChunk: 50, EqnChunkFactor: 1.5, EqChunkDelay: 10, useFontCache: false, linebreaks: {automatic: true}},
  tex2jax: {
    inlineMath: [['$~$', '$~$']],
    displayMath: [['$$~$', '$~$$']],
    processEscapes: true,
    preview: 'none',
  },
  showProcessingMessages: false,
  messageStyle: 'none',
  // http://docs.mathjax.org/en/latest/config-files.html#the-tex-ams-svg-configuration-file
  jax: ["input/TeX","output/SVG", "output/PreviewHTML"],
  extensions: ["tex2jax.js","MathMenu.js","MathZoom.js", "fast-preview.js", "AssistiveMML.js", "a11y/accessibility-menu.js"],
  TeX: { extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"] }
});</script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_SVG" async></script><script type="text/javascript" src="../arbital-demo-bundle.js"></script><script type="text/javascript">window.addEventListener('DOMContentLoaded', e=>window.loadAllDemos())
</script></head><body><header><h1 class="title">Valley of Dangerous Complacency</h1><div class="page-info"><p class="metadata-link"><a href="../metadata/complacency_valley.json.html">complacency_valley.json</a></p><p class="arbital-url"><a href="https://arbital.com/p/complacency_valley">https://arbital.com/p/complacency_valley</a></p><p class="creator">by
 <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a> Mar 23 2016</p></div><p class="clickbait">When the AGI works often enough that you let down your guard, but it still has bugs.  Imagine a robotic car that almost always steers perfectly, but sometimes heads off a cliff.</p><nav class="breadcrumbs"><nav class="breadcrumb" aria-label="Breadcrumb"><ul><li><a href="../index.html">Index</a></li><li>Valley of Dangerous Complacency</li></ul></nav><nav class="breadcrumb" aria-label="Breadcrumb"><ul><li><a href="ai_alignment.html">AI alignment</a></li><li><a href="advanced_safety.html">Advanced safety</a></li><li><a href="AI_safety_mindset.html">AI safety mindset</a></li><li>â€¦</li></ul></nav></nav></header><hr><main><p>The Valley of Dangerous Complacency is when a system works often enough that you let down your guard around it, but in fact the system is still dangerous enough that full vigilance is required.</p>
<ul>
<li>If a robotic car made the correct decision 99% of the time, you'd need to grab the steering wheel on a daily basis, you'd stay alert and your robot-car-overriding skills would stay sharp.</li>
<li>If a robotic car made the correct decision 100% of the time, you'd relax and let your guard down, but there wouldn't be anything wrong with that.</li>
<li>If the robotic car made the correct decision 99.99% of the time, so that you need to grab the steering wheel or else crash in 1 of 100 days, the task of monitoring the car would feel very unrewarding and the car would seem pretty safe.  You'd let your guard down and your driving skills would get rusty.  After a couple of months, the car would crash.</li>
</ul>
<p>Compare "<a href="https://en.wikipedia.org/wiki/Uncanny_valley">Uncanny Valley</a>" where a machine system is partially humanlike - humanlike enough that humans try to hold it to a human standard - but not humanlike enough to actually seem satisfactory when held to a human standard.  This means that in terms of user experience, there's a valley as the degree of humanlikeness of the system increases where the user experience actually gets worse before it gets better.  Similarly, if users become complacent, a 99.99% reliable system can be worse than a 99% reliable one, even though, with <em>enough</em> reliability, the degree of safety starts climbing back out of the valley.</p></main><hr><section class="comments"><h2>Comments</h2><div class="comment"><p><a class="page-link" href="../page/EricRogstad.html">Eric Rogstad</a></p><p><p>I'm reminded of this article: <a href="http://www.macroresilience.com/2011/12/29/people-make-poor-monitors-for-computers/,">http://www.macroresilience.com/2011/12/29/people-make-poor-monitors-for-computers/,</a> which provides some interesting examples.</p></p></div></section><footer><p class="likes"><h2>Likes</h2><span class="page-comma-list"><a class="page-link" href="../page/EricRogstad.html">Eric Rogstad</a>,
 <a class="page-link" href="../page/RyanCarey2.html">Ryan Carey</a></span></p></footer></body></html>