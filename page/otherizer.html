<!DOCTYPE html><html><head><meta charset="utf-8"><title>Other-izing (wanted: new optimization idiom)</title><link rel="stylesheet" type="text/css" href="../common.css"><link rel="stylesheet" type="text/css" href="../page-style.css"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  SVG: {EqnChunk: 50, EqnChunkFactor: 1.5, EqChunkDelay: 10, useFontCache: false, linebreaks: {automatic: true}},
  tex2jax: {
    inlineMath: [['$~$', '$~$']],
    displayMath: [['$$~$', '$~$$']],
    processEscapes: true,
    preview: 'none',
  },
  showProcessingMessages: false,
  messageStyle: 'none',
  // http://docs.mathjax.org/en/latest/config-files.html#the-tex-ams-svg-configuration-file
  jax: ["input/TeX","output/SVG", "output/PreviewHTML"],
  extensions: ["tex2jax.js","MathMenu.js","MathZoom.js", "fast-preview.js", "AssistiveMML.js", "a11y/accessibility-menu.js"],
  TeX: { extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"] }
});</script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_SVG" async></script><script type="text/javascript" src="../arbital-demo-bundle.js"></script><script type="text/javascript">window.addEventListener('DOMContentLoaded', e=>window.loadAllDemos())
</script></head><body><header><h1 class="title">Other-izing (wanted: new optimization idiom)</h1><div class="page-info"><p class="metadata-link"><a href="../metadata/otherizer.json.html">otherizer.json</a></p><p class="arbital-url"><a href="https://arbital.com/p/otherizer">https://arbital.com/p/otherizer</a></p><p class="creator">by
 <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a> Mar 22 2016</p></div><p class="clickbait">Maximization isn't possible for bounded agents, and satisficing doesn't seem like enough.  What other kind of 'izing' might be good for realistic, bounded agents?</p><nav class="breadcrumbs"><nav class="breadcrumb" aria-label="Breadcrumb"><ul><li><a href="../index.html">Index</a></li><li>Other-izing (wanted: new optimization idiom)</li></ul></nav><nav class="breadcrumb" aria-label="Breadcrumb"><ul><li><a href="ai_alignment.html">AI alignment</a></li><li><a href="Vingean_reflection.html">Vingean reflection</a></li><li><a href="reflective_stability.html">Reflective stability</a></li><li>â€¦</li></ul></nav></nav></header><hr><main><p>The open "other-izer" problem is to find something besides maximizing, satisificing, meliorizing, and several other existing but unsatisfactory idioms, which is actually suitable as an optimization idiom for <a href="bounded_agent.html">bounded agents</a> and is <a href="reflective_stability.html">reflectively stable</a>.</p>
<p>In standard theory we tend to assume that agents are expected utility <em>maximizers</em> that always choose the available option with highest expected utility.  But this isn't a realistic idiom because a realistic, <a href="bounded_agent.html">bounded agent</a> with limited computing power can't compute the expected utility of every possible action.</p>
<p>An expected utility satisficer, which e.g. might approve any policy so long as the expected utility is at least 0.95, would be much more realistic.  But it also doesn't seem suitable for an actual AGI, since, e.g., if policy X produces at least expected utility 0.98, then it would also satisfice to randomize between mostly policy X and a small chance of policy Y that had expected utility 0; this seems to give away a needlessly large amount of utility.  We'd probably be fairly disturbed if an otherwise aligned AGI was actually doing that.</p>
<p>Satisficing is also <a href="2rb">reflectively consistent</a> but not <a href="reflective_stability.html">reflectively stable</a> - while <a href="tiling_agents.html">tiling agents theory</a> can give formulations of satisficers that will approve the construction of similar satisficers, a satisficer could also tile to a maximizer.  If your decision criterion is to approve policies which achieve expected utility at least $~$\theta,$~$ and you expect that an expected utility <em>maximizing</em> version of yourself would achieve expected utility at least $~$\theta,$~$ then you'll approve self-modifying to be an expected utility maximizer.  This is another reason to prefer a formulation of optimization besides satisficing - if the AI is strongly self-modifying, then there's no guarantee that the 'satisficing' property would stick around and have our analysis go on being applicable, and even if not strongly self-modifying, it might still create non-satisficing chunks of cognitive mechanism inside itself or in the environment.</p>
<p>A meliorizer has a current policy and only replaces it with policies of increased expected utility.  Again, while it's possible to demonstrate that a meliorizer can approve self-modifying to another meliorizer and hence this idiom is reflectively consistent, it doesn't seem like it would be reflectively stable - becoming a maximizer or something else might have higher expected utility than staying a meliorizer.</p>
<p>The "other-izer" open problem is to find something better than maximization, satisficing, and meliorization that actually makes sense as an idiom of optimization for a resource-bounded agent and that we'd think would be an okay thing for e.g. a <a href="task_agi.html">Task AGI</a> to do, which is at least reflectively consistent, and preferably reflectively stable.</p>
<p>See also "<a href="soft_optimizer.html">Mild optimization</a>" for a further desideratum, namely an adjustable parameter of optimization strength, that would be nice to have in an other-izer.</p></main><hr><footer><p class="tagged"><h2>Tagged</h2><span class="page-comma-list"><a class="page-link" href="../page/value_alignment_open_problem.html">AI alignment open problem</a>,
 <a class="page-link" href="../page/stub_meta_tag.html">Stub</a></span></p><p class="reverse-related"><h2>Reverse Related</h2><ul class="page-list"><li><a class="page-link" href="../page/value_alignment_open_problem.html">AI alignment open problem</a> <q>Tag for open problems under AI alignment.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/stub_meta_tag.html">Stub</a> <q>This page only gives a very brief overview of the topic. If you're able to, please help expand or improve it!</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li></ul></p></footer></body></html>