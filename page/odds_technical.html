<!DOCTYPE html><html><head><meta charset="utf-8"><title>Odds: Technical explanation</title><link rel="stylesheet" type="text/css" href="../common.css"><link rel="stylesheet" type="text/css" href="../page-style.css"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  SVG: {EqnChunk: 50, EqnChunkFactor: 1.5, EqChunkDelay: 10, useFontCache: false, linebreaks: {automatic: true}},
  tex2jax: {
    inlineMath: [['$~$', '$~$']],
    displayMath: [['$$~$', '$~$$']],
    processEscapes: true,
    preview: 'none',
  },
  showProcessingMessages: false,
  messageStyle: 'none',
  // http://docs.mathjax.org/en/latest/config-files.html#the-tex-ams-svg-configuration-file
  jax: ["input/TeX","output/SVG", "output/PreviewHTML"],
  extensions: ["tex2jax.js","MathMenu.js","MathZoom.js", "fast-preview.js", "AssistiveMML.js", "a11y/accessibility-menu.js"],
  TeX: { extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"] }
});</script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_SVG" async></script><script type="text/javascript" src="../arbital-demo-bundle.js"></script><script type="text/javascript">window.addEventListener('DOMContentLoaded', e=>window.loadAllDemos())
</script></head><body><header><h1 class="title">Odds: Technical explanation</h1><div class="page-info"><p class="metadata-link"><a href="../metadata/odds_technical.json.html">odds_technical.json</a></p><p class="arbital-url"><a href="https://arbital.com/p/odds_technical">https://arbital.com/p/odds_technical</a></p><p class="creator">by
 <a class="page-link" href="../page/AlexeiAndreev.html">Alexei Andreev</a> Oct 11 2016 
updated
 Oct 13 2016</p></div><p class="clickbait">Formal definitions, alternate representations, and uses of odds and odds ratios (like a 1 : 2 chance of drawing a red ball vs. green ball from a barrel).</p><nav class="breadcrumbs"><nav class="breadcrumb" aria-label="Breadcrumb"><ul><li><a href="../index.html">Index</a></li><li>Odds: Technical explanation</li></ul></nav><nav class="breadcrumb" aria-label="Breadcrumb"><ul><li><a href="math.html">Mathematics</a></li><li><a href="probability_theory.html">Probability theory</a></li><li><a href="odds.html">Odds</a></li><li>…</li></ul></nav><nav class="breadcrumb" aria-label="Breadcrumb"><ul><li><a href="rationality.html">Rationality</a></li><li><a href="probability_theory.html">Probability theory</a></li><li><a href="odds.html">Odds</a></li><li>…</li></ul></nav></nav></header><hr><main><p>Odds express relative belief: we write "the odds for X versus Y are $~$17 : 2$~$" when we think that proposition X is 17/2 = 8.5 times as likely as proposition Y.%%note: The colon denotes that we are forming a set of odds. It does not denote division, as it might in French or German.%%</p>
<p>Odds don't say anything about how likely X or Y is in absolute terms. X might be "it will hail tomorrow" and Y might be "there will be a hurricane tomorrow." In that case, it might be the case that the odds for X versus Y are $~$17 : 2$~$, despite the fact that both X and Y are very unlikely. <a href="bayes_rule.html">Bayes&#39; rule</a> is an example of an important operation that makes use of relative belief.</p>
<p>Odds can be expressed between many different propositions at once. For example, let Z be the proposition "It will rain tomorrow," the odds for X vs Y vs Z might be $~$(17 : 2 : 100).$~$ When odds are expressed between only two propositions, they can be expressed using a single [-ratio]. For example, above, the odds ratio between X and Y is 17/2, the odds ratio between X and Z is 17/100, and the odds ratio between Y and Z is 2/100 = 1/50. This asserts that X is 8.5x more likely than Y, and that Z is 50x more likely than Y. When someone says "the odds ratio of sick to healthy is 2/3", they mean that the odds of sickness vs health are $~$2 : 3.$~$</p>
<p>[toc:]</p>
<h1 id="formaldefinition">Formal definition</h1>
<p>Given $~$n$~$ propositions $~$X_1, X_2, \ldots X_n,$~$ a set of odds between the propositions is a list $~$(x_1, x_2, \ldots, x_n)$~$ of non-negative <a href="real_number.html">real</a> numbers. Each $~$x_i$~$ in the set of odds is called a "term." Two sets of odds $~$(x_1, x_2, \ldots, x_n)$~$ and $~$(y_1, y_2, \ldots, y_n)$~$ are called "equivalent" if there is an $~$\alpha &gt; 0$~$ such that $~$ \alpha x_i = y_i$~$ for all $~$i$~$ from 1 to $~$n.$~$</p>
<p>When we write a set of odds using colons, like $~$(x_1 : x_2 : \ldots : x_n),$~$ it is understood that the '=' sign denotes this equivalence. Thus, $~$(3 : 6) = (9 : 18).$~$</p>
<p>A set of odds with only two terms can also be written as a fraction $~$\frac{x}{y},$~$ where it is understood that $~$\frac{x}{y}$~$ denotes the odds $~$(x : y).$~$ These fractions are often called "odds ratios."</p>
<h1 id="example">Example</h1>
<p>Suppose that in some forest, 40% of the trees are rotten and 60% of the trees are healthy.  There are then 2 rotten trees for every 3 healthy trees, so we say that the relative <em>odds</em> of rotten trees to healthy trees is 2 : 3. If we selected a tree at random from this forest, the <em>probability</em> of getting a rotten tree would be 2/5, but the <em>odds</em> would be 2 : 3 for rotten vs. healthy trees.</p>
<p><img src="https://i.imgur.com/GVZnz2c.png?0" alt="2 sick trees, 3 healthy trees" /></p>
<h1 id="conversionbetweenoddsandprobabilities">Conversion between odds and probabilities</h1>
<p>Consider three propositions, $~$X,$~$ $~$Y,$~$ and $~$Z,$~$ with odds of $~$(3 : 2 : 6).$~$ These odds assert  that $~$X$~$ is half as probable as $~$Z.$~$</p>
<p>When the set of propositions are <a href="exclusive_exhaustive.html">mutually exclusive and exhaustive</a>, we can convert a set of odds into a set of <a href="probability.html">probabilities</a> by <a href="normalize_probabilities.html">normalizing</a> the terms so that they sum to 1.  This can be done by summing all the components of the ratio, then dividing each component by the sum:</p>
<p>$$~$(x_1 : x_2 : \dots : x_n) = \left(\frac{x_1}{\sum_{i=1}^n x_i} : \frac{x_2}{\sum_{i=1}^n x_i} : \dots : \frac{x_n}{\sum_{i=1}^n x_i}\right)$~$$</p>
<p>For example, to obtain probabilities from the odds ratio 1/3, w write:</p>
<p>$$~$(1 : 3) = \left(\frac{1}{1+3}:\frac{3}{1+3}\right) = ( 0.25 : 0.75 )$~$$</p>
<p>which corresponds to the probabilities of 25% and 75%.</p>
<p>To go the other direction, recall that $~$\mathbb P(X) + \mathbb P(\neg X) = 1,$~$ where $~$\neg X$~$ is the negation of $~$X.$~$  So the odds for $~$X$~$ vs $~$\neg X$~$ are $~$\mathbb P(X) : \mathbb P(\neg X)$~$ $~$=$~$ $~$\mathbb P(X) : 1 - \mathbb P(X).$~$ If Alexander Hamilton has a 20% probability of winning the election, his odds for winning vs losing are $~$(0.2 : 1 - 0.2)$~$ $~$=$~$ $~$(0.2 : 0.8)$~$ $~$=$~$ $~$(1 : 4).$~$</p>
<h1 id="bayesrule">Bayes' rule</h1>
<p>Odds are exceptionally convenient when reasoning using <a href="bayes_rule.html">Bayes&#39; rule</a>, since the <a href="prior_probability.html">prior</a> odds can be term-by-term multiplied by a set of [relative_likelihoods relative likelihoods] to yield the <a href="posterior_probability.html">posterior</a> odds.  (The posterior odds in turn can be normalized to yield posterior probabilities, but if performing repeated updates, it's <a href="bayes_rule_multiple.html">more convenient</a> to multiply by all the likelihood ratios under consideration before normalizing at the end.)</p>
<p>$$~$\dfrac{\mathbb{P}(H_i\mid e_0)}{\mathbb{P}(H_j\mid e_0)} = \dfrac{\mathbb{P}(e_0\mid H_i)}{\mathbb{P}(e_0\mid H_j)} \cdot \dfrac{\mathbb{P}(H_i)}{\mathbb{P}(H_j)}$~$$</p>
<p>As a more striking illustration, suppose we receive emails on three subjects:  Business (60%), personal (30%), and spam (10%).  Suppose that business, personal, and spam emails are 60%, 10%, and 90% likely respectively to contain the word "money"; and that they are respectively 20%, 80%, and 10% likely to contain the word "probability".  Assume for the sake of discussion that a business email containing the word "money" [naive_bayes is thereby no more or less likely] to contain the word "probability", and similarly with personal and spam emails.  Then if we see an email containing both the words "money" and "probability":</p>
<p>$$~$(6 : 3 : 1) \times (6 : 1 : 9) \times (2 : 8 : 1) = (72 : 24 : 9) = (24 : 8: 3)$~$$</p>
<p>…so the posterior odds are 24 : 8 : 3 favoring the email being a business email, or roughly 69% probability after <a href="normalize_probabilities.html">normalizing</a>.</p>
<h1 id="logodds">Log odds</h1>
<p>The odds $~$\mathbb{P}(X) : \mathbb{P}(\neg X)$~$ can be viewed as a dimensionless scalar quantity $~$\frac{\mathbb{P}(X)}{\mathbb{P}(\neg X)}$~$ in the range $~$[0, +\infty]$~$.  If the odds of Alexander Hamilton becoming President are 0.75 to 0.25 in favor, we can also say that Andrew Jackson is 3 times as likely to become President as not.  Or if the odds were 0.4 to 0.6, we could say that Alexander Hamilton was 2/3rds as likely to become President as not.</p>
<p>The <strong>log odds</strong> are the logarithm of this dimensionless positive quantity, $~$\log\left(\frac{\mathbb{P}(X)}{\mathbb{P}(\neg X)}\right),$~$ e.g., $~$\log_2(1:4) = \log_2(0.25) = -2.$~$  Log odds fall in the [range_notation range] $~$[-\infty, +\infty]$~$ and are finite for probabilities inside the range $~$(0, 1).$~$</p>
<p>When using a log odds form of <a href="bayes_rule.html">Bayes&#39; rule</a>, the posterior log odds are equal to the prior log odds plus the log likelihood.  This means that the change in log odds can be identified with [ the strength of the evidence].  If the probability goes from 1/3 to 4/5, our odds have gone from 1:2 to 4:1 and the log odds have shifted from -1 bits to +2 bits.  So we must have seen evidence with a strength of +3 bits (a likelihood ratio of 8:1).</p>
<p>The convenience of this representation is what Han Solo refers to in <em>Star Wars</em> when he shouts:  "Never tell me the odds!", implying that he would much prefer to be told the logarithm of the odds ratio.</p>
<h2 id="directrepresentationofinfinitecertainty">Direct representation of infinite certainty</h2>
<p>In the log odds representation, the probabilities $~$0$~$ and $~$1$~$ are represented as $~$-\infty$~$ and $~$+\infty$~$ respectively.</p>
<p>This exposes the specialness of the classical probabilities $~$0$~$ and $~$1,$~$ and the ways in which these "infinite certainties" sometimes behave qualitatively differently from all finite credences.  If we don't start by being absolutely certain of a proposition, it will require infinitely strong evidence to shift our belief all the way out to infinity.  If we do start out absolutely certain of a proposition, no amount of ordinary evidence no matter how great can ever shift us away from infinity.</p>
<p>This reasoning is part of the justification of <a href="https://en.wikipedia.org/wiki/Cromwell%27s_rule">Cromwell's rule</a> which states that probabilities of exactly $~$0$~$ or $~$1$~$ should be avoided except for logical truths and falsities (and maybe <a href="http://lesswrong.com/lw/mo/infinite_certainty/">not even then</a>).  It also demonstrates how log odds are a good fit for measuring <em>strength of belief and evidence,</em> even if classical probabilities are a better representation of <em>degrees of caring</em> and betting odds.</p>
<pre>%%%comment: We are checking to see if users will click this button, even though we don&#39;t have the content for it yet.%%%</pre>
<p>%%hidden(Check my understanding):
Coming soon!
%%</p></main><hr><footer><p class="tagged"><h2>Tagged</h2><span class="page-comma-list"><a class="page-link" href="../page/c_class_meta_tag.html">C-Class</a></span></p><p class="all-creators"><h2>All Creators</h2><span class="page-comma-list"><a class="page-link" href="../page/AlexeiAndreev.html">Alexei Andreev</a>,
 <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></span></p><p class="likes"><h2>Likes</h2><span class="page-comma-list"><a class="page-link" href="../page/mariamyst.html">maria myst</a></span></p><p class="reverse-related"><h2>Reverse Related</h2><ul class="page-list"><li><a class="page-link" href="../page/c_class_meta_tag.html">C-Class</a> <q>This page has substantial content, but may not thoroughly cover the topic, may not meet style and prose standards, or may not explain the concept in a way the target audience will reliably understand.</q> - <a class="page-link" href="../page/EricBruylant.html">Eric Bruylant</a></li></ul></p></footer></body></html>