<!DOCTYPE html><html><head><meta charset="utf-8"><title>Consequentialist preferences are reflectively stable by default</title><link rel="stylesheet" type="text/css" href="../common.css"><link rel="stylesheet" type="text/css" href="../page-style.css"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  SVG: {EqnChunk: 50, EqnChunkFactor: 1.5, EqChunkDelay: 10, useFontCache: false, linebreaks: {automatic: true}},
  tex2jax: {
    inlineMath: [['$~$', '$~$']],
    displayMath: [['$$~$', '$~$$']],
    processEscapes: true,
    preview: 'none',
  },
  showProcessingMessages: false,
  messageStyle: 'none',
  // http://docs.mathjax.org/en/latest/config-files.html#the-tex-ams-svg-configuration-file
  jax: ["input/TeX","output/SVG", "output/PreviewHTML"],
  extensions: ["tex2jax.js","MathMenu.js","MathZoom.js", "fast-preview.js", "AssistiveMML.js", "a11y/accessibility-menu.js"],
  TeX: { extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"] }
});</script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_SVG" async></script><script type="text/javascript" src="../arbital-demo-bundle.js"></script><script type="text/javascript">window.addEventListener('DOMContentLoaded', e=>window.loadAllDemos())
</script></head><body><header><h1 class="title">Consequentialist preferences are reflectively stable by default</h1><div class="page-info"><p class="metadata-link"><a href="../metadata/preference_stability.json.html">preference_stability.json</a></p><p class="arbital-url"><a href="https://arbital.com/p/preference_stability">https://arbital.com/p/preference_stability</a></p><p class="creator">by
 <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a> May 21 2016 
updated
 May 22 2016</p></div><p class="clickbait">Gandhi wouldn't take a pill that made him want to kill people, because he knows in that case more people will be murdered.  A paperclip maximizer doesn't want to stop maximizing paperclips.</p><nav class="breadcrumbs"><nav class="breadcrumb" aria-label="Breadcrumb"><ul><li><a href="../index.html">Index</a></li><li>Consequentialist preferences are reflectively stable by default</li></ul></nav><nav class="breadcrumb" aria-label="Breadcrumb"><ul><li><a href="ai_alignment.html">AI alignment</a></li><li><a href="Vingean_reflection.html">Vingean reflection</a></li><li><a href="reflective_stability.html">Reflective stability</a></li><li>…</li></ul></nav><nav class="breadcrumb" aria-label="Breadcrumb"><ul><li><a href="ai_alignment.html">AI alignment</a></li><li><a href="advanced_agent_theory.html">Theory of (advanced) agents</a></li><li><a href="instrumental_convergence.html">Instrumental convergence</a></li><li><a href="convergent_strategies.html">Convergent instrumental strategies</a></li><li>…</li></ul></nav></nav></header><hr><main><p>Suppose that Gandhi doesn't want people to be murdered.  Imagine that you offer Gandhi a pill that will make him start <em>wanting</em> to kill people.  If Gandhi <em>knows</em> that this is what the pill does, Gandhi will refuse the pill, because Gandhi expects the result of taking the pill to be that future-Gandhi wants to murder people and then murders people and then more people will be murdered and Gandhi regards this as bad.  By a similar logic, a <a href="advanced_agent.html">sufficiently intelligent</a> <a href="paperclip_maximizer.html">paperclip maximizer</a> - an agent which always outputs the action it expects to lead to the greatest number of paperclips - will by default not perform any self-modification action that makes it not want to produce paperclips, because then future-Clippy will produce fewer paperclips, and then there will be fewer paperclips, so present-Clippy does not evaluate this self-modification as the action that produces the highest number of expected future paperclips.</p>
<p>Another way of stating this is that protecting the representation of the utility function, and creating only other agents with similar utility functions, are both <a href="instrumental_convergence.html">convergent instrumental strategies</a>, for consequentialist agents which <a href="big_picture_awareness.html">understand the big-picture relation</a> between their code and the real-world consequences.</p>
<p>Although the instrumental <em>incentive</em> to prefer stable preferences seems like it should follow from consequentialism plus big-picture understanding, less advanced consequentialists might not be <em>able</em> to self-modify in a way that preserves understanding - they might not understand which self-modifications or constructed successors lead to which kind of outcomes.  We could see this as a case of "The agent has no preference-preserving self-improvements in its subjective policy space, but would want an option like that if available."</p>
<p>That is:</p>
<ul>
<li>Wanting preference stability follows from <a href="consequentialist.html">Consequentialism</a> plus <a href="big_picture_awareness.html">Big-Picture Understanding</a>.</li>
<li>Actual preference stability furthermore requires some prerequisite level of skill at self-modification, which might perhaps be high, or too much caution to self-modify absent the policy option of preserving preferences.</li>
</ul></main><hr><footer><p class="tagged"><h2>Tagged</h2><span class="page-comma-list"><a class="page-link" href="../page/start_meta_tag.html">Start</a></span></p><p class="likes"><h2>Likes</h2><span class="page-comma-list"><a class="page-link" href="../page/EricBruylant.html">Eric Bruylant</a>,
 <a class="page-link" href="../page/MiddleKek.html">Middle Kek</a></span></p><p class="reverse-related"><h2>Reverse Related</h2><ul class="page-list"><li><a class="page-link" href="../page/start_meta_tag.html">Start</a> <q>This page gives a basic overview of the topic, but may be missing important information or have stylistic issues. If you're able to, please help expand or improve it!</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li></ul></p></footer></body></html>