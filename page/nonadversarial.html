<!DOCTYPE html><html><head><meta charset="utf-8"><title>Non-adversarial principle</title><link rel="stylesheet" type="text/css" href="../common.css"><link rel="stylesheet" type="text/css" href="../page-style.css"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  SVG: {EqnChunk: 50, EqnChunkFactor: 1.5, EqChunkDelay: 10, useFontCache: false, linebreaks: {automatic: true}},
  tex2jax: {
    inlineMath: [['$~$', '$~$']],
    displayMath: [['$$~$', '$~$$']],
    processEscapes: true,
    preview: 'none',
  },
  showProcessingMessages: false,
  messageStyle: 'none',
  // http://docs.mathjax.org/en/latest/config-files.html#the-tex-ams-svg-configuration-file
  jax: ["input/TeX","output/SVG", "output/PreviewHTML"],
  extensions: ["tex2jax.js","MathMenu.js","MathZoom.js", "fast-preview.js", "AssistiveMML.js", "a11y/accessibility-menu.js"],
  TeX: { extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"] }
});</script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_SVG" async></script><script type="text/javascript" src="../arbital-demo-bundle.js"></script><script type="text/javascript">window.addEventListener('DOMContentLoaded', e=>window.loadAllDemos())
</script></head><body><header><h1 class="title">Non-adversarial principle</h1><div class="page-info"><p class="metadata-link"><a href="../metadata/nonadversarial.json.html">nonadversarial.json</a></p><p class="arbital-url"><a href="https://arbital.com/p/nonadversarial">https://arbital.com/p/nonadversarial</a></p><p class="creator">by
 <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a> Jan 16 2017 
updated
 Jan 22 2017</p></div><p class="clickbait">At no point in constructing an Artificial General Intelligence should we construct a computation that tries to hurt us, and then try to stop it from hurting us.</p><nav class="breadcrumbs"><nav class="breadcrumb" aria-label="Breadcrumb"><ul><li><a href="../index.html">Index</a></li><li>Non-adversarial principle</li></ul></nav><nav class="breadcrumb" aria-label="Breadcrumb"><ul><li><a href="ai_alignment.html">AI alignment</a></li><li><a href="alignment_principle.html">Principles in AI alignment</a></li><li>â€¦</li></ul></nav></nav></header><hr><main><p>[summary:  The 'non-adversarial principle' states:  <em>By design, the human operators and the AGI should never come into conflict.</em></p>
<p>Since every event inside an AI is ultimately the causal result of choices by the human programmers, we should not choose so as to run computations that are searching for a way to hurt us.  At the point the AI is even <em>trying</em> to outwit us, we've already screwed up the design; we've made a foolish use of computing power.</p>
<p>E.g., according to this principle, if the AI's server center has <a href="shutdown_problem.html">a switch that shuts off the electricity</a>, our first thought should not be, "How do we have guards with guns defending this off-switch so the AI can't destroy it?"  Our first thought should be, "How do we make sure the AI <em>wants</em> this off-switch to exist?"]</p>
<p>The 'Non-Adversarial Principle' is a proposed design rule for <a href="sufficiently_advanced_ai.html">sufficiently advanced Artificial Intelligence</a> stating that:</p>
<p><em>By design, the human operators and the AGI should never come into conflict.</em></p>
<p>Special cases of this principle include <a href="niceness_defense.html">Niceness is the first line of defense</a> and [ai_wants_security The AI wants your safety measures].</p>
<p>According to this principle, if the AI has an off-switch, our first thought should not be, "How do we have guards with guns defending this off-switch so the AI can't destroy it?" but "How do we make sure the AI <em>wants</em> this off-switch to exist?"</p>
<p>If we think the AI is not ready to act on the Internet, our first thought should not be "How do we [airgapping airgap] the AI's computers from the Internet?" but "How do we construct an AI that wouldn't <em>try</em> to do anything on the Internet even if it got access?"  Afterwards we may go ahead and still not connect the AI to the Internet, but only as a fallback measure.  Like the containment shell of a nuclear power plant, the <em>plan</em> shouldn't call for the fallback measure to ever become necessary.  E.g., nuclear power plants have containment shells in case the core melts down.  But this is not because we're planning to have the core melt down on Tuesday and have that be okay because there's a containment shell.</p>
<h1 id="whyruncodethatdoesthewrongthing">Why run code that does the wrong thing?</h1>
<p>Ultimately, every event inside an AI--every RAM access and CPU instruction--is an event set in motion by our own design.  Even if the AI is modifying its own code, the modified code is a causal outcome of the original code (or the code that code wrote etcetera).  Everything that happens inside the computer is, in some sense, our fault and our choice.  Given that responsibility, we should not be constructing a computation that is <em>trying</em> to hurt us.  At the point that computation is running, we've already done something foolish--willfully shot ourselves in the foot.  Even if the AI doesn't find any way to do the bad thing, we are, at the very least, wasting computing power.</p>
<p>No aspect of the AI's design should ever put us in an adversarial position vis-a-vis the AI, or pit the AI's wits against our wits.  If a computation starts <em>looking</em> for a way to outwit us, then the design and methodology has <em>already</em> failed.  We just shouldn't be putting an AI in a box and then having the AI search for ways to get out of the box.  If you're building a toaster, you don't build one element that heats the toast and then add a tiny refrigerator that cools down the toast.</p>
<h1 id="notrunningsearchesforharmfulstrategies">Not running searches for harmful strategies</h1>
<p>Under the [agents_as_searches] viewpoint, we can see an AI as embodying computations that search out strategies within a domain.  E.g., an AI carrying out the "[one_damn_tomato put one damn strawberry on a damn plate]" task is searching the space of imaginable strategies for a strategy such that the consequence of executing that strategy is one strawberry being on the plate.</p>
<p>The non-adversarial principle then says that we should at no point be running a computation which is <em>searching</em> for ways to hurt us.</p>
<p>Suppose you don't want the AI to go on the Internet.  If your first thought is [airgapping airgapping] the AI's hardware from the Internet or otherwise <a href="AI_boxing.html">boxing the AI</a>, then what you're trying to do is ensure that <em>a computation searching for strategies to reach the Internet returns no result.</em></p>
<p>If the primary plan is "the AI searches, but doesn't find anything", then we need to worry that the AI's search <a href="uncontainability.html">considers some class of strategies we didn&#39;t imagine</a>.  We are pitting our own wits against a potentially <a href="uncontainability.html">weird</a> or <a href="superintelligent.html">superhuman</a> search.  Even if we did successfully rule out all possibilities for escape, it would be hard for us to ever be confident that we had done so.  It would be very hard to rule out the <em>conceptual</em> possibility of some entirely unexpected side-channel attack like the AI <a href="https://www.usenix.org/system/files/conference/usenixsecurity15/sec15-paper-guri-update.pdf">producing GSM cellphone signals by cleverly modulating its memory accesses</a>, or the AI [aibox_experiment persuading its human operators to let it out of the box].  (Except that neither of those two particular possibilities would be <em>unexpected;</em> any specific strategy we write down will be the kind of possibility that humans consider.)</p>
<p>Even if the AI's search does come up empty, you were tempting fate and wasting computing power by instantiating that search in the first place.</p>
<h1 id="oppositionalmeasuresarefallbacks">Oppositional measures are fallbacks</h1>
<p><em>After</em> we ensure that the escape-search computation is not running in the first place, we can think about fallback <a href="direct_limit_oppose.html">oppositional</a> lines of defense; e.g. putting the AI's processors inside a <a href="https://en.wikipedia.org/wiki/Faraday_cage">Faraday cage</a>.</p>
<p>But the <em>plan</em> is not that the AI tries to escape and we successfully prevent the escape.  If we imagine the AI trying to escape, we ought to [rule_of_surprise feel very surprised in our imagination].  If the AI actually does try to escape, we should be crying 'What?' and tracking down which strong-seeming assumption has failed.</p>
<p>No matter what clever design we use or how strong it seems, we'd probably <em>still</em> be wise to [airgapping airgap] an AI under development, not say "Oh it's supposed to be safe!" and connect it to the Internet and saunter on.  But that doesn't mean we <em>rely</em> on the airgap.  It doesn't mean the plan calls for the airgap to be necessary. </p>
<p>Nuclear power plants have concrete containment units in case the core melts down; but the <em>design</em> is not that they melt down.  The master plan doesn't say "And then on Tuesday the core melts down, but that's fine because of the containment unit."  By design, that enormous concrete shell isn't supposed to actually ever become necessary.  And then we build it anyway, because the best-laid plans etcetera.</p>
<p>Similarly, when designing an AI, we should pretend that the airgap doesn't exist or that the AI will <a href="omni_test.html">suddenly get Internet access anyway</a> on Tuesday; our <em>primary</em> thought should be to design AI that doesn't need an airgap to be safe.  And <em>then</em> we add the airgap, making sure that we're not thinking the equivalent of "Oh, it doesn't <em>really</em> matter if the core melts down, because we've got a containment structure there anyway."</p>
<h1 id="challengesinimplementingnonadversarialism">Challenges in implementing non-adversarialism</h1>
<p>The main difficulties foreseen so far for implementing the non-adversarial principle, tend to center around <a href="instrumental_convergence.html">Instrumental convergence</a> plus <a href="nearest_unblocked.html">Nearest unblocked strategy</a> behavior.</p>
<p>For example, if you build a <a href="shutdown_problem.html">shutdown button</a> for a <a href="task_agi.html">Task AGI</a> that suspends the AI to disk when pressed, the nonadversarial principle implies you must also ensure:</p>
<ul>
<li>That the AI <em>wants</em> there to be a shutdown button;</li>
<li>That the AI <em>wants</em> to be suspended to disk after this button is pressed;</li>
<li>That the AI <em>wants</em> the state of this shutdown button to reflect the dynamic decisions of the human operators;</li>
<li>That the AI does not <em>want</em> to influence the operators to decide to not press the switch, or to press it;</li>
<li>That the AI does <em>not</em> want anything <em>besides</em> an orderly suspend-to-disk to happen, or not happen, after this button is pressed.</li>
</ul>
<p><em>Or:</em></p>
<ul>
<li>The AI does not think about or make plans involving the shutdown button, e.g. because that domain was not [whitelisting whitelisted] for cognition.</li>
<li>None of the AI's other models end up reflecting the existence of the shutdown button or none of its other plans end up taking into account that part of the model.</li>
</ul>
<p>The difficulties here center around <a href="no_coffee_if_dead.html">&quot;You can&#39;t fetch the coffee if you&#39;re dead&quot;</a>.  This reasoning is very general, so <a href="patch_resistant.html">even if we try to make it not apply at one point, it tends to pop up somewhere else</a>:</p>
<ul>
<li>If you naively try to add in a special-case clause to the utility function for wanting a shutdown button to exist, the AI wants the shutdown button to not be pressed.</li>
<li>If you successfully add a special case saying that the AI wants the button to be pressed if the humans want that button to be pressed, the AI wants the humans to not want to press the button.</li>
<li>If you naively try to add in a special clause for the AI wanting to shut down after the button is pressed, the AI wants to create a [environmental_subagent subagent] to make sure the coffee gets pressed anyway.</li>
<li>If you try to make an AI that <a href="epistemic_exclusion.html">doesn&#39;t think about the shutdown button</a> or model it at all, this seems potentially difficult because in reality the best hypothesis to explain the world <em>does</em> contain a shutdown button.  A general search for good hypotheses may tend to create cognitive tokens that represent the shutdown button, and it's not clear (yet) how this could in general be prevented by trying to divide the world into domains.</li>
</ul>
<p>More generally: by default a lot of <em>high-level</em> searches we do want to run, <a href="instrumental_convergence.html">have <em>subsearches</em> we&#39;d prefer <em>not</em> to run</a>.  If we run an agent that searches <em>in general</em> for ways to fetch the coffee, that search would, by default and if smart enough, also search for ways to prevent itself from being shut down.</p>
<p>How exactly to implement the non-adversarial principle is thus a major open problem.  We may need to be more clever about shaping which computations give rise to which other computations than the default "Search for any action in any domain which achieves X."</p>
<h1 id="seealso">See also</h1>
<ul>
<li><a href="niceness_defense.html">Niceness is the first line of defense</a></li>
<li><a href="omni_test.html">The omnipotence/omniscience test</a></li>
<li><a href="nonadversarial_safety.html">The AI should not want to defeat your safety measures</a></li>
<li><a href="direct_limit_oppose.html">Directing, vs. limiting, vs. opposing</a></li>
</ul></main><hr><footer><p class="tagged"><h2>Tagged</h2><span class="page-comma-list"><a class="page-link" href="../page/value_alignment_open_problem.html">AI alignment open problem</a>,
 <a class="page-link" href="../page/taskagi_open_problems.html">Open subproblems in aligning a Task-based AGI</a></span></p><p class="all-creators"><h2>All Creators</h2><span class="page-comma-list"><a class="page-link" href="../page/AnanyaAloke.html">Ananya Aloke</a>,
 <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></span></p><p class="likes"><h2>Likes</h2><span class="page-comma-list"><a class="page-link" href="../page/AndrewMcKnight.html">Andrew McKnight</a></span></p><p class="related"><h2>Related</h2><ul class="page-list"><li><a class="page-link" href="../page/corrigibility.html">Corrigibility</a> <q>&quot;I can't let you do that, Dave.&quot;</q> - <a class="page-link" href="../page/NateSoares.html">Nate Soares</a></li></ul></p><p class="reverse-related"><h2>Reverse Related</h2><ul class="page-list"><li><a class="page-link" href="../page/value_alignment_open_problem.html">AI alignment open problem</a> <q>Tag for open problems under AI alignment.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/taskagi_open_problems.html">Open subproblems in aligning a Task-based AGI</a> <q>Open research problems, especially ones we can model today, in building an AGI that can &quot;paint all cars pink&quot; without turning its future light cone into pink-painted cars.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li></ul></p><p class="children"><h2>Children</h2><ul class="page-tree"><li><a class="page-link" href="../page/direct_limit_oppose.html">Directing, vs. limiting, vs. opposing</a> <q>Getting the AI to compute the right action in a domain; versus getting the AI to not compute at all in an unsafe domain; versus trying to prevent the AI from acting successfully.  (Prefer 1 &amp; 2.)</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/cognitive_alignment.html">Generalized principle of cognitive alignment</a> <q>When we're asking how we want the AI to think about an alignment problem, one source of inspiration is trying to have the AI mirror our own thoughts about that problem.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/niceness_defense.html">Niceness is the first line of defense</a> <q>The *first* line of defense in dealing with any partially superhuman AI system advanced enough to possibly be dangerous is that it does not *want* to hurt you or defeat your safety measures.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/omni_test.html">Omnipotence test for AI safety</a> <q>Would your AI produce disastrous outcomes if it suddenly gained omnipotence and omniscience? If so, why did you program something that *wants* to hurt you and is held back only by lacking the power?</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/nonadversarial_safety.html">The AI must tolerate your safety measures</a> <q>A corollary of the nonadversarial principle is that &quot;The AI must tolerate your safety measures.&quot;</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li></ul></p></footer></body></html>