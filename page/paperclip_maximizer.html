<!DOCTYPE html><html><head><meta charset="utf-8"><title>Paperclip maximizer</title><link rel="stylesheet" type="text/css" href="../common.css"><link rel="stylesheet" type="text/css" href="../page-style.css"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  SVG: {EqnChunk: 50, EqnChunkFactor: 1.5, EqChunkDelay: 10, useFontCache: false, linebreaks: {automatic: true}},
  tex2jax: {
    inlineMath: [['$~$', '$~$']],
    displayMath: [['$$~$', '$~$$']],
    processEscapes: true,
    preview: 'none',
  },
  showProcessingMessages: false,
  messageStyle: 'none',
  // http://docs.mathjax.org/en/latest/config-files.html#the-tex-ams-svg-configuration-file
  jax: ["input/TeX","output/SVG", "output/PreviewHTML"],
  extensions: ["tex2jax.js","MathMenu.js","MathZoom.js", "fast-preview.js", "AssistiveMML.js", "a11y/accessibility-menu.js"],
  TeX: { extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"] }
});</script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_SVG" async></script><script type="text/javascript" src="../arbital-demo-bundle.js"></script><script type="text/javascript">window.addEventListener('DOMContentLoaded', e=>window.loadAllDemos())
</script></head><body><header><h1 class="title">Paperclip maximizer</h1><div class="page-info"><p class="metadata-link"><a href="../metadata/paperclip_maximizer.json.html">paperclip_maximizer.json</a></p><p class="arbital-url"><a href="https://arbital.com/p/paperclip_maximizer">https://arbital.com/p/paperclip_maximizer</a></p><p class="creator">by
 <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a> Jul 16 2015 
updated
 Mar 3 2017</p></div><p class="clickbait">This agent will not stop until the entire universe is filled with paperclips.</p><nav class="breadcrumbs"><nav class="breadcrumb" aria-label="Breadcrumb"><ul><li><a href="../index.html">Index</a></li><li>Paperclip maximizer</li></ul></nav><nav class="breadcrumb" aria-label="Breadcrumb"><ul><li><a href="ai_alignment.html">AI alignment</a></li><li><a href="advanced_agent_theory.html">Theory of (advanced) agents</a></li><li><a href="instrumental_convergence.html">Instrumental convergence</a></li><li>…</li></ul></nav><nav class="breadcrumb" aria-label="Breadcrumb"><ul><li><a href="ai_alignment.html">AI alignment</a></li><li><a href="advanced_agent_theory.html">Theory of (advanced) agents</a></li><li><a href="orthogonality.html">Orthogonality Thesis</a></li><li>…</li></ul></nav></nav></header><hr><main><p>An expected paperclip maximizer is an agent that outputs the action it believes will lead to the greatest number of <a href="paperclip.html">paperclips</a> existing.  Or in more detail, its <a href="value_alignment_utility.html">utility function</a> is linear in the number of paperclips times the number of seconds that each paperclip lasts, over the lifetime of the universe.  See <a href="http://wiki.lesswrong.com/wiki/Paperclip_maximizer.">http://wiki.lesswrong.com/wiki/Paperclip_maximizer.</a></p>
<p>The agent may be a [ bounded maximizer] rather than an [ objective maximizer] without changing the key ideas; the core premise is just that, given actions A and B where the paperclip maximizer has evaluated the consequences of both actions, the paperclip maximizer always prefers the action that it expects to lead to more paperclips.</p>
<p>Some key ideas that the notion of an expected paperclip maximizer illustrates:</p>
<ul>
<li>A self-modifying paperclip maximizer <a href="preference_stability.html">does not change its own utility function</a> to something other than 'paperclips', since this would be expected to lead to fewer paperclips existing.</li>
<li>A paperclip maximizer instrumentally prefers the standard <a href="convergent_strategies.html">convergent instrumental strategies</a> - it will seek access to matter, energy, and negentropy in order to make paperclips; try to build efficient technology for <a href="cosmic_endowment.html">colonizing the galaxies</a> to transform into paperclips; do whatever science is necessary to gain the knowledge to build such technology optimally; etcetera.</li>
<li>"The AI does not hate you, nor does it love you, and you are made of atoms it can use for something else."</li>
</ul></main><hr><section class="comments"><h2>Comments</h2><div class="comment"><p><a class="page-link" href="../page/PatrickLaVictoir.html">Patrick LaVictoire</a></p><p><p>Maybe a useful thing to add: when we say things like "if X goes wrong, I expect your AI to become a paperclip maximizer", we don't necessarily mean that the AI will have a terminal goal as human-comprehensible and <a href="orthogonality.html">human-stupid</a> as "maximizing paperclips", we mean that it will actually seek to maximize a goal that isn't very near the exact direction of human preference, and thanks to <a href="instrumental_convergence.html">instrumental goals</a> and <a href="edge_instantiation.html">edge instantiation</a>, this results in a world that is just as worthless to human values as if we had let loose a paperclip maximizer.</p></p></div></section><footer><p class="tagged"><h2>Tagged</h2><span class="page-comma-list"><a class="page-link" href="../page/work_in_progress_meta_tag.html">Work in progress</a></span></p><p class="all-creators"><h2>All Creators</h2><span class="page-comma-list"><a class="page-link" href="../page/AlexeiAndreev.html">Alexei Andreev</a>,
 <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></span></p><p class="likes"><h2>Likes</h2><span class="page-comma-list"><a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a>,
 <a class="page-link" href="../page/EricBruylant.html">Eric Bruylant</a>,
 <a class="page-link" href="../page/NebuPookins.html">Nebu Pookins</a></span></p><p class="related"><h2>Related</h2><ul class="page-list"><li><a class="page-link" href="../page/not_more_paperclips.html">You can't get more paperclips that way</a> <q>Most arguments that &quot;A paperclip maximizer could get more paperclips by (doing nice things)&quot; are flawed.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li></ul></p><p class="reverse-related"><h2>Reverse Related</h2><ul class="page-list"><li><a class="page-link" href="../page/work_in_progress_meta_tag.html">Work in progress</a> <q>This page is being actively worked on by an editor. Check with them before making major changes.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li></ul></p><p class="children"><h2>Children</h2><ul class="page-tree"><li><a class="page-link" href="../page/paperclip.html">Paperclip</a> <q>A configuration of matter that we'd see as being worthless even from a very cosmopolitan perspective.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/random_utility_function.html">Random utility function</a> <q>A 'random' utility function is one chosen at random according to some simple probability measure (e.g. weight by Kolmorogov complexity) on a logical space of formal utility functions.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li></ul></p></footer></body></html>