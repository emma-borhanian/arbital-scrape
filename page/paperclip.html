<!DOCTYPE html><html><head><meta charset="utf-8"><title>Paperclip</title><link rel="stylesheet" type="text/css" href="../common.css"><link rel="stylesheet" type="text/css" href="../page-style.css"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  SVG: {EqnChunk: 50, EqnChunkFactor: 1.5, EqChunkDelay: 10, useFontCache: false, linebreaks: {automatic: true}},
  tex2jax: {
    inlineMath: [['$~$', '$~$']],
    displayMath: [['$$~$', '$~$$']],
    processEscapes: true,
    preview: 'none',
  },
  showProcessingMessages: false,
  messageStyle: 'none',
  // http://docs.mathjax.org/en/latest/config-files.html#the-tex-ams-svg-configuration-file
  jax: ["input/TeX","output/SVG", "output/PreviewHTML"],
  extensions: ["tex2jax.js","MathMenu.js","MathZoom.js", "fast-preview.js", "AssistiveMML.js", "a11y/accessibility-menu.js"],
  TeX: { extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"] }
});</script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_SVG" async></script><script type="text/javascript" src="../arbital-demo-bundle.js"></script><script type="text/javascript">window.addEventListener('DOMContentLoaded', e=>window.loadAllDemos())
</script></head><body><header><h1 class="title">Paperclip</h1><div class="page-info"><p class="metadata-link"><a href="../metadata/paperclip.json.html">paperclip.json</a></p><p class="arbital-url"><a href="https://arbital.com/p/paperclip">https://arbital.com/p/paperclip</a></p><p class="creator">by
 <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a> Jan 10 2017 
updated
 Jan 11 2017</p></div><p class="clickbait">A configuration of matter that we'd see as being worthless even from a very cosmopolitan perspective.</p><nav class="breadcrumbs"><nav class="breadcrumb" aria-label="Breadcrumb"><ul><li><a href="../index.html">Index</a></li><li>Paperclip</li></ul></nav><nav class="breadcrumb" aria-label="Breadcrumb"><ul><li><a href="ai_alignment.html">AI alignment</a></li><li><a href="advanced_agent_theory.html">Theory of (advanced) agents</a></li><li><a href="instrumental_convergence.html">Instrumental convergence</a></li><li><a href="paperclip_maximizer.html">Paperclip maximizer</a></li><li>…</li></ul></nav><nav class="breadcrumb" aria-label="Breadcrumb"><ul><li><a href="ai_alignment.html">AI alignment</a></li><li><a href="advanced_agent_theory.html">Theory of (advanced) agents</a></li><li><a href="orthogonality.html">Orthogonality Thesis</a></li><li><a href="paperclip_maximizer.html">Paperclip maximizer</a></li><li>…</li></ul></nav></nav></header><hr><main><p>[summary:  A 'paperclip', in the context of <a href="ai_alignment.html">AI alignment</a>, is any configuration of matter which would seem very boring and pointless even from a very <a href="value_cosmopolitan.html">cosmopolitan</a> perspective.  If some bizarre physics catastrophe, spreading out at the speed of light, permanently transformed all matter it touched into paperclips, this would be morally equivalent to a physics catastrophe that destroys the reachable universe outright.  There is no deep missing moral insight we could have, no broadening of perspective and understanding, that would make us realize that little bent pieces of metal are a wonderful use of a cosmic endowment that could otherwise be transformed into intelligent life.  This is why we consider actual non-metaphorical paperclips as the case in point of 'paperclips'.]</p>
<p>A 'paperclip', in the context of <a href="ai_alignment.html">AI alignment</a>, is any configuration of matter which would seem boring and <a href="value_alignment_value.html">valueless</a> even from a very <a href="value_cosmopolitan.html">cosmopolitan</a> perspective.</p>
<p>If some bizarre physics catastrophe, spreading out at the speed of light, permanently transformed all matter it touched into paperclips, this would be morally equivalent to a physics catastrophe that destroys the reachable universe outright.  There is no deep missing moral insight we could have, no broadening of perspective and understanding, that would make us realize that little bent pieces of metal without any thought or internal experiences are the best possible use of our <a href="cosmic_endowment.html">cosmic endowment</a>.  It's true that we don't know what epiphanies may lie in the future for us, but that <em>particular</em> paperclip-epiphany seems <em>improbable.</em>  If you are tempted to argue with this statement as it applies to actual non-metaphorical paperclips, you are probably being overly contrary.  This is why we consider actual non-metaphorical paperclips as the case in point of 'paperclips'.  </p>
<p>From our perspective, any entity that did in fact go around transforming almost all reachable matter into literal actual nonmetaphorical paperclips, would be doing something incredibly pointless; there would almost certainly be no hidden wisdom in the act that we could perceive on deeper examination or further growth of our own intellectual capacities.  By the definition of the concept, this would be equally true of anything more generally termed a <a href="paperclip_maximizer.html">paperclip maximizer</a>.  Anything claimed about 'paperclips' or a <a href="paperclip_maximizer.html">&#39;paperclip&#39; maximizer</a> (such as the claim that <a href="orthogonality.html">such an entity can exist without having any special intellectual defects</a>) must go through without any change for actual paperclips.  Actual paperclips are meant to be a central example of 'paperclips'.</p>
<p>The only distinction between paperclips and 'paperclips' is that the category 'paperclips' is far <em>wider</em> than the category 'actual non-metaphorical paperclips' and includes many more specific configurations of matter.  Pencil erasers, tiny molecular smileyfaces, and <a href="diamond_maximizer.html">enormous diamond masses</a> are all 'paperclips'.  Even under the <a href="orthogonality.html">Orthogonality Thesis</a>, an AI maximizing actual non-metaphorical paperclips would be an improbable <em>actual</em> outcome of screwing up on <a href="value_alignment_problem.html">value alignment</a>; but only because there are so many other possibilities.  A 'red actual-paperclip maximizer' would be even more improbable than an actual-paperclip maximizer to find in real life; but this is not because redness is antithetical to the nature of intelligent goals.  The 'redness' clause is one more added piece of complexity in the specification that drives down the probability of that exact outcome.</p>
<p>The popular press has sometimes distorted the notion of a paperclip maximizer into a story about an AI running a paperclip factory that takes over the universe.  (Needless to say, the kind of AI used in a paperclip-manufacturing facility is unlikely to be a frontier research AI.)  The concept of a 'paperclip' is not that it's an explicit goal somebody foolishly gave an AI, or even a goal comprehensible in human terms at all.  To imagine a central example of a supposed paperclip maximizer, imagine a research-level AI that did not stably preserve what its makers thought was supposed to be its utility function, or an AI with a poorly specified value learning rule, etcetera; such that the configuration of matter that <a href="unforeseen_maximum.html">actually happened to max out the AI&#39;s utility function</a> looks like a <a href="edge_instantiation.html">tiny</a> string of atoms in the shape of a paperclip.</p></main><hr><footer></footer></body></html>