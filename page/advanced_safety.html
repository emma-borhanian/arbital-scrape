<!DOCTYPE html><html><head><meta charset="utf-8"><title>Advanced safety</title><link rel="stylesheet" type="text/css" href="../common.css"><link rel="stylesheet" type="text/css" href="../page-style.css"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  SVG: {EqnChunk: 50, EqnChunkFactor: 1.5, EqChunkDelay: 10, useFontCache: false, linebreaks: {automatic: true}},
  tex2jax: {
    inlineMath: [['$~$', '$~$']],
    displayMath: [['$$~$', '$~$$']],
    processEscapes: true,
    preview: 'none',
  },
  showProcessingMessages: false,
  messageStyle: 'none',
  // http://docs.mathjax.org/en/latest/config-files.html#the-tex-ams-svg-configuration-file
  jax: ["input/TeX","output/SVG", "output/PreviewHTML"],
  extensions: ["tex2jax.js","MathMenu.js","MathZoom.js", "fast-preview.js", "AssistiveMML.js", "a11y/accessibility-menu.js"],
  TeX: { extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"] }
});</script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_SVG" async></script><script type="text/javascript" src="../arbital-demo-bundle.js"></script><script type="text/javascript">window.addEventListener('DOMContentLoaded', e=>window.loadAllDemos())
</script></head><body><header><h1 class="title">Advanced safety</h1><div class="page-info"><p class="metadata-link"><a href="../metadata/advanced_safety.json.html">advanced_safety.json</a></p><p class="arbital-url"><a href="https://arbital.com/p/advanced_safety">https://arbital.com/p/advanced_safety</a></p><p class="creator">by
 <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a> Mar 26 2015 
updated
 Dec 16 2015</p></div><p class="clickbait">An agent is *really* safe when it has the capacity to do anything, but chooses to do what the programmer wants.</p><nav class="breadcrumbs"><nav class="breadcrumb" aria-label="Breadcrumb"><ul><li><a href="../index.html">Index</a></li><li>Advanced safety</li></ul></nav><nav class="breadcrumb" aria-label="Breadcrumb"><ul><li><a href="ai_alignment.html">AI alignment</a></li><li>â€¦</li></ul></nav></nav></header><hr><main><p>A proposal meant to produce <a href="ai_alignment.html">value-aligned agents</a> is 'advanced-safe' if it succeeds, or fails safely, in <a href="advanced_agent.html">scenarios where the AI becomes much smarter than its human developers</a>. </p>
<h3 id="definition">Definition</h3>
<p>A proposal for a value-alignment methodology, or some aspect of that methodology, is alleged to be 'advanced-safe' if that proposal is claimed robust to scenarios where the agent:</p>
<ul>
<li>Knows more or has better probability estimates than us</li>
<li>Learns new facts unknown to us</li>
<li>Searches a larger strategy space than we can consider</li>
<li>Confronts new instrumental problems we didn't foresee in detail</li>
<li>Gains power quickly</li>
<li>Has access to greater levels of cognitive power than in the regime where it was previously tested</li>
<li>Wields strategies <a href="strong_uncontainability.html">that wouldn&#39;t make sense to us even if we were told about them in advance</a></li>
</ul>
<h3 id="importance">Importance</h3>
<p>It seems reasonable to expect that there will be difficulties of dealing with minds smarter than our own, doing things we didn't imagine, that will be qualitatively different from designing a toaster oven to not burn down a house, or from designing an AI system that is dumber than human.  This means that the concept of 'advanced safety' will end up importantly different from the concept of robust pre-advanced AI.</p>
<p>Concretely, it has been argued to be [ foreseeable] for several difficulties including e.g. <a href="programmer_deception.html">programmer deception</a> and <a href="unforeseen_maximum.html">unforeseen maximums</a>, that they won't materialize before an agent is advanced, or won't materialize in the same way, or won't materialize as severely.  This means that practice with dumber-than-human AIs may not train us against these difficulties, requiring a separate theory and mental discipline for making advanced AIs safe.</p>
<p>We have observed in practice that many proposals for 'AI safety' do not seem to have been thought through against advanced agent scenarios; thus, there seems to be a practical urgency to emphasizing the concept and the difference.</p>
<p>Key problems of advanced safety that are new or qualitatively different compared to pre-advanced AI safety include:</p>
<ul>
<li><a href="edge_instantiation.html">Edge instantiation</a></li>
<li><a href="unforeseen_maximum.html">Unforeseen maximums</a></li>
<li><a href="context_disaster.html">Context change problems</a></li>
<li><a href="programmer_deception.html">Programmer deception</a></li>
<li>[ Programmer maximization]</li>
<li>[ Philosophical competence]</li>
</ul>
<p>Non-advanced-safe methodologies may conceivably be useful if a [ known algorithm nonrecursive agent] can be created that is (a) <a href="relevant_powerful_agent.html">powerful enough to be relevant</a> and (b) can be known not to become advanced.  Even here there may be grounds for worry that such an agent finds unexpectedly strong strategies in some particular subdomain - that it exhibits flashes of domain-specific advancement that break a non-advanced-safe methodology.</p>
<h3 id="omnisafety">Omni-safety</h3>
<p>As an extreme case, an 'omni-safe' methodology allegedly remains value-aligned, or fails safely, even if the agent suddenly becomes omniscient and omnipotent (acquires delta probability distributions on all facts of interest and has all describable outcomes available as direct options). See: <a href="omni_test.html">real-world agents should be omni-safe</a>.</p></main><hr><section class="comments"><h2>Comments</h2><div class="comment"><p><a class="page-link" href="../page/KenziAmodei.html">Kenzi Amodei</a></p><p><p>I'm surprised you want to use the word "advanced" to for this concept; implies to me this is the <em>main</em> kind of high-level safety missing from standard "safety" models?  I guess the list of bullet points does cover a whole lot of scenarios.  It does make it sound sexy, and not like something you'd want to ignore.  Obvious alternative usage for the word advanced relative to safety would be for "actually" safe (over just claimed safe).  Maybe that has other words available to it like provably.</p>
<p>I have the intuition that many proposals fail against advanced agents; I don't see intuitively that it's the "advanced" that's the main problem (that would imply they <em>would</em> work as long as the agent didn't become advanced, I think?  What does that look like?  And is this like Asimov's three laws or tool AI or what?)</p>
<p>Are there any interesting intuition pumps that fall out of omniscience/omnipotence that don't fall easily out of the "advanced" concept?</p></p></div></section><footer><p class="all-creators"><h2>All Creators</h2><span class="page-comma-list"><a class="page-link" href="../page/AlexeiAndreev.html">Alexei Andreev</a>,
 <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></span></p><p class="children"><h2>Children</h2><ul class="page-tree"><li><a class="page-link" href="../page/AI_safety_mindset.html">AI safety mindset</a> <q>Asking how AI designs could go wrong, instead of imagining them going right.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a><ul class="page-tree"><li><a class="page-link" href="../page/hack.html">Ad-hoc hack (alignment theory)</a> <q>A &quot;hack&quot; is when you alter the behavior of your AI in a way that defies, or doesn't correspond to, a principled approach for that problem.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/direct_limit_oppose.html">Directing, vs. limiting, vs. opposing</a> <q>Getting the AI to compute the right action in a domain; versus getting the AI to not compute at all in an unsafe domain; versus trying to prevent the AI from acting successfully.  (Prefer 1 &amp; 2.)</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/dont_solve_whole_problem.html">Don't try to solve the entire alignment problem</a> <q>New to AI alignment theory?  Want to work in this area?  Already been working in it for years?  Don't try to solve the entire alignment problem with your next good idea!</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/load_bearing_premises.html">Flag the load-bearing premises</a> <q>If somebody says, &quot;This AI safety plan is going to fail, because X&quot; and you reply, &quot;Oh, that's fine because of Y and Z&quot;, then you'd better clearly flag Y and Z as &quot;load-bearing&quot; parts of your plan.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/show_broken.html">Show me what you've broken</a> <q>To demonstrate competence at computer security, or AI alignment, think in terms of breaking proposals and finding technically demonstrable flaws in them.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/complacency_valley.html">Valley of Dangerous Complacency</a> <q>When the AGI works often enough that you let down your guard, but it still has bugs.  Imagine a robotic car that almost always steers perfectly, but sometimes heads off a cliff.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li></ul></li><li><a class="page-link" href="../page/actual_effectiveness.html">Actual effectiveness</a> <q>If you want the AI's so-called 'utility function' to actually be steering the AI, you need to think about how it meshes up with beliefs, or what gets output to actions.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/context_disaster.html">Context disaster</a> <q>Some possible designs cause your AI to behave nicely while developing, and behave a lot less nicely when it's smarter.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/distinguish_advancement.html">Distinguish which advanced-agent properties lead to the foreseeable difficulty</a> <q>Say what kind of AI, or threshold level of intelligence, or key type of advancement, first produces the difficulty or challenge you're talking about.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/goodharts_curse.html">Goodhart's Curse</a> <q>The Optimizer's Curse meets Goodhart's Law.  For example, if our values are V, and an AI's utility function U is a proxy for V, optimizing for high U seeks out 'errors'--that is, high values of U - V.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/goodness_estimate_bias.html">Goodness estimate biaser</a> <q>Some of the main problems in AI alignment can be seen as scenarios where actual goodness is likely to be systematically lower than a broken way of estimating goodness.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/foreseeable_difficulties.html">Methodology of foreseeable difficulties</a> <q>Building a nice AI is likely to be hard enough, and contain enough gotchas that won't show up in the AI's early days, that we need to foresee problems coming in advance.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/unbounded_analysis.html">Methodology of unbounded analysis</a> <q>What we do and don't understand how to do, using unlimited computing power, is a critical distinction and important frontier.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a><ul class="page-tree"><li><a class="page-link" href="../page/AIXI.html">AIXI</a> <q>How to build an (evil) superintelligent AI using unlimited computing power and one page of Python code.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a><ul class="page-tree"><li><a class="page-link" href="../page/aixitl.html">AIXI-tl</a> <q>A time-bounded version of the ideal agent AIXI that uses an impossibly large finite computer instead of a hypercomputer.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li></ul></li><li><a class="page-link" href="../page/cartesian_agent.html">Cartesian agent</a> <q>Agents separated from their environments by impermeable barriers through which only sensory information can enter and motor output can exit.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a><ul class="page-tree"><li><a class="page-link" href="../page/cartesian_boundary.html">Cartesian agent-environment boundary</a> <q>If your agent is separated from the environment by an absolute border that can only be crossed by sensory information and motor outputs, it might just be a Cartesian agent.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li></ul></li><li><a class="page-link" href="../page/hypercomputer.html">Hypercomputer</a> <q>Some formalisms demand computers larger than the limit of all finite computers</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/mechanical_turk.html">Mechanical Turk (example)</a> <q>The 19th-century chess-playing automaton known as the Mechanical Turk actually had a human operator inside. People at the time had interesting thoughts about the possibility of mechanical chess.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/nofreelunch_irrelevant.html">No-Free-Lunch theorems are often irrelevant</a> <q>There's often a theorem proving that some problem has no optimal answer across every possible world.  But this may not matter, since the real world is a special case.  (E.g., a low-entropy universe.)</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/solomonoff_induction.html">Solomonoff induction</a> <q>A simple way to superintelligently predict sequences of data, given unlimited computing power.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a><ul class="page-tree"><li><a class="page-link" href="../page/1hh.html">Solomonoff induction: Intro Dialogue (Math 2)</a> <q>An introduction to Solomonoff induction for the unfamiliar reader who isn't bad at math</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li></ul></li><li><a class="page-link" href="../page/large_computer.html">Unphysically large finite computer</a> <q>The imaginary box required to run programs that require impossibly large, but finite, amounts of computing power.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li></ul></li><li><a class="page-link" href="../page/nearest_unblocked.html">Nearest unblocked strategy</a> <q>If you patch an agent's preference framework to avoid an undesirable solution, what can you expect to happen?</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/daemons.html">Optimization daemons</a> <q>When you optimize something so hard that it crystalizes into an optimizer, like the way natural selection optimized apes so hard they turned into human-level intelligences</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/safe_useless.html">Safe but useless</a> <q>Sometimes, at the end of locking down your AI so that it seems extremely safe, you'll end up with an AI that can't be used to do anything interesting.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li></ul></p></footer></body></html>