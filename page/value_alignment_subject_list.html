<!DOCTYPE html><html><head><meta charset="utf-8"><title>List: value-alignment subjects</title><link rel="stylesheet" type="text/css" href="../common.css"><link rel="stylesheet" type="text/css" href="../page-style.css"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  SVG: {EqnChunk: 50, EqnChunkFactor: 1.5, EqChunkDelay: 10, useFontCache: false, linebreaks: {automatic: true}},
  tex2jax: {
    inlineMath: [['$~$', '$~$']],
    displayMath: [['$$~$', '$~$$']],
    processEscapes: true,
    preview: 'none',
  },
  showProcessingMessages: false,
  messageStyle: 'none',
  // http://docs.mathjax.org/en/latest/config-files.html#the-tex-ams-svg-configuration-file
  jax: ["input/TeX","output/SVG", "output/PreviewHTML"],
  extensions: ["tex2jax.js","MathMenu.js","MathZoom.js", "fast-preview.js", "AssistiveMML.js", "a11y/accessibility-menu.js"],
  TeX: { extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"] }
});</script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_SVG" async></script><script type="text/javascript" src="../arbital-demo-bundle.js"></script><script type="text/javascript">window.addEventListener('DOMContentLoaded', e=>window.loadAllDemos())
</script></head><body><header><h1 class="title">List: value-alignment subjects</h1><div class="page-info"><p class="metadata-link"><a href="../metadata/value_alignment_subject_list.json.html">value_alignment_subject_list.json</a></p><p class="arbital-url"><a href="https://arbital.com/p/value_alignment_subject_list">https://arbital.com/p/value_alignment_subject_list</a></p><p class="creator">by
 <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a> Mar 31 2015 
updated
 Jan 27 2017</p></div><p class="clickbait">Bullet point list of core VAT subjects.</p><nav class="breadcrumbs"><nav class="breadcrumb" aria-label="Breadcrumb"><ul><li><a href="../index.html">Index</a></li><li>List: value-alignment subjects</li></ul></nav><nav class="breadcrumb" aria-label="Breadcrumb"><ul><li><a href="ai_alignment.html">AI alignment</a></li><li>â€¦</li></ul></nav></nav></header><hr><main><p>[summary: This page contains a thorough list of all <a href="ai_alignment.html">value-alignment</a> subjects.]</p>
<h3 id="safetyparadigmforadvancedagents">Safety paradigm for advanced agents</h3>
<ul>
<li><a href="advanced_safety.html">Advanced Safety</a></li>
<li><a href="advanced_agent.html">Advanced agents</a><ul>
<li><a href="efficiency.html">Efficient agents</a></li></ul></li>
<li><a href="AI_safety_mindset.html">AI safety mindset</a><ul>
<li><a href="omni_test.html">Omni Test</a></li></ul></li>
<li><a href="foreseeable_difficulties.html">Methodology of foreseeable difficulties</a></li>
<li><a href="context_disaster.html">Context Change</a> problems ("Treacherous problems"?)</li>
<li><a href="unbounded_analysis.html">Methodology of unbounded analysis</a></li>
<li>Priority of astronomical failures (those that destroy error recovery or are immediately catastrophic)</li>
</ul>
<h3 id="foreseendifficulties">Foreseen difficulties</h3>
<ul>
<li><a href="value_identification.html">Value identification</a></li>
<li><a href="edge_instantiation.html">Edge instantiation</a></li>
<li><a href="unforeseen_maximum.html">Unforeseen maximums</a></li>
<li><a href="ontology_identification.html">Ontology identification</a><ul>
<li>Cartesian boundary</li>
<li>Human identification</li></ul></li>
<li>Inductive value learning<ul>
<li><a href="inductive_ambiguity.html">Ambiguity-querying</a></li>
<li>Moral uncertainty<ul>
<li>Indifference</li></ul></li></ul></li>
<li><a href="patch_resistant.html">Patch resistance</a></li>
<li><a href="nearest_unblocked.html">Nearest Unblocked Neighbor</a></li>
<li><a href="corrigibility.html">Corrigibility</a></li>
<li>Anapartistic reasoning<ul>
<li>Programmer deception</li>
<li>Early conservatism</li>
<li>Reasoning under confusion</li></ul></li>
<li>User maximization / Unshielded argmax<ul>
<li>Hypothetical user maximization</li></ul></li>
<li><a href="task_agi.html">Genie theory</a></li>
<li>Limited AI<ul>
<li>Weak optimization<ul>
<li>Safe optimization measure (such that we are confident it has no Edge that secretly optimizes more)<ul>
<li>Factoring of an agent by stage/component optimization power</li></ul></li>
<li>'Checker' smarter than 'inventor / chooser'<ul>
<li>'Checker' can model humans, 'strategizer' cannot</li></ul></li></ul></li>
<li>Transparency</li>
<li>Domain restriction<ul>
<li><a href="behaviorist.html">Behaviorism</a></li></ul></li>
<li>Effable optimization (opposite of cognitive uncontainability; uses only comprehensible strategies)<ul>
<li><a href="4x.html">Minimal concepts</a> (simple, not simplest, that contains fewest whitelisted strategies)</li></ul></li></ul></li>
<li>Genie preferences<ul>
<li>Low-impact AGI<ul>
<li>Minimum Safe AA (just flip off switch and shut down safely)</li>
<li>Safe impact measure</li>
<li>Armstrong-style permitted output channels</li>
<li>Shutdown utility function</li></ul></li>
<li>Oracle utility function<ul>
<li>Safe indifference?</li></ul></li>
<li>Online checkability<ul>
<li>Reporting without programmer maximization</li></ul></li>
<li>Do What I Know I Mean</li></ul></li>
<li>Superintelligent security (all subproblems placing us in adversarial context vs. other SIs)</li>
<li>Bargaining<ul>
<li>Non-blackmailability</li>
<li>Secure counterfactual reasoning</li>
<li>First-mover penalty / epistemic low ground advantage</li>
<li>Division of gains from trade</li></ul></li>
<li>Epistemic exclusion of distant SIs<ul>
<li><a href="probable_environment_hacking.html">Distant superintelligences can coerce the most probable environment of your AI</a></li>
<li>Breaking out of hypotheses</li></ul></li>
<li>'Philosophical' problems</li>
<li>One True Prior<ul>
<li>Pascal's Mugging / leverage prior</li>
<li>Second-orderness</li>
<li>Anthropics<ul>
<li>How would an AI decide what to think about QTI?</li></ul></li></ul></li>
<li><a href="mindcrime.html">Mindcrime</a><ul>
<li>Nonperson predicates (and unblocked neighbor problem)</li></ul></li>
<li>Do What I Don't Know I Mean
       - CEV</li>
<li>Philosophical competence
     - Unprecedented excursions</li>
</ul>
<h3 id="reflectivityproblems">Reflectivity problems</h3>
<ul>
<li>Vingean reflection</li>
<li>Satisficing / meliorizing / staged maximization / ?<ul>
<li>Academic agenda: view current algorithms as finding a global logically-uncertain maximum, or teleporting to the current maximum, surveying, updating on a logical fact, and teleporting to the new maximum.</li></ul></li>
<li>Logical decision theory</li>
<li>Naturalized induction</li>
<li>Benja: Investigate multi-level representation of DBNs (with categorical structure)</li>
</ul>
<h3 id="foreseennormaldifficulties">Foreseen normal difficulties</h3>
<ul>
<li>Reproducibility</li>
<li>Oracle boxes</li>
<li>Triggers<ul>
<li>Ascent metrics</li></ul></li>
<li>Tripwires<ul>
<li>Honeypots</li></ul></li>
</ul>
<h3 id="generalagenttheory">General agent theory</h3>
<ul>
<li><a href="advanced_agent.html">Bounded rational agency</a></li>
<li>Instrumental convergence</li>
</ul>
<h3 id="valuetheory">Value theory</h3>
<ul>
<li><a href="orthogonality.html">Orthogonality Thesis</a></li>
<li><a href="complexity_of_value.html">Complexity of value</a></li>
<li>Complexity of <a href="object_level_goal.html">object-level</a> terminal values</li>
<li>Incompressibilities of value<ul>
<li>Bounded logical incompressibility</li>
<li>Terminal empirical incompressibility</li>
<li>Instrumental nonduplication of value</li>
<li>Economic incentives do not encode value</li>
<li>Selection among advanced agents would not encode value<ul>
<li>Strong selection among advanced agents would not encode value</li>
<li>Selection among advanced agents will be weak.</li></ul></li></ul></li>
<li>Fragility of value</li>
<li>Metaethics</li>
<li>Normative preferences are not compelling to a paperclip maximizer</li>
<li>Most 'random' stable AIs are like paperclip maximizers in this regard</li>
<li>It's okay for valid normative reasoning to be incapable of compelling a paperclip maximizer</li>
<li>Thick definitions of 'rationality' aren't part of what gets automatically produced by self-improvement</li>
<li>Alleged fallacies</li>
<li>Alleged fascination of One True Moral Command</li>
<li>Alleged rationalization of user-preferred options as formal-criterion-maximal options</li>
<li>Alleged metaethical alief that value must be internally morally compelling to all agents</li>
<li>Alleged alief that an AI must be stupid to do something inherently dispreferable </li>
</ul>
<h3 id="largerresearchagendas">Larger research agendas</h3>
<ul>
<li>Corrigible reflective unbounded safe genie</li>
<li>Bounding the theory</li>
<li>Derationalizing the theory (e.g. for a neuromorphic AI)<ul>
<li>Which machine learning systems do and don't behave like the corresponding ideal agents.</li></ul></li>
<li>Normative Sovereign</li>
<li>Approval-based agents</li>
<li>Mindblind AI (cognitively powerful in physical science and engineering, weak at modeling minds or agents, unreflective)</li>
</ul>
<h3 id="possiblefutureusecases">Possible future use-cases</h3>
<ul>
<li>A carefully designed bounded reflective agent.</li>
<li>An overpowered set of known algorithms, heavily constrained in what is authorized, with little recursion.</li>
</ul>
<h3 id="possibleescaperoutes">Possible escape routes</h3>
<ul>
<li>Some cognitively limited task which is relatively safe to carry out at great power, and resolves the larger problem.</li>
<li>Newcomers can't invent these well because they don't understand what is a cognitively limited task (e.g., "Tool AI" suggestions).</li>
<li>General cognitive tasks that seem boxable and resolve the larger problem.</li>
<li>Can you save the world by knowing which consequences of ZF a superintelligence could prove? It's unusually boxable, but what good is it?</li>
</ul>
<h3 id="background">Background</h3>
<ul>
<li>Intelligence explosion microeconomics</li>
<li>Civilizational adequacy/inadequacy</li>
</ul>
<h3 id="strategy">Strategy</h3>
<ul>
<li>Misleading Encouragement / context change / treacherous designs for naive projects</li>
<li>Programmer prediction &amp; infrahuman domains hide complexity of value</li>
<li>Context change problems</li>
<li>Problems that only appear in advanced regimes</li>
<li>Problem classes that seem debugged in infrahuman regimes and suddenly break again in advanced regimes</li>
<li>Methodologies that only work in infrahuman regimes</li>
<li>Programmer deception</li>
<li>Academic inadequacy</li>
<li>'Ethics' work neglects technical problems that need longest serial research times and fails to give priority to astronomical failures over survivable small hits, but 'ethics' work has higher prestige, higher publishability, and higher cognitive accessibility</li>
<li>Understanding of big technical picture currently very rare<ul>
<li>Most possible funding sources cannot predict for themselves what might be technically useful in 10 years</li>
<li>Many possible funding sources may not regard MIRI as trusted to discern this</li></ul></li>
<li>Noise problems<ul>
<li>Ethics research drowns out technical research<ul>
<li>And provokes counterreaction</li>
<li>And makes the field seem nontechnical</li></ul></li>
<li>Naive technical research drowns out sophisticated technical research<ul>
<li>And makes problems look more solvable than they really are</li>
<li>And makes tech problems look trivial, therefore nonprestigious</li>
<li>And distracts talent/funding from hard problems</li></ul></li>
<li>Bad methodology louder than good methodology<ul>
<li>So projects can appear safety-concerned while adopting bad methodologies</li></ul></li></ul></li>
<li>Future adequacy counterfactuals seem distant from the present regime</li>
<li>(To classify)</li>
<li><a href="4j.html">Coordinative development hypothetical</a></li>
</ul></main><hr><section class="comments"><h2>Comments</h2><div class="comment"><p><a class="page-link" href="../page/AlexeiAndreev.html">Alexei Andreev</a></p><p><p>Ideally we shouldn't have pages like this. It means that the hierarchy feature failed. Is this just meant to be temporary? Or do you foresee this as a permanent page?</p></p><div class="comment"><p><a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></p><p><p>I think one will often still need 'introductory' or 'tutorial' type pages that walk through the hierarchy as English text, but this exact page was something I whipped up during the recent Experimental Research Retreat as an alternative to just dumping the info and because I thought I might start filling it in as Arbital pages.</p></p></div><div class="comment"><p><a class="page-link" href="../page/AnnaSalamon.html">Anna Salamon</a></p><p><p>I'm finding this page helpful.  Alexei, does your theory think I shouldn't be?</p></p></div><div class="comment"><p><a class="page-link" href="../page/AlexeiAndreev.html">Alexei Andreev</a></p><p><p>I definitely think something like this should exist and will be helpful, but I think Arbital should be able to generate something like this automatically. Until it can, we are stuck doing it manually.</p>
<p>Expanding all children in the Children tab on the <a href="ai_alignment.html">AI alignment</a> page achieves something similar, but not quite as clean.</p></p></div></div><div class="comment"><p><a class="page-link" href="../page/MikeJohnson.html">Mike Johnson</a></p><p><p>Within the "Value Theory" section, I'd propose two subpoints:</p>
<ul>
<li><p>Unity of Value Thesis</p></li>
<li><p>Necessity of Physical Representation</p></li>
</ul>
<p>The 'Unity of Value Thesis' is simply what we get if the Complexity of Value Thesis is wrong. And it could be wrong- we just don't know. For what this could look like, see e.g. <a href="https://qualiacomputing.com/2016/11/19/the-tyranny-of-the-intentional-object/">https://qualiacomputing.com/2016/11/19/the-tyranny-of-the-intentional-object/</a></p>
<p>'Necessity of Physical Representation' refers to the notion that ultimately, a proper theory of value <em>must compile to physics</em>. We are made from physical stuff, and everything we interact with and value is made from the same physical stuff, and so <em>ethics ultimately is about how to move & arrange the physical stuff in our light-cone</em>. If a theory of value does not operate at this level, it can't be a final theory of value. See e.g., Tegmark's argument here: <a href="https://arxiv.org/abs/1409.0813">https://arxiv.org/abs/1409.0813</a></p></p></div></section><footer><p class="all-creators"><h2>All Creators</h2><span class="page-comma-list"><a class="page-link" href="../page/AlexeiAndreev.html">Alexei Andreev</a>,
 <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a>,
 <a class="page-link" href="../page/MatthewGraves.html">Matthew Graves</a>,
 <a class="page-link" href="../page/NateSoares.html">Nate Soares</a></span></p><p class="likes"><h2>Likes</h2><span class="page-comma-list"><a class="page-link" href="../page/AnnaSalamon.html">Anna Salamon</a>,
 <a class="page-link" href="../page/EliasBaixas.html">Elias Baixas</a>,
 <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a>,
 <a class="page-link" href="../page/StephanieZolayvar.html">Stephanie Zolayvar</a></span></p></footer></body></html>