<!DOCTYPE html><html><head><meta charset="utf-8"><title>Safe but useless</title><link rel="stylesheet" type="text/css" href="../common.css"><link rel="stylesheet" type="text/css" href="../page-style.css"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  SVG: {EqnChunk: 50, EqnChunkFactor: 1.5, EqChunkDelay: 10, useFontCache: false, linebreaks: {automatic: true}},
  tex2jax: {
    inlineMath: [['$~$', '$~$']],
    displayMath: [['$$~$', '$~$$']],
    processEscapes: true,
    preview: 'none',
  },
  showProcessingMessages: false,
  messageStyle: 'none',
  // http://docs.mathjax.org/en/latest/config-files.html#the-tex-ams-svg-configuration-file
  jax: ["input/TeX","output/SVG", "output/PreviewHTML"],
  extensions: ["tex2jax.js","MathMenu.js","MathZoom.js", "fast-preview.js", "AssistiveMML.js", "a11y/accessibility-menu.js"],
  TeX: { extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"] }
});</script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_SVG" async></script><script type="text/javascript" src="../arbital-demo-bundle.js"></script><script type="text/javascript">window.addEventListener('DOMContentLoaded', e=>window.loadAllDemos())
</script></head><body><header><h1 class="title">Safe but useless</h1><div class="page-info"><p class="metadata-link"><a href="../metadata/safe_useless.json.html">safe_useless.json</a></p><p class="arbital-url"><a href="https://arbital.com/p/safe_useless">https://arbital.com/p/safe_useless</a></p><p class="creator">by
 <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a> Jun 8 2016 
updated
 Jun 8 2016</p></div><p class="clickbait">Sometimes, at the end of locking down your AI so that it seems extremely safe, you'll end up with an AI that can't be used to do anything interesting.</p><nav class="breadcrumbs"><nav class="breadcrumb" aria-label="Breadcrumb"><ul><li><a href="../index.html">Index</a></li><li>Safe but useless</li></ul></nav><nav class="breadcrumb" aria-label="Breadcrumb"><ul><li><a href="ai_alignment.html">AI alignment</a></li><li><a href="advanced_safety.html">Advanced safety</a></li><li>â€¦</li></ul></nav></nav></header><hr><main><p>[summary:  Arguendo, when some particular proposed AI safety measures is alleged to be inherently opposed to the useful work the AI is meant to do.</p>
<p>We could use the metaphor of a scissors and its dangerous blades.  We can have a "safety scissors" that is only <em>just</em> sharp enough to cut paper, but this is still sharp enough to do some damage if you work at it.  If you make the scissors <em>even safer</em> by encasing the dangerous blades in foam rubber, the scissors can't cut paper any more; and if it <em>can</em> cut paper, it's still unsafe.  Maybe you can cut clay, but nobody knows how to do a <a href="pivotal.html">sufficiently large amount of good</a> by cutting clay.</p>
<p>Similarly, there's an obvious way to cut down the output of an <a href="oracle.html">Oracle AGI</a> to the point where <a href="ZF_provability_oracle.html">all it can do is tell us that a proposed theorem is provable from the axioms of Zermelo-Fraenkel set theory</a>.  Unfortunately, nobody knows how to use a ZF provability oracle to <a href="pivotal.html">save the world</a>.]</p>
<p>"This type of safety implies uselessness" (or conversely, "any AI powerful enough to be useful will still be unsafe") is an accusation leveled against a proposed AI safety measure that must, to make the AI safe, be enforced to the point that it will make the AI useless.</p>
<p>For a non-AI metaphor, consider a scissors and its dangerous blades.  We can have a "safety scissors" that is only <em>just</em> sharp enough to cut paper - but this is still sharp enough to do some damage if you work at it.  If you try to make the scissors <em>even safer</em> by encasing the dangerous blades in foam rubber, the scissors can't cut paper any more.  If the scissors <em>can</em> cut paper, it's still unsafe.  Maybe you could in principle cut clay with a scissors like that, but this is no defense unless you can tell us <a href="pivotal.html">something very useful</a> that can be done by cutting clay.</p>
<p>Similarly, there's an obvious way to try cutting down the allowed output of an <a href="oracle.html">Oracle AGI</a> to the point where <a href="ZF_provability_oracle.html">all it can do is tell us that a given theorem is provable from the axioms of Zermelo-Fraenkel set theory</a>.  This <a href="strong_uncontainability.html">might</a> prevent the AGI from hacking the human operators into letting it out, since all that can leave the box is a single yes-or-no bit, sent at some particular time.  An untrusted superintelligence inside this scheme would have the option of strategically not telling us when a theorem <em>is</em> provable in ZF; but if the bit from the proof-verifier said that the input theorem was ZF-provable, we could very likely trust that.</p>
<p>But now we run up against the problem that nobody knows how to <a href="pivotal.html">actually save the world</a> by virtue of sometimes knowing for sure that a theorem is provable in ZF.  The scissors has been blunted to where it's probably completely safe, but can only cut clay; and nobody knows how to <a href="pivotal.html">do <em>enough</em> good</a> by cutting clay.</p>
<h1 id="idealmodelsofsafebutuselessagents">Ideal models of "safe but useless" agents</h1>
<p>Should you have cause to do a mathematical study of this issue, then an excellent <a href="unbounded_analysis.html">ideal model</a> of a safe but useless agent, embodying maximal safety and minimum usefulness, would be a rock.</p></main><hr><footer><p class="likes"><h2>Likes</h2><span class="page-comma-list"><a class="page-link" href="../page/EricRogstad.html">Eric Rogstad</a></span></p></footer></body></html>