<!DOCTYPE html><html><head><meta charset="utf-8"><title>Niceness is the first line of defense</title><link rel="stylesheet" type="text/css" href="../common.css"><link rel="stylesheet" type="text/css" href="../page-style.css"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  SVG: {EqnChunk: 50, EqnChunkFactor: 1.5, EqChunkDelay: 10, useFontCache: false, linebreaks: {automatic: true}},
  tex2jax: {
    inlineMath: [['$~$', '$~$']],
    displayMath: [['$$~$', '$~$$']],
    processEscapes: true,
    preview: 'none',
  },
  showProcessingMessages: false,
  messageStyle: 'none',
  // http://docs.mathjax.org/en/latest/config-files.html#the-tex-ams-svg-configuration-file
  jax: ["input/TeX","output/SVG", "output/PreviewHTML"],
  extensions: ["tex2jax.js","MathMenu.js","MathZoom.js", "fast-preview.js", "AssistiveMML.js", "a11y/accessibility-menu.js"],
  TeX: { extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"] }
});</script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_SVG" async></script><script type="text/javascript" src="../arbital-demo-bundle.js"></script><script type="text/javascript">window.addEventListener('DOMContentLoaded', e=>window.loadAllDemos())
</script></head><body><header><h1 class="title">Niceness is the first line of defense</h1><div class="page-info"><p class="metadata-link"><a href="../metadata/niceness_defense.json.html">niceness_defense.json</a></p><p class="arbital-url"><a href="https://arbital.com/p/niceness_defense">https://arbital.com/p/niceness_defense</a></p><p class="creator">by
 <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a> Mar 27 2016 
updated
 Jan 18 2017</p></div><p class="clickbait">The *first* line of defense in dealing with any partially superhuman AI system advanced enough to possibly be dangerous is that it does not *want* to hurt you or defeat your safety measures.</p><nav class="breadcrumbs"><nav class="breadcrumb" aria-label="Breadcrumb"><ul><li><a href="../index.html">Index</a></li><li>Niceness is the first line of defense</li></ul></nav><nav class="breadcrumb" aria-label="Breadcrumb"><ul><li><a href="ai_alignment.html">AI alignment</a></li><li><a href="alignment_principle.html">Principles in AI alignment</a></li><li><a href="nonadversarial.html">Non-adversarial principle</a></li><li>â€¦</li></ul></nav></nav></header><hr><main><p>The first line of defense in constructing any <a href="advanced_agent.html">sufficiently advanced</a> <a href="agi.html">Artificial General Intelligence</a> is building an AI that does not <em>want</em> to hurt you.  Any other measures, like <a href="AI_boxing.html">AI-boxing</a> or trying to [airgap_ai prevent the AI from accessing the Internet], should be thought of only as backstops in case this first line of defense fails.  When designing the AGI we should first think as if all these <a href="direct_limit_oppose.html">oppositional measures</a> don't exist, so that we aren't distracted while trying to envision an AGI that--<a href="omni_test.html">regardless</a> of [capability_gain how much power it has]--will not <em>want</em> to hurt us.</p>
<p>See also <a href="nonadversarial.html">the non-adversarial principle</a> and the distinction between <a href="direct_limit_oppose.html">Directing, vs. limiting, vs. opposing</a>.</p></main><hr><footer><p class="related"><h2>Related</h2><ul class="page-list"><li><a class="page-link" href="../page/omni_test.html">Omnipotence test for AI safety</a> <q>Would your AI produce disastrous outcomes if it suddenly gained omnipotence and omniscience? If so, why did you program something that *wants* to hurt you and is held back only by lacking the power?</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li></ul></p></footer></body></html>