<!DOCTYPE html><html><head><meta charset="utf-8"><title>The AI must tolerate your safety measures</title><link rel="stylesheet" type="text/css" href="../common.css"><link rel="stylesheet" type="text/css" href="../page-style.css"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  SVG: {EqnChunk: 50, EqnChunkFactor: 1.5, EqChunkDelay: 10, useFontCache: false, linebreaks: {automatic: true}},
  tex2jax: {
    inlineMath: [['$~$', '$~$']],
    displayMath: [['$$~$', '$~$$']],
    processEscapes: true,
    preview: 'none',
  },
  showProcessingMessages: false,
  messageStyle: 'none',
  // http://docs.mathjax.org/en/latest/config-files.html#the-tex-ams-svg-configuration-file
  jax: ["input/TeX","output/SVG", "output/PreviewHTML"],
  extensions: ["tex2jax.js","MathMenu.js","MathZoom.js", "fast-preview.js", "AssistiveMML.js", "a11y/accessibility-menu.js"],
  TeX: { extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"] }
});</script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_SVG" async></script><script type="text/javascript" src="../arbital-demo-bundle.js"></script><script type="text/javascript">window.addEventListener('DOMContentLoaded', e=>window.loadAllDemos())
</script></head><body><header><h1 class="title">The AI must tolerate your safety measures</h1><div class="page-info"><p class="metadata-link"><a href="../metadata/nonadversarial_safety.json.html">nonadversarial_safety.json</a></p><p class="arbital-url"><a href="https://arbital.com/p/nonadversarial_safety">https://arbital.com/p/nonadversarial_safety</a></p><p class="creator">by
 <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a> Feb 13 2017</p></div><p class="clickbait">A corollary of the nonadversarial principle is that &quot;The AI must tolerate your safety measures.&quot;  </p><nav class="breadcrumbs"><nav class="breadcrumb" aria-label="Breadcrumb"><ul><li><a href="../index.html">Index</a></li><li>The AI must tolerate your safety measures</li></ul></nav><nav class="breadcrumb" aria-label="Breadcrumb"><ul><li><a href="ai_alignment.html">AI alignment</a></li><li><a href="alignment_principle.html">Principles in AI alignment</a></li><li><a href="nonadversarial.html">Non-adversarial principle</a></li><li>â€¦</li></ul></nav></nav></header><hr><main><p>A corollary of the <a href="nonadversarial.html">Non-adversarial principle</a>:  For every kind of safety measure proposed for a <a href="sufficiently_advanced_ai.html">Sufficiently advanced Artificial Intelligence</a>, we should immediately ask how to avoid this safety measure inducing an <a href="nonadversarial.html">adversarial context</a> between the human programmers and the agent being constructed.</p>
<p>A further corollary of the <a href="cognitive_alignment.html">generalized principle of cognitive alignment</a> would suggest that, if we know how to do it without inducing further problems, the AI should positively <em>want</em> the safety measure to be there.</p>
<p>E.g., if the safety measure we want is a <a href="shutdown_problem.html">suspend button</a> (off switch), our first thought should be, "How do we build an agent such that it doesn't mind the off-switch being pressed?"</p>
<p>At a higher level of alignment, if something damages the off-switch, the AI might be so configured that it naturally and spontaneously thinks, "Oh no!  The off-switch is damaged!" and reports this to the programmers, or failing any response there, tries to repair the off-switch itself.  But this would only be a good idea if we were pretty sure we knew this wouldn't lead to the AI substituting its own helpful ideas of what an off-switch would do, or shutting off extra hard.</p>
<p>Similarly, if you start thinking how nice it would be to have the AI operating inside a <a href="AI_boxing.html">box</a> rather than running around in the outside world, your first thought should not be "How do I enclose this box in 12 layers of Faraday cages, a virtual machine running a Java sandbox, and 15 meters of concrete?" but rather "How would I go about constructing an agent that only cared about things inside a box and experienced no motive to affect anything outside the box?"</p>
<p>At a higher level of alignment we might imagine constructing a sort of agent that, if something went wrong, would think "Oh no I am outside the box, that seems very unsafe, how do I go back in?"  But only if we were very sure that we were not thereby constructing a kind of agent that would, e.g., build a superintelligence outside the box just to make extra sure the original agent stayed inside it.</p>
<p>Many classes of safety measures are only meant to come into play after something else has already gone wrong, implying that other things may have gone wrong earlier and without notice.  This suggests that pragmatically we should focus on the principle of "The AI should leave the safety measures alone and not experience an incentive to change their straightforward operation" rather than tackling the more complicated problems of exact alignment inherent in "The AI should be enthusiastic about the safety measures and want them to work even better."</p>
<p>However, if the AI is <a href="tiling_agents.html">changing its own code or constructing subagents</a>, it is necessary for the AI to have at least <em>some</em> positive motivation relating to any safety measures embodied in the operation of an internal algorithm.  An AI indifferent to that code-based safety measure would tend to <a href="reflective_stability.html">just leave the uninteresting code out of the next self-modification</a>.</p></main><hr><footer><p class="tagged"><h2>Tagged</h2><span class="page-comma-list"><a class="page-link" href="../page/start_meta_tag.html">Start</a></span></p><p class="reverse-related"><h2>Reverse Related</h2><ul class="page-list"><li><a class="page-link" href="../page/start_meta_tag.html">Start</a> <q>This page gives a basic overview of the topic, but may be missing important information or have stylistic issues. If you're able to, please help expand or improve it!</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li></ul></p></footer></body></html>