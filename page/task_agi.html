<!DOCTYPE html><html><head><meta charset="utf-8"><title>Task-directed AGI</title><link rel="stylesheet" type="text/css" href="../common.css"><link rel="stylesheet" type="text/css" href="../page-style.css"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  SVG: {EqnChunk: 50, EqnChunkFactor: 1.5, EqChunkDelay: 10, useFontCache: false, linebreaks: {automatic: true}},
  tex2jax: {
    inlineMath: [['$~$', '$~$']],
    displayMath: [['$$~$', '$~$$']],
    processEscapes: true,
    preview: 'none',
  },
  showProcessingMessages: false,
  messageStyle: 'none',
  // http://docs.mathjax.org/en/latest/config-files.html#the-tex-ams-svg-configuration-file
  jax: ["input/TeX","output/SVG", "output/PreviewHTML"],
  extensions: ["tex2jax.js","MathMenu.js","MathZoom.js", "fast-preview.js", "AssistiveMML.js", "a11y/accessibility-menu.js"],
  TeX: { extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"] }
});</script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_SVG" async></script><script type="text/javascript" src="../arbital-demo-bundle.js"></script><script type="text/javascript">window.addEventListener('DOMContentLoaded', e=>window.loadAllDemos())
</script></head><body><header><h1 class="title">Task-directed AGI</h1><div class="page-info"><p class="metadata-link"><a href="../metadata/task_agi.json.html">task_agi.json</a></p><p class="arbital-url"><a href="https://arbital.com/p/task_agi">https://arbital.com/p/task_agi</a></p><p class="creator">by
 <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a> Jun 9 2015 
updated
 Mar 25 2017</p></div><p class="clickbait">An advanced AI that's meant to pursue a series of limited-scope goals given it by the user.  In Bostrom's terminology, a Genie.</p><nav class="breadcrumbs"><nav class="breadcrumb" aria-label="Breadcrumb"><ul><li><a href="../index.html">Index</a></li><li>Task-directed AGI</li></ul></nav><nav class="breadcrumb" aria-label="Breadcrumb"><ul><li><a href="ai_alignment.html">AI alignment</a></li><li><a href="AGI_typology.html">Strategic AGI typology</a></li><li>…</li></ul></nav><nav class="breadcrumb" aria-label="Breadcrumb"><ul><li><a href="ai_alignment.html">AI alignment</a></li><li>…</li></ul></nav></nav></header><hr><main><p>[summary:  A task-based AGI or "genie" is an AGI <a href="intended_goal.html">intended</a> to follow a series of human orders, rather than autonomously pursuing long-term goals.  A Task AGI might be easier to render safe, since:</p>
<ul>
<li>It's possible to <a href="user_querying.html">query the user</a> before and during a Task.</li>
<li><a href="task_goal.html">Tasks are satisficing</a> - they're of limited scope and can be fully accomplished using a limited effort.  (In other words, Tasks should not become more and more accomplished as more and more effort is put into them.)</li>
<li>Adequately <a href="task_identification.html">identifying</a> what it means to safely "cure cancer" might be simpler than adequately identifying <a href="value_alignment_value.html">all normative value</a>.</li>
<li>Task AGIs can be limited in various ways, rather than self-improving as far as possible, so long as they can still carry out at least some <a href="pivotal.html">pivotal</a> Tasks.</li>
</ul>
<p>The obvious disadvantage of a Task AGI is <a href="moral_hazard.html">moral hazard</a> - it may tempt the users in ways that an autonomous AI would not.</p>
<p>The problem of making a safe Task AGI invokes numerous subtopics such as <a href="low_impact.html">low impact</a>, <a href="soft_optimizer.html">mild optimization</a>, and <a href="conservative_concept.html">conservatism</a> as well as numerous standard AGI safety problems like [ goal identification] and <a href="reflective_stability.html">reflective stability</a>.]</p>
<p>A task-based AGI is an AGI <a href="intended_goal.html">intended</a> to follow a series of human-originated orders, with these orders each being of limited scope - "satisficing" in the sense that they can be <a href="task_goal.html">accomplished using bounded amounts of effort and resources</a> (as opposed to the goals being more and more fulfillable using more and more effort).</p>
<p>In <a href="AGI_typology.html">Bostrom&#39;s typology</a>, this is termed a "Genie".  It contrasts with a "Sovereign" AGI that acts autonomously in the pursuit of long-term real-world goals.</p>
<p>Building a safe Task AGI might be easier than building a safe Sovereign for the following reasons:</p>
<ul>
<li>A Task AGI can be "online"; the AGI can potentially <a href="user_querying.html">query the user</a> before and during Task performance.  (Assuming an ambiguous situation arises, and is successfully identified as ambiguous.)</li>
<li>A Task AGI can potentially be <a href="limited_agi.html">limited</a> in various ways, since a Task AGI doesn't need to be <em>as powerful as possible</em> in order to accomplish its limited-scope Tasks.  A Sovereign would presumably engage in all-out self-improvement.  (This isn't to say Task AGIs would automatically not self-improve, only that it's possible <em>in principle</em> to limit the power of a Task AGI to only the level required to do the targeted Tasks, <em>if</em> the associated safety problems can be solved.)</li>
<li>Tasks, by assumption, are limited in scope - they can be accomplished and done, inside some limited region of space and time, using some limited amount of effort which is then complete.  (To gain this advantage, a state of Task accomplishment should not go higher and higher in preference as more and more effort is expended on it open-endedly.)</li>
<li>Assuming that users can figure out <a href="intended_goal.html">intended goals</a> for the AGI that are <a href="value_alignment_value.html">valuable</a> and <a href="pivotal.html">pivotal</a>, the <a href="task_identification.html">identification problem</a> for describing what constitutes a safe performance of that Task, might be simpler than giving the AGI a [ complete description] of <a href="value_alignment_value.html">normativity in general</a>.  That is, the problem of communicating to an AGI an adequate description of "cure cancer" (without killing patients or causing other side effects), while still difficult, might be simpler than an adequate description of all normative value.  Task AGIs fall on the narrow side of <a href="ambitious_vs_narrow_value_learning.html">Ambitious vs. narrow value learning</a>.</li>
</ul>
<p>Relative to the problem of building a Sovereign, trying to build a Task AGI instead might step down the problem from "impossibly difficult" to "insanely difficult", while still maintaining enough power in the AI to perform <a href="pivotal.html">pivotal acts</a>.</p>
<p>The obvious disadvantage of a Task AGI is <a href="moral_hazard.html">moral hazard</a> - it may tempt the users in ways that a Sovereign would not.  A Sovereign has moral hazard chiefly during the development phase, when the programmers and users are perhaps not yet in a position of special relative power.  A Task AGI has ongoing moral hazard as it is used.</p>
<p><a href="EliezerYudkowsky.html">Eliezer Yudkowsky</a> has suggested that people only confront many important problems in value alignment when they are thinking about Sovereigns, but that at the same time, Sovereigns may be impossibly hard in practice.  Yudkowsky advocates that people think about Sovereigns first and list out all the associated issues before stepping down their thinking to Task AGIs, because thinking about Task AGIs may result in premature pruning, while thinking about Sovereigns is more likely to generate a complete list of problems that can then be checked against particular Task AGI approaches to see if those problems have become any easier.</p>
<p>Three distinguished subtypes of Task AGI are these:</p>
<ul>
<li><strong><a href="oracle.html">Oracles</a></strong>, an AI that is intended to only answer questions, possibly from some restricted question set.</li>
<li><strong><a href="KANSI.html">Known-algorithm AIs</a></strong>, which are not self-modifying or very weakly self-modifying, such that their algorithms and representations are mostly known and mostly stable.</li>
<li><strong><a href="behaviorist.html">Behaviorist Genies</a></strong>, which are meant to not model human minds or model them in only very limited ways, while having great material understanding (e.g., potentially the ability to invent and deploy nanotechnology).</li>
</ul>
<h1 id="subproblems">Subproblems</h1>
<p>The problem of making a safe genie invokes numerous subtopics such as <a href="low_impact.html">low impact</a>, <a href="soft_optimizer.html">mild optimization</a>, and <a href="conservative_concept.html">conservatism</a> as well as numerous standard AGI safety problems like <a href="reflective_stability.html">reflective stability</a> and safe <a href="value_identification.html">identification</a> of <a href="intended_goal.html">intended goals</a>.</p>
<p>(<a href="taskagi_open_problems.html">See here for a separate page on open problems in Task AGI safety that might be ready for current research.</a>)</p>
<p>Some further problems beyond those appearing in the page above are:</p>
<ul>
<li><strong>Oracle utility functions</strong> (that make the Oracle not wish to leave its box or optimize its programmers)</li>
<li><strong>Effable optimization</strong> (the opposite of <a href="uncontainability.html">cognitive uncontainability</a>)</li>
<li><strong>Online checkability</strong></li>
<li>Explaining things to programmers <a href="30b.html">without putting the programmers inside an argmax</a> for how well you are 'explaining' things to them</li>
<li><strong>Transparency</strong></li>
<li><a href="dwim.html">Do What I Mean</a></li>
</ul></main><hr><section class="comments"><h2>Comments</h2><div class="comment"><p><a class="page-link" href="../page/PaulChristiano.html">Paul Christiano</a></p><p><p>If the distinguishing characteristic of a genie is "primarily relying on the human ability to discern short-term strategies that achieve long-term value," then I guess that includes all <a href="https://medium.com/ai-control/act-based-agents-8ec926c79e9c">act-based agents</a>. I don't especially like this terminology.</p>
<p>Note that, logically speaking, "human ability" in the above sentence should refer to the ability of humans working in concert with other genies. This really seems like a key fact to me (it also doesn't seem like it should be controversial).</p></p></div></section><footer><p class="tagged"><h2>Tagged</h2><span class="page-comma-list"><a class="page-link" href="../page/work_in_progress_meta_tag.html">Work in progress</a></span></p><p class="all-creators"><h2>All Creators</h2><span class="page-comma-list"><a class="page-link" href="../page/AlexeiAndreev.html">Alexei Andreev</a>,
 <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a>,
 <a class="page-link" href="../page/MalcolmOcean.html">Malcolm Ocean</a>,
 <a class="page-link" href="../page/NateSoares.html">Nate Soares</a></span></p><p class="likes"><h2>Likes</h2><span class="page-comma-list"><a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a>,
 <a class="page-link" href="../page/RolandPihlakas.html">Roland Pihlakas</a>,
 <a class="page-link" href="../page/RyanCarey2.html">Ryan Carey</a></span></p><p class="related"><h2>Related</h2><ul class="page-list"><li><a class="page-link" href="../page/neural_genie_metaphor.html">Neutral genie metaphor</a> <q>Definition. A neutral-genie metaphor is an attempt to illustrate a possible formal problem via an in…</q> - <a class="page-link" href="../page/AlexeiAndreev.html">Alexei Andreev</a></li></ul></p><p class="reverse-related"><h2>Reverse Related</h2><ul class="page-list"><li><a class="page-link" href="../page/work_in_progress_meta_tag.html">Work in progress</a> <q>This page is being actively worked on by an editor. Check with them before making major changes.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li></ul></p><p class="children"><h2>Children</h2><ul class="page-tree"><li><a class="page-link" href="../page/behaviorist.html">Behaviorist genie</a> <q>An advanced agent that's forbidden to model minds in too much detail.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/AI_boxing.html">Boxed AI</a> <q>Idea: what if we limit how AI can interact with the world. That'll make it safe, right??</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a><ul class="page-tree"><li><a class="page-link" href="../page/ZF_provability_oracle.html">Zermelo-Fraenkel provability oracle</a> <q>We might be able to build a system that can safely inform us that a theorem has a proof in set theory, but we can't see how to use that capability to save the world.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li></ul></li><li><a class="page-link" href="../page/conservative_concept.html">Conservative concept boundary</a> <q>Given N example burritos, draw a boundary around what is a 'burrito' that is relatively simple and allows as few positive instances as possible.  Helps make sure the next thing generated is a burrito.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/epistemic_exclusion.html">Epistemic exclusion</a> <q>How would you build an AI that, no matter what else it learned about the world, never knew or wanted to know what was inside your basement?</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/faithful_simulation.html">Faithful simulation</a> <q>How would you identify, to a Task AGI (aka Genie), the problem of scanning a human brain, and then running a sufficiently accurate simulation of it for the simulation to not be crazy or psychotic?</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/limited_agi.html">Limited AGI</a> <q>Task-based AGIs don't need unlimited cognitive and material powers to carry out their Tasks; which means their powers can potentially be limited.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/low_impact.html">Low impact</a> <q>The open problem of having an AI carry out tasks in ways that cause minimum side effects and change as little of the rest of the universe as possible.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a><ul class="page-tree"><li><a class="page-link" href="../page/abortable.html">Abortable plans</a> <q>Plans that can be undone, or switched to having low further impact.  If the AI builds abortable nanomachines, they'll have a quiet self-destruct option that includes any replicated nanomachines.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/shutdown_utility_function.html">Shutdown utility function</a> <q>A special case of a low-impact utility function where you just want the AGI to switch itself off harmlessly (and not create subagents to make absolutely sure it stays off, etcetera).</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li></ul></li><li><a class="page-link" href="../page/soft_optimizer.html">Mild optimization</a> <q>An AGI which, if you ask it to paint one car pink, just paints one car pink and doesn't tile the universe with pink-painted cars, because it's not trying *that* hard to max out its car-painting score.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/taskagi_open_problems.html">Open subproblems in aligning a Task-based AGI</a> <q>Open research problems, especially ones we can model today, in building an AGI that can &quot;paint all cars pink&quot; without turning its future light cone into pink-painted cars.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/oracle.html">Oracle</a> <q>System designed to safely answer questions.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a><ul class="page-tree"><li><a class="page-link" href="../page/ZF_provability_oracle.html">Zermelo-Fraenkel provability oracle</a> <q>We might be able to build a system that can safely inform us that a theorem has a proof in set theory, but we can't see how to use that capability to save the world.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li></ul></li><li><a class="page-link" href="../page/user_querying.html">Querying the AGI user</a> <q>Postulating that an advanced agent will check something with its user, probably comes with some standard issues and gotchas (e.g., prioritizing what to query, not manipulating the user, etc etc).</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/safe_plan_identification.html">Safe plan identification and verification</a> <q>On a particular task or problem, the issue of how to communicate to the AGI what you want it to do and all the things you don't want it to do.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a><ul class="page-tree"><li><a class="page-link" href="../page/dwim.html">Do-What-I-Mean hierarchy</a> <q>Successive levels of &quot;Do What I Mean&quot; or AGIs that understand their users increasingly well</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li></ul></li><li><a class="page-link" href="../page/task_goal.html">Task (AI goal)</a> <q>When building the first AGIs, it may be wiser to assign them only goals that are bounded in space and time, and can be satisfied by bounded efforts.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/task_identification.html">Task identification problem</a> <q>If you have a task-based AGI (Genie) then how do you pinpoint exactly what you want it to do (and not do)?</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a><ul class="page-tree"><li><a class="page-link" href="../page/pointing_finger.html">Look where I'm pointing, not at my finger</a> <q>When trying to communicate the concept &quot;glove&quot;, getting the AGI to focus on &quot;gloves&quot; rather than &quot;my user's decision to label something a glove&quot; or &quot;anything that depresses the glove-labeling button&quot;.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li></ul></li></ul></p></footer></body></html>