<!DOCTYPE html><html><head><meta charset="utf-8"><title>Difference Between Weights and Biases: Another way of Looking at Forward Propagation</title><link rel="stylesheet" type="text/css" href="../common.css"><link rel="stylesheet" type="text/css" href="../page-style.css"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  SVG: {EqnChunk: 50, EqnChunkFactor: 1.5, EqChunkDelay: 10, useFontCache: false, linebreaks: {automatic: true}},
  tex2jax: {
    inlineMath: [['$~$', '$~$']],
    displayMath: [['$$~$', '$~$$']],
    processEscapes: true,
    preview: 'none',
  },
  showProcessingMessages: false,
  messageStyle: 'none',
  // http://docs.mathjax.org/en/latest/config-files.html#the-tex-ams-svg-configuration-file
  jax: ["input/TeX","output/SVG", "output/PreviewHTML"],
  extensions: ["tex2jax.js","MathMenu.js","MathZoom.js", "fast-preview.js", "AssistiveMML.js", "a11y/accessibility-menu.js"],
  TeX: { extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"] }
});</script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_SVG" async></script><script type="text/javascript" src="../arbital-demo-bundle.js"></script><script type="text/javascript">window.addEventListener('DOMContentLoaded', e=>window.loadAllDemos())
</script></head><body><header><h1 class="title">Difference Between Weights and Biases: Another way of Looking at Forward Propagation</h1><div class="page-info"><p class="metadata-link"><a href="../metadata/8r4.json.html">8r4.json</a></p><p class="arbital-url"><a href="https://arbital.com/p/8r4">https://arbital.com/p/8r4</a></p><p class="creator">by
 <a class="page-link" href="../page/AltoClef.html">Alto Clef</a> Oct 15 2017 
updated
 Oct 15 2017</p></div><p class="clickbait">My understanding on Forward Propagation</p><nav class="breadcrumbs"><nav class="breadcrumb" aria-label="Breadcrumb"><ul><li><a href="../index.html">Index</a></li><li>Difference Between Weights and Biases: Another way of Looking at Forward Propagation</li></ul></nav></nav></header><hr><main><h2 id="whatareweightsandbiases">What are Weights and Biases</h2>
<p>Consider the following forward propagation algorithm:
$$~$
\vec{y_{n}}=\mathbf{W_n}^T \times  \vec{y_{n-1}} + \vec{b_n}
$~$$
where $~$n$~$ is the number of the layers, $~$\vec{y_n}$~$ is the output of the $~$n^{th}$~$ layer, expressed as a $~$l_n \times 1$~$ ($~$l_n$~$ is the number of neurons of the $~$n^th$~$ layer) vector. $~$\mathbf{W_n}$~$ is a $~$l_{n-1} \times l_{n}$~$ matrix storing all the weights of every connection between layer $~$n$~$ and $~$n-1$~$, thus needing to be transposed for the sake of the product. $~$\vec{b_n}$~$, again, is the biases of the connections between the $~$n^th$~$ and $~$(n-1)^th$~$ layers, in the shape of $~$l_n\times1$~$.</p>
<p>As one can see, both weights and biases are just changeable and derivable(thus trainable) factors that contributes to the final results.</p>
<h2 id="whydoweneedbothofthemandwhyarebiasesoptional">Why do we need both of them, and why are Biases Optional?</h2>
<p>Neural network, indeed a better version of the perceptron model, where the output of each neuron(perceptron) owns a linear correlation with the output, rather than simply outputting plain 0/1. (This relation is further more projected to the activation function to make it non-linear, which will be discussed later) </p>
<p>To create a linear correlation, the easiest way is to scale the input with a certain coefficient $~$w$~$, output the scaled input. 
$$~$
f(x)=w\times x
$~$$</p>
<p>This model works alright, even with one neuron it could perfectly fit a linear function like $~$f(x)=m\times x$~$, and certain non-linear relations could be fit with neurons work in layers. </p>
<p>However, this new neuron without biases, lack of a significant ability even comparing to perceptron: it always fires regardless the input thus failing to fit functions like $~$y=mx+b$~$. It's impossible to disable the output of a specific neuron on certain threshold value of the input. Even that adding more layers and neurons a lot eases and hides this issue, neural networks without biases are likely to perform a worse job than those with biases.(Consider the total layers/neurons are the same)</p>
<p>In conclusion, the biases are supplements to the weights to help a network better fit the pattern, which are not necessary but helps the network to perform better. </p>
<h2 id="anotherwayofwritingtheforwardpropagation">Another way of writing the Forward Propagation</h2>
<p>Interestingly, the forward propagation algorithm 
$$~$
\vec{y_{n}}=\mathbf{W_n}^T \times  \vec{y_{n-1}} + 1 \times \vec{b_n}
$~$$
could also be written like this:
$$~$
\vec{y_{n}}=
\left[ \begin{array}{c}
                x, \\ 1
\end{array} \right]^T
\cdot
\left[ \begin{array}{c}
                \mathbf{W_n},
                \\ \vec{b_n}
\end{array} \right]
$~$$,which is
$$~$
\vec{y_{n}} = \vec{y_{new_{n-1}}}^T \times \vec{W_{new}} 
$~$$.
This is a way of rewriting the equation makes the adjustment by gradient really easy to write.</p>
<h2 id="howtoupdatethem">How to update them?</h2>
<p>It's super easy after the rewrite:
$$~$
\vec{W_{new}} =\vec{W_{new}}-\frac{\delta W_{new}}{\delta Error}
$~$$.</p>
<h2 id="theactivationfunction">The Activation Function</h2>
<p>There is one more compoment yet to be mentioned--the Activation Function. It's basically a function takes the output of a neuron as an input and output whatever value defined as the final output of the neuron.
$$~$
\vec{W_{new}} =Activation(\vec{W_{new}}-\frac{\delta W_{new}}{\delta Error})
$~$$
There are copious types of them around, but all of them have at least one shared property that there are all <em>Non-linear</em>! </p>
<p>That's basically what they are designed for. Activation Functions project output to a non-linear function, thus introducing non-linearity into the model. </p>
<p>Consider non-linear-seperatable problems like the the XOR problem, giving the network the ability to draw non-linear sperators may help the classification.</p>
<p>Also, there's another purpose of the activation function, which is to project a huge input, into the space between -1 and 1, thus making the followed-up calculations easier and faster. </p>
<p>2017/10/15</p></main><hr><footer><p class="likes"><h2>Likes</h2><span class="page-comma-list"><a class="page-link" href="../page/AltoClef.html">Alto Clef</a></span></p></footer></body></html>