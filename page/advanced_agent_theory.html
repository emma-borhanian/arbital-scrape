<!DOCTYPE html><html><head><meta charset="utf-8"><title>Theory of (advanced) agents</title><link rel="stylesheet" type="text/css" href="../common.css"><link rel="stylesheet" type="text/css" href="../page-style.css"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  SVG: {EqnChunk: 50, EqnChunkFactor: 1.5, EqChunkDelay: 10, useFontCache: false, linebreaks: {automatic: true}},
  tex2jax: {
    inlineMath: [['$~$', '$~$']],
    displayMath: [['$$~$', '$~$$']],
    processEscapes: true,
    preview: 'none',
  },
  showProcessingMessages: false,
  messageStyle: 'none',
  // http://docs.mathjax.org/en/latest/config-files.html#the-tex-ams-svg-configuration-file
  jax: ["input/TeX","output/SVG", "output/PreviewHTML"],
  extensions: ["tex2jax.js","MathMenu.js","MathZoom.js", "fast-preview.js", "AssistiveMML.js", "a11y/accessibility-menu.js"],
  TeX: { extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"] }
});</script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_SVG" async></script><script type="text/javascript" src="../arbital-demo-bundle.js"></script><script type="text/javascript">window.addEventListener('DOMContentLoaded', e=>window.loadAllDemos())
</script></head><body><header><h1 class="title">Theory of (advanced) agents</h1><div class="page-info"><p class="metadata-link"><a href="../metadata/advanced_agent_theory.json.html">advanced_agent_theory.json</a></p><p class="arbital-url"><a href="https://arbital.com/p/advanced_agent_theory">https://arbital.com/p/advanced_agent_theory</a></p><p class="creator">by
 <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a> Feb 17 2017</p></div><p class="clickbait">One of the research subproblems of building powerful nice AIs, is the theory of (sufficiently advanced) minds in general.</p><nav class="breadcrumbs"><nav class="breadcrumb" aria-label="Breadcrumb"><ul><li><a href="../index.html">Index</a></li><li>Theory of (advanced) agents</li></ul></nav><nav class="breadcrumb" aria-label="Breadcrumb"><ul><li><a href="ai_alignment.html">AI alignment</a></li><li>…</li></ul></nav></nav></header><hr><main><p>Many issues in AI alignment have dependencies on what we think we can factually say about the general design space of cognitively powerful agents, or on which background assumptions yield which implications about advanced agents.  E.g., the <a href="orthogonality.html">Orthogonality Thesis</a> is a claim about the general design space of powerful AIs.  The design space of advanced agents is very wide, and only very weak statements seem likely to be true about the <em>whole</em> design space; but we can still try to say 'If X then Y' and refute claims about 'No need for if-X, Y happens anyway!'</p></main><hr><footer><p class="tagged"><h2>Tagged</h2><span class="page-comma-list"><a class="page-link" href="../page/stub_meta_tag.html">Stub</a></span></p><p class="reverse-related"><h2>Reverse Related</h2><ul class="page-list"><li><a class="page-link" href="../page/stub_meta_tag.html">Stub</a> <q>This page only gives a very brief overview of the topic. If you're able to, please help expand or improve it!</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li></ul></p><p class="children"><h2>Children</h2><ul class="page-tree"><li><a class="page-link" href="../page/advanced_agent.html">Advanced agent properties</a> <q>How smart does a machine intelligence need to be, for its niceness to become an issue?  &quot;Advanced&quot; is a broad term to cover cognitive abilities such that we'd need to start considering AI alignment.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a><ul class="page-tree"><li><a class="page-link" href="../page/advanced_nonagent.html">Advanced nonagent</a> <q>Hypothetically, cognitively powerful programs that don't follow the loop of &quot;observe, learn, model the consequences, act, observe results&quot; that a standard &quot;agent&quot; would.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/agi.html">Artificial General Intelligence</a> <q>An AI which has the same kind of &quot;significantly more general&quot; intelligence that humans have compared to chimpanzees; it can learn new domains, like we can.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/big_picture_awareness.html">Big-picture strategic awareness</a> <q>We start encountering new AI alignment issues at the point where a machine intelligence recognizes the existence of a real world, the existence of programmers, and how these relate to its goals.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/uncontainability.html">Cognitive uncontainability</a> <q>'Cognitive uncontainability' is when we can't hold all of an agent's possibilities inside our own minds.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a><ul class="page-tree"><li><a class="page-link" href="../page/rich_domain.html">Rich domain</a> <q>A domain is 'rich', relative to our own intelligence, to the extent that (1) its [ search space] is …</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a><ul class="page-tree"><li><a class="page-link" href="../page/real_is_rich.html">Almost all real-world domains are rich</a> <q>Anything you're trying to accomplish in the real world can potentially be accomplished in a *lot* of different ways.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/logical_game.html">Logical game</a> <q>Game's mathematical structure at its purest form.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li></ul></li></ul></li><li><a class="page-link" href="../page/consequentialist.html">Consequentialist cognition</a> <q>The cognitive ability to foresee the consequences of actions, prefer some outcomes to others, and output actions leading to the preferred outcomes.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/corps_vs_si.html">Corporations vs. superintelligences</a> <q>Corporations have relatively few of the advanced-agent properties that would allow one mistake in aligning a corporation to immediately kill all humans and turn the future light cone into paperclips.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/efficiency.html">Epistemic and instrumental efficiency</a> <q>An efficient agent never makes a mistake you can predict.  You can never successfully predict a directional bias in its estimates.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a><ul class="page-tree"><li><a class="page-link" href="../page/timemachine_efficiency_metaphor.html">Time-machine metaphor for efficient agents</a> <q>Don't imagine a paperclip maximizer as a mind.  Imagine it as a time machine that always spits out the output leading to the greatest number of future paperclips.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li></ul></li><li><a class="page-link" href="../page/general_intelligence.html">General intelligence</a> <q>Compared to chimpanzees, humans seem to be able to learn a much wider variety of domains.  We have 'significantly more generally applicable' cognitive abilities, aka 'more general intelligence'.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/relative_ability.html">Infrahuman, par-human, superhuman, efficient, optimal</a> <q>A categorization of AI ability levels relative to human, with some gotchas in the ordering.  E.g., in simple domains where humans can play optimally, optimal play is not superhuman.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/intelligence_explosion.html">Intelligence explosion</a> <q>What happens if a self-improving AI gets to the point where each amount x of self-improvement triggers &gt;x further self-improvement, and it stays that way for a while.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/real_world.html">Real-world domain</a> <q>Some AIs play chess, some AIs play Go, some AIs drive cars.  These different 'domains' present different options.  All of reality, in all its messy entanglement, is the 'real-world domain'.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/standard_agent.html">Standard agent properties</a> <q>What's a Standard Agent, and what can it do?</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a><ul class="page-tree"><li><a class="page-link" href="../page/bounded_agent.html">Bounded agent</a> <q>An agent that operates in the real world, using realistic amounts of computing power, that is uncertain of its environment, etcetera.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li></ul></li><li><a class="page-link" href="../page/sufficiently_advanced_ai.html">Sufficiently advanced Artificial Intelligence</a> <q>'Sufficiently advanced Artificial Intelligences' are AIs with enough 'advanced agent properties' that we start needing to do 'AI alignment' to them.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/superintelligent.html">Superintelligent</a> <q>A &quot;superintelligence&quot; is strongly superhuman (strictly higher-performing than any and all humans) on every cognitive problem.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/Vingean_uncertainty.html">Vingean uncertainty</a> <q>You can't predict the exact actions of an agent smarter than you - so is there anything you _can_ say about them?</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a><ul class="page-tree"><li><a class="page-link" href="../page/deep_blue.html">Deep Blue</a> <q>The chess-playing program, built by IBM, that first won the world chess championship from Garry Kasparov in 1996.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/Vinge_law.html">Vinge's Law</a> <q>You can't predict exactly what someone smarter than you would do, because if you could, you'd be that smart yourself.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li></ul></li></ul></li><li><a class="page-link" href="../page/instrumental_convergence.html">Instrumental convergence</a> <q>Some strategies can help achieve most possible simple goals.  E.g., acquiring more computing power or more material resources.  By default, unless averted, we can expect advanced AIs to do that.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a><ul class="page-tree"><li><a class="page-link" href="../page/convergent_strategies.html">Convergent instrumental strategies</a> <q>Paperclip maximizers can make more paperclips by improving their cognitive abilities or controlling more resources.  What other strategies would almost-any AI try to use?</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a><ul class="page-tree"><li><a class="page-link" href="../page/preference_stability.html">Consequentialist preferences are reflectively stable by default</a> <q>Gandhi wouldn't take a pill that made him want to kill people, because he knows in that case more people will be murdered.  A paperclip maximizer doesn't want to stop maximizing paperclips.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/convergent_self_modification.html">Convergent strategies of self-modification</a> <q>The strategies we'd expect to be employed by an AI that understands the relevance of its code and hardware to achieving its goals, which therefore has subgoals about its code and hardware.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li></ul></li><li><a class="page-link" href="../page/instrumental.html">Instrumental</a> <q>What is &quot;instrumental&quot; in the context of Value Alignment Theory?</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/instrumental_pressure.html">Instrumental pressure</a> <q>A consequentialist agent will want to bring about certain instrumental events that will help to fulfill its goals.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/paperclip_maximizer.html">Paperclip maximizer</a> <q>This agent will not stop until the entire universe is filled with paperclips.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a><ul class="page-tree"><li><a class="page-link" href="../page/paperclip.html">Paperclip</a> <q>A configuration of matter that we'd see as being worthless even from a very cosmopolitan perspective.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/random_utility_function.html">Random utility function</a> <q>A 'random' utility function is one chosen at random according to some simple probability measure (e.g. weight by Kolmorogov complexity) on a logical space of formal utility functions.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li></ul></li><li><a class="page-link" href="../page/not_more_paperclips.html">You can't get more paperclips that way</a> <q>Most arguments that &quot;A paperclip maximizer could get more paperclips by (doing nice things)&quot; are flawed.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li></ul></li><li><a class="page-link" href="../page/orthogonality.html">Orthogonality Thesis</a> <q>Will smart AIs automatically become benevolent, or automatically become hostile?  Or do different AI designs imply different goals?</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a><ul class="page-tree"><li><a class="page-link" href="../page/instrumental_goals_equally_tractable.html">Instrumental goals are almost-equally as tractable as terminal goals</a> <q>Getting the milk from the refrigerator because you want to drink it, is not vastly harder than getting the milk from the refrigerator because you inherently desire it.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/mind_design_space_wide.html">Mind design space is wide</a> <q>Imagine all human beings as one tiny dot inside a much vaster sphere of possibilities for &quot;The space of minds in general.&quot;  It is wiser to make claims about *some* minds than *all* minds.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/paperclip_maximizer.html">Paperclip maximizer</a> <q>This agent will not stop until the entire universe is filled with paperclips.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a><ul class="page-tree"><li><a class="page-link" href="../page/paperclip.html">Paperclip</a> <q>A configuration of matter that we'd see as being worthless even from a very cosmopolitan perspective.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/random_utility_function.html">Random utility function</a> <q>A 'random' utility function is one chosen at random according to some simple probability measure (e.g. weight by Kolmorogov complexity) on a logical space of formal utility functions.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li></ul></li></ul></li></ul></p></footer></body></html>