<!DOCTYPE html><html><head><meta charset="utf-8"><title>Informed oversight</title><link rel="stylesheet" type="text/css" href="../common.css"><link rel="stylesheet" type="text/css" href="../page-style.css"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  SVG: {EqnChunk: 50, EqnChunkFactor: 1.5, EqChunkDelay: 10, useFontCache: false, linebreaks: {automatic: true}},
  tex2jax: {
    inlineMath: [['$~$', '$~$']],
    displayMath: [['$$~$', '$~$$']],
    processEscapes: true,
    preview: 'none',
  },
  showProcessingMessages: false,
  messageStyle: 'none',
  // http://docs.mathjax.org/en/latest/config-files.html#the-tex-ams-svg-configuration-file
  jax: ["input/TeX","output/SVG", "output/PreviewHTML"],
  extensions: ["tex2jax.js","MathMenu.js","MathZoom.js", "fast-preview.js", "AssistiveMML.js", "a11y/accessibility-menu.js"],
  TeX: { extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"] }
});</script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_SVG" async></script><script type="text/javascript" src="../arbital-demo-bundle.js"></script><script type="text/javascript">window.addEventListener('DOMContentLoaded', e=>window.loadAllDemos())
</script></head><body><header><h1 class="title">Informed oversight</h1><div class="page-info"><p class="metadata-link"><a href="../metadata/informed_oversight.json.html">informed_oversight.json</a></p><p class="arbital-url"><a href="https://arbital.com/p/informed_oversight">https://arbital.com/p/informed_oversight</a></p><p class="creator">by
 <a class="page-link" href="../page/JessicaTaylor.html">Jessica Taylor</a> Mar 23 2016 
updated
 Mar 24 2016</p></div><p class="clickbait">Incentivize a reinforcement learner that's less smart than you to accomplish some task</p><nav class="breadcrumbs"><nav class="breadcrumb" aria-label="Breadcrumb"><ul><li><a href="../index.html">Index</a></li><li>Informed oversight</li></ul></nav><nav class="breadcrumb" aria-label="Breadcrumb"><ul><li><a href="ai_alignment.html">AI alignment</a></li><li>â€¦</li></ul></nav></nav></header><hr><main><p>It can be difficult for one agent to oversee another agent that is maximizing a reward controlled by the first agent, even if the first agent is smarter than the second agent.  This is a serious obstacle to <a href="approval_directed_agents.html">Approval directed agents</a> and other approaches based on reinforcement learning.</p>
<p>More formally, suppose a strong agent B with a utility function U is overseeing a weak agent A.  Agent A produces an action y and gets a reward B(y) determined by agent B.  B wants to give rewards such that agent A is incentivized to make its best effort at maximizing U(a).  </p>
<p>If B(y) is an <a href="efficiency.html">efficient</a> estimate of U(y) relative to A, then indeed maximizing B(y) is equivalent to maximizing U(y) from A's perspective.  Unfortunately, it is difficult for this to be the case even when B is smarter than A, because A may think thoughts B does not in the course of selecting y.  Here are a couple concrete examples (each having the flavor of "easy to generate a solution, hard to check a solution"):</p>
<ul>
<li>Suppose A is outputting a partial Sudoku solutions.  U assigns a high value to partial Sudoku solutions consistent with some solution to the entire Sudoku problem.  A comes up with some solution to the Sudoku puzzle and outputs part of this solution.  B cannot tell that this partial solution is actually part of a full solution, even though B is smarter than A.  (B is able to come up with its own solutions but most aren't consistent with A's partial solution).</li>
<li>Suppose U assigns a high value to cat pictures that don't contain steganographic messages.  B has no way of distinguishing cat pictures that contain steganographic messages from ones that don't.  Thus, B has to give all cat pictures the same score, regardless of whether the pictures contain steganographic messages.  This either fails to reward A for producing cat pictures not containing steganographic messages, or rewards A for producing cat pictures containing steganographic messages.</li>
</ul>
<p>Here are a couple analogous examples that seem easier to analyze:</p>
<ol>
<li>Let O be a one-way function (e.g. SHA256).  U assigns a high value to binary strings y equal to O(111x) for some x.  B cannot tell whether a particular string y is equal to O(111x) for some x (even if B is smarter than A), so B has no good way of giving rewards to A.  </li>
<li>Same as the previous problem, except U assigns a high value to binary strings y that are <em>not</em> equal to O(111x) for any short string x.  A's intended strategy is to generate a uniformly random string (or generate a string in some other way that doesn't introduce a bias towards generating strings of the form O(111x)), but B has no way of knowing that A's string was uniformly random (rather than equal to O(111x) for a uniformly random x).</li>
</ol>
<p>One attempted solution is for agent B to see A's thought processes in addition to the action y.  Then, agent B can use all the information A has, so that B(y) is an efficient estimate of U(y) from the perspective of A.  However, this requires B to understand something about A's mental architecture.  It would be good to have a more generic solution that requires only weak assumptions about A's mental architecture.</p>
<p>An attempted solution to problem 2 is <a href="https://agentfoundations.org/item?id=700">for B to incentivize A to generate its action in a provably random manner</a>.  However, this doesn't appear to generalize well to problems significantly different from 2.</p>
<h1 id="furtherreading">Further reading</h1>
<p><a href="https://medium.com/ai-control/adequate-oversight-25fadf1edce9#.bv5lq13fw">Paul Christiano on adequate oversight</a></p>
<p><a href="https://medium.com/ai-control/the-informed-oversight-problem-1b51b4f66b35#.o82tjmhll">Paul Christiano on the informed oversight problem</a></p></main><hr><section class="comments"><h2>Comments</h2><div class="comment"><p><a class="page-link" href="../page/PaulChristiano.html">Paul Christiano</a></p><p><p>In the sudoku and first OWF example, the agent can justify their answer, and its easy to incentivize them to reveal it. In the steganography and second OWF example, there is no short proof that something is good, only a proof that something is bad. In realistic settings there will be lots of arguments on both sides. Another way of looking at the question is: how do you elicit the negative arguments?</p>
<p>Katja wrote about a scheme <a href="https://meteuphoric.wordpress.com/2014/07/21/how-to-buy-a-truth-from-a-liar">here</a>. I think it's a nice idea that feels like it might be relevant. But if you include it as part of the agent's reward, and the agent also picks the action, then you get actions optimized to be info-rich (as discussed in "maximizing B + info" <a href="https://medium.com/ai-control/the-informed-oversight-problem-1b51b4f66b35#.sbod7hmj1">here</a>).</p></p></div></section><footer><p class="tagged"><h2>Tagged</h2><span class="page-comma-list"><a class="page-link" href="../page/taskagi_open_problems.html">Open subproblems in aligning a Task-based AGI</a></span></p><p class="likes"><h2>Likes</h2><span class="page-comma-list"><a class="page-link" href="../page/PatrickLaVictoir.html">Patrick LaVictoire</a></span></p><p class="reverse-related"><h2>Reverse Related</h2><ul class="page-list"><li><a class="page-link" href="../page/taskagi_open_problems.html">Open subproblems in aligning a Task-based AGI</a> <q>Open research problems, especially ones we can model today, in building an AGI that can &quot;paint all cars pink&quot; without turning its future light cone into pink-painted cars.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li></ul></p></footer></body></html>