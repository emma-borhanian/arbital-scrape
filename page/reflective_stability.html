<!DOCTYPE html><html><head><meta charset="utf-8"><title>Reflective stability</title><link rel="stylesheet" type="text/css" href="../common.css"><link rel="stylesheet" type="text/css" href="../page-style.css"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  SVG: {EqnChunk: 50, EqnChunkFactor: 1.5, EqChunkDelay: 10, useFontCache: false, linebreaks: {automatic: true}},
  tex2jax: {
    inlineMath: [['$~$', '$~$']],
    displayMath: [['$$~$', '$~$$']],
    processEscapes: true,
    preview: 'none',
  },
  showProcessingMessages: false,
  messageStyle: 'none',
  // http://docs.mathjax.org/en/latest/config-files.html#the-tex-ams-svg-configuration-file
  jax: ["input/TeX","output/SVG", "output/PreviewHTML"],
  extensions: ["tex2jax.js","MathMenu.js","MathZoom.js", "fast-preview.js", "AssistiveMML.js", "a11y/accessibility-menu.js"],
  TeX: { extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"] }
});</script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_SVG" async></script><script type="text/javascript" src="../arbital-demo-bundle.js"></script><script type="text/javascript">window.addEventListener('DOMContentLoaded', e=>window.loadAllDemos())
</script></head><body><header><h1 class="title">Reflective stability</h1><div class="page-info"><p class="metadata-link"><a href="../metadata/reflective_stability.json.html">reflective_stability.json</a></p><p class="arbital-url"><a href="https://arbital.com/p/reflective_stability">https://arbital.com/p/reflective_stability</a></p><p class="creator">by
 <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a> Dec 28 2015 
updated
 May 21 2016</p></div><p class="clickbait">Wanting to think the way you currently think, building other agents and self-modifications that think the same way.</p><nav class="breadcrumbs"><nav class="breadcrumb" aria-label="Breadcrumb"><ul><li><a href="../index.html">Index</a></li><li>Reflective stability</li></ul></nav><nav class="breadcrumb" aria-label="Breadcrumb"><ul><li><a href="ai_alignment.html">AI alignment</a></li><li><a href="Vingean_reflection.html">Vingean reflection</a></li><li>â€¦</li></ul></nav></nav></header><hr><main><p>[summary:  An agent is "reflectively stable" in some regard if it only self-modifies (or constructs successors) to think similarly in that regard.  For example,  <a href="paperclip_maximizer.html">an agent with a utility function that only values paperclips</a> will construct successors that only value paperclips, so having a paperclip utility function is "reflectively stable" (and [goals_reflectively_stable so are most other utility functions]).  Contrast "<a href="reflective_consistency.html">reflectively consistent</a>".]</p>
<p>An <a href="advanced_agent.html">agent</a> is "reflectively stable" in some regard, if having a choice of how to construct a successor agent or modify its own code, the agent will <em>only</em> construct a successor that thinks similarly in that regard.</p>
<ul>
<li>In <a href="tiling_agents.html">tiling agent theory</a>, an [ expected utility satisficer] is reflectively <em>consistent</em>, since it will approve of building another EU satisficer, but an EU satisficer is not reflectively <em>stable</em>, since it may also approve of building an expected utility maximizer (it expects the consequences of building the maximizer to satisfice).</li>
<li>Having a <a href="utility_function.html">utility function</a> that <a href="paperclip_maximizer.html">only weighs paperclips</a> is "reflectively stable" because paperclip maximizers <em>only</em> try to build other paperclip maximizers.</li>
</ul>
<p>If, thinking the way you currently do (in some regard), it seems unacceptable to not think that way (in that regard), then you are reflectively stable (in that regard).</p>
<p>[todo: untangle possible confusion about reflective stability not being "good" and wanting reflectively unstable agents because it seems bad to them if a paperclip maximizer stays a paperclip maximizer, or they imagine causal decision theorists building something incrementally saner than casual decision theorists.]</p></main><hr><footer><p class="tagged"><h2>Tagged</h2><span class="page-comma-list"><a class="page-link" href="../page/stub_meta_tag.html">Stub</a></span></p><p class="likes"><h2>Likes</h2><span class="page-comma-list"><a class="page-link" href="../page/MYass.html">M Yass</a>,
 <a class="page-link" href="../page/TomEveritt.html">Tom Everitt</a></span></p><p class="reverse-related"><h2>Reverse Related</h2><ul class="page-list"><li><a class="page-link" href="../page/stub_meta_tag.html">Stub</a> <q>This page only gives a very brief overview of the topic. If you're able to, please help expand or improve it!</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li></ul></p><p class="children"><h2>Children</h2><ul class="page-tree"><li><a class="page-link" href="../page/preference_stability.html">Consequentialist preferences are reflectively stable by default</a> <q>Gandhi wouldn't take a pill that made him want to kill people, because he knows in that case more people will be murdered.  A paperclip maximizer doesn't want to stop maximizing paperclips.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/otherizer.html">Other-izing (wanted: new optimization idiom)</a> <q>Maximization isn't possible for bounded agents, and satisficing doesn't seem like enough.  What other kind of 'izing' might be good for realistic, bounded agents?</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/reflective_degree_of_freedom.html">Reflectively consistent degree of freedom</a> <q>When an instrumentally efficient, self-modifying AI can be like X or like X' in such a way that X wants to be X and X' wants to be X', that's a reflectively consistent degree of freedom.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a><ul class="page-tree"><li><a class="page-link" href="../page/humean_free_boundary.html">Humean degree of freedom</a> <q>A concept includes 'Humean degrees of freedom' when the intuitive borders of the human version of that concept depend on our values, making that concept less natural for AIs to learn.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/value_laden.html">Value-laden</a> <q>Cure cancer, but avoid any bad side effects?  Categorizing &quot;bad side effects&quot; requires knowing what's &quot;bad&quot;.  If an agent needs to load complex human goals to evaluate something, it's &quot;value-laden&quot;.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li></ul></li></ul></p></footer></body></html>