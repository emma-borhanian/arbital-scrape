<!DOCTYPE html><html><head><meta charset="utf-8"><title>&quot;It seems critical to distin...&quot;</title><link rel="stylesheet" type="text/css" href="../common.css"><link rel="stylesheet" type="text/css" href="../page-style.css"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  SVG: {EqnChunk: 50, EqnChunkFactor: 1.5, EqChunkDelay: 10, useFontCache: false, linebreaks: {automatic: true}},
  tex2jax: {
    inlineMath: [['$~$', '$~$']],
    displayMath: [['$$~$', '$~$$']],
    processEscapes: true,
    preview: 'none',
  },
  showProcessingMessages: false,
  messageStyle: 'none',
  // http://docs.mathjax.org/en/latest/config-files.html#the-tex-ams-svg-configuration-file
  jax: ["input/TeX","output/SVG", "output/PreviewHTML"],
  extensions: ["tex2jax.js","MathMenu.js","MathZoom.js", "fast-preview.js", "AssistiveMML.js", "a11y/accessibility-menu.js"],
  TeX: { extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"] }
});</script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_SVG" async></script><script type="text/javascript" src="../arbital-demo-bundle.js"></script><script type="text/javascript">window.addEventListener('DOMContentLoaded', e=>window.loadAllDemos())
</script></head><body><header><h1 class="title">&quot;It seems critical to distin...&quot;</h1><div class="page-info"><p class="metadata-link"><a href="../metadata/2ql.json.html">2ql.json</a></p><p class="arbital-url"><a href="https://arbital.com/p/2ql">https://arbital.com/p/2ql</a></p><p class="creator">by
 <a class="page-link" href="../page/PaulChristiano.html">Paul Christiano</a> Mar 19 2016</p></div><nav class="breadcrumbs"><nav class="breadcrumb" aria-label="Breadcrumb"><ul><li><a href="../index.html">Index</a></li><li>&quot;It seems critical to distin...&quot;</li></ul></nav><nav class="breadcrumb" aria-label="Breadcrumb"><ul><li><a href="ai_alignment.html">AI alignment</a></li><li><a href="AGI_typology.html">Strategic AGI typology</a></li><li><a href="task_agi.html">Task-directed AGI</a></li><li><a href="taskagi_open_problems.html">Open subproblems in aligning a Task-based AGI</a></li><li>…</li></ul></nav><nav class="breadcrumb" aria-label="Breadcrumb"><ul><li><a href="ai_alignment.html">AI alignment</a></li><li><a href="task_agi.html">Task-directed AGI</a></li><li><a href="taskagi_open_problems.html">Open subproblems in aligning a Task-based AGI</a></li><li>…</li></ul></nav></nav></header><hr><main><blockquote class="comment-context">To put it another way, the task is to have the AI generate a safe burrito\.  <mark>One way to try to do this is making sure that the AI's explicit training data contains a burrito with butolinum toxin, labeled as a negative example, so that the AI knows not to include butolinum\.  The hope is that via conservatism we can avoid needing to think of every possible way that our training data might not properly stabilize the 'simplest explanation' along every dimension of potentially fatal variance, and shift some of the workload to just showing the AI positive examples which happen not to contain butolinum toxin\.</mark></blockquote>
<p>It seems critical to distinguish the cases where</p>
<ol>
<li>We are hoping the AI generalizes the concept of "burrito" in the intended way to new data,</li>
<li>The definition of burrito is "something our burrito-identifier would identify as a burrito given enough time," and we are just hoping the AI doesn't make mistakes. (The burrito-identifier is some process that we can actually run in order to determine whether something is a burrito.)</li>
</ol>
<p>As you've probably gathered, I feel hopeless about case (1).</p>
<p>In case (2), any agent that can learn the concept "definitely a burrito" could use this concept to produce definitely-burritos and thereby achieve high reward in the RL game. So the mere existence of the easy-to-learn definitely-a-burrito concept seems to imply that our learner will behave well. We don't have to actually explicitly do any work about conservative concepts (except to better understand the behavior of our learner).</p>
<p>I've never managed to get quite clear on your picture. My impression is that:</p>
<ul>
<li>you think that case (2) is doomed because there is no realistic prospect for creating a good enough burrito-evaluator, </li>
<li>you think that even with a good enough burrito-evaluator, you would still have serious trouble because of errors.</li>
</ul>
<p>I think your optimism about case (1) is defensible; I disagree, but not for super straightforward reasons.  The main disagreement is probably about case (2).</p>
<p>I think that your concern about generating a good enough burrito-evaluator is also defensible; I am optimistic, but even on my view this would require resolving a number of big research problems.</p>
<p>I think your concern about mistakes, and especially about something like "conservative concepts" as a way to reduce the scope for mistakes, is less defensible. I don't feel like this is as complex an issue---the case for delegating this to the learning algorithm seems quite strong, and I don't feel you've really given a case on the other side.</p>
<p>Note that this is related to what you've been calling <a href="inductive_ambiguity.html">Identifying ambiguous inductions</a>, and I do think that there are techniques in that space that could help avoid mistakes. (Though I would definitely frame that problem differently.) So it's possible we're not really disagreeing here either. But my best guess is that you are underestimating to the extent to which some of these issues could/should be delegated to the learner itself, supposing that we could resolve your other concerns (i.e. supposing that we could construct a good enough burrito-evaluator).</p></main><hr><section class="comments"><h2>Comments</h2><div class="comment"><p><a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></p><p><blockquote>
  <p>As you've probably gathered, I feel hopeless about case (1).</p>
</blockquote>
<p>Okay, I didn't understand this.  My reaction was something like "Isn't conservatively generalizing burritos from sample burritos a much simpler problem than defining an ideal criterion for burritos which probably requires something like an ideal advisor theory over extrapolated humans to talk about all the poisons that people could detect given enough computing power?" but I think I should maybe just ask you to clarify what you mean.  The interpretation my brain generated was something like "Predicting a human 9p's Go moves is easier than generating 9p-level Go moves" which seems clearly false to me so I probably misunderstood you.</p>
<blockquote>
  <p>In case (2), any agent that can learn the concept "definitely a burrito" could use this concept to produce definitely-burritos and thereby achieve high reward in the RL game. So the mere existence of the easy-to-learn definitely-a-burrito concept seems to imply that our learner will behave well. We don't have to actually explicitly do any work about conservative concepts (except to better understand the behavior of our learner).</p>
</blockquote>
<p>I don't understand this at all.  Are we supposing that we have an inviolable physical machine that outputs burrito ratings and can't be shorted by seizing control of the reward channel or by including poisons that the machine-builders didn't know about?  …actually I should just ask you to clarify this paragraph.</p></p></div><div class="comment"><p><a class="page-link" href="../page/PaulChristiano.html">Paul Christiano</a></p><p><p>I think the <a href="https://medium.com/ai-control/the-informed-oversight-problem-1b51b4f66b35#.lqcjngn6w">key question</a> is whether:</p>
<ol>
<li>the burrito judge needs to be extremely powerful, or</li>
<li>the burrito judge needs to be modestly more powerful than the burrito producer.</li>
</ol>
<p>In world 1 I agree that the burrito-evaluator seems pretty tough to build. We certainly have disagreements about that case, but I'm happy to set it aside for now.</p>
<p>In world 2 things seem much less scary. Because <a href="https://medium.com/ai-control/counterfactual-human-in-the-loop-a7822e36f399">I only need to run these evaluations with e.g. 1% probability</a>, the judge can use 50x more resources than the burrito producer. So it's imaginable that the judge can be more powerful than the producer.</p>
<p>You seem to think that we are in world 1. I think that we are probably in world 2, but I'm certainly not sure. I discuss the issue in <a href="https://medium.com/ai-control/the-informed-oversight-problem-1b51b4f66b35#.lqcjngn6w">this post</a>.</p>
<p>Some observations:</p>
<ul>
<li>The judge's job is easier if they are evaluating steps of the plan, before those steps are taken, rather than actually letting the burrito producer take actions. So let's do it that way.</li>
<li>The judge can look at the burrito producer's computation, and at the training process that produced that computation, and can change the burrito producer's training procedure to make that computation more understandable.</li>
<li>If the judge were epistemically efficient with respect to the producer, then maximizing the judge's expectation of a burrito's quality would be the same as maximizing the burrito producer's expectation of a burrito's quality. That's basically what we want. So the real issue is narrower than you might expect, it's some kind of epistemic version of "offense vs. defense," where the producer can think particular thoughts that the judge doesn't happen to think, and so the producer might expect to be able to deceive/attack the judge even though the judge is smarter. This is what the judge is trying to avoid by looking at the producer's computation.</li>
</ul>
<p>So I don't think that we can just ask the judge to evaluate the burrito; but the judge has enough going for her that I expect we can find some strategy that lets her win. I think this is the biggest open problem for my current approach.</p></p></div></section><footer></footer></body></html>