<!DOCTYPE html><html><head><meta charset="utf-8"><title>Goal-concept identification</title><link rel="stylesheet" type="text/css" href="../common.css"><link rel="stylesheet" type="text/css" href="../page-style.css"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  SVG: {EqnChunk: 50, EqnChunkFactor: 1.5, EqChunkDelay: 10, useFontCache: false, linebreaks: {automatic: true}},
  tex2jax: {
    inlineMath: [['$~$', '$~$']],
    displayMath: [['$$~$', '$~$$']],
    processEscapes: true,
    preview: 'none',
  },
  showProcessingMessages: false,
  messageStyle: 'none',
  // http://docs.mathjax.org/en/latest/config-files.html#the-tex-ams-svg-configuration-file
  jax: ["input/TeX","output/SVG", "output/PreviewHTML"],
  extensions: ["tex2jax.js","MathMenu.js","MathZoom.js", "fast-preview.js", "AssistiveMML.js", "a11y/accessibility-menu.js"],
  TeX: { extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"] }
});</script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_SVG" async></script><script type="text/javascript" src="../arbital-demo-bundle.js"></script><script type="text/javascript">window.addEventListener('DOMContentLoaded', e=>window.loadAllDemos())
</script></head><body><header><h1 class="title">Goal-concept identification</h1><div class="page-info"><p class="metadata-link"><a href="../metadata/identify_goal_concept.json.html">identify_goal_concept.json</a></p><p class="arbital-url"><a href="https://arbital.com/p/identify_goal_concept">https://arbital.com/p/identify_goal_concept</a></p><p class="creator">by
 <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a> Apr 14 2016</p></div><p class="clickbait">Figuring out how to say &quot;strawberry&quot; to an AI that you want to bring you strawberries (and not fake plastic strawberries, either).</p><nav class="breadcrumbs"><nav class="breadcrumb" aria-label="Breadcrumb"><ul><li><a href="../index.html">Index</a></li><li>Goal-concept identification</li></ul></nav><nav class="breadcrumb" aria-label="Breadcrumb"><ul><li><a href="ai_alignment.html">AI alignment</a></li><li><a href="value_identification.html">Value identification problem</a></li><li>…</li></ul></nav></nav></header><hr><main><p>[summary:  The problem of communicating to an AI a very simple local concept on the order of "strawberries" or "give me a strawberry".  This level of the problem is meant to include subproblems of local categorization like "I meant a real strawberry like the ones I already showed you, not a fake strawberry that looks similar to your webcam".  It isn't meant to include larger problems like <a href="safe_plan_identification.html">verifying that a plan uses only known, whitelisted methods</a> or <a href="value_identification.html">identifying all possible harmful effects we could care about</a>.]</p>
<p>The problem of trying to figure out how to communicate to an AGI an <a href="intended_goal.html">intended</a> goal [ concept] on the order of "give me a strawberry, and not a fake plastic strawberry either".</p>
<p>At this level of the problem, we're not concerned with e.g. larger problems of <a href="safe_plan_identification.html">safe plan identification</a> such as not mugging people for strawberries, or <a href="low_impact.html">minimizing side effects</a>.  We're not (at this level of the problem) concerned with identifying <a href="value_identification.html">each and every one of the components of human value</a>, as they might be impacted by side effects more distant in the causal graph.  We're not concerned with [ philosophical uncertainty] about what we [normativity should] mean by "strawberry".  We suppose that in an intuitive sense, we do have a pretty good idea of what we <a href="intended_goal.html">intend</a> by "strawberry", such that there are things that are definitely strawberries and we're pretty happy with our sense of that so long as nobody is deliberately trying to fool it.</p>
<p>We just want to communicate a <em>local</em> goal concept that distinguishes edible strawberries from plastic strawberries, or nontoxic strawberries from poisonous strawberries.  That is: we want to say "strawberry" in an understandable way that's suitable for fulfilling a <a href="task_agi.html">task</a> of "just give Sally a strawberry", possibly in conjunction with other features like <a href="conservative_concept.html">conservatism</a> or <a href="low_impact.html">low impact</a> or <a href="soft_optimizer.html">mild optimization</a>.</p>
<p>For some open subproblems of the obvious approach that goes through showing actual strawberries to the AI's webcam, see "<a href="identify_causal_goals.html">Identifying causal goal concepts from sensory data</a>" and "<a href="pointing_finger.html">Look where I&#39;m pointing, not at my finger</a>".</p></main><hr><section class="comments"><h2>Comments</h2><div class="comment"><p><a class="page-link" href="../page/PaulChristiano.html">Paul Christiano</a></p><p><p>I think it's going to be hard to talk or think clearly about these problems (even at the level of separating them into distinct problems or telling which are real problems) until we get more specific about what a goal is, what a concept is, etc. What does the overall system actually look like, even very roughly?</p>
<p>I guess your take is that this is tied up in a very hard-to-separate way from the design of AI itself.</p>
<p>I understand that it is good to throw out some concrete problems before embarking on the project of clarifying our models of powerful AI systems. But I suspect you need at least <em>some</em> model of a powerful AI system where the questions make sense, just to keep things vaguely on track.</p></p><div class="comment"><p><a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></p><p><p>Questions like these seem to me to have obvious <a href="unbounded_analysis.html">unbounded</a> formulations.  If we're talking about a modern policy-reinforcement neural network, then yes, the notion of a separable goal is more ephemeral.  Does this seem to agree with your own state of mind, or would you disagree that we understand the notion of 'goal concepts' in unbounded formulations, or…?</p>
<p>A concept is something that discretely or fuzzily classifies states of the world, or states of a slice through the world, into positive or negative instances.  A "goal concept", for a satisficing agent, then describes the set of worlds that it's trying to steer us into.  The more general version of this is a utility function.</p></p></div><div class="comment"><p><a class="page-link" href="../page/PaulChristiano.html">Paul Christiano</a></p><p><p>On this definition, what is the difference between "communicating a goal concept" and "communicating a goal"?</p>
<p>Is the problem in this post equivalent to the special case of value learning where the values to be learned are simple (to us), local, and philosophically unproblematic?</p></p></div><div class="comment"><p><a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></p><p><p>I think we're going to have to specialize the terminology so we have separate words for "learn any goal concept" and "learn human normativity" instead of calling these both "value", which is something I'm currently trying to think how to revise.  But if by value learning you mean "outcome-preference-criterion learning" and not <a href="value_alignment_value.html">value</a> learning, then yes, we're looking for outcome-preference-criterion learning where the criterion seems simple to us, is hopefully local, and is philosophically unproblematic by our own standards.  Like, say, having the outcome be one in which we just have a damn strawberry.</p>
<blockquote>
  <p>On this definition, what is the difference between "communicating a goal concept" and "communicating a goal"?</p>
</blockquote>
<p>In the language being used here, it sounds to me like "communicating a goal" should parse to "communicating a goal concept to an agent which will then optimize for the outcome-preference-criterion you're about to communicate to it."</p></p></div></div></section><footer><p class="tagged"><h2>Tagged</h2><span class="page-comma-list"><a class="page-link" href="../page/stub_meta_tag.html">Stub</a></span></p><p class="reverse-related"><h2>Reverse Related</h2><ul class="page-list"><li><a class="page-link" href="../page/stub_meta_tag.html">Stub</a> <q>This page only gives a very brief overview of the topic. If you're able to, please help expand or improve it!</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li></ul></p></footer></body></html>