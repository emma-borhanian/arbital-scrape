<!DOCTYPE html><html><head><meta charset="utf-8"><title>Logical decision theories</title><link rel="stylesheet" type="text/css" href="../common.css"><link rel="stylesheet" type="text/css" href="../page-style.css"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  SVG: {EqnChunk: 50, EqnChunkFactor: 1.5, EqChunkDelay: 10, useFontCache: false, linebreaks: {automatic: true}},
  tex2jax: {
    inlineMath: [['$~$', '$~$']],
    displayMath: [['$$~$', '$~$$']],
    processEscapes: true,
    preview: 'none',
  },
  showProcessingMessages: false,
  messageStyle: 'none',
  // http://docs.mathjax.org/en/latest/config-files.html#the-tex-ams-svg-configuration-file
  jax: ["input/TeX","output/SVG", "output/PreviewHTML"],
  extensions: ["tex2jax.js","MathMenu.js","MathZoom.js", "fast-preview.js", "AssistiveMML.js", "a11y/accessibility-menu.js"],
  TeX: { extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"] }
});</script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_SVG" async></script><script type="text/javascript" src="../arbital-demo-bundle.js"></script><script type="text/javascript">window.addEventListener('DOMContentLoaded', e=>window.loadAllDemos())
</script></head><body><header><h1 class="title">Logical decision theories</h1><div class="page-info"><p class="metadata-link"><a href="../metadata/logical_dt.json.html">logical_dt.json</a></p><p class="arbital-url"><a href="https://arbital.com/p/logical_dt">https://arbital.com/p/logical_dt</a></p><p class="creator">by
 <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a> Jul 8 2016 
updated
 Jun 1 2018</p></div><p class="clickbait">Root page for topics on logical decision theory, with multiple intros for different audiences.</p><nav class="breadcrumbs"><nav class="breadcrumb" aria-label="Breadcrumb"><ul><li><a href="../index.html">Index</a></li><li>Logical decision theories</li></ul></nav><nav class="breadcrumb" aria-label="Breadcrumb"><ul><li><a href="decision_theory.html">Decision theory</a></li><li>â€¦</li></ul></nav></nav></header><hr><main><p>"Logical decision theories" are algorithms for making choices which embody some variant of "Decide as though you determine the logical output of your decision algorithm."</p>
<p>Besides the intros in the tabs above, <a href="https://arxiv.org/abs/1710.05060">a formal paper on the &quot;functional decision theory&quot; variant of LDT is now available</a>.  This is the best current introduction for those who prefer to read papers.</p></main><hr><section class="comments"><h2>Comments</h2><div class="comment"><p><a class="page-link" href="../page/WilfA.html">Wilf A</a></p><p><p>The link to the formal paper doesn't have anything.</p></p></div></section><footer><p class="tagged"><h2>Tagged</h2><span class="page-comma-list"><a class="page-link" href="../page/concept_meta_tag.html">Concept</a>,
 <a class="page-link" href="../page/stub_meta_tag.html">Stub</a></span></p><p class="likes"><h2>Likes</h2><span class="page-comma-list"><a class="page-link" href="../page/EricBruylant.html">Eric Bruylant</a>,
 <a class="page-link" href="../page/RonnyFernandez.html">Ronny Fernandez</a></span></p><p class="reverse-related"><h2>Reverse Related</h2><ul class="page-list"><li><a class="page-link" href="../page/concept_meta_tag.html">Concept</a> <q>Add this meta tag to pages which are concepts.</q> - <a class="page-link" href="../page/AlexeiAndreev.html">Alexei Andreev</a></li><li><a class="page-link" href="../page/stub_meta_tag.html">Stub</a> <q>This page only gives a very brief overview of the topic. If you're able to, please help expand or improve it!</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li></ul></p><p class="children"><h2>Children</h2><ul class="page-tree"><li><a class="page-link" href="../page/5kv.html">An Introduction to Logical Decision Theory for Everyone Else</a> <q>So like what the heck is 'logical decision theory' in terms a normal person can understand?</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/fair_problem_class.html">Fair problem class</a> <q>A problem is 'fair' (according to logical decision theory) when only the results matter and not how we get there.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/ldt_guide.html">Guide to Logical Decision Theory</a> <q>The entry point for learning about logical decision theory.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/ldt_intro_phil.html">Introduction to Logical Decision Theory for Analytic Philosophers</a> <q>Why &quot;choose as if controlling the logical output of your decision algorithm&quot; is the most appealing candidate for the principle of rational choice.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/ldt_intro_compsci.html">Introduction to Logical Decision Theory for Computer Scientists</a> <q>'Logical decision theory' from a math/programming standpoint, including how two agents with mutual knowledge of each other's code can cooperate on the Prisoner's Dilemma.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/ldt_intro_econ.html">Introduction to Logical Decision Theory for Economists</a> <q>An introduction to 'logical decision theory' and its implications for the Ultimatum Game, voting in elections, bargaining problems, and more.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/newcomblike.html">Newcomblike decision problems</a> <q>Decision problems in which your choice correlates with something other than its physical consequences (say, because somebody has predicted you very well) can do weird things to some decision theories.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a><ul class="page-tree"><li><a class="page-link" href="../page/rationality_of_voting.html">'Rationality' of voting in elections</a> <q>&quot;A single vote is very unlikely to swing the election, so your vote is unlikely to have an effect&quot; versus &quot;Many people similar to you are making a similar decision about whether to vote.&quot;</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/pd_tournament_99ldt_1cdt.html">99LDT x 1CDT oneshot PD tournament as arguable counterexample to LDT doing better than CDT</a> <q>Arguendo, if 99 LDT agents and 1 CDT agent are facing off in a one-shot Prisoner's Dilemma tournament, the CDT agent does better on a problem that CDT considers 'fair'.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/absentminded_driver.html">Absent-Minded Driver dilemma</a> <q>A road contains two identical intersections.  An absent-minded driver wants to turn right at the second intersection.  &quot;With what probability should the driver turn right?&quot; argue decision theorists.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/death_in_damascus.html">Death in Damascus</a> <q>Death tells you that It is coming for you tomorrow.  You can stay in Damascus or flee to Aleppo.  Whichever decision you actually make is the wrong one.  This gives some decision theories trouble.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/newcombs_problem.html">Newcomb's Problem</a> <q>There are two boxes in front of you, Box A and Box B.  You can take both boxes, or only Box B.  Box A contains $1000.  Box B contains $1,000,000 if and only if Omega predicted you'd take only Box B.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/parfits_hitchhiker.html">Parfit's Hitchhiker</a> <q>You are dying in the desert.  A truck-driver who is very good at reading faces finds you, and offers to drive you into the city if you promise to pay $1,000 on arrival.  You are a selfish rationalist.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/prisoners_dilemma.html">Prisoner's Dilemma</a> <q>You and an accomplice have been arrested.  Both of you must decide, in isolation, whether to testify against the other prisoner--which subtracts one year from your sentence, and adds two to theirs.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a><ul class="page-tree"><li><a class="page-link" href="../page/true_prisoners_dilemma.html">True Prisoner's Dilemma</a> <q>A scenario that would reproduce the ideal payoff matrix of the Prisoner's Dilemma about human beings who care about their public reputation and each other.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li></ul></li><li><a class="page-link" href="../page/toxoplasmosis_dilemma.html">Toxoplasmosis dilemma</a> <q>A parasitic infection, carried by cats, may make humans enjoy petting cats more.  A kitten, now in front of you, isn't infected.  But if you *want* to pet it, you may already be infected.  Do you?</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/transparent_newcombs_problem.html">Transparent Newcomb's Problem</a> <q>Omega has left behind a transparent Box A containing $1000, and a transparent Box B containing $1,000,000 or nothing.  Box B is full iff Omega thinks you one-box on seeing a full Box B.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/ultimatum_game.html">Ultimatum Game</a> <q>A Proposer decides how to split $10 between themselves and the Responder.  The Responder can take what is offered, or refuse, in which case both parties get nothing.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li></ul></li><li><a class="page-link" href="../page/omega_troll.html">Omega (alien philosopher-troll)</a> <q>The entity that sets up all those trolley problems.  An alien philosopher/troll imbued with unlimited powers, excellent predictive ability, and very odd motives.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/updateless_dt.html">Updateless decision theories</a> <q>Decision theories that maximize their policies (mappings from sense inputs to actions), rather than using their sense inputs to update their beliefs and then selecting actions.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li></ul></p></footer></body></html>