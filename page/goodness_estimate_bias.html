<!DOCTYPE html><html><head><meta charset="utf-8"><title>Goodness estimate biaser</title><link rel="stylesheet" type="text/css" href="../common.css"><link rel="stylesheet" type="text/css" href="../page-style.css"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  SVG: {EqnChunk: 50, EqnChunkFactor: 1.5, EqChunkDelay: 10, useFontCache: false, linebreaks: {automatic: true}},
  tex2jax: {
    inlineMath: [['$~$', '$~$']],
    displayMath: [['$$~$', '$~$$']],
    processEscapes: true,
    preview: 'none',
  },
  showProcessingMessages: false,
  messageStyle: 'none',
  // http://docs.mathjax.org/en/latest/config-files.html#the-tex-ams-svg-configuration-file
  jax: ["input/TeX","output/SVG", "output/PreviewHTML"],
  extensions: ["tex2jax.js","MathMenu.js","MathZoom.js", "fast-preview.js", "AssistiveMML.js", "a11y/accessibility-menu.js"],
  TeX: { extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"] }
});</script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_SVG" async></script><script type="text/javascript" src="../arbital-demo-bundle.js"></script><script type="text/javascript">window.addEventListener('DOMContentLoaded', e=>window.loadAllDemos())
</script></head><body><header><h1 class="title">Goodness estimate biaser</h1><div class="page-info"><p class="metadata-link"><a href="../metadata/goodness_estimate_bias.json.html">goodness_estimate_bias.json</a></p><p class="arbital-url"><a href="https://arbital.com/p/goodness_estimate_bias">https://arbital.com/p/goodness_estimate_bias</a></p><p class="creator">by
 <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a> Jul 7 2016 
updated
 Jul 8 2016</p></div><p class="clickbait">Some of the main problems in AI alignment can be seen as scenarios where actual goodness is likely to be systematically lower than a broken way of estimating goodness.</p><nav class="breadcrumbs"><nav class="breadcrumb" aria-label="Breadcrumb"><ul><li><a href="../index.html">Index</a></li><li>Goodness estimate biaser</li></ul></nav><nav class="breadcrumb" aria-label="Breadcrumb"><ul><li><a href="ai_alignment.html">AI alignment</a></li><li><a href="advanced_safety.html">Advanced safety</a></li><li>…</li></ul></nav></nav></header><hr><main><p>A "<a href="value_alignment_value.html">goodness</a> estimate [statistical_bias biaser]" is a system setup or phenomenon that seems <a href="foreseeable_difficulties.html">foreseeably</a> likely to cause the actual goodness of some AI plan to be systematically lower than the AI's estimate of that plan's goodness.  We want the AI's estimate to be [statistically_unbiased unbiased].</p>
<h2 id="ordinaryexamples">Ordinary examples</h2>
<p>Subtle and unsubtle [statistical_bias estimate-biasing] issues in machine learning are well-known and appear far short of <a href="advanced_agent.html">advanced agency</a>:</p>
<p>&#x25CF;  A machine learning algorithm's performance on the training data is not an unbiased estimate of its performance on the test data.  Some of what the algorithm seems to learn may be particular to noise in the training data.  This fitted noise will not be fitted within the test data.  So test performance is not just unequal to, but <em>systematically lower than,</em> training performance; if we were treating the training performance as an estimate of test performance, it would not be an [statistically_unbiased unbiased] estimate.</p>
<p>&#x25CF;  The <a href="https://en.wikipedia.org/wiki/Winner%27s_curse">Winner's Curse</a> from auction theory observes that if bidders have noise in their unbiased estimates of the auctioned item's value, then the <em>highest</em> bidder, who receives the item, is more likely to have upward noise in their individually unbiased estimate, <a href="bayes_update.html">conditional</a> on their having won.  (E.g., three bidders with Gaussian noise in their value estimates submit bids on an item whose true value to them is 1.0; the winning bidder is likely to have valued the item at more than 1.0.)</p>
<p>The analogous <a href="https://faculty.fuqua.duke.edu/~jes9/bio/The_Optimizers_Curse.pdf">Optimizer's Curse</a> observes that if we make locally unbiased but noisy estimates of the [subjective_expected_utility subjective expected utility] of several plans, then selecting the plan with 'highest expected utility' is likely to select an estimate with upward noise.  Barring compensatory adjustments, this means that actual utility will be systematically lower than expected utility, even if all expected utility estimates are individually unbiased.  Worse, if we have 10 plans whose expected utility can be unbiasedly estimated with low noise, plus 10 plans whose expected utility can be unbiasedly estimated with high noise, then selecting the plan with apparently highest expected utility favors the noisiest estimates!</p>
<h2 id="inaialignment">In AI alignment</h2>
<p>We can see many of the alleged <a href="foreseeable_difficulties.html">foreseeable difficulties</a> in <a href="ai_alignment.html">AI alignment</a> as involving similar processes that allegedly produce systematic downward biases in what we see as actual <a href="value_alignment_value.html">goodness</a>, compared to an AI's estimate of goodness:</p>
<p>&#x25CF;  <a href="edge_instantiation.html">Edge instantiation</a> suggests that if we take an imperfectly or incompletely learned value function, then looking for the <em>maximum</em> or <em>extreme</em> of that value function is much more likely than usual to magnify what we see as the gaps or imperfections (because of [fragile_value fragility of value], plus the Optimizer's Curse); or destroy whatever aspects of value the AI didn't learn about (because optimizing a subset of properties is liable to set all other properties to extreme values).</p>
<p>We can see this as implying both "The AI's apparent goodness in non-extreme cases is an upward-biased estimate of its goodness in extreme cases" and "If the AI learns its goodness estimator less than <a href="total_alignment.html">perfectly</a>, the AI's estimates of the goodness of its best plans will systematically overestimate what we see as the actual goodness."</p>
<p>&#x25CF;  <a href="nearest_unblocked.html">Nearest unblocked strategy</a> generally, and especially over [instrumental_incorrigibility instrumentally convergent incorrigibility], suggests that if there are naturally-arising AI behaviors we see as bad (e.g. routing around <a href="shutdown_problem.html">shutdown</a>), there may emerge a pseudo-adversarial selection of strategies that route around our attempted <a href="patch_resistant.html">patches</a> to those problems.  E.g., the AI constructs an environmental subagent to continue carrying on its goals, while cheerfully obeying 'the letter of the law' by allowing its current hardware to be shut down.  This pseudo-adversarial selection (though the AI does not have an explicit goal of thwarting us or selecting low-goodness strategies per se) again implies that actual <a href="value_alignment_value.html">goodness</a> is likely to be systematically lower than the AI's estimate of what it's learned as 'goodness'; again to an <a href="context_disaster.html">increasing degree</a> as the AI becomes <a href="uncontainability.html">smarter</a> and <a href="unforeseen_maximum.html">searches a wider policy space</a>.</p>
<p><a href="soft_optimizer.html">Mild optimization</a> and <a href="conservative_concept.html">conservative strategies</a> can be seen as proposals to 'regularize' powerful optimization in a way that <em>decreases</em> the degree to which goodness in training is a biased (over)estimate of goodness in execution.</p></main><hr><section class="comments"><h2>Comments</h2><div class="comment"><p><a class="page-link" href="../page/NateSoares.html">Nate Soares</a></p><p><blockquote class="comment-context">●  Nearest unblocked strategy generally, and especially over <mark>instrumentally convergent corrigibility incorrigibility</mark>, suggests that if there are naturally\-arising AI behaviors we see as bad \(e\.g\. routing around shutdown\), there may emerge a pseudo\-adversarial selection of best strategies that happen to route around our attempted patches to those problems\.  E\.g\., the AI constructs an environmental subagent to continue carrying on its goals, while cheerfully obeying 'the letter of the law' with respect to allowing its current hardware to be shut down\.  This pseudo\-adversarial selection \(though obviously the AI does not actually have a goal of thwarting us or selecting low\-goodness strategies per se\) again implies that operational goodness is likely to be systematically lower than the AI's partially\-learned estimate of goodness; again to an increasing degree as the AI becomes smarter and searches a wider policy space\.</blockquote>
<p>wut</p></p><div class="comment"><p><a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></p><p><p>Oh my God you don't know about instrumentally convergent corrigibility incorrigibility</p>
<p>How could I have neglected to tell you this</p>
<p>The world is doomed</p>
<p>(Edited to fix.)</p></p></div></div></section><footer><p class="likes"><h2>Likes</h2><span class="page-comma-list"><a class="page-link" href="../page/EricBruylant.html">Eric Bruylant</a></span></p><p class="related"><h2>Related</h2><ul class="page-list"><li><a class="page-link" href="../page/edge_instantiation.html">Edge instantiation</a> <q>When you ask the AI to make people happy, and it tiles the universe with the smallest objects that can be happy.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/goodharts_curse.html">Goodhart's Curse</a> <q>The Optimizer's Curse meets Goodhart's Law.  For example, if our values are V, and an AI's utility function U is a proxy for V, optimizing for high U seeks out 'errors'--that is, high values of U - V.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li></ul></p></footer></body></html>