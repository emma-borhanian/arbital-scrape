<!DOCTYPE html><html><head><meta charset="utf-8"><title>Safe training procedures for human-imitators</title><link rel="stylesheet" type="text/css" href="../common.css"><link rel="stylesheet" type="text/css" href="../page-style.css"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  SVG: {EqnChunk: 50, EqnChunkFactor: 1.5, EqChunkDelay: 10, useFontCache: false, linebreaks: {automatic: true}},
  tex2jax: {
    inlineMath: [['$~$', '$~$']],
    displayMath: [['$$~$', '$~$$']],
    processEscapes: true,
    preview: 'none',
  },
  showProcessingMessages: false,
  messageStyle: 'none',
  // http://docs.mathjax.org/en/latest/config-files.html#the-tex-ams-svg-configuration-file
  jax: ["input/TeX","output/SVG", "output/PreviewHTML"],
  extensions: ["tex2jax.js","MathMenu.js","MathZoom.js", "fast-preview.js", "AssistiveMML.js", "a11y/accessibility-menu.js"],
  TeX: { extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"] }
});</script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_SVG" async></script><script type="text/javascript" src="../arbital-demo-bundle.js"></script><script type="text/javascript">window.addEventListener('DOMContentLoaded', e=>window.loadAllDemos())
</script></head><body><header><h1 class="title">Safe training procedures for human-imitators</h1><div class="page-info"><p class="metadata-link"><a href="../metadata/safe_training_for_imitators.json.html">safe_training_for_imitators.json</a></p><p class="arbital-url"><a href="https://arbital.com/p/safe_training_for_imitators">https://arbital.com/p/safe_training_for_imitators</a></p><p class="creator">by
 <a class="page-link" href="../page/JessicaTaylor.html">Jessica Taylor</a> Mar 24 2016 
updated
 Mar 24 2016</p></div><p class="clickbait">How does one train a reinforcement learner to act like a human?</p><nav class="breadcrumbs"><nav class="breadcrumb" aria-label="Breadcrumb"><ul><li><a href="../index.html">Index</a></li><li>Safe training procedures for human-imitators</li></ul></nav><nav class="breadcrumb" aria-label="Breadcrumb"><ul><li><a href="ai_alignment.html">AI alignment</a></li><li>â€¦</li></ul></nav></nav></header><hr><main><p>How do we train a reinforcement learning system to imitate a human producing complex outputs such as strings?  Existing approaches are not entirely satisfactory. </p>
<p>Concretely, suppose there is some set of questions.  A human can answer each question with a string.  We have a set of (question, answer) pairs as training data.  Train a model that, given a question, will produce an answer similar to the answer a human would give.  Here are some existing approaches to this problem:</p>
<h1 id="generativeadversarialmodels">Generative adversarial models</h1>
<p>In a <a href="http://arxiv.org/abs/1406.2661">generative adversarial model</a>, one agent (the <em>imitator</em>) outputs an answer and another agent (the <em>distinguisher</em>) tries to determine if the answer was produced by the imitator or by an actual human.  The imitator gets a score for fooling the distinguisher.</p>
<p>Advantages:</p>
<ol>
<li>It's possible to tell when the imitator is not imitating the human well.  Example: humans always solve Sudoku puzzles.  The imitator cannot solve Sudoku puzzles.  The distinguisher knows humans always solve Sudoku puzzles and can check candidate solutions to see if they actually solve the Sudoku puzzle, to distinguish the imitator from the human.</li>
<li>Whenever the human can be imitated efficiently, the imitator is able to get a good score by doing this.</li>
</ol>
<p>Disadvantages:</p>
<ol>
<li>The imitator may differ from human behavior in important but hard-to-detect ways.  Example: humans always produce photos of cat drawings.  The imitator produces photos of cat drawings that contain steganographic messages.  The distinguisher cannot detect these messages, and therefore cannot tell the imitator apart from the human.</li>
</ol>
<h1 id="retrodiction">Retrodiction</h1>
<p>Given a question, the imitator outputs some representation of a distribution.  It is scored according to the estimated probability that this distribution assigns to the human's actual answer (by log probability).  This is called "retrodiction" because it must be possible to show that the actual answer had high probability in hindsight.</p>
<p>Practical examples of this approach include <a href="http://arxiv.org/abs/1312.6114">variational autoencoders</a> and <a href="http://www.cs.cornell.edu/gomes/papers/ModelCounting-SAT-Handbook-prelim.pdf">approximate model counting</a>.  In theory, perhaps <a href="https://agentfoundations.org/item?id=632">importance sampling</a> is a good general theory of retrodiction.</p>
<p>Advantages:</p>
<ol>
<li>No steganography problem.</li>
</ol>
<p>Disadvantages:</p>
<ol>
<li>The computational complexity properties are unclear.  We have no guarantee of the form "if it's efficient to imitate a human doing something, then it's efficient to create a probabilistic model that provably assigns a high probability to the human's behavior".</li>
<li>It's hard to tell when the model is performing badly in an absolute sense (as in the Sudoku example).</li>
</ol></main><hr><footer><p class="tagged"><h2>Tagged</h2><span class="page-comma-list"><a class="page-link" href="../page/taskagi_open_problems.html">Open subproblems in aligning a Task-based AGI</a></span></p><p class="reverse-related"><h2>Reverse Related</h2><ul class="page-list"><li><a class="page-link" href="../page/taskagi_open_problems.html">Open subproblems in aligning a Task-based AGI</a> <q>Open research problems, especially ones we can model today, in building an AGI that can &quot;paint all cars pink&quot; without turning its future light cone into pink-painted cars.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li></ul></p></footer></body></html>