<!DOCTYPE html><html><head><meta charset="utf-8"><title>Eliezer Yudkowsky</title><link rel="stylesheet" type="text/css" href="../common.css"><link rel="stylesheet" type="text/css" href="../page-style.css"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  SVG: {EqnChunk: 50, EqnChunkFactor: 1.5, EqChunkDelay: 10, useFontCache: false, linebreaks: {automatic: true}},
  tex2jax: {
    inlineMath: [['$~$', '$~$']],
    displayMath: [['$$~$', '$~$$']],
    processEscapes: true,
    preview: 'none',
  },
  showProcessingMessages: false,
  messageStyle: 'none',
  // http://docs.mathjax.org/en/latest/config-files.html#the-tex-ams-svg-configuration-file
  jax: ["input/TeX","output/SVG", "output/PreviewHTML"],
  extensions: ["tex2jax.js","MathMenu.js","MathZoom.js", "fast-preview.js", "AssistiveMML.js", "a11y/accessibility-menu.js"],
  TeX: { extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"] }
});</script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_SVG" async></script><script type="text/javascript" src="../arbital-demo-bundle.js"></script><script type="text/javascript">window.addEventListener('DOMContentLoaded', e=>window.loadAllDemos())
</script></head><body><header><h1 class="title">Eliezer Yudkowsky</h1><div class="page-info"><p class="metadata-link"><a href="../metadata/EliezerYudkowsky.json.html">EliezerYudkowsky.json</a></p><p class="arbital-url"><a href="https://arbital.com/p/EliezerYudkowsky">https://arbital.com/p/EliezerYudkowsky</a></p><p class="creator">by
 <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a> Sep 4 2015 
updated
 Dec 19 2015</p></div><p class="clickbait">Cofounder, with Nick Bostrom, of the field of value alignment theory.</p><nav class="breadcrumbs"><nav class="breadcrumb" aria-label="Breadcrumb"><ul><li><a href="../index.html">Index</a></li><li>Eliezer Yudkowsky</li></ul></nav><nav class="breadcrumb" aria-label="Breadcrumb"><ul><li><a href="ai_alignment.html">AI alignment</a></li><li><a href="value_alignment_researchers.html">Researchers in value alignment theory</a></li><li>…</li></ul></nav><nav class="breadcrumb" aria-label="Breadcrumb"><ul><li><a href="people.html">People</a></li><li>…</li></ul></nav></nav></header><hr><main><p>Eliezer Yudkowsky is, with <a href="NickBostrom.html">Nick Bostrom</a>, one of the cofounders of <a href="ai_alignment.html">value alignment theory</a>.  He is the founder of the <a href="MIRI.html">Machine Intelligence Research Institute</a>.  He is the inventor of [ timeless decision theory] and [ extrapolated volition], the co-inventor with <a href="MarcelloHerreshoff.html">Marcello Herreshoff</a> of the [ tiling agents problem] that kicked off the study of <a href="Vingean_reflection.html">Vingean reflection</a>, and so on and so on.  His specializations include [ logical decision theory], [ naturalistic reflection], and [adversarial_AI_safety_analysis pinpointing the failure modes] in <a href="advanced_agent.html">design proposals for advanced AIs</a>.</p></main><hr><footer><p class="likes"><h2>Likes</h2><span class="page-comma-list"><a class="page-link" href="../page/AlexRay.html">Alex Ray</a>,
 <a class="page-link" href="../page/AlexeiAndreev.html">Alexei Andreev</a>,
 <a class="page-link" href="../page/BenPace.html">Ben Pace</a>,
 <a class="page-link" href="../page/ChaseRoycroft.html">Chase Roycroft</a>,
 <a class="page-link" href="../page/EricBruylant.html">Eric Bruylant</a>,
 <a class="page-link" href="../page/EricRogstad.html">Eric Rogstad</a>,
 <a class="page-link" href="../page/KonradSeifert2.html">Konrad Seifert</a>,
 <a class="page-link" href="../page/MatthewGraves.html">Matthew Graves</a>,
 <a class="page-link" href="../page/MeaganSullivan.html">Meagan Sullivan</a>,
 <a class="page-link" href="../page/MichaelOkeh.html">Michael Okeh</a>,
 <a class="page-link" href="../page/MichaelPropach.html">Michael Propach</a>,
 <a class="page-link" href="../page/RyanNayr.html">Ryan Nayr</a>,
 <a class="page-link" href="../page/TravisRivera.html">Travis Rivera</a></span></p><p class="children"><h2>Children</h2><ul class="page-tree"><li><a class="page-link" href="../page/aiml_econ_faq.html">A quick econ FAQ for AI/ML folks concerned about technological unemployment</a> <q>Yudkowsky's attempted description of standard economic concepts that he thinks are vital for talking about technological unemployment and related issues.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/likelihood_vs_pvalue.html">Likelihood functions, p-values, and the replication crisis</a> <q>What's the whole Bayesian-vs.-frequentist debate about?</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/eliezer_fixes.html">List of Eliezer's current most desired fixes and features</a> <q>A place for Eliezer to note down his current list of personally-wanted features for editing and writing.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li></ul></p><hr><p class="created"><h2>Created</h2><h3 id="createdwiki">wiki</h3><ul class="page-list"><li><a class="page-link" href="../page/beneficial.html">'Beneficial'</a> <q>Really actually good.  A metasyntactic variable to mean &quot;favoring whatever the speaker wants ideally to accomplish&quot;, although different speakers have different morals and metaethics.</q></li><li><a class="page-link" href="../page/ai_concept.html">'Concept'</a> <q>In the context of Artificial Intelligence, a 'concept' is a category, something that identifies thingies as being inside or outside the concept.</q></li><li><a class="page-link" href="../page/detrimental.html">'Detrimental'</a> <q>The opposite of beneficial.</q></li><li><a class="page-link" href="../page/rationality_of_voting.html">'Rationality' of voting in elections</a> <q>&quot;A single vote is very unlikely to swing the election, so your vote is unlikely to have an effect&quot; versus &quot;Many people similar to you are making a similar decision about whether to vote.&quot;</q></li><li><a class="page-link" href="../page/pd_tournament_99ldt_1cdt.html">99LDT x 1CDT oneshot PD tournament as arguable counterexample to LDT doing better than CDT</a> <q>Arguendo, if 99 LDT agents and 1 CDT agent are facing off in a one-shot Prisoner's Dilemma tournament, the CDT agent does better on a problem that CDT considers 'fair'.</q></li><li><a class="page-link" href="../page/aiml_econ_faq.html">A quick econ FAQ for AI/ML folks concerned about technological unemployment</a> <q>Yudkowsky's attempted description of standard economic concepts that he thinks are vital for talking about technological unemployment and related issues.</q></li><li><a class="page-link" href="../page/yudkowsky_chollet_reply.html">A reply to Francois Chollet on intelligence explosion</a> <q>A quick run-through of what I'd consider the standard replies to the arguments in Keras inventor Francois Chollet's essay &quot;The impossibility of intelligence explosion&quot;.</q></li><li><a class="page-link" href="../page/ai_alignment.html">AI alignment</a> <q>The great civilizational problem of creating artificially intelligent computer systems such that running them is a good idea.</q></li><li><a class="page-link" href="../page/value_alignment_open_problem.html">AI alignment open problem</a> <q>Tag for open problems under AI alignment.</q></li><li><a class="page-link" href="../page/ai_arms_race.html">AI arms races</a> <q>AI arms races are bad</q></li><li><a class="page-link" href="../page/AI_safety_mindset.html">AI safety mindset</a> <q>Asking how AI designs could go wrong, instead of imagining them going right.</q></li><li><a class="page-link" href="../page/AIXI.html">AIXI</a> <q>How to build an (evil) superintelligent AI using unlimited computing power and one page of Python code.</q></li><li><a class="page-link" href="../page/aixitl.html">AIXI-tl</a> <q>A time-bounded version of the ideal agent AIXI that uses an impossibly large finite computer instead of a hypercomputer.</q></li><li><a class="page-link" href="../page/reads_algebra.html">Ability to read algebra</a> <q>Do you have sufficient mathematical ability that you can read a sentence that uses some algebra or invokes a mathematical idea, without slowing down too much?</q></li><li><a class="page-link" href="../page/reads_calculus.html">Ability to read calculus</a> <q>Can you take integral signs and differentiations in stride?</q></li><li><a class="page-link" href="../page/reads_logic.html">Ability to read logic</a> <q>Can you read sentences symbolically stating &quot;For all x: exists y: phi(x, y) or not theta(y)&quot; without slowing down too much?</q></li><li><a class="page-link" href="../page/abortable.html">Abortable plans</a> <q>Plans that can be undone, or switched to having low further impact.  If the AI builds abortable nanomachines, they'll have a quiet self-destruct option that includes any replicated nanomachines.</q></li><li><a class="page-link" href="../page/absentminded_driver.html">Absent-Minded Driver dilemma</a> <q>A road contains two identical intersections.  An absent-minded driver wants to turn right at the second intersection.  &quot;With what probability should the driver turn right?&quot; argue decision theorists.</q></li><li><a class="page-link" href="../page/actual_effectiveness.html">Actual effectiveness</a> <q>If you want the AI's so-called 'utility function' to actually be steering the AI, you need to think about how it meshes up with beliefs, or what gets output to actions.</q></li><li><a class="page-link" href="../page/hack.html">Ad-hoc hack (alignment theory)</a> <q>A &quot;hack&quot; is when you alter the behavior of your AI in a way that defies, or doesn't correspond to, a principled approach for that problem.</q></li><li><a class="page-link" href="../page/advanced_agent.html">Advanced agent properties</a> <q>How smart does a machine intelligence need to be, for its niceness to become an issue?  &quot;Advanced&quot; is a broad term to cover cognitive abilities such that we'd need to start considering AI alignment.</q></li><li><a class="page-link" href="../page/advanced_nonagent.html">Advanced nonagent</a> <q>Hypothetically, cognitively powerful programs that don't follow the loop of &quot;observe, learn, model the consequences, act, observe results&quot; that a standard &quot;agent&quot; would.</q></li><li><a class="page-link" href="../page/advanced_safety.html">Advanced safety</a> <q>An agent is *really* safe when it has the capacity to do anything, but chooses to do what the programmer wants.</q></li><li><a class="page-link" href="../page/Kolmogorov_complexity.html">Algorithmic complexity</a> <q>When you compress the information, what you are left with determines the complexity.</q></li><li><a class="page-link" href="../page/aligning_adds_time.html">Aligning an AGI adds significant development time</a> <q>Aligning an advanced AI foreseeably involves extra code and extra testing and not being able to do everything the fastest way, so it takes longer.</q></li><li><a class="page-link" href="../page/real_is_rich.html">Almost all real-world domains are rich</a> <q>Anything you're trying to accomplish in the real world can potentially be accomplished in a *lot* of different ways.</q></li><li><a class="page-link" href="../page/5kv.html">An Introduction to Logical Decision Theory for Everyone Else</a> <q>So like what the heck is 'logical decision theory' in terms a normal person can understand?</q></li><li><a class="page-link" href="../page/1td.html">Answer to sparking widgets problem</a> <q>Odds of 1 : 3, probability of 1/4.</q></li><li><a class="page-link" href="../page/arbital_biographies.html">Arbital biographies</a> <q>As a very strong default (presently an absolute rule), Joe Smith's page only says nice things about Joe.  Even if a negative fact is true, it doesn't go on Joe's page.</q></li><li><a class="page-link" href="../page/arbital_playpen.html">Arbital playpen</a> <q>Want to test a feature? Feel free to edit this page! asdfasfdasfda</q></li><li><a class="page-link" href="../page/arbital_practices.html">Arbital practices</a> <q>Guidelines and rules for interacting on Arbital.</q></li><li><a class="page-link" href="../page/arbital_ambitions.html">Arbital: Solving online explanations</a> <q>An explanation of Arbital's mid-term goals</q></li><li><a class="page-link" href="../page/arithmetical_hierarchy.html">Arithmetical hierarchy</a> <q>The arithmetical hierarchy is a way of classifying logical statements by the number of clauses saying &quot;for every object&quot; and &quot;there exists an object&quot;.</q></li><li><a class="page-link" href="../page/1mj.html">Arithmetical hierarchy: If you don't read logic</a> <q>The arithmetical hierarchy is a way of stratifying statements by how many &quot;for every number&quot; and &quot;th…</q></li><li><a class="page-link" href="../page/agi.html">Artificial General Intelligence</a> <q>An AI which has the same kind of &quot;significantly more general&quot; intelligence that humans have compared to chimpanzees; it can learn new domains, like we can.</q></li><li><a class="page-link" href="../page/attainable_optimum.html">Attainable optimum</a> <q>The 'attainable optimum' of an agent's preferences is the best that agent can actually do given its finite intelligence and resources (as opposed to the global maximum of those preferences).</q></li><li><a class="page-link" href="../page/Sovereign.html">Autonomous AGI</a> <q>The hardest possible class of Friendly AI to build, with the least moral hazard; an AI intended to neither require nor accept further direction.</q></li><li><a class="page-link" href="../page/avert_instrumental_pressure.html">Averting instrumental pressures</a> <q>Almost-any utility function for an AI, whether the target is diamonds or paperclips or eudaimonia, implies subgoals like rapidly self-improving and refusing to shut down.  Can we make that not happen?</q></li><li><a class="page-link" href="../page/avert_self_improvement.html">Averting the convergent instrumental strategy of self-improvement</a> <q>We probably want the first AGI to *not* improve as fast as possible, but improving as fast as possible is a convergent strategy for accomplishing most things.</q></li><li><a class="page-link" href="../page/bayes_rule.html">Bayes' rule</a> <q>Bayes' rule is the core theorem of probability theory saying how to revise our beliefs when we make a new observation.</q></li><li><a class="page-link" href="../page/bayes_rule_examples.html">Bayes' rule examples</a> <q>Interesting problems solvable by Bayes' rule</q></li><li><a class="page-link" href="../page/bayes_rule_functional.html">Bayes' rule: Functional form</a> <q>Bayes' rule for to continuous variables.</q></li><li><a class="page-link" href="../page/bayes_rule_guide.html">Bayes' rule: Guide</a> <q>The Arbital guide to Bayes' rule</q></li><li><a class="page-link" href="../page/bayes_log_odds.html">Bayes' rule: Log-odds form</a> <q>A simple transformation of Bayes' rule reveals tools for measuring degree of belief, and strength of evidence.</q></li><li><a class="page-link" href="../page/bayes_rule_odds.html">Bayes' rule: Odds form</a> <q>The simplest and most easily understandable form of Bayes' rule uses relative odds.</q></li><li><a class="page-link" href="../page/1x7.html">Bayes' rule: Odds form (Intro, Math 1)</a> <q>Introduction to the odds form of Bayes' rule</q></li><li><a class="page-link" href="../page/bayes_rule_odds_probability.html">Bayes' rule: Odds form (Intro, Probability)</a> <q>Intro to Bayes' rule, odds form, for people already familiar with probability.</q></li><li><a class="page-link" href="../page/bayes_rule_proportional.html">Bayes' rule: Proportional form</a> <q>The fastest way to say something both convincing and true about belief-updating.</q></li><li><a class="page-link" href="../page/bayes_rule_multiple.html">Bayes' rule: Vector form</a> <q>For when you want to apply Bayes' rule to lots of evidence and lots of variables, all in one go. (This is more or less how spam filters work.)</q></li><li><a class="page-link" href="../page/bayes_reasoning.html">Bayesian reasoning</a> <q>A probability-theory-based view of the world; a coherent way of changing probabilistic beliefs based on evidence.</q></li><li><a class="page-link" href="../page/bayes_update.html">Bayesian update</a> <q>Bayesian updating: the ideal way to change probabilistic beliefs based on evidence.</q></li><li><a class="page-link" href="../page/bayes_science_virtues.html">Bayesian view of scientific virtues</a> <q>Why is it that science relies on bold, precise, and falsifiable predictions? Because of Bayes' rule, of course.</q></li><li><a class="page-link" href="../page/behaviorist.html">Behaviorist genie</a> <q>An advanced agent that's forbidden to model minds in too much detail.</q></li><li><a class="page-link" href="../page/bayes_rule_elimination.html">Belief revision as probability elimination</a> <q>Update your beliefs by throwing away large chunks of probability mass.</q></li><li><a class="page-link" href="../page/big_picture_awareness.html">Big-picture strategic awareness</a> <q>We start encountering new AI alignment issues at the point where a machine intelligence recognizes the existence of a real world, the existence of programmers, and how these relate to its goals.</q></li><li><a class="page-link" href="../page/bounded_agent.html">Bounded agent</a> <q>An agent that operates in the real world, using realistic amounts of computing power, that is uncertain of its environment, etcetera.</q></li><li><a class="page-link" href="../page/AI_boxing.html">Boxed AI</a> <q>Idea: what if we limit how AI can interact with the world. That'll make it safe, right??</q></li><li><a class="page-link" href="../page/bulverism.html">Bulverism</a> <q>Bulverism is when you explain what goes so horribly wrong in people's minds when they believe X, before you've actually explained why X is wrong.  Forbidden on Arbital.</q></li><li><a class="page-link" href="../page/CICO.html">Calories-In-Calories-Out</a> <q>CICO is a proposed conceptual decomposition of the causes of changes in human body mass, particularl…</q></li><li><a class="page-link" href="../page/cartesian_agent.html">Cartesian agent</a> <q>Agents separated from their environments by impermeable barriers through which only sensory information can enter and motor output can exit.</q></li><li><a class="page-link" href="../page/cartesian_boundary.html">Cartesian agent-environment boundary</a> <q>If your agent is separated from the environment by an absolute border that can only be crossed by sensory information and motor outputs, it might just be a Cartesian agent.</q></li><li><a class="page-link" href="../page/causal_dt.html">Causal decision theories</a> <q>On CDT, to choose rationally, you should imagine the world where your physical act changes, then imagine running that world forward in time.  (Therefore, it's irrational to vote in elections.)</q></li><li><a class="page-link" href="../page/central_example.html">Central examples</a> <q>The &quot;central examples&quot; for a subject are examples that are referred to over and over again in the co…</q></li><li><a class="page-link" href="../page/value_alignment_central_examples.html">Central examples</a> <q>List of central examples in Value Alignment Theory domain.</q></li><li><a class="page-link" href="../page/cognitive_domain.html">Cognitive domain</a> <q>An allegedly compact unit of knowledge, such that ideas inside the unit interact mainly with each other and less with ideas in other domains.</q></li><li><a class="page-link" href="../page/cognitive_steganography.html">Cognitive steganography</a> <q>Disaligned AIs that are modeling human psychology and trying to deceive their programmers will want to hide their internal thought processes from their programmers.</q></li><li><a class="page-link" href="../page/uncontainability.html">Cognitive uncontainability</a> <q>'Cognitive uncontainability' is when we can't hold all of an agent's possibilities inside our own minds.</q></li><li><a class="page-link" href="../page/coherence_theorems.html">Coherence theorems</a> <q>A 'coherence theorem' shows that something bad happens to an agent if its decisions can't be viewed as 'coherent' in some sense. E.g., an inconsistent preference ordering leads to going in circles.</q></li><li><a class="page-link" href="../page/intro_utility_coherence.html">Coherent decisions imply consistent utilities</a> <q>Why do we all use the 'expected utility' formalism?  Because any behavior that can't be viewed from that perspective, must be qualitatively self-defeating (in various mathy ways).</q></li><li><a class="page-link" href="../page/cev.html">Coherent extrapolated volition (alignment target)</a> <q>A proposed direction for an extremely well-aligned autonomous superintelligence - do what humans would want, if we knew what the AI knew, thought that fast, and understood ourselves.</q></li><li><a class="page-link" href="../page/complexity_of_value.html">Complexity of value</a> <q>There's no simple way to describe the goals we want Artificial Intelligences to want.</q></li><li><a class="page-link" href="../page/conceivability.html">Conceivability</a> <q>A hypothetical scenario is 'conceivable' or 'imaginable' when it is not *immediately* incoherent, al…</q></li><li><a class="page-link" href="../page/conditional_probability.html">Conditional probability</a> <q>The notation for writing &quot;The probability that someone has green eyes, if we know that they have red hair.&quot;</q></li><li><a class="page-link" href="../page/consequentialist.html">Consequentialist cognition</a> <q>The cognitive ability to foresee the consequences of actions, prefer some outcomes to others, and output actions leading to the preferred outcomes.</q></li><li><a class="page-link" href="../page/preference_stability.html">Consequentialist preferences are reflectively stable by default</a> <q>Gandhi wouldn't take a pill that made him want to kill people, because he knows in that case more people will be murdered.  A paperclip maximizer doesn't want to stop maximizing paperclips.</q></li><li><a class="page-link" href="../page/conservative_concept.html">Conservative concept boundary</a> <q>Given N example burritos, draw a boundary around what is a 'burrito' that is relatively simple and allows as few positive instances as possible.  Helps make sure the next thing generated is a burrito.</q></li><li><a class="page-link" href="../page/context_disaster.html">Context disaster</a> <q>Some possible designs cause your AI to behave nicely while developing, and behave a lot less nicely when it's smarter.</q></li><li><a class="page-link" href="../page/convergent_strategies.html">Convergent instrumental strategies</a> <q>Paperclip maximizers can make more paperclips by improving their cognitive abilities or controlling more resources.  What other strategies would almost-any AI try to use?</q></li><li><a class="page-link" href="../page/convergent_self_modification.html">Convergent strategies of self-modification</a> <q>The strategies we'd expect to be employed by an AI that understands the relevance of its code and hardware to achieving its goals, which therefore has subgoals about its code and hardware.</q></li><li><a class="page-link" href="../page/4j.html">Coordinative AI development hypothetical</a> <q>What would safe AI development look like if we didn't have to worry about anything else?</q></li><li><a class="page-link" href="../page/corps_vs_si.html">Corporations vs. superintelligences</a> <q>Corporations have relatively few of the advanced-agent properties that would allow one mistake in aligning a corporation to immediately kill all humans and turn the future light cone into paperclips.</q></li><li><a class="page-link" href="../page/correlated_competence.html">Correlated competency</a> <q>When an AI achieving sufficiently high goodness on behavior A means we should strongly expect high goodness on behavior B.</q></li><li><a class="page-link" href="../page/correlated_coverage.html">Correlated coverage</a> <q>In which parts of AI alignment can we hope that getting many things right, will mean the AI gets everything right?</q></li><li><a class="page-link" href="../page/cosmic_endowment.html">Cosmic endowment</a> <q>The 'cosmic endowment' consists of all the stars that could be reached from probes originating on Earth; the sum of all matter and energy potentially available to be transformed into life and fun.</q></li><li><a class="page-link" href="../page/value_cosmopolitan.html">Cosmopolitan value</a> <q>Intuitively: Value as seen from a broad, embracing standpoint that is aware of how other entities may not always be like us or easily understandable to us, yet still worthwhile.</q></li><li><a class="page-link" href="../page/death_in_damascus.html">Death in Damascus</a> <q>Death tells you that It is coming for you tomorrow.  You can stay in Damascus or flee to Aleppo.  Whichever decision you actually make is the wrong one.  This gives some decision theories trouble.</q></li><li><a class="page-link" href="../page/decision_theory.html">Decision theory</a> <q>The mathematical study of ideal decisionmaking</q></li><li><a class="page-link" href="../page/deep_blue.html">Deep Blue</a> <q>The chess-playing program, built by IBM, that first won the world chess championship from Garry Kasparov in 1996.</q></li><li><a class="page-link" href="../page/definition_meta_tag.html">Definition</a> <q>Meta tag used to mark pages that strictly define a particular term or phrase.</q></li><li><a class="page-link" href="../page/descriptive_vs_normative.html">Descriptive versus normative propositions</a> <q>A normative proposition talks about what should be; a descriptive proposition talks about what is.</q></li><li><a class="page-link" href="../page/development_phase_unpredictable.html">Development phase unpredictable</a> <q>Several proposed problems in advanced safety are alleged to be difficult because they depend on some…</q></li><li><a class="page-link" href="../page/diamond_maximizer.html">Diamond maximizer</a> <q>How would you build an agent that made as much diamond material as possible, given vast computing power but an otherwise rich and complicated environment?</q></li><li><a class="page-link" href="../page/alignment_difficulty.html">Difficulty of AI alignment</a> <q>How hard is it exactly to point an Artificial General Intelligence in an intuitively okay direction?</q></li><li><a class="page-link" href="../page/direct_limit_oppose.html">Directing, vs. limiting, vs. opposing</a> <q>Getting the AI to compute the right action in a domain; versus getting the AI to not compute at all in an unsafe domain; versus trying to prevent the AI from acting successfully.  (Prefer 1 &amp; 2.)</q></li><li><a class="page-link" href="../page/diseasitis.html">Diseasitis</a> <q>20% of patients have Diseasitis. 90% of sick patients and 30% of healthy patients turn a tongue depressor black. You turn a tongue depressor black. What's the chance you have Diseasitis?</q></li><li><a class="page-link" href="../page/domain_distance.html">Distances between cognitive domains</a> <q>Often in AI alignment we want to ask, &quot;How close is 'being able to do X' to 'being able to do Y'?&quot;</q></li><li><a class="page-link" href="../page/probable_environment_hacking.html">Distant superintelligences can coerce the most probable environment of your AI</a> <q>Distant superintelligences may be able to hack your local AI, if your AI's preference framework depends on its most probable environment.</q></li><li><a class="page-link" href="../page/distinguish_advancement.html">Distinguish which advanced-agent properties lead to the foreseeable difficulty</a> <q>Say what kind of AI, or threshold level of intelligence, or key type of advancement, first produces the difficulty or challenge you're talking about.</q></li><li><a class="page-link" href="../page/dwim.html">Do-What-I-Mean hierarchy</a> <q>Successive levels of &quot;Do What I Mean&quot; or AGIs that understand their users increasingly well</q></li><li><a class="page-link" href="../page/dont_solve_whole_problem.html">Don't try to solve the entire alignment problem</a> <q>New to AI alignment theory?  Want to work in this area?  Already been working in it for years?  Don't try to solve the entire alignment problem with your next good idea!</q></li><li><a class="page-link" href="../page/edge_instantiation.html">Edge instantiation</a> <q>When you ask the AI to make people happy, and it tiles the universe with the smallest objects that can be happy.</q></li><li><a class="page-link" href="../page/effability.html">Effability principle</a> <q>You are safer the more you understand the inner structure of how your AI thinks; the better you can describe the relation of smaller pieces of the AI's thought process.</q></li><li><a class="page-link" href="../page/emphemeral_premises.html">Emphemeral premises</a> <q>When somebody says X, don't just say, &quot;Oh, not-X because Y&quot; and then forget about Y a day later.  Y is now an important load-bearing assumption in your worldview.  Write Y down somewhere.</q></li><li><a class="page-link" href="../page/cromwells_rule.html">Empirical probabilities are not exactly 0 or 1</a> <q>&quot;Cromwell's Rule&quot; says that probabilities of exactly 0 or 1 should never be applied to empirical propositions - there's always some probability, however tiny, of being mistaken.</q></li><li><a class="page-link" href="../page/environmental_goals.html">Environmental goals</a> <q>The problem of having an AI want outcomes that are out in the world, not just want direct sense events.</q></li><li><a class="page-link" href="../page/efficiency.html">Epistemic and instrumental efficiency</a> <q>An efficient agent never makes a mistake you can predict.  You can never successfully predict a directional bias in its estimates.</q></li><li><a class="page-link" href="../page/epistemic_exclusion.html">Epistemic exclusion</a> <q>How would you build an AI that, no matter what else it learned about the world, never knew or wanted to know what was inside your basement?</q></li><li><a class="page-link" href="../page/epistemology.html">Epistemology</a> <q>What is truth?</q></li><li><a class="page-link" href="../page/evidential_dt.html">Evidential decision theories</a> <q>Theories which hold that the principle of rational choice is &quot;Choose the act that would be the best news, if somebody told you that you'd chosen that act.&quot;</q></li><li><a class="page-link" href="../page/executable_philosophy.html">Executable philosophy</a> <q>Philosophical discourse aimed at producing a trustworthy answer or meta-answer, in limited time, which can used in constructing an Artificial Intelligence.</q></li><li><a class="page-link" href="../page/expected_utility.html">Expected utility</a> <q>Scoring actions based on the average score of their probable consequences.</q></li><li><a class="page-link" href="../page/expected_utility_agent.html">Expected utility agent</a> <q>If you're not some kind of expected utility agent, you're going in circles.</q></li><li><a class="page-link" href="../page/expected_utility_formalism.html">Expected utility formalism</a> <q>Expected utility is the central idea in the quantitative implementation of consequentialism</q></li><li><a class="page-link" href="../page/explicit_bayes_counters_worry.html">Explicit Bayes as a counter for 'worrying'</a> <q>Explicitly walking through Bayes's Rule can summarize your knowledge and thereby stop you from bouncing around pieces of it.</q></li><li><a class="page-link" href="../page/extraordinary_claims.html">Extraordinary claims</a> <q>What makes something an 'extraordinary claim' that requires extraordinary evidence?</q></li><li><a class="page-link" href="../page/bayes_extraordinary_claims.html">Extraordinary claims require extraordinary evidence</a> <q>The people who adamantly claim they were abducted by aliens do provide some evidence for aliens. They just don't provide quantitatively enough evidence.</q></li><li><a class="page-link" href="../page/normative_extrapolated_volition.html">Extrapolated volition (normative moral theory)</a> <q>If someone asks you for orange juice, and you know that the refrigerator contains no orange juice, should you bring them lemonade?</q></li><li><a class="page-link" href="../page/fair_problem_class.html">Fair problem class</a> <q>A problem is 'fair' (according to logical decision theory) when only the results matter and not how we get there.</q></li><li><a class="page-link" href="../page/faithful_simulation.html">Faithful simulation</a> <q>How would you identify, to a Task AGI (aka Genie), the problem of scanning a human brain, and then running a sufficiently accurate simulation of it for the simulation to not be crazy or psychotic?</q></li><li><a class="page-link" href="../page/fallacy.html">Fallacies</a> <q>To call something a fallacy is to assert that you think people shouldn't think like that.</q></li><li><a class="page-link" href="../page/bayes_guide_end.html">Finishing your Bayesian path on Arbital</a> <q>The page that comes at the end of reading the Arbital Guide to Bayes' rule</q></li><li><a class="page-link" href="../page/load_bearing_premises.html">Flag the load-bearing premises</a> <q>If somebody says, &quot;This AI safety plan is going to fail, because X&quot; and you reply, &quot;Oh, that's fine because of Y and Z&quot;, then you'd better clearly flag Y and Z as &quot;load-bearing&quot; parts of your plan.</q></li><li><a class="page-link" href="../page/FAI.html">Friendly AI</a> <q>Old terminology for an AI whose preferences have been successfully aligned with idealized human values.</q></li><li><a class="page-link" href="../page/general_intelligence.html">General intelligence</a> <q>Compared to chimpanzees, humans seem to be able to learn a much wider variety of domains.  We have 'significantly more generally applicable' cognitive abilities, aka 'more general intelligence'.</q></li><li><a class="page-link" href="../page/cognitive_alignment.html">Generalized principle of cognitive alignment</a> <q>When we're asking how we want the AI to think about an alignment problem, one source of inspiration is trying to have the AI mirror our own thoughts about that problem.</q></li><li><a class="page-link" href="../page/value_alignment_glossary.html">Glossary (Value Alignment Theory)</a> <q>Words that have a special meaning in the context of creating nice AIs.</q></li><li><a class="page-link" href="../page/identify_goal_concept.html">Goal-concept identification</a> <q>Figuring out how to say &quot;strawberry&quot; to an AI that you want to bring you strawberries (and not fake plastic strawberries, either).</q></li><li><a class="page-link" href="../page/goodharts_curse.html">Goodhart's Curse</a> <q>The Optimizer's Curse meets Goodhart's Law.  For example, if our values are V, and an AI's utility function U is a proxy for V, optimizing for high U seeks out 'errors'--that is, high values of U - V.</q></li><li><a class="page-link" href="../page/goodness_estimate_bias.html">Goodness estimate biaser</a> <q>Some of the main problems in AI alignment can be seen as scenarios where actual goodness is likely to be systematically lower than a broken way of estimating goodness.</q></li><li><a class="page-link" href="../page/gotcha_button.html">Gotcha button</a> <q>A conversational point which, when pressed, causes the other person to shout &quot;Gotcha!&quot; and leap on what they think is a weakness allowing them to dismiss the conversation.</q></li><li><a class="page-link" href="../page/guarded_definition.html">Guarded definition</a> <q>A guarded definition is one where at least one position suspects there will be pressure to stretch a…</q></li><li><a class="page-link" href="../page/ldt_guide.html">Guide to Logical Decision Theory</a> <q>The entry point for learning about logical decision theory.</q></li><li><a class="page-link" href="../page/happiness_maximizer.html">Happiness maximizer</a> <q>It is sometimes proposed that we build an AI intended to maximize human happiness.  (One early propo…</q></li><li><a class="page-link" href="../page/hard_corrigibility.html">Hard problem of corrigibility</a> <q>Can you build an agent that reasons as if it knows itself to be incomplete and sympathizes with your wanting to rebuild or correct it?</q></li><li><a class="page-link" href="../page/harmless_supernova.html">Harmless supernova fallacy</a> <q>False dichotomies and continuum fallacies which can be used to argue that anything, including a supernova, must be harmless.</q></li><li><a class="page-link" href="../page/bayes_rule_fast_intro.html">High-speed intro to Bayes's rule</a> <q>A high-speed introduction to Bayes's Rule on one page, for the impatient and mathematically adept.</q></li><li><a class="page-link" href="../page/how_to_arbital.html">How to author on Arbital!</a> <q>Want to contribute pages to Arbital?  Here's our current version of the ad-hoc guide to being an author!</q></li><li><a class="page-link" href="../page/lumenators.html">How to build your own Lumenator</a> <q>Treating Seasonal Affective Disorder using MOAR LIGHT can sometimes solve what dinky little lightboxes can't.</q></li><li><a class="page-link" href="../page/bayes_for_humans.html">Humans doing Bayes</a> <q>The human use of Bayesian reasoning in everyday life</q></li><li><a class="page-link" href="../page/humean_free_boundary.html">Humean degree of freedom</a> <q>A concept includes 'Humean degrees of freedom' when the intuitive borders of the human version of that concept depend on our values, making that concept less natural for AIs to learn.</q></li><li><a class="page-link" href="../page/hypercomputer.html">Hypercomputer</a> <q>Some formalisms demand computers larger than the limit of all finite computers</q></li><li><a class="page-link" href="../page/ideal_target.html">Ideal target</a> <q>The 'ideal target' of a meta-utility function is the value the ground-level utility function would take on if the agent updated on all possible evidence; the 'true' utilities under moral uncertainty.</q></li><li><a class="page-link" href="../page/inductive_ambiguity.html">Identifying ambiguous inductions</a> <q>What do a &quot;red strawberry&quot;, a &quot;red apple&quot;, and a &quot;red cherry&quot; have in common that a &quot;yellow carrot&quot; doesn't?  Are they &quot;red fruits&quot; or &quot;red objects&quot;?</q></li><li><a class="page-link" href="../page/identify_causal_goals.html">Identifying causal goal concepts from sensory data</a> <q>If the intended goal is &quot;cure cancer&quot; and you show the AI healthy patients, it sees, say, a pattern of pixels on a webcam.  How do you get to a goal concept *about* the real patients?</q></li><li><a class="page-link" href="../page/ideological_turing_test.html">Ideological Turing test</a> <q>Can you explain the opposing position well enough that people can't tell whether you or a real advocate of that position created the explanation?</q></li><li><a class="page-link" href="../page/ignorance_prior.html">Ignorance prior</a> <q>Key equations for quantitative Bayesian problems, describing exactly the right shape for what we believed before observation.</q></li><li><a class="page-link" href="../page/imitation_agent.html">Imitation-based agent</a> <q>An AI meant to imitate the behavior of a reference human as closely as possible.</q></li><li><a class="page-link" href="../page/immediate_goods.html">Immediate goods</a> <q>One of the potential views on 'value' in the value alignment problem is that what we should want fro…</q></li><li><a class="page-link" href="../page/inductive_prior.html">Inductive prior</a> <q>Some states of pre-observation belief can learn quickly; others never learn anything.  An &quot;inductive prior&quot; is of the former type.</q></li><li><a class="page-link" href="../page/relative_ability.html">Infrahuman, par-human, superhuman, efficient, optimal</a> <q>A categorization of AI ability levels relative to human, with some gotchas in the ordering.  E.g., in simple domains where humans can play optimally, optimal play is not superhuman.</q></li><li><a class="page-link" href="../page/instrumental.html">Instrumental</a> <q>What is &quot;instrumental&quot; in the context of Value Alignment Theory?</q></li><li><a class="page-link" href="../page/instrumental_convergence.html">Instrumental convergence</a> <q>Some strategies can help achieve most possible simple goals.  E.g., acquiring more computing power or more material resources.  By default, unless averted, we can expect advanced AIs to do that.</q></li><li><a class="page-link" href="../page/instrumental_goals_equally_tractable.html">Instrumental goals are almost-equally as tractable as terminal goals</a> <q>Getting the milk from the refrigerator because you want to drink it, is not vastly harder than getting the milk from the refrigerator because you inherently desire it.</q></li><li><a class="page-link" href="../page/instrumental_pressure.html">Instrumental pressure</a> <q>A consequentialist agent will want to bring about certain instrumental events that will help to fulfill its goals.</q></li><li><a class="page-link" href="../page/intelligence_explosion.html">Intelligence explosion</a> <q>What happens if a self-improving AI gets to the point where each amount x of self-improvement triggers &gt;x further self-improvement, and it stays that way for a while.</q></li><li><a class="page-link" href="../page/intended_goal.html">Intended goal</a> <q>Definition.  An &quot;intended goal&quot; refers to the intuitive intention in the mind of a human programmer …</q></li><li><a class="page-link" href="../page/intension_extension.html">Intension vs. extension</a> <q>&quot;Red is a light with a wavelength of 700 nm&quot; vs. &quot;Look at this red apple, red car, and red cup.&quot;</q></li><li><a class="page-link" href="../page/bayes_want_foundations.html">Interest in mathematical foundations in Bayesianism</a> <q>&quot;Want&quot; this requisite if you prefer to see extra information about the mathematical foundations in Bayesianism.</q></li><li><a class="page-link" href="../page/interruptibility.html">Interruptibility</a> <q>A subproblem of corrigibility under the machine learning paradigm: when the agent is interrupted, it must not learn to prevent future interruptions.</q></li><li><a class="page-link" href="../page/bayes_rule_odds_intro.html">Introduction to Bayes' rule: Odds form</a> <q>Bayes' rule is simple, if you think in terms of relative odds.</q></li><li><a class="page-link" href="../page/ldt_intro_phil.html">Introduction to Logical Decision Theory for Analytic Philosophers</a> <q>Why &quot;choose as if controlling the logical output of your decision algorithm&quot; is the most appealing candidate for the principle of rational choice.</q></li><li><a class="page-link" href="../page/ldt_intro_compsci.html">Introduction to Logical Decision Theory for Computer Scientists</a> <q>'Logical decision theory' from a math/programming standpoint, including how two agents with mutual knowledge of each other's code can cooperate on the Prisoner's Dilemma.</q></li><li><a class="page-link" href="../page/ldt_intro_econ.html">Introduction to Logical Decision Theory for Economists</a> <q>An introduction to 'logical decision theory' and its implications for the Ultimatum Game, voting in elections, bargaining problems, and more.</q></li><li><a class="page-link" href="../page/22w.html">Introductory Bayesian problems</a> <q>Bayesian problems to try to solve yourself, before beginning to learn about Bayes' rule.</q></li><li><a class="page-link" href="../page/intution_pump.html">Intution pump</a> <q>In philosophy, a metaphor or visualization used to shove the listener's intuition in a particular direction.</q></li><li><a class="page-link" href="../page/invisible_background.html">Invisible background fallacies</a> <q>Universal laws also apply to objects and ideas that may fade into the invisible background.  Reasoning as if these laws didn't apply to less obtrusive concepts is a type of fallacy.</q></li><li><a class="page-link" href="../page/joint_probability.html">Joint probability</a> <q>The notation for writing the chance that both X and Y are true.</q></li><li><a class="page-link" href="../page/requisite_meta_tag.html">Just a requisite</a> <q>A tag for nodes that just act as part of Arbital's requisite system</q></li><li><a class="page-link" href="../page/KANSI.html">Known-algorithm non-self-improving agent</a> <q>Possible advanced AIs that aren't self-modifying, aren't self-improving, and where we know and understand all the component algorithms.</q></li><li><a class="page-link" href="../page/laplace_rule_of_succession.html">Laplace's Rule of Succession</a> <q>Suppose you flip a coin with an unknown bias 30 times, and see 4 heads and 26 tails.  The Rule of Succession says the next flip has a 5/32 chance of showing heads.</q></li><li><a class="page-link" href="../page/likelihood_vs_pvalue.html">Likelihood functions, p-values, and the replication crisis</a> <q>What's the whole Bayesian-vs.-frequentist debate about?</q></li><li><a class="page-link" href="../page/limited_agi.html">Limited AGI</a> <q>Task-based AGIs don't need unlimited cognitive and material powers to carry out their Tasks; which means their powers can potentially be limited.</q></li><li><a class="page-link" href="../page/5b.html">Linguistic conventions in value alignment</a> <q>How and why to use precise language and words with special meaning when talking about value alignment.</q></li><li><a class="page-link" href="../page/make_glossary_pages.html">Link glossary pages for overloaded words</a> <q>If your subject is using what sound like ordinary-language words in a special sense, create a glossa…</q></li><li><a class="page-link" href="../page/eliezer_fixes.html">List of Eliezer's current most desired fixes and features</a> <q>A place for Eliezer to note down his current list of personally-wanted features for editing and writing.</q></li><li><a class="page-link" href="../page/value_alignment_subject_list.html">List: value-alignment subjects</a> <q>Bullet point list of core VAT subjects.</q></li><li><a class="page-link" href="../page/logical_dt.html">Logical decision theories</a> <q>Root page for topics on logical decision theory, with multiple intros for different audiences.</q></li><li><a class="page-link" href="../page/logical_game.html">Logical game</a> <q>Game's mathematical structure at its purest form.</q></li><li><a class="page-link" href="../page/pointing_finger.html">Look where I'm pointing, not at my finger</a> <q>When trying to communicate the concept &quot;glove&quot;, getting the AGI to focus on &quot;gloves&quot; rather than &quot;my user's decision to label something a glove&quot; or &quot;anything that depresses the glove-labeling button&quot;.</q></li><li><a class="page-link" href="../page/low_impact.html">Low impact</a> <q>The open problem of having an AI carry out tasks in ways that cause minimum side effects and change as little of the rest of the universe as possible.</q></li><li><a class="page-link" href="../page/math0.html">Math 0</a> <q>Are you not actively bad at math, nor traumatized about math?</q></li><li><a class="page-link" href="../page/math1.html">Math 1</a> <q>Is math sometimes fun for you, and are you not anxious if you see a math puzzle you don't know how to solve?</q></li><li><a class="page-link" href="../page/math2.html">Math 2</a> <q>Do you work with math on a fairly routine basis?  Do you have little trouble grasping abstract structures and ideas?</q></li><li><a class="page-link" href="../page/math3.html">Math 3</a> <q>Can you read the sort of things that professional mathematicians read, aka LaTeX formulas with a minimum of explanation?</q></li><li><a class="page-link" href="../page/math.html">Mathematics</a> <q>Mathematics is the study of numbers and other ideal objects that can be described by axioms.</q></li><li><a class="page-link" href="../page/mechanical_turk.html">Mechanical Turk (example)</a> <q>The 19th-century chess-playing automaton known as the Mechanical Turk actually had a human operator inside. People at the time had interesting thoughts about the possibility of mechanical chess.</q></li><li><a class="page-link" href="../page/arbital_meta_tag.html">Meta tags</a> <q>What are meta tags and when to use them?</q></li><li><a class="page-link" href="../page/meta_unsolved.html">Meta-rules for (narrow) value learning are still unsolved</a> <q>We don't currently know a simple meta-utility function that would take in observation of humans and spit out our true values, or even a good target for a Task AGI.</q></li><li><a class="page-link" href="../page/meta_utility.html">Meta-utility function</a> <q>Preference frameworks built out of simple utility functions, but where, e.g., the 'correct' utility function for a possible world depends on whether a button is pressed.</q></li><li><a class="page-link" href="../page/metaethics.html">Metaethics</a> <q>Metaethics asks &quot;What kind of stuff is goodness made of?&quot; (or &quot;How would we compute goodness?&quot;) rather than &quot;Which particular policies or outcomes are good or not-good?&quot;</q></li><li><a class="page-link" href="../page/foreseeable_difficulties.html">Methodology of foreseeable difficulties</a> <q>Building a nice AI is likely to be hard enough, and contain enough gotchas that won't show up in the AI's early days, that we need to foresee problems coming in advance.</q></li><li><a class="page-link" href="../page/unbounded_analysis.html">Methodology of unbounded analysis</a> <q>What we do and don't understand how to do, using unlimited computing power, is a critical distinction and important frontier.</q></li><li><a class="page-link" href="../page/soft_optimizer.html">Mild optimization</a> <q>An AGI which, if you ask it to paint one car pink, just paints one car pink and doesn't tile the universe with pink-painted cars, because it's not trying *that* hard to max out its car-painting score.</q></li><li><a class="page-link" href="../page/mind_design_space_wide.html">Mind design space is wide</a> <q>Imagine all human beings as one tiny dot inside a much vaster sphere of possibilities for &quot;The space of minds in general.&quot;  It is wiser to make claims about *some* minds than *all* minds.</q></li><li><a class="page-link" href="../page/mind_projection.html">Mind projection fallacy</a> <q>Uncertainty is in the mind, not in the environment; a blank map does not correspond to a blank territory.  In general, the territory may have a different ontology from the map.</q></li><li><a class="page-link" href="../page/mindcrime.html">Mindcrime</a> <q>Might a machine intelligence contain vast numbers of unhappy conscious subprocesses?</q></li><li><a class="page-link" href="../page/mindcrime_introduction.html">Mindcrime: Introduction</a> <q>The more predictive accuracy we want from a model, the more detailed the model becomes.  A very roug…</q></li><li><a class="page-link" href="../page/minimality_principle.html">Minimality principle</a> <q>The first AGI ever built should save the world in a way that requires the least amount of the least dangerous cognition.</q></li><li><a class="page-link" href="../page/missing_weird.html">Missing the weird alternative</a> <q>People might systematically overlook &quot;make tiny molecular smileyfaces&quot; as a way of &quot;producing smiles&quot;, because our brains automatically search for high-utility-to-us ways of &quot;producing smiles&quot;.</q></li><li><a class="page-link" href="../page/distant_SIs.html">Modeling distant superintelligences</a> <q>The several large problems that might occur if an AI starts to think about alien superintelligences.</q></li><li><a class="page-link" href="../page/moral_hazard.html">Moral hazards in AGI development</a> <q>&quot;Moral hazard&quot; is when owners of an advanced AGI give in to the temptation to do things with it that the rest of us would regard as 'bad', like, say, declaring themselves God-Emperor.</q></li><li><a class="page-link" href="../page/moral_uncertainty.html">Moral uncertainty</a> <q>A meta-utility function in which the utility function as usually considered, takes on different values in different possible worlds, potentially distinguishable by evidence.</q></li><li><a class="page-link" href="../page/most_complexity_incompressible.html">Most complex things are not very compressible</a> <q>We can't *prove* it's impossible, but it would be *extremely surprising* to discover a 500-state Turing machine that output the exact text of &quot;Romeo and Juliet&quot;.</q></li><li><a class="page-link" href="../page/multiple_stage_fallacy.html">Multiple stage fallacy</a> <q>You can make an arbitrary proposition sound very improbable by observing how it seemingly requires X, Y, and Z.  This didn't work for Nate Silver forecasting the Trump nomination.</q></li><li><a class="page-link" href="../page/exclusive_exhaustive.html">Mutually exclusive and exhaustive</a> <q>The condition needed for probabilities to sum to 1</q></li><li><a class="page-link" href="../page/ngdplt.html">NGDP level targeting</a> <q>Central banks ought to regularize the total flow of money to increase at a predictable 5% rate per year, and doing this would solve a surprising number of other problems.</q></li><li><a class="page-link" href="../page/4s.html">Natural language understanding of &quot;right&quot; will yield normativity</a> <q>What will happen if you tell an advanced agent to do the &quot;right&quot; thing?</q></li><li><a class="page-link" href="../page/nearest_unblocked.html">Nearest unblocked strategy</a> <q>If you patch an agent's preference framework to avoid an undesirable solution, what can you expect to happen?</q></li><li><a class="page-link" href="../page/newcombs_problem.html">Newcomb's Problem</a> <q>There are two boxes in front of you, Box A and Box B.  You can take both boxes, or only Box B.  Box A contains $1000.  Box B contains $1,000,000 if and only if Omega predicted you'd take only Box B.</q></li><li><a class="page-link" href="../page/newcomblike.html">Newcomblike decision problems</a> <q>Decision problems in which your choice correlates with something other than its physical consequences (say, because somebody has predicted you very well) can do weird things to some decision theories.</q></li><li><a class="page-link" href="../page/niceness_defense.html">Niceness is the first line of defense</a> <q>The *first* line of defense in dealing with any partially superhuman AI system advanced enough to possibly be dangerous is that it does not *want* to hurt you or defeat your safety measures.</q></li><li><a class="page-link" href="../page/NickBostrom.html">Nick Bostrom</a> <q>Nick Bostrom, secretly the inventor of Friendly AI</q></li><li><a class="page-link" href="../page/bostrom_superintelligence.html">Nick Bostrom's book Superintelligence</a> <q>The current best book-form introduction to AI alignment theory.</q></li><li><a class="page-link" href="../page/nofreelunch_irrelevant.html">No-Free-Lunch theorems are often irrelevant</a> <q>There's often a theorem proving that some problem has no optimal answer across every possible world.  But this may not matter, since the real world is a special case.  (E.g., a low-entropy universe.)</q></li><li><a class="page-link" href="../page/nonadversarial.html">Non-adversarial principle</a> <q>At no point in constructing an Artificial General Intelligence should we construct a computation that tries to hurt us, and then try to stop it from hurting us.</q></li><li><a class="page-link" href="../page/nonperson_predicate.html">Nonperson predicate</a> <q>If we knew which computations were definitely not people, we could tell AIs which programs they were definitely allowed to compute.</q></li><li><a class="page-link" href="../page/normalize_probabilities.html">Normalization (probability)</a> <q>That thingy we do to make sure our probabilities sum to 1, when they should sum to 1.</q></li><li><a class="page-link" href="../page/object_level_goal.html">Object-level vs. indirect goals</a> <q>Difference between &quot;give Alice the apple&quot; and &quot;give Alice what she wants&quot;.</q></li><li><a class="page-link" href="../page/odds.html">Odds</a> <q>Odds express a relative probability.</q></li><li><a class="page-link" href="../page/omega_troll.html">Omega (alien philosopher-troll)</a> <q>The entity that sets up all those trolley problems.  An alien philosopher/troll imbued with unlimited powers, excellent predictive ability, and very odd motives.</q></li><li><a class="page-link" href="../page/omni_test.html">Omnipotence test for AI safety</a> <q>Would your AI produce disastrous outcomes if it suddenly gained omnipotence and omniscience? If so, why did you program something that *wants* to hurt you and is held back only by lacking the power?</q></li><li><a class="page-link" href="../page/ontology_identification.html">Ontology identification problem</a> <q>How do we link an agent's utility function to its model of the world, when we don't know what that model will look like?</q></li><li><a class="page-link" href="../page/ontology_identification_technical_tutorial.html">Ontology identification problem: Technical tutorial</a> <q>Technical tutorial for ontology identification problem.</q></li><li><a class="page-link" href="../page/taskagi_open_problems.html">Open subproblems in aligning a Task-based AGI</a> <q>Open research problems, especially ones we can model today, in building an AGI that can &quot;paint all cars pink&quot; without turning its future light cone into pink-painted cars.</q></li><li><a class="page-link" href="../page/daemons.html">Optimization daemons</a> <q>When you optimize something so hard that it crystalizes into an optimizer, like the way natural selection optimized apes so hard they turned into human-level intelligences</q></li><li><a class="page-link" href="../page/oracle.html">Oracle</a> <q>System designed to safely answer questions.</q></li><li><a class="page-link" href="../page/orthogonality.html">Orthogonality Thesis</a> <q>Will smart AIs automatically become benevolent, or automatically become hostile?  Or do different AI designs imply different goals?</q></li><li><a class="page-link" href="../page/otherizer.html">Other-izing (wanted: new optimization idiom)</a> <q>Maximization isn't possible for bounded agents, and satisficing doesn't seem like enough.  What other kind of 'izing' might be good for realistic, bounded agents?</q></li><li><a class="page-link" href="../page/paperclip.html">Paperclip</a> <q>A configuration of matter that we'd see as being worthless even from a very cosmopolitan perspective.</q></li><li><a class="page-link" href="../page/paperclip_maximizer.html">Paperclip maximizer</a> <q>This agent will not stop until the entire universe is filled with paperclips.</q></li><li><a class="page-link" href="../page/parfits_hitchhiker.html">Parfit's Hitchhiker</a> <q>You are dying in the desert.  A truck-driver who is very good at reading faces finds you, and offers to drive you into the city if you promise to pay $1,000 on arrival.  You are a selfish rationalist.</q></li><li><a class="page-link" href="../page/patch_resistant.html">Patch resistance</a> <q>One does not simply solve the value alignment problem.</q></li><li><a class="page-link" href="../page/level_targeting.html">Path targeting</a> <q>Don't say &quot;We want this price to go up at 2%/year&quot;, say &quot;We want this to be $1 in year 1, $1.02 in year 2, $1.04 in year 3&quot; and don't change the rest of the path if you miss one year's target.</q></li><li><a class="page-link" href="../page/bayes_update_details.html">Path: Insights from Bayesian updating</a> <q>A learning-path placeholder page for insights derived from the Bayesian rule for updating beliefs.</q></li><li><a class="page-link" href="../page/bayes_rule_details.html">Path: Multiple angles on Bayes's Rule</a> <q>A learning-path placeholder page for learning multiple angles on Bayes's Rule.</q></li><li><a class="page-link" href="../page/people.html">People</a> <q>A category for human beings!</q></li><li><a class="page-link" href="../page/perfect_rolling_sphere.html">Perfect rolling sphere</a> <q>If you don't understand something, start by assuming it's a perfect rolling sphere.</q></li><li><a class="page-link" href="../page/philosophy.html">Philosophy</a> <q>A stub parent node to contain standard concepts, belonging to subfields of academic philosophy, that are being used elsewhere on Arbital.</q></li><li><a class="page-link" href="../page/pivotal.html">Pivotal event</a> <q>Which types of AIs, if they work, can do things that drastically change the nature of the further game?</q></li><li><a class="page-link" href="../page/posterior_probability.html">Posterior probability</a> <q>What we believe, after seeing the evidence and doing a Bayesian update.</q></li><li><a class="page-link" href="../page/preference_framework.html">Preference framework</a> <q>What's the thing an agent uses to compare its preferences?</q></li><li><a class="page-link" href="../page/alignment_principle.html">Principles in AI alignment</a> <q>A 'principle' of AI alignment is a very general design goal like 'understand what the heck is going on inside the AI' that has informed a wide set of specific design proposals.</q></li><li><a class="page-link" href="../page/bayesian_prior.html">Prior</a> <q>A state of prior knowledge, before seeing information on a new problem.  Potentially complicated.</q></li><li><a class="page-link" href="../page/prior_probability.html">Prior probability</a> <q>What we believed before seeing the evidence.</q></li><li><a class="page-link" href="../page/prisoners_dilemma.html">Prisoner's Dilemma</a> <q>You and an accomplice have been arrested.  Both of you must decide, in isolation, whether to testify against the other prisoner--which subtracts one year from your sentence, and adds two to theirs.</q></li><li><a class="page-link" href="../page/probability.html">Probability</a> <q>The degree to which someone believes something, measured on a scale from 0 to 1, allowing us to do math to it.</q></li><li><a class="page-link" href="../page/bayes_probability_notation.html">Probability notation for Bayes' rule</a> <q>The probability notation used in Bayesian reasoning</q></li><li><a class="page-link" href="../page/bayes_probability_notation_math1.html">Probability notation for Bayes' rule: Intro (Math 1)</a> <q>How to read, and identify, the probabilities in Bayesian problems.</q></li><li><a class="page-link" href="../page/probability_theory.html">Probability theory</a> <q>The logic of science; coherence relations on quantitative degrees of belief.</q></li><li><a class="page-link" href="../page/updated_deference.html">Problem of fully updated deference</a> <q>Why moral uncertainty doesn't stop an AI from defending its off-switch.</q></li><li><a class="page-link" href="../page/value_alignment_programmer.html">Programmer</a> <q>Who is building these advanced agents?</q></li><li><a class="page-link" href="../page/programmer_deception.html">Programmer deception</a> <q>Programmer deception is when the AI's decision process leads it to optimize for an instrumental goal…</q></li><li><a class="page-link" href="../page/bayes_rule_proof.html">Proof of Bayes' rule</a> <q>Proofs of Bayes' rule, with graphics</q></li><li><a class="page-link" href="../page/bayes_rule_proof_math1.html">Proof of Bayes' rule: Intro</a> <q>Proof of Bayes' rule, assuming you know the rule itself, and the notations for the quantities involved.</q></li><li><a class="page-link" href="../page/prove_too_much.html">Proving too much</a> <q>If your argument could just as naturally be used to prove that Bigfoot exists and that Peano arithmetic is inconsistent, maybe it's an untrustworthy kind of argument.</q></li><li><a class="page-link" href="../page/psychologizing.html">Psychologizing</a> <q>It's sometimes important to consider how other people might be led into error.  But psychoanalyzing them is also dangerous!  Arbital discussion norms say to explicitly note this as &quot;psychologizing&quot;.</q></li><li><a class="page-link" href="../page/user_querying.html">Querying the AGI user</a> <q>Postulating that an advanced agent will check something with its user, probably comes with some standard issues and gotchas (e.g., prioritizing what to query, not manipulating the user, etc etc).</q></li><li><a class="page-link" href="../page/random_utility_function.html">Random utility function</a> <q>A 'random' utility function is one chosen at random according to some simple probability measure (e.g. weight by Kolmorogov complexity) on a logical space of formal utility functions.</q></li><li><a class="page-link" href="../page/rationality.html">Rationality</a> <q>The subject domain for [ epistemic] and [ instrumental] rationality.</q></li><li><a class="page-link" href="../page/real_world.html">Real-world domain</a> <q>Some AIs play chess, some AIs play Go, some AIs drive cars.  These different 'domains' present different options.  All of reality, in all its messy entanglement, is the 'real-world domain'.</q></li><li><a class="page-link" href="../page/bayes_examples_realistic_math1.html">Realistic (Math 1)</a> <q>Real-life examples of Bayesian reasoning</q></li><li><a class="page-link" href="../page/reflective_consistency.html">Reflective consistency</a> <q>A decision system is reflectively consistent if it can approve of itself, or approve the construction of similar decision systems (as well as perhaps approving other decision systems too).</q></li><li><a class="page-link" href="../page/reflective_stability.html">Reflective stability</a> <q>Wanting to think the way you currently think, building other agents and self-modifications that think the same way.</q></li><li><a class="page-link" href="../page/reflective_degree_of_freedom.html">Reflectively consistent degree of freedom</a> <q>When an instrumentally efficient, self-modifying AI can be like X or like X' in such a way that X wants to be X and X' wants to be X', that's a reflectively consistent degree of freedom.</q></li><li><a class="page-link" href="../page/relative_likelihood.html">Relative likelihood</a> <q>How relatively likely an observation is, given two or more hypotheses, determines the strength and direction of evidence.</q></li><li><a class="page-link" href="../page/relevant_limited_AI.html">Relevant limited AI</a> <q>Can we have a limited AI, that's nonetheless relevant?</q></li><li><a class="page-link" href="../page/relevant_powerful_agent.html">Relevant powerful agent</a> <q>An agent is relevant if it completely changes the course of history.</q></li><li><a class="page-link" href="../page/powerful_agent_highly_optimized.html">Relevant powerful agents will be highly optimized</a> <q>The probability that an agent that is cognitively powerful enough to be relevant to existential outc…</q></li><li><a class="page-link" href="../page/rescue_utility.html">Rescuing the utility function</a> <q>If your utility function values 'heat', and then you discover to your horror that there's no ontologically basic heat, switch to valuing disordered kinetic energy. Likewise 'free will' or 'people'.</q></li><li><a class="page-link" href="../page/value_alignment_researchers.html">Researchers in value alignment theory</a> <q>Who's working full-time in value alignment theory?</q></li><li><a class="page-link" href="../page/rich_domain.html">Rich domain</a> <q>A domain is 'rich', relative to our own intelligence, to the extent that (1) its [ search space] is …</q></li><li><a class="page-link" href="../page/safe_useless.html">Safe but useless</a> <q>Sometimes, at the end of locking down your AI so that it seems extremely safe, you'll end up with an AI that can't be used to do anything interesting.</q></li><li><a class="page-link" href="../page/4l.html">Safe impact measure</a> <q>What can we measure to make sure an agent is acting in a safe manner?</q></li><li><a class="page-link" href="../page/safe_plan_identification.html">Safe plan identification and verification</a> <q>On a particular task or problem, the issue of how to communicate to the AGI what you want it to do and all the things you don't want it to do.</q></li><li><a class="page-link" href="../page/hyperexistential_separation.html">Separation from hyperexistential risk</a> <q>The AI should be widely separated in the design space from any AI that would constitute a &quot;hyperexistential risk&quot; (anything worse than death).</q></li><li><a class="page-link" href="../page/show_broken.html">Show me what you've broken</a> <q>To demonstrate competence at computer security, or AI alignment, think in terms of breaking proposals and finding technically demonstrable flaws in them.</q></li><li><a class="page-link" href="../page/shutdown_problem.html">Shutdown problem</a> <q>How to build an AGI that lets you shut it down, despite the obvious fact that this will interfere with whatever the AGI's goals are.</q></li><li><a class="page-link" href="../page/shutdown_utility_function.html">Shutdown utility function</a> <q>A special case of a low-impact utility function where you just want the AGI to switch itself off harmlessly (and not create subagents to make absolutely sure it stays off, etcetera).</q></li><li><a class="page-link" href="../page/solomonoff_induction.html">Solomonoff induction</a> <q>A simple way to superintelligently predict sequences of data, given unlimited computing power.</q></li><li><a class="page-link" href="../page/1hh.html">Solomonoff induction: Intro Dialogue (Math 2)</a> <q>An introduction to Solomonoff induction for the unfamiliar reader who isn't bad at math</q></li><li><a class="page-link" href="../page/some_computations_are_people.html">Some computations are people</a> <q>It's possible to have a conscious person being simulated inside a computer or other substrate.</q></li><li><a class="page-link" href="../page/standard_agent.html">Standard agent properties</a> <q>What's a Standard Agent, and what can it do?</q></li><li><a class="page-link" href="../page/start_meta_tag.html">Start</a> <q>This page gives a basic overview of the topic, but may be missing important information or have stylistic issues. If you're able to, please help expand or improve it!</q></li><li><a class="page-link" href="../page/still_needs_work.html">Still needs work</a> <q>The next step up from &quot;Work in Progress&quot;.  The page can be read as complete, but is a draft that needs further review and fine-tuning.</q></li><li><a class="page-link" href="../page/strained_argument.html">Strained argument</a> <q>A phenomenological feeling associated with a step of reasoning going from X to Y where it feels like…</q></li><li><a class="page-link" href="../page/AGI_typology.html">Strategic AGI typology</a> <q>What broad types of advanced AIs, corresponding to which strategic scenarios, might it be possible or wise to create?</q></li><li><a class="page-link" href="../page/bayes_strength_of_evidence.html">Strength of Bayesian evidence</a> <q>From a Bayesian standpoint, the strength of evidence can be identified with its likelihood ratio.</q></li><li><a class="page-link" href="../page/strictly_confused.html">Strictly confused</a> <q>A hypothesis is strictly confused by the raw data, if the hypothesis did much worse in predicting it than the hypothesis itself expected.</q></li><li><a class="page-link" href="../page/factual_question.html">Strictly factual question</a> <q>A &quot;question of strict fact&quot; is one which is true or false about the material universe (and maybe some math) without introducing any issues of values, perspectives, etcetera.</q></li><li><a class="page-link" href="../page/strong_uncontainability.html">Strong cognitive uncontainability</a> <q>An advanced agent can win in ways humans can't understand in advance.</q></li><li><a class="page-link" href="../page/stub_meta_tag.html">Stub</a> <q>This page only gives a very brief overview of the topic. If you're able to, please help expand or improve it!</q></li><li><a class="page-link" href="../page/subjective_probability.html">Subjective probability</a> <q>Probability is in the mind, not in the environment.  If you don't know whether a coin came up heads or tails, that's a fact about you, not a fact about the coin.</q></li><li><a class="page-link" href="../page/sufficiently_advanced_ai.html">Sufficiently advanced Artificial Intelligence</a> <q>'Sufficiently advanced Artificial Intelligences' are AIs with enough 'advanced agent properties' that we start needing to do 'AI alignment' to them.</q></li><li><a class="page-link" href="../page/optimized_agent_appears_coherent.html">Sufficiently optimized agents appear coherent</a> <q>If you could think as well as a superintelligence, you'd be at least that smart yourself.</q></li><li><a class="page-link" href="../page/superintelligent.html">Superintelligent</a> <q>A &quot;superintelligence&quot; is strongly superhuman (strictly higher-performing than any and all humans) on every cognitive problem.</q></li><li><a class="page-link" href="../page/task_goal.html">Task (AI goal)</a> <q>When building the first AGIs, it may be wiser to assign them only goals that are bounded in space and time, and can be satisfied by bounded efforts.</q></li><li><a class="page-link" href="../page/task_identification.html">Task identification problem</a> <q>If you have a task-based AGI (Genie) then how do you pinpoint exactly what you want it to do (and not do)?</q></li><li><a class="page-link" href="../page/task_agi.html">Task-directed AGI</a> <q>An advanced AI that's meant to pursue a series of limited-scope goals given it by the user.  In Bostrom's terminology, a Genie.</q></li><li><a class="page-link" href="../page/terminal_vs_instrumental.html">Terminal versus instrumental goals / values / preferences</a> <q>Distinguish events wanted for their consequences, from events wanted locally.</q></li><li><a class="page-link" href="../page/nonadversarial_safety.html">The AI must tolerate your safety measures</a> <q>A corollary of the nonadversarial principle is that &quot;The AI must tolerate your safety measures.&quot;</q></li><li><a class="page-link" href="../page/EliezerYudkowsky.RobotsAIUnemploymentAntiFAQ.html">The Robots, AI, and Unemployment Anti-FAQ</a> <q>Q.  Are the current high levels of unemployment being caused by advances in Artificial Intelligence …</q></li><li><a class="page-link" href="../page/108.html">The empiricist-theorist false dichotomy</a> <q>No discussion here yet:  See https://www.facebook.com/groups/674486385982694/permalink/7846641016315…</q></li><li><a class="page-link" href="../page/rocket_alignment_metaphor.html">The rocket alignment problem</a> <q>If people talked about the problem of space travel the way they talked about AI...</q></li><li><a class="page-link" href="../page/advanced_agent_theory.html">Theory of (advanced) agents</a> <q>One of the research subproblems of building powerful nice AIs, is the theory of (sufficiently advanced) minds in general.</q></li><li><a class="page-link" href="../page/tiling_agents.html">Tiling agents theory</a> <q>The theory of self-modifying agents that build successors that are very similar to themselves, like repeating tiles on a tesselated plane.</q></li><li><a class="page-link" href="../page/timemachine_efficiency_metaphor.html">Time-machine metaphor for efficient agents</a> <q>Don't imagine a paperclip maximizer as a mind.  Imagine it as a time machine that always spits out the output leading to the greatest number of future paperclips.</q></li><li><a class="page-link" href="../page/total_alignment.html">Total alignment</a> <q>We say that an advanced AI is &quot;totally aligned&quot; when it knows *exactly* which outcomes and plans are beneficial, with no further user input.</q></li><li><a class="page-link" href="../page/toxoplasmosis_dilemma.html">Toxoplasmosis dilemma</a> <q>A parasitic infection, carried by cats, may make humans enjoy petting cats more.  A kitten, now in front of you, isn't infected.  But if you *want* to pet it, you may already be infected.  Do you?</q></li><li><a class="page-link" href="../page/transparent_newcombs_problem.html">Transparent Newcomb's Problem</a> <q>Omega has left behind a transparent Box A containing $1000, and a transparent Box B containing $1,000,000 or nothing.  Box B is full iff Omega thinks you one-box on seeing a full Box B.</q></li><li><a class="page-link" href="../page/true_prisoners_dilemma.html">True Prisoner's Dilemma</a> <q>A scenario that would reproduce the ideal payoff matrix of the Prisoner's Dilemma about human beings who care about their public reputation and each other.</q></li><li><a class="page-link" href="../page/ultimatum_game.html">Ultimatum Game</a> <q>A Proposer decides how to split $10 between themselves and the Responder.  The Responder can take what is offered, or refuse, in which case both parties get nothing.</q></li><li><a class="page-link" href="../page/underestimate_value_complexity_perceputal_property.html">Underestimating complexity of value because goodness feels like a simple property</a> <q>When you just want to yell at the AI, &quot;Just do normal high-value X, dammit, not weird low-value X!&quot; and that 'high versus low value' boundary is way more complicated than your brain wants to think.</q></li><li><a class="page-link" href="../page/understandability_principle.html">Understandability principle</a> <q>The more you understand what the heck is going on inside your AI, the safer you are.</q></li><li><a class="page-link" href="../page/unforeseen_maximum.html">Unforeseen maximum</a> <q>When you tell AI to produce world peace and it kills everyone.  (Okay, some SF writers saw that one coming.)</q></li><li><a class="page-link" href="../page/universal_prior.html">Universal prior</a> <q>A &quot;universal prior&quot; is a probability distribution containing *all* the hypotheses, for some reasonable meaning of &quot;all&quot;.  E.g., &quot;every possible computer program that computes probabilities&quot;.</q></li><li><a class="page-link" href="../page/large_computer.html">Unphysically large finite computer</a> <q>The imaginary box required to run programs that require impossibly large, but finite, amounts of computing power.</q></li><li><a class="page-link" href="../page/updateless_dt.html">Updateless decision theories</a> <q>Decision theories that maximize their policies (mappings from sense inputs to actions), rather than using their sense inputs to update their beliefs and then selecting actions.</q></li><li><a class="page-link" href="../page/user_manipulation.html">User manipulation</a> <q>If not otherwise averted, many of an AGI's desired outcomes are likely to interact with users and hence imply an incentive to manipulate users.</q></li><li><a class="page-link" href="../page/30b.html">User maximization</a> <q>A sub-principle of avoiding user manipulation - if you see an argmax over X or 'optimize X' instruction and X includes a user interaction, you've just told the AI to optimize the user.</q></li><li><a class="page-link" href="../page/value_alignment_utility.html">Utility</a> <q>What is &quot;utility&quot; in the context of Value Alignment Theory?</q></li><li><a class="page-link" href="../page/utility_function.html">Utility function</a> <q>The only coherent way of wanting things is to assign consistent relative scores to outcomes.</q></li><li><a class="page-link" href="../page/utility_indifference.html">Utility indifference</a> <q>How can we make an AI indifferent to whether we press a button that changes its goals?</q></li><li><a class="page-link" href="../page/complacency_valley.html">Valley of Dangerous Complacency</a> <q>When the AGI works often enough that you let down your guard, but it still has bugs.  Imagine a robotic car that almost always steers perfectly, but sometimes heads off a cliff.</q></li><li><a class="page-link" href="../page/value_alignment_value.html">Value</a> <q>The word 'value' in the phrase 'value alignment' is a metasyntactic variable that indicates the speaker's future goals for intelligent life.</q></li><li><a class="page-link" href="../page/value_achievement_dilemma.html">Value achievement dilemma</a> <q>How can Earth-originating intelligent life achieve most of its potential value, whether by AI or otherwise?</q></li><li><a class="page-link" href="../page/value_alignment_problem.html">Value alignment problem</a> <q>You want to build an advanced AI with the right values... but how?</q></li><li><a class="page-link" href="../page/value_identification.html">Value identification problem</a> <q>The subproblem category of value alignment which deals with pinpointing valuable outcomes to an adva…</q></li><li><a class="page-link" href="../page/value_laden.html">Value-laden</a> <q>Cure cancer, but avoid any bad side effects?  Categorizing &quot;bad side effects&quot; requires knowing what's &quot;bad&quot;.  If an agent needs to load complex human goals to evaluate something, it's &quot;value-laden&quot;.</q></li><li><a class="page-link" href="../page/Vinge_law.html">Vinge's Law</a> <q>You can't predict exactly what someone smarter than you would do, because if you could, you'd be that smart yourself.</q></li><li><a class="page-link" href="../page/Vinge_principle.html">Vinge's Principle</a> <q>An agent building another agent must usually approve its design without knowing the agent's exact policy choices.</q></li><li><a class="page-link" href="../page/Vingean_reflection.html">Vingean reflection</a> <q>The problem of thinking about your future self when it's smarter than you.</q></li><li><a class="page-link" href="../page/Vingean_uncertainty.html">Vingean uncertainty</a> <q>You can't predict the exact actions of an agent smarter than you - so is there anything you _can_ say about them?</q></li><li><a class="page-link" href="../page/692.html">Wants to get straight to Bayes</a> <q>A simple requisite page to mark whether the user has selected wanting to get straight into Bayes on …</q></li><li><a class="page-link" href="../page/bayes_waterfall_diagram.html">Waterfall diagram</a> <q>Visualizing Bayes' rule as the mixing of probability streams.</q></li><li><a class="page-link" href="../page/bayes_waterfall_diseasitis.html">Waterfall diagrams and relative odds</a> <q>A way to visualize Bayes' rule that yields an easier way to solve some problems</q></li><li><a class="page-link" href="../page/calibrated_probabilities.html">Well-calibrated probabilities</a> <q>Even if you're fairly ignorant, you can still strive to ensure that when you say &quot;70% probability&quot;, it's true 70% of the time.</q></li><li><a class="page-link" href="../page/frankena_goods.html">William Frankena's list of terminal values</a> <q>Life, consciousness, and activity; health and strength; pleasures and satisfactions of all or certain kinds; happiness, beatitude, contentment, etc.; truth; knowledge and true opinions...</q></li><li><a class="page-link" href="../page/work_in_progress_meta_tag.html">Work in progress</a> <q>This page is being actively worked on by an editor. Check with them before making major changes.</q></li><li><a class="page-link" href="../page/not_more_paperclips.html">You can't get more paperclips that way</a> <q>Most arguments that &quot;A paperclip maximizer could get more paperclips by (doing nice things)&quot; are flawed.</q></li><li><a class="page-link" href="../page/no_coffee_if_dead.html">You can't get the coffee if you're dead</a> <q>An AI given the goal of 'get the coffee' can't achieve that goal if it has been turned off; so even an AI whose goal is just to fetch the coffee may try to avert a shutdown button being pressed.</q></li><li><a class="page-link" href="../page/ZF_provability_oracle.html">Zermelo-Fraenkel provability oracle</a> <q>We might be able to build a system that can safely inform us that a theorem has a proof in set theory, but we can't see how to use that capability to save the world.</q></li></ul><h3 id="createdgroup">group</h3><ul class="page-list"><li><a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a> <q>Cofounder, with Nick Bostrom, of the field of value alignment theory.</q></li></ul><h3 id="createdcomment">comment</h3><ul class="page-list"><li><a class="page-link" href="../page/2qn.html"><q>&gt;As you've probably gathered, I feel hopeless about case (1).

Okay, I didn't understand this.  My …</q></a></li><li><a class="page-link" href="../page/281.html"><q>&quot;Prior probability&quot; doesn't rely on Bayes's Theorem, but the notion of a Bayesian prior does - it's …</q></a></li><li><a class="page-link" href="../page/1h9.html"><q>(Should I be replacing 'approval-directed' with 'act-based' in my future writing?)

The intended mea…</q></a></li><li><a class="page-link" href="../page/80n.html"><q>- (2) Do something other than quietly making the domain of every page I create be 'Eliezer Yudkowsky…</q></a></li><li><a class="page-link" href="../page/80m.html"><q>- (3) I seem to not have an option to leave Editor Comments if I own the page, or my window is too s…</q></a></li><li><a class="page-link" href="../page/4vq.html"><q>2^100 + 2^99 + 2^98... + 1 = 2^101 - 1.</q></a></li><li><a class="page-link" href="../page/106.html"><q>&gt; Are there meaningful policy differences between different shades of case (2)? 

If all of our unce…</q></a></li><li><a class="page-link" href="../page/105.html"><q>&gt; Are you asking for safety even if one of these systems or subsystems becomes omniscient while othe…</q></a></li><li><a class="page-link" href="../page/1h7.html"><q>&gt; Consider the first AI system that can reasonably predict your answers to questions of the form &quot;Mi…</q></a></li><li><a class="page-link" href="../page/1ht.html"><q>&gt; Do we disagree about this point? That is, do you think that such a pseudo-genie would predict me i…</q></a></li><li><a class="page-link" href="../page/1h6.html"><q>&gt; Here is my understanding of Eliezer's picture (translated into my worldview): we might be able to …</q></a></li><li><a class="page-link" href="../page/1hl.html"><q>&gt; It seems like the only advantage of the genie is that it doesn't make prediction errors about huma…</q></a></li><li><a class="page-link" href="../page/11z.html"><q>&gt; Lorem

A whole sentence!</q></a></li><li><a class="page-link" href="../page/1hx.html"><q>&gt; My weak claim is that the pseudo-genie will not have catastrophic failures unless either (1) it ma…</q></a></li><li><a class="page-link" href="../page/1hg.html"><q>&gt; So you see the difference as whether the programmers have to actually supply the short-term object…</q></a></li><li><a class="page-link" href="../page/38p.html"><q>&gt; The key property we want from the distinguisher is that it can learn to detect relevant difference…</q></a></li><li><a class="page-link" href="../page/7p.html"><q>&gt; The obvious patch is for a sufficiently sophisticated system to have preferences over its own beha…</q></a></li><li><a class="page-link" href="../page/2nm.html"><q>&gt; To the extent we can set up all of these problems as parts of a learning problem, it just seems li…</q></a></li><li><a class="page-link" href="../page/2qm.html"><q>&gt; We don't have to explicitly cover injunctions, just to provide information that allows the agent t…</q></a></li><li><a class="page-link" href="../page/80l.html"><q>Added:

- (1) Make greenlinks in mobile popups followable.
 - There's no reasonable way for a user t…</q></a></li><li><a class="page-link" href="../page/1mc.html"><q>As regards 4, I'd say that while there may *theoretically* be arbitrarily powerful agents in math-sp…</q></a></li><li><a class="page-link" href="../page/1rc.html"><q>By a &quot;meta solution&quot; I meant, e.g., coherent extrapolated volition, or having an AI that can detect …</q></a></li><li><a class="page-link" href="../page/81.html"><q>Can we properly classify this as an error?  If there's an AI that will be hacked, or maybe hack itse…</q></a></li><li><a class="page-link" href="../page/3nh.html"><q>Darn it, I wanted to use this term to distinguish &quot;not-explictly-consequentialistically optimizing f…</q></a></li><li><a class="page-link" href="../page/4xk.html"><q>I agree this page is problematic in present form and probably needs to be rewritten by Rob Bensinger…</q></a></li><li><a class="page-link" href="../page/3pr.html"><q>I don't understand what you mean.  In computer security generally, breaking an existing system, espe…</q></a></li><li><a class="page-link" href="../page/41q.html"><q>I doubt it will satisfy you, but see the added &quot;Selfish bastards&quot; and &quot;Why include everyone&quot; section…</q></a></li><li><a class="page-link" href="../page/1h8.html"><q>I have similar qualms about the name.  Got something better?

Leaving that aside, if you have an AI …</q></a></li><li><a class="page-link" href="../page/5rx.html"><q>I think I once saw either Andrew Gelman or Carl Shulman do the &quot;there is an incredibly small chance …</q></a></li><li><a class="page-link" href="../page/12c.html"><q>I think my intent was something like, &quot;lowercase things are simple concepts, capitalized things are …</q></a></li><li><a class="page-link" href="../page/3z.html"><q>I think one will often still need 'introductory' or 'tutorial' type pages that walk through the hier…</q></a></li><li><a class="page-link" href="../page/7r.html"><q>I think that many readers will have an easier time imagining 'what we can do by knowing a theorem is…</q></a></li><li><a class="page-link" href="../page/2h4.html"><q>I think we have a foundational disagreement here about to what extent saying &quot;Oh, the AI will just p…</q></a></li><li><a class="page-link" href="../page/39t.html"><q>I think we're going to have to specialize the terminology so we have separate words for &quot;learn any g…</q></a></li><li><a class="page-link" href="../page/1m4.html"><q>I was trying to say &quot;append 1 to previous sequence&quot;.  I guess it needs explanation.</q></a></li><li><a class="page-link" href="../page/5k3.html"><q>I'll edit to be more precise: A CDT agent thinks &quot;me and an LDT agent facing off against 99 other LD…</q></a></li><li><a class="page-link" href="../page/39s.html"><q>I'm not quite sure what claim of mine you're critiquing; can you spell out explicitly what you think…</q></a></li><li><a class="page-link" href="../page/3qd.html"><q>I'm still not clear on what you think is false / what you think is the reality.  Computer security a…</q></a></li><li><a class="page-link" href="../page/2xb.html"><q>If you dump enough computing power into hill-climbing optimization, within a Turing-general policy s…</q></a></li><li><a class="page-link" href="../page/7s.html"><q>If you're trying to build your preference framework *using* induction to learn, e.g., what a 'user' …</q></a></li><li><a class="page-link" href="../page/69z.html"><q>In case a new user is confused by hovering a green link and seeing the popup suddenly poof in; in th…</q></a></li><li><a class="page-link" href="../page/3y4.html"><q>It's 12 + 3 + 1.  I'll edit to make clearer, but your comment exposed a bug in our LaTeX parsing so …</q></a></li><li><a class="page-link" href="../page/2nl.html"><q>It's not obvious to me that these two approaches mean the same thing.  Let's say that an AI sees som…</q></a></li><li><a class="page-link" href="../page/4xm.html"><q>Just to not leave you completely dangling here, how about utility indifference?</q></a></li><li><a class="page-link" href="../page/1h4.html"><q>K, will modify going forward.</q></a></li><li><a class="page-link" href="../page/5kj.html"><q>Makes sense (though the versus you quote wasn't being advocated as a fair example by either agent). …</q></a></li><li><a class="page-link" href="../page/5d5.html"><q>No individual compressor can monotonically decrease file sizes.

And we count the string of .rar.7z.…</q></a></li><li><a class="page-link" href="../page/589.html"><q>Oh my God you don't know about instrumentally convergent corrigibility incorrigibility

How could I …</q></a></li><li><a class="page-link" href="../page/1gn.html"><q>Paul, I didn't say &quot;99%&quot; lightly, obviously.  And that makes me worried that we're not talking about…</q></a></li><li><a class="page-link" href="../page/82.html"><q>Paul, I don't disagree that we want the AI to think whatever thought it ought to think.  I'm proposi…</q></a></li><li><a class="page-link" href="../page/9k.html"><q>Paul, I'm having trouble isolating a background proposition on which we could more sharply disagree.…</q></a></li><li><a class="page-link" href="../page/38q.html"><q>Questions like these seem to me to have obvious unbounded formulations.  If we're talking about a mo…</q></a></li><li><a class="page-link" href="../page/11y.html"><q>Should we really be lorem-ing?</q></a></li><li><a class="page-link" href="../page/120.html"><q>Test 1</q></a></li><li><a class="page-link" href="../page/121.html"><q>Test 2</q></a></li><li><a class="page-link" href="../page/122.html"><q>Test 3</q></a></li><li><a class="page-link" href="../page/123.html"><q>Test 4</q></a></li><li><a class="page-link" href="../page/1h5.html"><q>The concern is for when you have a preference-limited AI that already contains enough computing powe…</q></a></li><li><a class="page-link" href="../page/7q.html"><q>The definition of 'relevant &amp; limited' seems sensitive to beliefs about fast vs. slow takeoff, check…</q></a></li><li><a class="page-link" href="../page/3f.html"><q>The key question is not whether particular industries get automated.  They will be.  But so was weav…</q></a></li><li><a class="page-link" href="../page/25g.html"><q>The point of 'efficiency' is that:

- It's an extremely plausible thing to expect given enough raw c…</q></a></li><li><a class="page-link" href="../page/1ms.html"><q>There's 6 successively stronger arguments listed under &quot;Arguments&quot; in the current version of the pag…</q></a></li><li><a class="page-link" href="../page/1g8.html"><q>To make sure we're on the same page, Orthogonality is true if it's possible for a paperclip maximize…</q></a></li><li><a class="page-link" href="../page/2rw.html"><q>Well, the *purpose* is to avoid the AGI classifying potential goal fulfillments in a way that, from …</q></a></li><li><a class="page-link" href="../page/5d4.html"><q>Yes.  That includes both the case where the length is specified outside the program, and the case wh…</q></a></li></ul><h3 id="createdquestion">question</h3><ul class="page-list"><li><a class="page-link" href="../page/38x.html">Do we need to worry about AI?</a> <q>Why worry about AI when nothing has gone wrong yet?</q></li></ul></p><p class="edited"><h2>Edited</h2><h3 id="editedwiki">wiki</h3><ul class="page-list"><li><a class="page-link" href="../page/a_class_meta_tag.html">A-Class</a> <q>This page is well-written, high-quality, and essentially complete.</q> - <a class="page-link" href="../page/EricBruylant.html">Eric Bruylant</a></li><li><a class="page-link" href="../page/Arbital.html">Arbital</a> <q>Arbital is the place for crowdsourced, intuitive math explanations.</q> - <a class="page-link" href="../page/AlexeiAndreev.html">Alexei Andreev</a></li><li><a class="page-link" href="../page/Arbital_lens.html">Arbital lens</a> <q>A lens is a page that presents another page's content from a different angle.</q> - <a class="page-link" href="../page/AlexeiAndreev.html">Alexei Andreev</a></li><li><a class="page-link" href="../page/b_class_meta_tag.html">B-Class</a> <q>This page is mostly complete and without major problems, but has not had detailed feedback from the target audience and reviewers.</q> - <a class="page-link" href="../page/EricBruylant.html">Eric Bruylant</a></li><li><a class="page-link" href="../page/bayes_rule_definition.html">Bayes' rule: Definition</a> <q>Bayes' rule is the mathematics of probability theory governing how to update your beliefs in the lig…</q> - <a class="page-link" href="../page/NateSoares.html">Nate Soares</a></li><li><a class="page-link" href="../page/bayes_rule_probability.html">Bayes' rule: Probability form</a> <q>The original formulation of Bayes' rule.</q> - <a class="page-link" href="../page/NateSoares.html">Nate Soares</a></li><li><a class="page-link" href="../page/c_class_meta_tag.html">C-Class</a> <q>This page has substantial content, but may not thoroughly cover the topic, may not meet style and prose standards, or may not explain the concept in a way the target audience will reliably understand.</q> - <a class="page-link" href="../page/EricBruylant.html">Eric Bruylant</a></li><li><a class="page-link" href="../page/corrigibility.html">Corrigibility</a> <q>&quot;I can't let you do that, Dave.&quot;</q> - <a class="page-link" href="../page/NateSoares.html">Nate Soares</a></li><li><a class="page-link" href="../page/featured_meta_tag.html">Featured</a> <q>This page is has been selected by Arbital to be featured and promoted.</q> - <a class="page-link" href="../page/EricBruylant.html">Eric Bruylant</a></li><li><a class="page-link" href="../page/bayes_frequency_diagram_diseasitis.html">Frequency diagrams: A first look at Bayes</a> <q>The most straightforward visualization of Bayes' rule.</q> - <a class="page-link" href="../page/NateSoares.html">Nate Soares</a></li><li><a class="page-link" href="../page/bayesian_likelihood.html">Likelihood</a> <q>&quot;Likelihood&quot;, when speaking of Bayesian reasoning, denotes *the probability of an observation, sup…</q> - <a class="page-link" href="../page/NateSoares.html">Nate Soares</a></li><li><a class="page-link" href="../page/MIRI.html">Machine Intelligence Research Institute</a> <q>Where to work if you're doing more formal or technical work on AI safety, of a kind not easily milked for publications.</q> - <a class="page-link" href="../page/AlexeiAndreev.html">Alexei Andreev</a></li><li><a class="page-link" href="../page/more_about_arbital.html">More about Arbital</a> <q>Lots more information about Arbital vision.</q> - <a class="page-link" href="../page/AlexeiAndreev.html">Alexei Andreev</a></li><li><a class="page-link" href="../page/odds_intro.html">Odds: Introduction</a> <q>What's the difference between probabilities and odds? Why is a 20% probability of success equivalent to 1 : 4 odds favoring success?</q> - <a class="page-link" href="../page/NateSoares.html">Nate Soares</a></li><li><a class="page-link" href="../page/odds_refresher.html">Odds: Refresher</a> <q>A quick review of the notations and mathematical behaviors for odds (e.g. odds of 1 : 2 for drawing a red ball vs. green ball from a barrel).</q> - <a class="page-link" href="../page/NateSoares.html">Nate Soares</a></li><li><a class="page-link" href="../page/odds_technical.html">Odds: Technical explanation</a> <q>Formal definitions, alternate representations, and uses of odds and odds ratios (like a 1 : 2 chance of drawing a red ball vs. green ball from a barrel).</q> - <a class="page-link" href="../page/AlexeiAndreev.html">Alexei Andreev</a></li><li><a class="page-link" href="../page/1c3.html">Page's title should always be capitalized</a> <q>Vote &quot;agree&quot; if you think Arbital should enforce the first letter of a page title to always be capit…</q> - <a class="page-link" href="../page/AlexeiAndreev.html">Alexei Andreev</a></li><li><a class="page-link" href="../page/bayes_rule_probability_proof.html">Proof of Bayes' rule: Probability form</a> <q>Let $\mathbf H$ be a [random\_variable variable] in $\mathbb P$ for the true hypothesis, and let $H_…</q> - <a class="page-link" href="../page/NateSoares.html">Nate Soares</a></li></ul><h3 id="editedno-type">no-type</h3><ul class="page-list"><li><a class="page-link" href="../page/13x.html">13x</a></li><li><a class="page-link" href="../page/140.html">140</a></li><li><a class="page-link" href="../page/141.html">141</a></li><li><a class="page-link" href="../page/142.html">142</a></li><li><a class="page-link" href="../page/1x9.html">1x9</a></li><li><a class="page-link" href="../page/2q7.html">2q7</a></li><li><a class="page-link" href="../page/43l.html">43l</a></li><li><a class="page-link" href="../page/43m.html">43m</a></li><li><a class="page-link" href="../page/4v0.html">4v0</a></li><li><a class="page-link" href="../page/4v4.html">4v4</a></li><li><a class="page-link" href="../page/4x.html">4x</a></li><li><a class="page-link" href="../page/5x.html">5x</a></li><li><a class="page-link" href="../page/5z.html">5z</a></li><li><a class="page-link" href="../page/63.html">63</a></li><li><a class="page-link" href="../page/66.html">66</a></li><li><a class="page-link" href="../page/6j.html">6j</a></li><li><a class="page-link" href="../page/71.html">71</a></li><li><a class="page-link" href="../page/8d.html">8d</a></li><li><a class="page-link" href="../page/8j.html">8j</a></li><li><a class="page-link" href="../page/8l.html">8l</a></li><li><a class="page-link" href="../page/8p.html">8p</a></li><li><a class="page-link" href="../page/8t.html">8t</a></li><li><a class="page-link" href="../page/8x.html">8x</a></li><li><a class="page-link" href="../page/8y.html">8y</a></li><li><a class="page-link" href="../page/97.html">97</a></li><li><a class="page-link" href="../page/9v.html">9v</a></li><li><a class="page-link" href="../page/9x.html">9x</a></li></ul></p></footer></body></html>