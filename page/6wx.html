<!DOCTYPE html><html><head><meta charset="utf-8"><title> CFAR should explicitly focus on AI safety</title><link rel="stylesheet" type="text/css" href="../common.css"><link rel="stylesheet" type="text/css" href="../page-style.css"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  SVG: {EqnChunk: 50, EqnChunkFactor: 1.5, EqChunkDelay: 10, useFontCache: false, linebreaks: {automatic: true}},
  tex2jax: {
    inlineMath: [['$~$', '$~$']],
    displayMath: [['$$~$', '$~$$']],
    processEscapes: true,
    preview: 'none',
  },
  showProcessingMessages: false,
  messageStyle: 'none',
  // http://docs.mathjax.org/en/latest/config-files.html#the-tex-ams-svg-configuration-file
  jax: ["input/TeX","output/SVG", "output/PreviewHTML"],
  extensions: ["tex2jax.js","MathMenu.js","MathZoom.js", "fast-preview.js", "AssistiveMML.js", "a11y/accessibility-menu.js"],
  TeX: { extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"] }
});</script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_SVG" async></script><script type="text/javascript" src="../arbital-demo-bundle.js"></script><script type="text/javascript">window.addEventListener('DOMContentLoaded', e=>window.loadAllDemos())
</script></head><body><header><h1 class="title"> CFAR should explicitly focus on AI safety</h1><div class="page-info"><p class="metadata-link"><a href="../metadata/6wx.json.html">6wx.json</a></p><p class="arbital-url"><a href="https://arbital.com/p/6wx">https://arbital.com/p/6wx</a></p><p class="creator">by
 <a class="page-link" href="../page/StephanieZolayvar.html">Stephanie Zolayvar</a> Dec 16 2016</p></div><nav class="breadcrumbs"><nav class="breadcrumb" aria-label="Breadcrumb"><ul><li><a href="../index.html">Index</a></li><li> CFAR should explicitly focus on AI safety</li></ul></nav></nav></header><hr><main><p>The Center for Applied Rationality has historically had a "cause-neutral" mission but has recently revised its mission to partly be focused on AI safety efforts in particular.</p></main><hr><section class="comments"><h2>Comments</h2><div class="comment"><p><a class="page-link" href="../page/AnnaSalamon.html">Anna Salamon</a></p><p><p>I want a wrong question button!! :/</p></p><div class="comment"><p><a class="page-link" href="../page/AnnaSalamon.html">Anna Salamon</a></p><p><p>CFAR should be about "Rationality for its own sake, for the sake of existential risk".  Which is totally different.  I just, um, haven't figured out how to say the actual thing clearly.  Help very welcome.</p></p></div><div class="comment"><p><a class="page-link" href="../page/EricRogstad.html">Eric Rogstad</a></p><p><p>In other words, promoting this claim as worded, is misleading?</p></p></div><div class="comment"><p><a class="page-link" href="../page/AnnaSalamon.html">Anna Salamon</a></p><p><p>Uh, well, it's hard to reply-to, or something?  Like, it wants to jam the conversation into questions about whether the claim is "true" or "false", instead of on questions about what is meant by it or what 3rd alternatives might be available or something?</p></p></div><div class="comment"><p><a class="page-link" href="../page/EricRogstad.html">Eric Rogstad</a></p><p><p>I'd be interested to know if you find yourself having that feeling a lot, while interacting with claims.</p>
<p>If it's a small minority of the time, I think the solution is a "wrong question" button. If it happens a lot, we might need another object type --something like a prompt-for-discussion rather than a claim-to-be-agreed with.</p></p></div></div><div class="comment"><p><a class="page-link" href="../page/TimothyChu.html">Timothy Chu</a></p><p><p>Addressing the post, a focus on AI risk feels like something worth experimenting with. </p>
<p><a href="70q.html">My lame model suggests that the main downside is that it risks the brand</a>. If so, experimenting with AI risk in the CFAR context seems like a potentially high value avenue of exploration, and brand damage can be mitigated. </p>
<p><a href="70r.html">For example, if it turned out to be toxic for the CFAR brand, the same group of people could spin off a new program called something else, and people may not remember or care that it was the old CFAR folks</a>.</p></p></div><div class="comment"><p><a class="page-link" href="../page/ConnorFlexman3.html">Connor Flexman</a></p><p><p>Along with "Growing EA is net-positive", anything with a large search space + value judgment seems like it's going to have this issue. </p></p></div></section><footer><p class="likes"><h2>Likes</h2><span class="page-comma-list"><a class="page-link" href="../page/AlexeiAndreev.html">Alexei Andreev</a></span></p></footer></body></html>