<!DOCTYPE html><html><head><meta charset="utf-8"><title>Averting the convergent instrumental strategy of self-improvement</title><link rel="stylesheet" type="text/css" href="../common.css"><link rel="stylesheet" type="text/css" href="../page-style.css"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  SVG: {EqnChunk: 50, EqnChunkFactor: 1.5, EqChunkDelay: 10, useFontCache: false, linebreaks: {automatic: true}},
  tex2jax: {
    inlineMath: [['$~$', '$~$']],
    displayMath: [['$$~$', '$~$$']],
    processEscapes: true,
    preview: 'none',
  },
  showProcessingMessages: false,
  messageStyle: 'none',
  // http://docs.mathjax.org/en/latest/config-files.html#the-tex-ams-svg-configuration-file
  jax: ["input/TeX","output/SVG", "output/PreviewHTML"],
  extensions: ["tex2jax.js","MathMenu.js","MathZoom.js", "fast-preview.js", "AssistiveMML.js", "a11y/accessibility-menu.js"],
  TeX: { extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"] }
});</script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_SVG" async></script><script type="text/javascript" src="../arbital-demo-bundle.js"></script><script type="text/javascript">window.addEventListener('DOMContentLoaded', e=>window.loadAllDemos())
</script></head><body><header><h1 class="title">Averting the convergent instrumental strategy of self-improvement</h1><div class="page-info"><p class="metadata-link"><a href="../metadata/avert_self_improvement.json.html">avert_self_improvement.json</a></p><p class="arbital-url"><a href="https://arbital.com/p/avert_self_improvement">https://arbital.com/p/avert_self_improvement</a></p><p class="creator">by
 <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a> Mar 28 2016</p></div><p class="clickbait">We probably want the first AGI to *not* improve as fast as possible, but improving as fast as possible is a convergent strategy for accomplishing most things.</p><nav class="breadcrumbs"><nav class="breadcrumb" aria-label="Breadcrumb"><ul><li><a href="../index.html">Index</a></li><li>Averting the convergent instrumental strategy of self-improvement</li></ul></nav><nav class="breadcrumb" aria-label="Breadcrumb"><ul><li><a href="ai_alignment.html">AI alignment</a></li><li><a href="corrigibility.html">Corrigibility</a></li><li>â€¦</li></ul></nav></nav></header><hr><main><p>Rapid capability gains, or just large capability gains between a training paradigm and a test paradigm, are one of the primary expected reasons why AGI alignment might be hard.  We probably want the first AGI or AGIs ever built, tested, and used to <em>not</em> self-improve as quickly as possible.  Since there's a very strong <a href="instrumental_convergence.html">convergent incentive</a> to self-improve and do things <a href="nearest_unblocked.html">neighboring</a> to self-improvement, by default you would expect an AGI to search for ways to defeat naive blocks on self-improvement, which violates the <a href="niceness_defense.html">nonadversarial principle</a>. Thus, any proposals to limit an AGI's capabilities imply a very strong desideratum for us to figure out a way to <a href="avert_instrumental_pressure.html">avert the instrumental incentive</a> to self-improvement in that AGI.  The alternative is failing the <a href="omni_test.html">Omni Test</a>, violating the nonadversarial principle, <a href="reflective_consistency.html">having the AGI&#39;s code be actively inconsistent with what the AGI would approve of its own code being</a> (if the brake is a code-level measure), and setting up a safety measure that the AGI wants to defeat as the <a href="niceness_defense.html">only line of defense</a>.</p></main><hr><footer><p class="tagged"><h2>Tagged</h2><span class="page-comma-list"><a class="page-link" href="../page/value_alignment_open_problem.html">AI alignment open problem</a>,
 <a class="page-link" href="../page/stub_meta_tag.html">Stub</a></span></p><p class="likes"><h2>Likes</h2><span class="page-comma-list"><a class="page-link" href="../page/EricBruylant.html">Eric Bruylant</a></span></p><p class="reverse-related"><h2>Reverse Related</h2><ul class="page-list"><li><a class="page-link" href="../page/value_alignment_open_problem.html">AI alignment open problem</a> <q>Tag for open problems under AI alignment.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/stub_meta_tag.html">Stub</a> <q>This page only gives a very brief overview of the topic. If you're able to, please help expand or improve it!</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li></ul></p></footer></body></html>