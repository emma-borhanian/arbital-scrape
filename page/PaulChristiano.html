<!DOCTYPE html><html><head><meta charset="utf-8"><title>Paul Christiano</title><link rel="stylesheet" type="text/css" href="../common.css"><link rel="stylesheet" type="text/css" href="../page-style.css"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  SVG: {EqnChunk: 50, EqnChunkFactor: 1.5, EqChunkDelay: 10, useFontCache: false, linebreaks: {automatic: true}},
  tex2jax: {
    inlineMath: [['$~$', '$~$']],
    displayMath: [['$$~$', '$~$$']],
    processEscapes: true,
    preview: 'none',
  },
  showProcessingMessages: false,
  messageStyle: 'none',
  // http://docs.mathjax.org/en/latest/config-files.html#the-tex-ams-svg-configuration-file
  jax: ["input/TeX","output/SVG", "output/PreviewHTML"],
  extensions: ["tex2jax.js","MathMenu.js","MathZoom.js", "fast-preview.js", "AssistiveMML.js", "a11y/accessibility-menu.js"],
  TeX: { extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"] }
});</script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_SVG" async></script><script type="text/javascript" src="../arbital-demo-bundle.js"></script><script type="text/javascript">window.addEventListener('DOMContentLoaded', e=>window.loadAllDemos())
</script></head><body><header><h1 class="title">Paul Christiano</h1><div class="page-info"><p class="metadata-link"><a href="../metadata/PaulChristiano.json.html">PaulChristiano.json</a></p><p class="arbital-url"><a href="https://arbital.com/p/PaulChristiano">https://arbital.com/p/PaulChristiano</a></p><p class="creator">by
 <a class="page-link" href="../page/PaulChristiano.html">Paul Christiano</a> Sep 4 2015</p></div><nav class="breadcrumbs"><nav class="breadcrumb" aria-label="Breadcrumb"><ul><li><a href="../index.html">Index</a></li><li>Paul Christiano</li></ul></nav></nav></header><hr><main><p>Automatically generated page for "Paul Christiano" user. Click <a href="https://arbital.com/edit/3">here to edit</a>.</p></main><hr><footer><p class="related"><h2>Related</h2><ul class="page-list"><li><a class="page-link" href="../page/imitation_agent.html">Imitation-based agent</a> <q>An AI meant to imitate the behavior of a reference human as closely as possible.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li></ul></p><hr><p class="created"><h2>Created</h2><h3 id="createdwiki">wiki</h3><ul class="page-list"><li><a class="page-link" href="../page/possible_stance_ai_control_research.html">A possible stance for AI control research</a> <q>I think that AI control research should focus on finding [scalable](https://arbital.com/pages/492374…</q></li><li><a class="page-link" href="../page/ai_control_on_cheap.html">AI control on the cheap</a> <q>Ideally, we will build aligned AI systems without sacrificing any efficiency.

I think this is a rea…</q></li><li><a class="page-link" href="../page/abstract_approval_direction.html">Abstract approval-direction</a> <q>Consider the following design for an agent, which I first described [here](https://arbital.com/p/1t7…</q></li><li><a class="page-link" href="../page/act_based_agents.html">Act based agents </a> <q>I’ve recently discussed three kinds of learning systems:

- [Approval-directed agents](https://arbit…</q></li><li><a class="page-link" href="../page/active_learning_powerful_predictors.html">Active learning for opaque, powerful predictors</a> <q>(An open theoretical question relevant to AI control.)

Suppose that I have a very powerful predicti…</q></li><li><a class="page-link" href="../page/adversarial_collaboration.html">Adversarial collaboration </a> <q>Suppose that I have hired a group of employees who are much smarter than I am. For some tasks it’s…</q></li><li><a class="page-link" href="../page/advisor_games.html">Advisor games</a> <q>Machine learning algorithms often learn models or policies that are inscrutable to humans. We belie…</q></li><li><a class="page-link" href="../page/against_mimicry.html">Against mimicry</a> <q>One simple and apparently safe AI system is a “copycat:” an agent that predicts what its user woul…</q></li><li><a class="page-link" href="../page/ambitious_vs_narrow_value_learning.html">Ambitious vs. narrow value learning</a> <q>Suppose I’m trying to build an AI system that “learns what I want” and helps me get it. I think tha…</q></li><li><a class="page-link" href="../page/apprenticeship_learning_mimicry.html">Apprenticeship learning and mimicry</a> <q>This post compares my [recent proposal](https://arbital.com/p/1vp/mimicry_meeting_halfway) with [Ab…</q></li><li><a class="page-link" href="../page/approval_directed_agents.html">Approval directed agents</a> <q>Research in AI is steadily progressing towards more flexible, powerful, and autonomous goal-directe…</q></li><li><a class="page-link" href="../page/1qr.html">Approval-based agents</a> <q>An alternative to goal-directed behavior</q></li><li><a class="page-link" href="../page/approval_directed_bootstrapping.html">Approval-directed bootstrapping</a> <q>Approval-directed behavior works best when the overseer is very smart. Where can we find a smart o…</q></li><li><a class="page-link" href="../page/automated_assistants.html">Automated assistants </a> <q>In my [last post](https://arbital.com/p/1th?title=implementing-our-considered-judgment), I describ…</q></li><li><a class="page-link" href="../page/challenges_safe_ai_rl.html">Challenges for safe AI from RL</a> <q>In this post, I’ll describe and discuss a few big problems for the proposal from [my last post](htt…</q></li><li><a class="page-link" href="../page/concrete_approval_directed_agents.html">Concrete approval-directed agents</a> <q>This post lays out my current concrete “[approval-directed agents](https://arbital.com/p/1t7)” propo…</q></li><li><a class="page-link" href="../page/counterfactual_oversight_vs_training_data.html">Counterfactual oversight vs. training data</a> <q>I have written a lot recently about [counterfactual human oversight](https://arbital.com/p/1tj?titl…</q></li><li><a class="page-link" href="../page/Delegating_mixed_crowd.html">Delegating to a mixed crowd</a> <q>### 

Suppose I have ten programs, each a human-level agent. I suspect that at least one or two of…</q></li><li><a class="page-link" href="../page/efficient_feedback.html">Efficient feedback</a> <q>In some machine learning domains, such as image classification, we can produce a bunch of labelled t…</q></li><li><a class="page-link" href="../page/elaborations_apprenticeship_learning.html">Elaborations on apprenticeship learning</a> <q>Apprenticeship learning (AL) is an intuitively appealing approach to AI control. In AL, a human expe…</q></li><li><a class="page-link" href="../page/handingling_adversarial_errors.html">Handling adversarial errors</a> <q>Even a very powerful learning system can’t do everything perfectly at first — it requires time to l…</q></li><li><a class="page-link" href="../page/handling_error_with_arguments.html">Handling errors with arguments</a> <q>My [recent proposal](https://arbital.com/p/1v7?title=steps-towards-safe-ai-from-online-learning) f…</q></li><li><a class="page-link" href="../page/how_common_is_imitation.html">How common is imitation?</a> <q>How often do we train machine learning systems to imitate human behavior?

Some researchers explicit…</q></li><li><a class="page-link" href="../page/human_arguments_ai_control.html">Human arguments and AI control</a> <q>### Explanation and AI control

Consider the definition:

&gt; An action is good to the extent that I w…</q></li><li><a class="page-link" href="../page/human_counterfactual_loop.html">Human in counterfactual loop</a> <q>Consider an autonomous system which is buying or selling assets, operating heavy machinery, or mak…</q></li><li><a class="page-link" href="../page/Human_consulting_HCH.html">Humans consulting HCH</a> <q>Consider a human who has access to a question-answering machine. Suppose the machine answers questio…</q></li><li><a class="page-link" href="../page/IRL_VOI.html">IRL and VOI</a> <q>Consider the following straightforward algorithm based on inverse reinforcement learning:

- Given…</q></li><li><a class="page-link" href="../page/imitation_justification.html">Imitation and justification</a> <q>Suppose that I am training an AI system to play Go. One approach is to have the AI observe human m…</q></li><li><a class="page-link" href="../page/implementing_considered_judgement.html">Implementing our considered judgment</a> <q>Suppose I had a very powerful prediction algorithm. How might I use this algorithm to build a smar…</q></li><li><a class="page-link" href="../page/implicit_consequentialism.html">Implicit consequentialism</a> <q>Consider a machine that does exactly what its user [would tell it to do](https://arbital.com/p/1tj?t…</q></li><li><a class="page-link" href="../page/in_defense_maximization.html">In defense of maximization</a> <q>I’ve been thinking about [AI systems that take actions their users would most approve of](https://…</q></li><li><a class="page-link" href="../page/Indirect_decision_theory.html">Indirect decision theory</a> <q>In which I argue that understanding decision theory can be delegated to AI.

### Indirect normativit…</q></li><li><a class="page-link" href="../page/learn_policies_goals.html">Learn policies or goals?</a> <q>I’ve [recently proposed](https://arbital.com/p/1t7/approval_directed_agents) training agents to ma…</q></li><li><a class="page-link" href="../page/learning_logic.html">Learning and logic</a> <q>In most machine learning tasks, the learner maximizes a concrete, empirical performance measure: i…</q></li><li><a class="page-link" href="../page/Learning_representations.html">Learning representations</a> <q>Many AI systems form internal representations of their current environment or of particular data. Pr…</q></li><li><a class="page-link" href="../page/mimicry_meeting_halfway.html">Mimicry and meeting halfway</a> <q>I’ve talked recently about two different model-free decision procedures:

- At each step, pick the …</q></li><li><a class="page-link" href="../page/modeling_ai_control_with_humans.html">Modeling AI control with humans</a> <q>I’ve been trying to build an aligned AI out of reward-maximizing modules. A successful scheme could …</q></li><li><a class="page-link" href="../page/arguments_wagers.html">Of arguments and wagers</a> <q>(In which I explore an unusual way of combining the two.)

Suppose that Alice and Bob disagree, and …</q></li><li><a class="page-link" href="../page/simulations_inductive_definitions.html">Of simulations and inductive definitions</a> <q>_(Warning: weird.)_

Consider a simple AI system, named A, that carries out a task by predicting wha…</q></li><li><a class="page-link" href="../page/heterogenous_objectives.html">On heterogeneous objectives</a> <q>Eliezer Yudkowsky [has said](https://www.facebook.com/yudkowsky/posts/10153748345169228):

&gt; If you …</q></li><li><a class="page-link" href="../page/1vc.html">Online guarantees and AI control</a> <q>I’m interested in claims of the form: “If we had an AI that could do X well, then we could build a…</q></li><li><a class="page-link" href="../page/optimization_goals.html">Optimization and goals</a> <q>If we want to write a program that _doesn’t_ pursue a goal, we can have two kinds of trouble:

1. We…</q></li><li><a class="page-link" href="../page/optimizing_with_comparisons.html">Optimizing with comparisons</a> <q>I could [elicit a user’s approval](https://arbital.com/p/1w5) of an action _a_ by having them supply…</q></li><li><a class="page-link" href="../page/paul_ai_control.html">Paul Christiano's AI control blog</a> <q>Speculations on the design of safe, efficient AI systems.</q></li><li><a class="page-link" href="../page/safe_ai_episode_rl.html">Problem: safe AI from episodic RL</a> <q>In [a previous post](https://arbital.com/p/1tv?title=the-steering-problem), I posed the steering pr…</q></li><li><a class="page-link" href="../page/reinforcement_learning_linguistic_convention.html">Reinforcement learning and linguistic convention</a> <q>Existing machine learning techniques are most effective when we can provide concrete feedback — such…</q></li><li><a class="page-link" href="../page/research_directions_ai_control.html">Research directions in AI control</a> <q>What research would best advance our understanding of AI control?

I’ve been thinking about this qu…</q></li><li><a class="page-link" href="../page/reward_engineering.html">Reward engineering</a> <q>This post gestures at a handful of research questions with a loose thematic connection.

### The id…</q></li><li><a class="page-link" href="../page/safe_ai_from_question_answering.html">Safe AI from question-answering</a> <q>_(Warning: minimal new content. Just a clearer framing.)_

Suppose that I have a question-answering…</q></li><li><a class="page-link" href="../page/1ql.html">Scalable AI Control</a> <q>By AI control, I mean the problem of getting AI systems to do what we want them to do, to the best o…</q></li><li><a class="page-link" href="../page/Scalable_ai_control.html">Scalable AI control</a> <q>By AI control, I mean the problem of getting AI systems to do what we want them to do, to the best…</q></li><li><a class="page-link" href="../page/stable_self_improvement.html">Stable self-improvement as an AI safety problem</a> <q>“Stable self-improvement” seems to be a primary focus of MIRI’s work. As I understand it, the proble…</q></li><li><a class="page-link" href="../page/steps_towards_safe_ai_online_learning.html">Steps towards safe AI from online learning</a> <q>### Steps towards safe AI from online learning

Suppose that we have a good algorithm for episodic r…</q></li><li><a class="page-link" href="../page/synthesizing_training_data.html">Synthesizing training data</a> <q>[Counterfactual oversight](https://arbital.com/p/1tj?title=human-in-counterfactual-loop) requires th…</q></li><li><a class="page-link" href="../page/technical_socail_approach_ai_safety.html">Technical and social approaches to AI safety</a> <q>I often divide solutions to the AI control problem into two parts: technical and social. I think a…</q></li><li><a class="page-link" href="../page/absentee_billionaire.html">The absentee billionaire</a> <q>Once each day, Hugh wakes for 10 minutes. During these 10 minutes, he spends 10 million dollars. The…</q></li><li><a class="page-link" href="../page/Easy_goal_inference_problem_still_hard.html">The easy goal inference problem is still hard</a> <q>Goal inference and inverse reinforcement learning
------------------------------------------------…</q></li><li><a class="page-link" href="../page/state_of_steering_problem.html">The state of the steering problem</a> <q>The [steering problem](https://arbital.com/p/1tv?title=the-steering-problem) asks: given some powe…</q></li><li><a class="page-link" href="../page/steering_problem.html">The steering problem</a> <q>Most AI research focuses on reproducing human abilities: to learn, infer, and reason; to perceive,…</q></li><li><a class="page-link" href="../page/unsupervised_learning_ai_control.html">Unsupervised learning and AI control</a> <q>Reinforcement learning systems optimize for an objective defined by external feedback — anything fro…</q></li></ul><h3 id="createdgroup">group</h3><ul class="page-list"><li><a class="page-link" href="../page/PaulChristiano.html">Paul Christiano</a></li></ul><h3 id="createdcomment">comment</h3><ul class="page-list"><li><a class="page-link" href="../page/1gq.html"><q>(This is hard without threaded conversations. Responding to the &quot;agree/disagree&quot; from Eliezer)

&gt;The…</q></a></li><li><a class="page-link" href="../page/1gg.html"><q>(Understandable to focus on explanation for now. Threaded replies to replies would also be great eve…</q></a></li><li><a class="page-link" href="../page/3q0.html"><q>&gt;  If you want to demonstrate competence... you should first think in terms of exposing technically …</q></a></li><li><a class="page-link" href="../page/1j2.html"><q>&gt; &gt;This is like one step of ten in the act-based approach, and so to the extent that we disagree it …</q></a></li><li><a class="page-link" href="../page/3cy.html"><q>&gt; Even so, while the outputs are still abstract and not-yet-computed, Alice doesn't have much of a p…</q></a></li><li><a class="page-link" href="../page/1jl.html"><q>&gt; on my view it seems extremely probable that, whatever we have in the way of AI algorithms short of…</q></a></li><li><a class="page-link" href="../page/1k.html"><q>A 5% change in mortality / 15% change in other endpoints would be surprisingly large to me. Does thi…</q></a></li><li><a class="page-link" href="../page/1hf.html"><q>Act-based is a more general designation, that includes e.g. imitation learning (and value learning w…</q></a></li><li><a class="page-link" href="../page/7h.html"><q>As I see it, there are two cases that are meaningfully distinct:   
  
(1) what we want is so simp…</q></a></li><li><a class="page-link" href="../page/7b.html"><q>Consider an AI system composed of many interacting subsystems, or a world containing many AI systems…</q></a></li><li><a class="page-link" href="../page/2hr.html"><q>Eliezer [objects](https://arbital.com/p/2fr/?l=2fr#subpage-2h4) to this post's optimism about robust…</q></a></li><li><a class="page-link" href="../page/1gp.html"><q>Eliezer seems to have, and this page seems to reflect, strong intuitions about &quot;self-modification&quot; b…</q></a></li><li><a class="page-link" href="../page/1h1.html"><q>Eliezer, I find your position confusing.

Consider the first AI system that can reasonably predict y…</q></a></li><li><a class="page-link" href="../page/2gh.html"><q>I agree that reflective degrees of freedom won't &quot;fix themselves&quot; automatically, and that this is a …</q></a></li><li><a class="page-link" href="../page/7w.html"><q>I agree. You can use the results of easier/earlier inferences to guide harder/later inferences, but …</q></a></li><li><a class="page-link" href="../page/1fr.html"><q>I am pretty surprised by how confident the voters are!

Is &quot;arbitrarily powerful&quot; intended to includ…</q></a></li><li><a class="page-link" href="../page/1fj.html"><q>I can imagine this concept becoming relevant one day. But it seems sufficiently improbable that it d…</q></a></li><li><a class="page-link" href="../page/7g.html"><q>I don't see indirect specifications as encountering these difficulties; all of the contenders so far…</q></a></li><li><a class="page-link" href="../page/7v.html"><q>I don't see why getting the satisfying assignment really matters. If your AI sometimes declines to a…</q></a></li><li><a class="page-link" href="../page/75z.html"><q>I don't think the existence of such a colony would directly mitigate AI risk, but it could help in t…</q></a></li><li><a class="page-link" href="../page/1hn.html"><q>I don't think you've correctly diagnosed the disagreement yet (your strawman position is obviously c…</q></a></li><li><a class="page-link" href="../page/1hc.html"><q>I expect you know my answer on this one.

I agree that if there is a *really* fast transition (e.g. …</q></a></li><li><a class="page-link" href="../page/1gl.html"><q>I obviously disagree with &quot;under intelligence explosion scenarios a Singleton seems like a quite pro…</q></a></li><li><a class="page-link" href="../page/2hs.html"><q>I responded [here](https://arbital.com/p/1w4/?l=1w4#subpage-2hr).

&gt; Some of this probably reflects …</q></a></li><li><a class="page-link" href="../page/4x8.html"><q>I share the concern that people working on value alignment won't understand what has been done befor…</q></a></li><li><a class="page-link" href="../page/37c.html"><q>I think it's going to be hard to talk or think clearly about these problems (even at the level of se…</q></a></li><li><a class="page-link" href="../page/180.html"><q>I think that using the uniform prior over observers constitutes a critical learning failure. Calling…</q></a></li><li><a class="page-link" href="../page/2qs.html"><q>I think the [key question](https://medium.com/ai-control/the-informed-oversight-problem-1b51b4f66b35…</q></a></li><li><a class="page-link" href="../page/1hj.html"><q>I was comparing act-based agents to what you are calling a genie. Both get objectives from humans an…</q></a></li><li><a class="page-link" href="../page/2nc.html"><q>I was talking to Chelsea Finn about IRL a few weeks ago, and she said that they had encountered the …</q></a></li><li><a class="page-link" href="../page/1gy.html"><q>I wouldn't call this &quot;Christiano's hack.&quot; I appreciate the implicit praise that I can think up esote…</q></a></li><li><a class="page-link" href="../page/1gj.html"><q>If the distinguishing characteristic of a genie is &quot;primarily relying on the human ability to discer…</q></a></li><li><a class="page-link" href="../page/1l.html"><q>If there aren't side effects, it seems like the answer is probably yes, since vitamin D deficiency s…</q></a></li><li><a class="page-link" href="../page/1ff.html"><q>In practice, Eliezer often invokes this concept in settings where there *isn't* yet an intelligent a…</q></a></li><li><a class="page-link" href="../page/7k.html"><q>In the long run automation will increase the share of income going to capital. I think theory is agn…</q></a></li><li><a class="page-link" href="../page/2w5.html"><q>In the sudoku and first OWF example, the agent can justify their answer, and its easy to incentivize…</q></a></li><li><a class="page-link" href="../page/2ql.html"><q>It seems critical to distinguish the cases where

1. We are hoping the AI generalizes the concept of…</q></a></li><li><a class="page-link" href="../page/1h3.html"><q>It seems unlikely we'll ever build systems that &quot;maximize X, but rule out some bad solutions with th…</q></a></li><li><a class="page-link" href="../page/1j9.html"><q>It's easy to equivocate between &quot;can be viewed as&quot; and &quot;is.&quot; Indeed, any rational agent &quot;can be view…</q></a></li><li><a class="page-link" href="../page/1j8.html"><q>It's worth pointing out that in our discussions of AI safety, the author (I assume Eliezer, hereafte…</q></a></li><li><a class="page-link" href="../page/38w.html"><q>Methodologically, I am trying to understand what approaches may or may not work and what the key dif…</q></a></li><li><a class="page-link" href="../page/7j.html"><q>My knee-jerk response to this problem (just as with mind crime and corrigibility) is to try to build…</q></a></li><li><a class="page-link" href="../page/7x.html"><q>My views about Eliezer's preferences may depend on the reason that I am running X, rather than merel…</q></a></li><li><a class="page-link" href="../page/7m.html"><q>Normally I think that you set the bar too high for yourself. In this case, I think that you would be…</q></a></li><li><a class="page-link" href="../page/1fh.html"><q>Of course, the game is typically about costs and benefits. Saying &quot;it is good to adopt the security …</q></a></li><li><a class="page-link" href="../page/7f.html"><q>Often complaints are with the particular problems which purportedly will require novel solutions or …</q></a></li><li><a class="page-link" href="../page/2nb.html"><q>On the act-based model, the user would say something like &quot;paint all the cars pink,&quot; and the AI woul…</q></a></li><li><a class="page-link" href="../page/396.html"><q>On this definition, what is the difference between &quot;communicating a goal concept&quot; and &quot;communicating…</q></a></li><li><a class="page-link" href="../page/37b.html"><q>One natural standard: it should be hard to distinguish an adequate model from the system-to-be-model…</q></a></li><li><a class="page-link" href="../page/2nd.html"><q>Presumably the advantage of this approach---rather than simply learning to imitate the human burrito…</q></a></li><li><a class="page-link" href="../page/4xd.html"><q>Re: &quot;poking holes in things,&quot; what is an example of a proposal you would ask people to poke a hole i…</q></a></li><li><a class="page-link" href="../page/1gs.html"><q>Re: simulating a hostile superintelligence:

I find this concern really unconcerning. 

Some points:…</q></a></li><li><a class="page-link" href="../page/23w.html"><q>Regarding corporations:

I have seen very few arguments about superintelligence that rest on epistem…</q></a></li><li><a class="page-link" href="../page/1fq.html"><q>Six months and several discussions later this still seems like a serious concern (Nick Bostrom seeme…</q></a></li><li><a class="page-link" href="../page/1hv.html"><q>Sorry, I tried to be concrete about what we were discussing, but I will try harder:

Consider some p…</q></a></li><li><a class="page-link" href="../page/394.html"><q>Superficially, there are two quite different concerns:

1. You optimize a system for X. You are unha…</q></a></li><li><a class="page-link" href="../page/7l.html"><q>The ZFC provability box is equivalent to a good SAT solver, up to a constant factor (and I don't see…</q></a></li><li><a class="page-link" href="../page/7d.html"><q>The obvious patch is for a sufficiently sophisticated system to have preferences over its own behavi…</q></a></li><li><a class="page-link" href="../page/1fp.html"><q>There seems to be some equivocation here between two motivations for studying corrigibility.

As far…</q></a></li><li><a class="page-link" href="../page/1st.html"><q>These arguments seem weak to me.

 - I think the basic issue is that you are not properly handling u…</q></a></li><li><a class="page-link" href="../page/7n.html"><q>This (and many of your concerns) seem basically sensible to me. But I tend to read them more broadly…</q></a></li><li><a class="page-link" href="../page/7c.html"><q>This is a more general pattern in theoretical research. When you first start to attack a hard proble…</q></a></li><li><a class="page-link" href="../page/3pj.html"><q>This isn't the case in modern cryptography, except perhaps for the design of ciphers. It seems at be…</q></a></li><li><a class="page-link" href="../page/7t.html"><q>This seems like a good example to have at hand. I'm skeptical that it's much easier than what we rea…</q></a></li><li><a class="page-link" href="../page/2qh.html"><q>This seems like a straw alternative. More realistically, we could imagine an agent which avoids pert…</q></a></li><li><a class="page-link" href="../page/1gk.html"><q>This topic consistently frustrates me; the proposed typology is obviously incomplete, and I don't th…</q></a></li><li><a class="page-link" href="../page/2nh.html"><q>To me, the most natural way to approach this is to take a probability distribution over &quot;what it mea…</q></a></li><li><a class="page-link" href="../page/181.html"><q>To the extent that humans can imagine these kinds of scenarios, it seems pretty futile to try to pre…</q></a></li><li><a class="page-link" href="../page/79.html"><q>We can imagine two regimes of this problem: in the weak regime the AI may make a small number of err…</q></a></li><li><a class="page-link" href="../page/2rv.html"><q>Yeah, thanks.</q></a></li><li><a class="page-link" href="../page/188.html"><q>Your characterization of utility indifference doesn't seem quite right. More accurate would be: the …</q></a></li></ul></p><p class="edited"><h2>Edited</h2><ul class="page-list"><li><a class="page-link" href="../page/neural_genie_metaphor.html">Neutral genie metaphor</a> <q>Definition. A neutral-genie metaphor is an attempt to illustrate a possible formal problem via an in…</q> - <a class="page-link" href="../page/AlexeiAndreev.html">Alexei Andreev</a></li><li><a class="page-link" href="../page/value_alignment_researchers.html">Researchers in value alignment theory</a> <q>Who's working full-time in value alignment theory?</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li></ul></p></footer></body></html>