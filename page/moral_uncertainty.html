<!DOCTYPE html><html><head><meta charset="utf-8"><title>Moral uncertainty</title><link rel="stylesheet" type="text/css" href="../common.css"><link rel="stylesheet" type="text/css" href="../page-style.css"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  SVG: {EqnChunk: 50, EqnChunkFactor: 1.5, EqChunkDelay: 10, useFontCache: false, linebreaks: {automatic: true}},
  tex2jax: {
    inlineMath: [['$~$', '$~$']],
    displayMath: [['$$~$', '$~$$']],
    processEscapes: true,
    preview: 'none',
  },
  showProcessingMessages: false,
  messageStyle: 'none',
  // http://docs.mathjax.org/en/latest/config-files.html#the-tex-ams-svg-configuration-file
  jax: ["input/TeX","output/SVG", "output/PreviewHTML"],
  extensions: ["tex2jax.js","MathMenu.js","MathZoom.js", "fast-preview.js", "AssistiveMML.js", "a11y/accessibility-menu.js"],
  TeX: { extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"] }
});</script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_SVG" async></script><script type="text/javascript" src="../arbital-demo-bundle.js"></script><script type="text/javascript">window.addEventListener('DOMContentLoaded', e=>window.loadAllDemos())
</script></head><body><header><h1 class="title">Moral uncertainty</h1><div class="page-info"><p class="metadata-link"><a href="../metadata/moral_uncertainty.json.html">moral_uncertainty.json</a></p><p class="arbital-url"><a href="https://arbital.com/p/moral_uncertainty">https://arbital.com/p/moral_uncertainty</a></p><p class="creator">by
 <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a> Feb 8 2017</p></div><p class="clickbait">A meta-utility function in which the utility function as usually considered, takes on different values in different possible worlds, potentially distinguishable by evidence.</p><nav class="breadcrumbs"><nav class="breadcrumb" aria-label="Breadcrumb"><ul><li><a href="../index.html">Index</a></li><li>Moral uncertainty</li></ul></nav><nav class="breadcrumb" aria-label="Breadcrumb"><ul><li><a href="ai_alignment.html">AI alignment</a></li><li><a href="value_alignment_problem.html">Value alignment problem</a></li><li><a href="preference_framework.html">Preference framework</a></li><li>â€¦</li></ul></nav></nav></header><hr><main><p>[summary: "Moral uncertainty" in the context of AI refers to an agent with an "uncertain utility function". That is, we can view the agent as pursuing a <a href="utility_function.html">utility function</a> that takes on different values in different subsets of possible worlds.</p>
<p>For example, an agent might have a <a href="meta_utility.html">meta-utility function</a> saying that eating cake has a utility of &euro;8 in worlds where Lee Harvey Oswald shot John F. Kennedy and that eating cake has a utility of &euro;10 in worlds where it was the other way around.  This agent will be motivated to inquire into political history to find out which utility function is probably the 'correct' one (relative to this meta-utility function).]</p>
<p>"Moral uncertainty" in the context of AI refers to an agent with an "uncertain utility function".  That is, we can view the agent as pursuing a <a href="utility_function.html">utility function</a> that takes on different values in different subsets of possible worlds.</p>
<p>For example, an agent might have a <a href="meta_utility.html">meta-utility function</a> saying that eating cake has a utility of &euro;8 in worlds where Lee Harvey Oswald shot John F. Kennedy and that eating cake has a utility of &euro;10 in worlds where it was the other way around.  This agent will be motivated to inquire into political history to find out which utility function is probably the 'correct' one (relative to this meta-utility function), though it will never be <a href="cromwells_rule.html">absolutely sure</a>.</p>
<p>Moral uncertainty must be resolvable by some conceivable observation in order to function as uncertainty.  Suppose for example that an agent's probability distribution $~$\Delta U$~$ over the 'true' utility function $~$U$~$ asserts a dependency on a fair quantum coin that was flipped inside a sealed box then destroyed by explosives: the utility function is $~$U_1$~$ over outcomes in the worlds where the coin came up heads, and if the coin came up tails the utility function is $~$U_2.$~$  If the agent thinks it has no way of ever figuring out what happened inside the box, it will thereafter behave as if it had a single, constant, certain utility function equal to $~$0.5 \cdot U_1 + 0.5 \cdot U_2.$~$</p></main><hr><footer><p class="tagged"><h2>Tagged</h2><span class="page-comma-list"><a class="page-link" href="../page/start_meta_tag.html">Start</a></span></p><p class="reverse-related"><h2>Reverse Related</h2><ul class="page-list"><li><a class="page-link" href="../page/start_meta_tag.html">Start</a> <q>This page gives a basic overview of the topic, but may be missing important information or have stylistic issues. If you're able to, please help expand or improve it!</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li></ul></p><p class="children"><h2>Children</h2><ul class="page-tree"><li><a class="page-link" href="../page/ideal_target.html">Ideal target</a> <q>The 'ideal target' of a meta-utility function is the value the ground-level utility function would take on if the agent updated on all possible evidence; the 'true' utilities under moral uncertainty.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li></ul></p></footer></body></html>