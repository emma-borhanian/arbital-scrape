<!DOCTYPE html><html><head><meta charset="utf-8"><title>Generalized principle of cognitive alignment</title><link rel="stylesheet" type="text/css" href="../common.css"><link rel="stylesheet" type="text/css" href="../page-style.css"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  SVG: {EqnChunk: 50, EqnChunkFactor: 1.5, EqChunkDelay: 10, useFontCache: false, linebreaks: {automatic: true}},
  tex2jax: {
    inlineMath: [['$~$', '$~$']],
    displayMath: [['$$~$', '$~$$']],
    processEscapes: true,
    preview: 'none',
  },
  showProcessingMessages: false,
  messageStyle: 'none',
  // http://docs.mathjax.org/en/latest/config-files.html#the-tex-ams-svg-configuration-file
  jax: ["input/TeX","output/SVG", "output/PreviewHTML"],
  extensions: ["tex2jax.js","MathMenu.js","MathZoom.js", "fast-preview.js", "AssistiveMML.js", "a11y/accessibility-menu.js"],
  TeX: { extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"] }
});</script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_SVG" async></script><script type="text/javascript" src="../arbital-demo-bundle.js"></script><script type="text/javascript">window.addEventListener('DOMContentLoaded', e=>window.loadAllDemos())
</script></head><body><header><h1 class="title">Generalized principle of cognitive alignment</h1><div class="page-info"><p class="metadata-link"><a href="../metadata/cognitive_alignment.json.html">cognitive_alignment.json</a></p><p class="arbital-url"><a href="https://arbital.com/p/cognitive_alignment">https://arbital.com/p/cognitive_alignment</a></p><p class="creator">by
 <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a> Feb 13 2017</p></div><p class="clickbait">When we're asking how we want the AI to think about an alignment problem, one source of inspiration is trying to have the AI mirror our own thoughts about that problem.</p><nav class="breadcrumbs"><nav class="breadcrumb" aria-label="Breadcrumb"><ul><li><a href="../index.html">Index</a></li><li>Generalized principle of cognitive alignment</li></ul></nav><nav class="breadcrumb" aria-label="Breadcrumb"><ul><li><a href="ai_alignment.html">AI alignment</a></li><li><a href="alignment_principle.html">Principles in AI alignment</a></li><li><a href="nonadversarial.html">Non-adversarial principle</a></li><li>â€¦</li></ul></nav></nav></header><hr><main><p>A generalization of the <a href="nonadversarial.html">Non-adversarial principle</a> is that whenever we are asking how we want an AI algorithm to execute with respect to some alignment or safety issue, we might ask how we ourselves are thinking about that problem, and whether we can have the AI think conjugate thoughts.   This may sometimes seem like a much more complicated or dangerous-seeming approach than simpler avenues, but it's often a source of useful inspiration.</p>
<p>For example, with respect to the <a href="shutdown_problem.html">Shutdown problem</a>, this principle might lead us to ask:  "Is there some way we can have the AI <a href="hard_corrigibility.html">truly understand that its own programmers may have built the wrong AI</a>, including the wrong definition of exactly what it means to have 'built the wrong AI', such that <a href="updated_deference.html">the AI thinks it <em>cannot</em> recover the matter by optimizing any kind of preference already built into it</a>, so that the AI itself wants to shut down before having a great impact, because when the AI sees the programmers trying to press the button or contemplates the possibility of the programmers pressing the button, updating on this information causes the AI to expect its further operation to have a net bad impact in some sense that it can't overcome through any kind of clever strategy besides just shutting down?"</p>
<p>This in turn might imply a complicated mind-state we're not sure how to get right, such that we would prefer a simpler approach to shutdownability along the lines of a perfected <a href="utility_indifference.html">utility indifference</a> scheme.  If we're shutting down the AI at all, it means something has gone wrong, which implies that something else may have gone wrong earlier before we noticed.  That seems like a bad time to have the AI be enthusiastic about shutting down even better than in its original design (unless we can get the AI to <a href="hard_corrigibility.html">understand even <em>that</em> part too</a>, the danger of that kind of 'improvement', during its normal operation).</p>
<p>Trying for maximum cognitive alignment isn't always a good idea; but it's almost always worth trying to think through a safety problem from that perspective for inspiration on what we'd ideally want the AI to be doing.  It's often a good idea to move closer to that ideal when this doesn't introduce greater complication or other problems.</p></main><hr><footer><p class="tagged"><h2>Tagged</h2><span class="page-comma-list"><a class="page-link" href="../page/c_class_meta_tag.html">C-Class</a></span></p><p class="reverse-related"><h2>Reverse Related</h2><ul class="page-list"><li><a class="page-link" href="../page/c_class_meta_tag.html">C-Class</a> <q>This page has substantial content, but may not thoroughly cover the topic, may not meet style and prose standards, or may not explain the concept in a way the target audience will reliably understand.</q> - <a class="page-link" href="../page/EricBruylant.html">Eric Bruylant</a></li></ul></p></footer></body></html>