<!DOCTYPE html><html><head><meta charset="utf-8"><title>&quot;Can we properly classify th...&quot;</title><link rel="stylesheet" type="text/css" href="../common.css"><link rel="stylesheet" type="text/css" href="../page-style.css"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  SVG: {EqnChunk: 50, EqnChunkFactor: 1.5, EqChunkDelay: 10, useFontCache: false, linebreaks: {automatic: true}},
  tex2jax: {
    inlineMath: [['$~$', '$~$']],
    displayMath: [['$$~$', '$~$$']],
    processEscapes: true,
    preview: 'none',
  },
  showProcessingMessages: false,
  messageStyle: 'none',
  // http://docs.mathjax.org/en/latest/config-files.html#the-tex-ams-svg-configuration-file
  jax: ["input/TeX","output/SVG", "output/PreviewHTML"],
  extensions: ["tex2jax.js","MathMenu.js","MathZoom.js", "fast-preview.js", "AssistiveMML.js", "a11y/accessibility-menu.js"],
  TeX: { extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"] }
});</script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_SVG" async></script><script type="text/javascript" src="../arbital-demo-bundle.js"></script><script type="text/javascript">window.addEventListener('DOMContentLoaded', e=>window.loadAllDemos())
</script></head><body><header><h1 class="title">&quot;Can we properly classify th...&quot;</h1><div class="page-info"><p class="metadata-link"><a href="../metadata/81.json.html">81.json</a></p><p class="arbital-url"><a href="https://arbital.com/p/81">https://arbital.com/p/81</a></p><p class="creator">by
 <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a> Jun 19 2015 
updated
 Dec 30 2015</p></div><nav class="breadcrumbs"><nav class="breadcrumb" aria-label="Breadcrumb"><ul><li><a href="../index.html">Index</a></li><li>&quot;Can we properly classify th...&quot;</li></ul></nav><nav class="breadcrumb" aria-label="Breadcrumb"><ul><li><a href="ai_alignment.html">AI alignment</a></li><li><a href="distant_SIs.html">Modeling distant superintelligences</a></li><li><a href="probable_environment_hacking.html">Distant superintelligences can coerce the most probable environment of your AI</a></li><li><a href="79.html">&quot;We can imagine two regimes ...&quot;</a></li><li>…</li></ul></nav><nav class="breadcrumb" aria-label="Breadcrumb"><ul><li><a href="ai_alignment.html">AI alignment</a></li><li><a href="distant_SIs.html">Modeling distant superintelligences</a></li><li><a href="probable_environment_hacking.html">Distant superintelligences can coerce the most probable environment of your AI</a></li><li>…</li></ul></nav></nav></header><hr><main><p>Can we properly classify this as an error?  If there's an AI that will be hacked, or maybe hack itself, only if it <em>correctly</em> forecasts that distant superintelligences are creating millions more simulations than the actual AI, then I'd expect distant superintelligences to create millions of simulations.  Simulating a pre-intelligence-explosion AI is extremely cheap.  Sure, <em>not</em> doing it is even <em>cheaper</em>, but if the AI has a sufficiently good model of the distant SI to not be fooled by fakeouts in one decision that get corrected by another decision, then the distant SI will expend the resources to actually simulate.</p>
<p>It seems to me that we'd have to address this issue in a way that's robust to the case where the distant SI is actually simulating a million copies of our local AI that our local AI can't distinguish from itself.  If we only correct erroneous beliefs about such simulation by processes that only work to eject false beliefs, then perhaps the distant SI can hack us by making the local AI's belief not be erroneous.</p></main><hr><footer></footer></body></html>