<!DOCTYPE html><html><head><meta charset="utf-8"><title>Autonomous AGI</title><link rel="stylesheet" type="text/css" href="../common.css"><link rel="stylesheet" type="text/css" href="../page-style.css"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  SVG: {EqnChunk: 50, EqnChunkFactor: 1.5, EqChunkDelay: 10, useFontCache: false, linebreaks: {automatic: true}},
  tex2jax: {
    inlineMath: [['$~$', '$~$']],
    displayMath: [['$$~$', '$~$$']],
    processEscapes: true,
    preview: 'none',
  },
  showProcessingMessages: false,
  messageStyle: 'none',
  // http://docs.mathjax.org/en/latest/config-files.html#the-tex-ams-svg-configuration-file
  jax: ["input/TeX","output/SVG", "output/PreviewHTML"],
  extensions: ["tex2jax.js","MathMenu.js","MathZoom.js", "fast-preview.js", "AssistiveMML.js", "a11y/accessibility-menu.js"],
  TeX: { extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"] }
});</script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_SVG" async></script><script type="text/javascript" src="../arbital-demo-bundle.js"></script><script type="text/javascript">window.addEventListener('DOMContentLoaded', e=>window.loadAllDemos())
</script></head><body><header><h1 class="title">Autonomous AGI</h1><div class="page-info"><p class="metadata-link"><a href="../metadata/Sovereign.json.html">Sovereign.json</a></p><p class="arbital-url"><a href="https://arbital.com/p/Sovereign">https://arbital.com/p/Sovereign</a></p><p class="creator">by
 <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a> Dec 28 2015 
updated
 Jun 6 2016</p></div><p class="clickbait">The hardest possible class of Friendly AI to build, with the least moral hazard; an AI intended to neither require nor accept further direction.</p><nav class="breadcrumbs"><nav class="breadcrumb" aria-label="Breadcrumb"><ul><li><a href="../index.html">Index</a></li><li>Autonomous AGI</li></ul></nav><nav class="breadcrumb" aria-label="Breadcrumb"><ul><li><a href="ai_alignment.html">AI alignment</a></li><li><a href="AGI_typology.html">Strategic AGI typology</a></li><li>â€¦</li></ul></nav></nav></header><hr><main><p>An autonomous or self-directed <a href="advanced_agent.html">advanced agent</a>, a machine intelligence which acts in the real world in pursuit of its preferences without further user intervention or steering.  In <a href="NickBostrom.html">Bostrom</a>'s <a href="AGI_typology.html">typology</a> of <a href="advanced_agent.html">advanced agents</a>, this is a "Sovereign" and distinguished from a "<a href="task_agi.html">Genie</a>" or an "<a href="oracle.html">Oracle</a>".  ("Sovereign" in this sense means self-sovereign, and is not to be confused with the concept of a <a href="http://www.nickbostrom.com/fut/singleton.html">Bostromian singleton</a> or any particular kind of social governance.)</p>
<p>Usually, when we say "Sovereign" or "self-directed", we'll be talking about a supposedly <a href="value_alignment_problem.html">aligned</a> AI that acts autonomously <em>by design</em>.  Failure to solve the alignment problem probably means the resulting AI is self-directed-by-default.</p>
<p>Trying to construct an autonomous Friendly AI suggests that we trust the AI more than the <a href="value_alignment_programmer.html">programmers</a> in any conflict between them, and we're okay with removing all constraints and off-switches except those the agent voluntarily takes upon itself.</p>
<p>A <em>successfully</em> aligned autonomous AGI would carry the least <a href="moral_hazard.html">moral hazard</a> of any scenario, since it hands off steering to some fixed <a href="preference_framework.html">preference framework</a> or objective that the programmers can no longer modify.  Nonetheless, being really really really that sure, not just getting it right but <em>knowing</em> we've gotten it right, seems like a large enough problem that perhaps we shouldn't be trying to build this class of AI <em>for our first try</em>, and should first target a <a href="task_agi.html">Task AGI</a> instead, or something else involving ongoing user steering.</p>
<p>An autonomous <a href="superintelligent.html">superintelligence</a> would be the most difficult possible class of AGI to <a href="value_alignment_problem.html">align</a>, requiring <a href="total_alignment.html">total alignment</a>.  <a href="cev.html">Coherent extrapolated volition</a> is a proposed alignment target for an autonomous superintelligence, but again, probably not something we should attempt to do on our first try.</p></main><hr><section class="comments"><h2>Comments</h2><div class="comment"><p><a class="page-link" href="../page/PaulChristiano.html">Paul Christiano</a></p><p><p>This topic consistently frustrates me; the proposed typology is obviously incomplete, and I don't think it produces any useful conclusions except by either equivocating between definitions (e.g. when establishing that X is a sovereign and later that sovereigns have property P), by assuming exhaustiveness without justification, or by straightforwardly smuggling in associations.</p>
<p>Note that "an AI intended to act freely in the world according to its own preferences" need not entail "without further direction," since the preferences of the AI may make reference to human direction. And neither of these directly entail the need to get it right on the first try to any greater extent than any other AI system.</p>
<p>And the complement of these properties doesn't really imply anything at all, certainly not that a system is a genie or an oracle.</p></p></div><div class="comment"><p><a class="page-link" href="../page/PaulChristiano.html">Paul Christiano</a></p><p><p>I obviously disagree with "under intelligence explosion scenarios a Singleton seems like a quite probable result of constructing a Sovereign." </p>
<p>This is true in an uninteresting sense, namely: in the very long run a singleton seems pretty likely. If technological/economic/social change accelerates enough, then from the outside it may look like a singleton appears immediately. But that's not a useful notion for forecasting the character of that singleton or the future trajectory of civilization, and the resulting singleton has little more relation to the early AI than it has to us.</p>
<p>Relatedly, I feel that "sovereign" is a <em>really</em> bad name.</p></p></div></section><footer><p class="related"><h2>Related</h2><ul class="page-list"><li><a class="page-link" href="../page/cev.html">Coherent extrapolated volition (alignment target)</a> <q>A proposed direction for an extremely well-aligned autonomous superintelligence - do what humans would want, if we knew what the AI knew, thought that fast, and understood ourselves.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li></ul></p></footer></body></html>