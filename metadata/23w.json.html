<!DOCTYPE html><html><head><meta charset="utf-8"><title>&quot;Regarding corporations:

I ...&quot;</title><link rel="stylesheet" type="text/css" href="../common.css"></head><body><pre style="white-space:pre-wrap">{
  localUrl: '<a href="../page/23w.html">../page/23w.html</a>',
  arbitalUrl: '<a href="https://arbital.com/p/23w">https://arbital.com/p/23w</a>',
  rawJsonUrl: '<a href="../raw/23w.json">../raw/23w.json</a>',
  likeableId: '<a href="1057.json.html">1057</a>',
  likeableType: 'page',
  myLikeValue: '0',
  likeCount: '2',
  dislikeCount: '0',
  likeScore: '2',
  individualLikes: [
    '<a href="GlennWillen.json.html">GlennWillen</a>',
    '<a href="EricRogstad.json.html">EricRogstad</a>'
  ],
  pageId: '<a href="23w.json.html">23w</a>',
  edit: '1',
  editSummary: '',
  prevEdit: '0',
  currentEdit: '1',
  wasPublished: 'true',
  type: 'comment',
  title: '&quot;Regarding corporations:\n\nI ...&quot;',
  clickbait: '',
  textLength: '2869',
  alias: '23w',
  externalUrl: '',
  sortChildrenBy: 'recentFirst',
  hasVote: 'false',
  voteType: '',
  votesAnonymous: 'false',
  editCreatorId: '<a href="PaulChristiano.json.html">PaulChristiano</a>',
  editCreatedAt: '2016-02-26 00:47:23',
  pageCreatorId: '<a href="PaulChristiano.json.html">PaulChristiano</a>',
  pageCreatedAt: '2016-02-26 00:47:23',
  seeDomainId: '<a href="0.json.html">0</a>',
  editDomainId: '<a href="EliezerYudkowsky.json.html">EliezerYudkowsky</a>',
  submitToDomainId: '<a href="0.json.html">0</a>',
  isAutosave: 'false',
  isSnapshot: 'false',
  isLiveEdit: 'true',
  isMinorEdit: 'false',
  indirectTeacher: 'false',
  todoCount: '0',
  isEditorComment: 'false',
  isApprovedComment: 'true',
  isResolved: 'false',
  snapshotText: '',
  anchorContext: '',
  anchorText: '',
  anchorOffset: '0',
  mergedInto: '',
  isDeleted: 'false',
  viewCount: '1680',
  text: 'Regarding corporations:\n\nI have seen very few arguments about superintelligence that rest on epistemic efficiency. Roughly speaking, epistemic efficiency with respect to X might be interpreted as &quot;smarter in every way than X.&quot; But we usually talk about systems that are &quot;smarter in some ways than humans.&quot; And the safety problem doesn&#39;t seem to change in a qualitative way at the threshold of &quot;smarter in every way.&quot; Nor does economic value, or most indicators of interest. The only measure on which that&#39;s a fundamental threshold is &quot;how hard it is for humans to do anything useful&quot; (but this is not an indicator people are talking about if they talk about corporations, for obvious reasons...)\n\nSo while I might agree that a corporation is not a superintelligence on Nick&#39;s definition, this doesn&#39;t seem to have much bearing on the way in which the analogy is invoked in discussions.  In general, this notion of superintelligence is a *sufficient* condition for lots of interesting phenomena, but not a necessary condition for almost anything.\n\nIt just seems like you semantically disagreeing about the use of the word &quot;superhuman.&quot; This seems like a missed opportunity to help communicate about the value alignment problem. (Also, though it&#39;s not precisely related, I think that people really do think about AI better when they think about it as &quot;idealized corporation&quot; rather than &quot;idealized human.&quot; Corporatization seems to be a better baseline than anthropomorphization, though neither is great.)\n\nTo make things more concrete, I think it is roughly as reasonable to ask about the &quot;value alignment problem for organizations,&quot; and that many solutions to the value alignment problem for AI will also be applicable to organizations (and conversely, if someone came to me with an actually good proposal for value alignment for organizations, I would consider it worth-looking-at). Of course I think that value alignment problem for AI systems is much more important, and so where the two problems are disanalogous I care about the AI version (and also I want to reserve undecorated &quot;value alignment&quot; as a technical term referring to the AI version of the problem). But that judgment is unrelated to the epistemic efficiency of corporations---I&#39;d think the same thing even if corporations were epistemically efficient, and I presume so would you.\n\nYour basic complaint with people&#39;s use of corporations as an analogy really seems to be that AI systems will become very much more powerful, and that they will never have the same peculiar mix of abilities as human organizations. \n\n(Indeed, assuming that an agent is smarter in every way simply makes the safety problem easier, and many of our disagreements about safety are based on me being willing to assume something like &quot;epistemic efficiency with respect to an average college graduate,&quot; at least as a first step.)',
  metaText: '',
  isTextLoaded: 'true',
  isSubscribedToDiscussion: 'false',
  isSubscribedToUser: 'false',
  isSubscribedAsMaintainer: 'false',
  discussionSubscriberCount: '2',
  maintainerCount: '1',
  userSubscriberCount: '0',
  lastVisit: '2016-02-27 00:11:32',
  hasDraft: 'false',
  votes: [],
  voteSummary: 'null',
  muVoteSummary: '0',
  voteScaling: '0',
  currentUserVote: '-2',
  voteCount: '0',
  lockedVoteType: '',
  maxEditEver: '0',
  redLinkCount: '0',
  lockedBy: '',
  lockedUntil: '',
  nextPageId: '',
  prevPageId: '',
  usedAsMastery: 'false',
  proposalEditNum: '0',
  permissions: {
    edit: {
      has: 'false',
      reason: 'You don&#39;t have domain permission to edit this page'
    },
    proposeEdit: {
      has: 'true',
      reason: ''
    },
    delete: {
      has: 'false',
      reason: 'You don&#39;t have domain permission to delete this page'
    },
    comment: {
      has: 'false',
      reason: 'You can&#39;t comment in this domain because you are not a member'
    },
    proposeComment: {
      has: 'true',
      reason: ''
    }
  },
  summaries: {},
  creatorIds: [
    '<a href="PaulChristiano.json.html">PaulChristiano</a>'
  ],
  childIds: [],
  parentIds: [
    '<a href="efficiency.json.html">efficiency</a>'
  ],
  commentIds: [
    '<a href="25g.json.html">25g</a>'
  ],
  questionIds: [],
  tagIds: [],
  relatedIds: [],
  markIds: [],
  explanations: [],
  learnMore: [],
  requirements: [],
  subjects: [],
  lenses: [],
  lensParentId: '',
  pathPages: [],
  learnMoreTaughtMap: {},
  learnMoreCoveredMap: {},
  learnMoreRequiredMap: {},
  editHistory: {},
  domainSubmissions: {},
  answers: [],
  answerCount: '0',
  commentCount: '0',
  newCommentCount: '0',
  linkedMarkCount: '0',
  changeLogs: [
    {
      likeableId: '<a href="0.json.html">0</a>',
      likeableType: 'changeLog',
      myLikeValue: '0',
      likeCount: '0',
      dislikeCount: '0',
      likeScore: '0',
      individualLikes: [],
      id: '7839',
      pageId: '<a href="23w.json.html">23w</a>',
      userId: '<a href="PaulChristiano.json.html">PaulChristiano</a>',
      edit: '1',
      type: 'newEdit',
      createdAt: '2016-02-26 00:47:23',
      auxPageId: '',
      oldSettingsValue: '',
      newSettingsValue: ''
    },
    {
      likeableId: '<a href="0.json.html">0</a>',
      likeableType: 'changeLog',
      myLikeValue: '0',
      likeCount: '0',
      dislikeCount: '0',
      likeScore: '0',
      individualLikes: [],
      id: '7833',
      pageId: '<a href="23w.json.html">23w</a>',
      userId: '<a href="PaulChristiano.json.html">PaulChristiano</a>',
      edit: '0',
      type: 'newParent',
      createdAt: '2016-02-26 00:20:16',
      auxPageId: '<a href="efficiency.json.html">efficiency</a>',
      oldSettingsValue: '',
      newSettingsValue: ''
    }
  ],
  feedSubmissions: [],
  searchStrings: {},
  hasChildren: 'false',
  hasParents: 'true',
  redAliases: {},
  improvementTagIds: [],
  nonMetaTagIds: [],
  todos: [],
  slowDownMap: 'null',
  speedUpMap: 'null',
  arcPageIds: 'null',
  contentRequests: {}
}</pre></body></html>