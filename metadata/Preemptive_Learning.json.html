<!DOCTYPE html><html><head><meta charset="utf-8"><title>Preemptive Learning</title><link rel="stylesheet" type="text/css" href="../common.css"></head><body><pre style="white-space:pre-wrap">{
  localUrl: '<a href="../page/Preemptive_Learning.html">../page/Preemptive_Learning.html</a>',
  arbitalUrl: '<a href="https://arbital.com/p/Preemptive_Learning">https://arbital.com/p/Preemptive_Learning</a>',
  rawJsonUrl: '<a href="../raw/8bt.json">../raw/8bt.json</a>',
  likeableId: '<a href="0.json.html">0</a>',
  likeableType: 'page',
  myLikeValue: '0',
  likeCount: '0',
  dislikeCount: '0',
  likeScore: '0',
  individualLikes: [],
  pageId: '<a href="Preemptive_Learning.json.html">Preemptive_Learning</a>',
  edit: '10',
  editSummary: '',
  prevEdit: '9',
  currentEdit: '10',
  wasPublished: 'true',
  type: 'wiki',
  title: 'Preemptive Learning',
  clickbait: 'One of the basic theorems of logical inductors, used to derive many other theorems that rely on &quot;buy low, sell high&quot; strategies.',
  textLength: '11933',
  alias: 'Preemptive_Learning',
  externalUrl: '',
  sortChildrenBy: 'likes',
  hasVote: 'false',
  voteType: '',
  votesAnonymous: 'false',
  editCreatorId: '<a href="AlexAppel.json.html">AlexAppel</a>',
  editCreatedAt: '2017-05-17 04:11:14',
  pageCreatorId: '<a href="AlexAppel.json.html">AlexAppel</a>',
  pageCreatedAt: '2017-05-07 02:48:28',
  seeDomainId: '<a href="0.json.html">0</a>',
  editDomainId: '<a href="812.json.html">812</a>',
  submitToDomainId: '<a href="0.json.html">0</a>',
  isAutosave: 'false',
  isSnapshot: 'false',
  isLiveEdit: 'true',
  isMinorEdit: 'false',
  indirectTeacher: 'false',
  todoCount: '0',
  isEditorComment: 'false',
  isApprovedComment: 'false',
  isResolved: 'false',
  snapshotText: '',
  anchorContext: '',
  anchorText: '',
  anchorOffset: '0',
  mergedInto: '',
  isDeleted: 'false',
  viewCount: '13',
  text: 'If $\\overline{A}\\in\\mathcal{BCS}(\\overline{\\mathbb{P}})$, then $\\displaystyle\\liminf_{n\\to\\infty}\\mathbb{P}_{n}(A_{n})=\\liminf_{n\\to\\infty}\\sup_{m\\ge n}\\mathbb{P}_{m}(A_{n})$ and $\\displaystyle\\limsup_{n\\to\\infty}\\mathbb{P}_{n}(A_{n})=\\limsup_{n\\to\\infty}\\inf_{m\\ge n}\\mathbb{P}_{m}(A_{n})$\n\n%%hidden(BCS(P)??):\nIt&#39;s the set of all sequences of $\\mathbb{R}$-combinations (bundles of cash and shares), that are $\\overline{\\mathbb{P}}$-generable (There&#39;s a sequence of $\\mathcal{EF}$-combinations that can be written down in poly-time that will turn into the $\\mathbb{R}$-combinations given market data), and bounded (there&#39;s some $b$ such that the L1 norm, the sum of the absolute value of the coefficients, is $\\le b$ for all elements of the sequence.)\n\nThese sound like weird conditions to impose, but $\\mathbb{R}$-combinations can be used to encode constraints between sentences like &quot;3 of these 7 sentences are true&quot; and &quot;A xor B&quot; by having the $\\mathbb{R}$-combination for that group of sentences have a value of 3, and 1, respectively.\n\nThe $\\overline{\\mathbb{P}}$-generable condition is important to impose so the trader is actually able to *write down* the combination in the first place to buy and sell it.\n\nThe &quot;boundedness&quot; part is mainly useful to scale down an unruly sequence so the magnitude of each combination is 1 or less, to make it feasible to analyze.\n\nBecause of these essential properties, $\\mathcal{BCS}(\\overline{\\mathbb{P}})$ could be thought of as the set of $\\mathbb{R}$-combination-sequences it&#39;s actually possible to prove stuff about and that the logical inductor can feasibly analyze. It will be showing up in most future theorems, as our set of &quot;sufficiently nice&quot; bundles of cash and shares.\n%%\n\n## Basic Intuition ##\n\nImagine a giant grid, with the index of the $\\mathbb{R}$-combinations on the $x$-axis, and the day on the $y$-axis. Imagine it as a landscape where the elevation at a spot is the value of the combination on a particular day. \n\nWe&#39;ll be walking along the diagonal, in this particular case. Imagine there&#39;s a series of signs along that diagonal, that list the greatest and least elevation/value you&#39;d ever reach if you just walked north forever. (this corresponds to $\\sup_{m\\ge n}\\mathbb{P}_{m}(A_{n})$ and $\\inf$, respectively) What this theorem says is that if you walk along the diagonal, the asymptotic lower bound on the elevation/value is the same as the asymptotic lower bound of the &quot;greatest elevation to the north/greatest future value&quot; advertised on the series of signs. Similarly, the asymptotic upper bound on the elevation/value is the same as the asyptotic upper bound of the &quot;lowest elevation to the north/lowest future value&quot;\n\nPut another way, the &quot;asymptotic lower bound of greatest future value&quot; and &quot;asymptotic upper bound of lowest future value&quot; eventually get projected back into bounds on the value of the combinations along the diagonal. This links the far-distant values of combinations back to their &quot;immediate&quot; values along the diagonal, if we wait long enough.\n\n## Proof Formulation ##\n\nWell, since we&#39;ve already proved the [88p], we can assume that preemptive-learning property is false, and then show that it lets us fulfill the four $\\varepsilon$-ROI lemma prerequisites that&#39;ll result in a contradiction with the assumption that we have a logical inductor.\n\nAs a recap, they are as follows:\n\n - $\\varepsilon$-ROI on all &quot;employees&quot;\n - The sequence of &quot;employees&quot; is efficiently emulatable so they can be combined into a &quot;trading firm&quot;\n - There is a $\\overline{\\mathbb{P}}$-generable sequence $\\overline\\alpha$ that describes the magnitude of the sequence of &quot;employees&quot;\n - $\\displaystyle\\lim_{k\\to\\infty}\\alpha_{k}\\neq 0$.\n\nWhat would happen if the preemptive learning property, $\\displaystyle\\liminf_{n\\to\\infty}\\mathbb{P}_{n}(A_{n})=\\liminf_{n\\to\\infty}\\sup_{m\\ge n}\\mathbb{P}_{m}(A_{n})$ was false?\n\nWell, straightaway, $\\sup_{m\\ge n}\\mathbb{P}_{m}(A_{n})\\ge\\mathbb{P}_{n}(A_{n})$. So if it&#39;s going to fail at all, it must be because $\\displaystyle\\liminf_{n\\to\\infty}\\mathbb{P}_{n}(A_{n})&lt;\\liminf_{n\\to\\infty}\\sup_{m\\ge n}\\mathbb{P}_{m}(A_{n})$.\n\nThis means that there&#39;s a gap, so there&#39;s some $b$ and some $\\varepsilon&gt;0$ where $\\displaystyle\\liminf_{n\\to\\infty}\\mathbb{P}_{n}(A_{n})&lt;b-\\varepsilon&lt;b+\\varepsilon&lt;\\liminf_{n\\to\\infty}\\sup_{m\\ge n}\\mathbb{P}_{m}(A_{n})$.\n\nThis suggests the buy low, sell high strategy for the employees. Buy an $\\mathbb{R}$-combination when its starting price along the diagonal is $b-\\varepsilon$ or less, and resell it when its price inevitably goes above $b+\\varepsilon$, and pick up a little bit of guaranteed cash for $\\varepsilon$-ROI.\n\nThere&#39;s problems with this, of course. You can&#39;t have a sharp cutoff-function for buying and selling, you have to use the continuous indicator functions. Also, the price will only &quot;inevitably&quot; go above $b+\\varepsilon$ in the limit. Let&#39;s analyze it a bit more, until a path reveals itself.\n\nSo, for infinitely many $n$, $\\mathbb{P}_{n}(A_{n})&lt;b-\\varepsilon$ by the definition of lim inf.\n\nAnd there&#39;s *some* far-distant day $s_{e}$ where $\\forall n&gt;s_{e}:\\sup_{m\\ge n}\\mathbb{P}_{n}(A_{n})&gt;b+\\varepsilon$.\n\n%%hidden(Wait, what?):\nLets say it was false, then. Then $\\forall s_{e}\\exists n&gt;s_{e}:\\sup_{m\\ge n}\\mathbb{P}_{n}(A_{n})\\le b+\\varepsilon$. Or, put another way, there would be infinitely many $n$ where the greatest future value is $b+\\varepsilon$ or less. Oh wait, by the definition of lim inf, lim inf(greatest future value) would be $b+\\varepsilon$ or less. But it&#39;s greater than $b+\\varepsilon$, and thus, a contradiction.\n%%\n\nNotice, the previous statement means that, along the diagonal, you&#39;ll eventually get to the point where the highest future value on all combinations further along the diagonal is over $b+\\varepsilon$. **It does not mean that all combinations in the entire sequence will eventually go over $b+\\varepsilon$.**\n\nIf we imagine our trading firm going along the diagonal and leaving one &quot;employee&quot; behind for each combination, which&#39;ll continue north forever, checking the value of the same combination... That means that our &quot;trading firm&quot; could just leave employees that&#39;ll do nothing, until day/combination $s_{e}$, and then it&#39;d start getting serious and outputting employees that buy low (if their combination is underpriced on the diagonal), wait for it to inevitably become overpriced, and sell their entire combination back then, because after day/combination $s_{e}$, *every* combination is going to go above $b+\\varepsilon$ value, *eventually*. \n\nAlso, instead of buying and selling precisely at $b-\\varepsilon$ and $b+\\varepsilon$, we could have our indicator functions start buying and selling a bit closer to the middle, with those bounds just denoting when to go all-in (all-in corresponds to buying/selling 1 $\\mathbb{R}$-combination/whatever&#39;s left). Then since the value on a combination is guaranteed to go above $b+\\varepsilon$ eventually, each employee past day $s_{e}$ is going to end up unloading everything it bought on its first day (note: not the actual first day), and get $\\varepsilon$ guaranteed cash. \n\nAlso, since the $\\mathbb{R}$-combinations are bounded, that means its possible to scale them all down so they have a magnitude of 1 or less and preserve the theorem. That takes care of the first condition, the $\\varepsilon$-ROI one, since $\\varepsilon$ guaranteed cash on a trade magnitude of 1 or less is an $\\varepsilon$ fraction of the total trade magnitude.\n\nThe second condition about being able to emulate the employees also looks fairly simple if we can define a &quot;percent of original purchase left&quot; feature, since a &quot;sell the easily-definable thing if the price goes too low&quot; trigger is pretty easy to implement.\n\nThe third condition about being able to emulate the magnitude of the employees using market data can be fulfilled without too much trouble, too. Since all the employees will end up selling back everything they bought by the argument for point 1, the magnitude is twice as big as the magnitude of the original trade. Just sum up whatever expressible features were used to make the original trade and bam, you&#39;ve got an expressible feature that&#39;ll produce the magnitude of the employee when exposed to market data. (the expressible features of the first trade can be written down in $poly(k)$ time, remember)\n\nThe fourth condition is trivially taken care of by that &quot;infinitely often&quot; clause about how often the value of the combinations dips below $b-\\varepsilon$ on the diagonal.\n\n%%hidden(Full Proof):\nAlright, first, we&#39;ll define our series of traders. If the trader&#39;s index is $&lt;s_{e}$, it&#39;ll just output 0 forever. Otherwise, there are three phases. For $n&lt;k$, the k&#39;th employee outputs the 0 trade. For $n=k$, we want to buy an $\\mathbb{R}$-combination if it&#39;s underpriced, and for $n&gt;k$, we want to sell the remaining amount of the combination back if it&#39;s overpriced. Let&#39;s define the initial trade, the trade made by the k&#39;th trader on the k&#39;th day.\n$$T_{k}^{k}:=Ind_{\\varepsilon/2}(A_{k}^{\\dagger * k}&lt;b-\\varepsilon/2)\\cdot (A_{k}^{\\dagger}-A_{k}^{\\dagger * k})$$\nThis is basically just saying &quot;start buying the $\\mathbb{R}$-combination $A_{k}$ at $b-\\varepsilon/2$, and go all the way up to buying one of the combination at $b-\\varepsilon$&quot;. And also the $\\mathbb{R}$-combination is going to have a price of 1 or less because we scaled it down, and we could do that because it was in $\\mathcal{BCS}(\\overline{\\mathbb{P}})$, so we could divide it by the upper bound.\n\nNow, for the later parts. We&#39;ll need a &quot;fraction of original purchase to sell off&quot; expressible feature.\n$$F_{n}:=Ind_{\\varepsilon/2}(A_{k}^{\\dagger * n}&gt;b+\\varepsilon/2)\\left(1-\\sum_{k&lt;i&lt;n}F_{i}\\right)$$\nThe first part of it is the indicator to start buying at $b+\\varepsilon/2$ and go all-in at $b+\\varepsilon$, and the second part is the percent of original purchase left over (1-all the previous sellings)\n\nFinally, for $n&gt;k$, we&#39;ll need to use that expressible feature to determine how much to sell off. So the trade would be\n$$T_{n}^{k}:=-F_{n}\\cdot T_{k}^{k}$$\nJust use the &quot;fraction to sell off&quot; feature to scale down the original purchase, and sell it off.\n\nOk, so we need to establish $\\varepsilon$-ROI, that what we just defined is an efficiently emulatable sequence of traders, that the magnitudes of the traders is $\\overline{\\mathbb{P}}$-generable, and the sequence of trader magnitudes doesn&#39;t go to zero.\n\nLet&#39;s start with the second. The k&#39;th &quot;employee&quot; makes no trades before day k. \n\nThe algorithm for the k&#39;th employee can be written down in poly(k) time, because the $\\mathcal{EF}$-combination $A_{k}^{dagger}$ is computable in $poly(k)$ time, by the definition of $\\mathcal{BCS}(\\overline{\\mathbb{P}})$ (remember, being $\\overline{\\mathbb{P}}$-generable was one of the conditions, and that implies that it takes $poly(k)$ time to write down $A_{k}^{\\dagger}$). And the recursive definition for &quot;fraction of purchase left&quot; is the same for each trader. And the test to see whether the trader&#39;s index is $&lt;s_{e}$ also takes fairly little time.\n\nAnd each individual employee has to be able to write down its trade in poly(n) time. By caching past trades, it would only take $poly(n)$ time to compute the &quot;fraction of original purchase left&quot; feature is, and the $\\mathbb{R}$-combination is just the same every time and doesn&#39;t change.\n\nThus, since the k&#39;th employee makes no trades before day $k$, the k&#39;th employee can be written down in $poly(k)$ time, and they all run in $poly(n)$ time, it&#39;s an efficiently emulatable sequence of traders.\n\nNext up: Showing that all the original purchase will be sold off *eventually*.\n\nBy induction on $F_{n}$, we can find that $\\sum_{k&lt;i\\le n}F_{i}\\le 1$ and $F_{n}\\ge 0$. Remember, by our original assumption, there&#39;s *some* day $m$ where $\\mathbb{P}_{m}(A_{k})&gt;b+\\varepsilon$. On that day, the indicator function will be 1, so\n\n(will return later)\n%%\n\n',
  metaText: '',
  isTextLoaded: 'true',
  isSubscribedToDiscussion: 'false',
  isSubscribedToUser: 'false',
  isSubscribedAsMaintainer: 'false',
  discussionSubscriberCount: '1',
  maintainerCount: '1',
  userSubscriberCount: '0',
  lastVisit: '',
  hasDraft: 'false',
  votes: [],
  voteSummary: 'null',
  muVoteSummary: '0',
  voteScaling: '0',
  currentUserVote: '-2',
  voteCount: '0',
  lockedVoteType: '',
  maxEditEver: '0',
  redLinkCount: '0',
  lockedBy: '',
  lockedUntil: '',
  nextPageId: '',
  prevPageId: '',
  usedAsMastery: 'false',
  proposalEditNum: '0',
  permissions: {
    edit: {
      has: 'false',
      reason: 'You don&#39;t have domain permission to edit this page'
    },
    proposeEdit: {
      has: 'true',
      reason: ''
    },
    delete: {
      has: 'false',
      reason: 'You don&#39;t have domain permission to delete this page'
    },
    comment: {
      has: 'false',
      reason: 'You can&#39;t comment in this domain because you are not a member'
    },
    proposeComment: {
      has: 'true',
      reason: ''
    }
  },
  summaries: {},
  creatorIds: [
    '<a href="AlexAppel.json.html">AlexAppel</a>'
  ],
  childIds: [],
  parentIds: [],
  commentIds: [],
  questionIds: [],
  tagIds: [],
  relatedIds: [],
  markIds: [],
  explanations: [],
  learnMore: [],
  requirements: [],
  subjects: [],
  lenses: [],
  lensParentId: '',
  pathPages: [],
  learnMoreTaughtMap: {},
  learnMoreCoveredMap: {},
  learnMoreRequiredMap: {},
  editHistory: {},
  domainSubmissions: {},
  answers: [],
  answerCount: '0',
  commentCount: '0',
  newCommentCount: '0',
  linkedMarkCount: '0',
  changeLogs: [
    {
      likeableId: '<a href="0.json.html">0</a>',
      likeableType: 'changeLog',
      myLikeValue: '0',
      likeCount: '0',
      dislikeCount: '0',
      likeScore: '0',
      individualLikes: [],
      id: '22530',
      pageId: '<a href="Preemptive_Learning.json.html">Preemptive_Learning</a>',
      userId: '<a href="AlexAppel.json.html">AlexAppel</a>',
      edit: '10',
      type: 'newEdit',
      createdAt: '2017-05-17 04:11:14',
      auxPageId: '',
      oldSettingsValue: '',
      newSettingsValue: ''
    },
    {
      likeableId: '<a href="0.json.html">0</a>',
      likeableType: 'changeLog',
      myLikeValue: '0',
      likeCount: '0',
      dislikeCount: '0',
      likeScore: '0',
      individualLikes: [],
      id: '22529',
      pageId: '<a href="Preemptive_Learning.json.html">Preemptive_Learning</a>',
      userId: '<a href="AlexAppel.json.html">AlexAppel</a>',
      edit: '9',
      type: 'newEdit',
      createdAt: '2017-05-17 03:13:10',
      auxPageId: '',
      oldSettingsValue: '',
      newSettingsValue: ''
    },
    {
      likeableId: '<a href="0.json.html">0</a>',
      likeableType: 'changeLog',
      myLikeValue: '0',
      likeCount: '0',
      dislikeCount: '0',
      likeScore: '0',
      individualLikes: [],
      id: '22522',
      pageId: '<a href="Preemptive_Learning.json.html">Preemptive_Learning</a>',
      userId: '<a href="AlexAppel.json.html">AlexAppel</a>',
      edit: '8',
      type: 'newEdit',
      createdAt: '2017-05-07 17:49:11',
      auxPageId: '',
      oldSettingsValue: '',
      newSettingsValue: ''
    },
    {
      likeableId: '<a href="0.json.html">0</a>',
      likeableType: 'changeLog',
      myLikeValue: '0',
      likeCount: '0',
      dislikeCount: '0',
      likeScore: '0',
      individualLikes: [],
      id: '22521',
      pageId: '<a href="Preemptive_Learning.json.html">Preemptive_Learning</a>',
      userId: '<a href="AlexAppel.json.html">AlexAppel</a>',
      edit: '7',
      type: 'newEdit',
      createdAt: '2017-05-07 05:50:57',
      auxPageId: '',
      oldSettingsValue: '',
      newSettingsValue: ''
    },
    {
      likeableId: '<a href="0.json.html">0</a>',
      likeableType: 'changeLog',
      myLikeValue: '0',
      likeCount: '0',
      dislikeCount: '0',
      likeScore: '0',
      individualLikes: [],
      id: '22520',
      pageId: '<a href="Preemptive_Learning.json.html">Preemptive_Learning</a>',
      userId: '<a href="AlexAppel.json.html">AlexAppel</a>',
      edit: '6',
      type: 'newEdit',
      createdAt: '2017-05-07 05:47:03',
      auxPageId: '',
      oldSettingsValue: '',
      newSettingsValue: ''
    },
    {
      likeableId: '<a href="0.json.html">0</a>',
      likeableType: 'changeLog',
      myLikeValue: '0',
      likeCount: '0',
      dislikeCount: '0',
      likeScore: '0',
      individualLikes: [],
      id: '22519',
      pageId: '<a href="Preemptive_Learning.json.html">Preemptive_Learning</a>',
      userId: '<a href="AlexAppel.json.html">AlexAppel</a>',
      edit: '5',
      type: 'newEdit',
      createdAt: '2017-05-07 05:46:13',
      auxPageId: '',
      oldSettingsValue: '',
      newSettingsValue: ''
    },
    {
      likeableId: '<a href="0.json.html">0</a>',
      likeableType: 'changeLog',
      myLikeValue: '0',
      likeCount: '0',
      dislikeCount: '0',
      likeScore: '0',
      individualLikes: [],
      id: '22518',
      pageId: '<a href="Preemptive_Learning.json.html">Preemptive_Learning</a>',
      userId: '<a href="AlexAppel.json.html">AlexAppel</a>',
      edit: '4',
      type: 'newEdit',
      createdAt: '2017-05-07 05:32:31',
      auxPageId: '',
      oldSettingsValue: '',
      newSettingsValue: ''
    },
    {
      likeableId: '<a href="0.json.html">0</a>',
      likeableType: 'changeLog',
      myLikeValue: '0',
      likeCount: '0',
      dislikeCount: '0',
      likeScore: '0',
      individualLikes: [],
      id: '22517',
      pageId: '<a href="Preemptive_Learning.json.html">Preemptive_Learning</a>',
      userId: '<a href="AlexAppel.json.html">AlexAppel</a>',
      edit: '3',
      type: 'newEdit',
      createdAt: '2017-05-07 05:31:26',
      auxPageId: '',
      oldSettingsValue: '',
      newSettingsValue: ''
    },
    {
      likeableId: '<a href="0.json.html">0</a>',
      likeableType: 'changeLog',
      myLikeValue: '0',
      likeCount: '0',
      dislikeCount: '0',
      likeScore: '0',
      individualLikes: [],
      id: '22515',
      pageId: '<a href="Preemptive_Learning.json.html">Preemptive_Learning</a>',
      userId: '<a href="AlexAppel.json.html">AlexAppel</a>',
      edit: '0',
      type: 'newAlias',
      createdAt: '2017-05-07 05:01:11',
      auxPageId: '',
      oldSettingsValue: 'Affine_Preemptive',
      newSettingsValue: 'Preemptive_Learning'
    },
    {
      likeableId: '<a href="0.json.html">0</a>',
      likeableType: 'changeLog',
      myLikeValue: '0',
      likeCount: '0',
      dislikeCount: '0',
      likeScore: '0',
      individualLikes: [],
      id: '22516',
      pageId: '<a href="Preemptive_Learning.json.html">Preemptive_Learning</a>',
      userId: '<a href="AlexAppel.json.html">AlexAppel</a>',
      edit: '2',
      type: 'newEdit',
      createdAt: '2017-05-07 05:01:11',
      auxPageId: '',
      oldSettingsValue: '',
      newSettingsValue: ''
    },
    {
      likeableId: '<a href="0.json.html">0</a>',
      likeableType: 'changeLog',
      myLikeValue: '0',
      likeCount: '0',
      dislikeCount: '0',
      likeScore: '0',
      individualLikes: [],
      id: '22514',
      pageId: '<a href="Preemptive_Learning.json.html">Preemptive_Learning</a>',
      userId: '<a href="AlexAppel.json.html">AlexAppel</a>',
      edit: '1',
      type: 'newEdit',
      createdAt: '2017-05-07 02:48:28',
      auxPageId: '',
      oldSettingsValue: '',
      newSettingsValue: ''
    }
  ],
  feedSubmissions: [],
  searchStrings: {},
  hasChildren: 'false',
  hasParents: 'false',
  redAliases: {},
  improvementTagIds: [],
  nonMetaTagIds: [],
  todos: [],
  slowDownMap: 'null',
  speedUpMap: 'null',
  arcPageIds: 'null',
  contentRequests: {}
}</pre></body></html>