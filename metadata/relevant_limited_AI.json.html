<!DOCTYPE html><html><head><meta charset="utf-8"><title>Relevant limited AI</title><link rel="stylesheet" type="text/css" href="../common.css"></head><body><pre style="white-space:pre-wrap">{
  localUrl: '<a href="../page/relevant_limited_AI.html">../page/relevant_limited_AI.html</a>',
  arbitalUrl: '<a href="https://arbital.com/p/relevant_limited_AI">https://arbital.com/p/relevant_limited_AI</a>',
  rawJsonUrl: '<a href="../raw/2y.json">../raw/2y.json</a>',
  likeableId: '<a href="1855.json.html">1855</a>',
  likeableType: 'page',
  myLikeValue: '0',
  likeCount: '0',
  dislikeCount: '0',
  likeScore: '0',
  individualLikes: [],
  pageId: '<a href="relevant_limited_AI.json.html">relevant_limited_AI</a>',
  edit: '7',
  editSummary: '',
  prevEdit: '6',
  currentEdit: '7',
  wasPublished: 'true',
  type: 'wiki',
  title: 'Relevant limited AI',
  clickbait: 'Can we have a limited AI, that&#39;s nonetheless relevant?',
  textLength: '3023',
  alias: 'relevant_limited_AI',
  externalUrl: '',
  sortChildrenBy: 'likes',
  hasVote: 'false',
  voteType: '',
  votesAnonymous: 'false',
  editCreatorId: '<a href="RobBensinger2.json.html">RobBensinger2</a>',
  editCreatedAt: '2016-09-25 00:04:01',
  pageCreatorId: '<a href="EliezerYudkowsky.json.html">EliezerYudkowsky</a>',
  pageCreatedAt: '2015-03-27 01:26:01',
  seeDomainId: '<a href="0.json.html">0</a>',
  editDomainId: '<a href="EliezerYudkowsky.json.html">EliezerYudkowsky</a>',
  submitToDomainId: '<a href="0.json.html">0</a>',
  isAutosave: 'false',
  isSnapshot: 'false',
  isLiveEdit: 'true',
  isMinorEdit: 'false',
  indirectTeacher: 'false',
  todoCount: '4',
  isEditorComment: 'false',
  isApprovedComment: 'true',
  isResolved: 'false',
  snapshotText: '',
  anchorContext: '',
  anchorText: '',
  anchorOffset: '0',
  mergedInto: '',
  isDeleted: 'false',
  viewCount: '115',
  text: 'It is an open problem to propose a [5b3 limited AI] that would be [2s relevant] to the [2z value achievement dilemma] - an agent cognitively constrained along some dimensions that render it much safer, but still able to perform some task useful enough to prevent catastrophe.\n\n### Basic difficulty\n\nConsider an [6x Oracle AI] that is so constrained as to be allowed only to output proofs in HOL of input theorems; these proofs are then verified by a simple and secure-seeming verifier in a sandbox whose exact code is unknown to the Oracle, and this verifier outputs 1 if the proof is true and 0 otherwise, then discards the proof-data.  Suppose also that the Oracle is in a shielded box, etcetera.\n\nIt&#39;s possible that this Provability Oracle has been so constrained that it is [2j cognitively containable] (it has no classes of options we don&#39;t know about).  If the verifier is unhackable, it gives us trustworthy knowledge that a theorem is provable.  But this limited system is not obviously useful in a way that enables humanity to extricate itself from its larger dilemma.  Nobody has yet stated a plan which could save the world *if only* we had a superhuman capacity to detect which theorems were provable in Zermelo-Fraenkel set theory.\n\nSaying &quot;The solution is for humanity to only build Provability Oracles!&quot; does not resolve the [2z value achievement dilemma] because humanity does not have the coordination ability to &#39;choose&#39; to develop only one kind of AI over the indefinite future, and the Provability Oracle has no obvious use that prevents non-Oracle AIs from ever being developed.  Thus our larger value achievement dilemma would remain unsolved.  It&#39;s not obvious how the Provability Oracle would even constitute significant strategic progress.\n\n### Open problem\n\nDescribe a cognitive task or real-world task for a AI to carry out, *that makes great progress upon the [2z value achievement dilemma] if executed correctly*, and that can be done with a *limited* AI that:\n\n1.  Has a real-world solution state that is exceptionally easy to pinpoint using a utility function, thereby avoiding some of [2w edge instantiation], [47 unforeseen maximums], [6q context change], [ programmer maximization], and the other pitfalls of [2l advanced safety], if there is otherwise a trustworthy solution for [ low-impact AI]; or\n2.  Seems exceptionally implementable using a [1fy known-algorithm non-self-improving agent], thereby averting problems of stable self-modification, if there is otherwise a trustworthy solution for a known-algorithm non-self-improving agent; or\n3.  Constrains the agent&#39;s option space so drastically as to make the strategy space not be rich (and the agent hence containable), while still containing a trustworthy, otherwise unfindable solution to some challenge that resolves the larger dilemma.\n\n[todo: ### Additional difficulties]\n\n[todo: (Fill in this section later; all the things that go wrong when somebody eagerly says something along the lines of &quot;We just need AI that does X!&quot;)]',
  metaText: '',
  isTextLoaded: 'true',
  isSubscribedToDiscussion: 'false',
  isSubscribedToUser: 'false',
  isSubscribedAsMaintainer: 'false',
  discussionSubscriberCount: '1',
  maintainerCount: '1',
  userSubscriberCount: '0',
  lastVisit: '2016-02-26 01:58:42',
  hasDraft: 'false',
  votes: [],
  voteSummary: 'null',
  muVoteSummary: '0',
  voteScaling: '0',
  currentUserVote: '-2',
  voteCount: '0',
  lockedVoteType: '',
  maxEditEver: '0',
  redLinkCount: '0',
  lockedBy: '',
  lockedUntil: '',
  nextPageId: '',
  prevPageId: '',
  usedAsMastery: 'false',
  proposalEditNum: '0',
  permissions: {
    edit: {
      has: 'false',
      reason: 'You don&#39;t have domain permission to edit this page'
    },
    proposeEdit: {
      has: 'true',
      reason: ''
    },
    delete: {
      has: 'false',
      reason: 'You don&#39;t have domain permission to delete this page'
    },
    comment: {
      has: 'false',
      reason: 'You can&#39;t comment in this domain because you are not a member'
    },
    proposeComment: {
      has: 'true',
      reason: ''
    }
  },
  summaries: {},
  creatorIds: [
    '<a href="EliezerYudkowsky.json.html">EliezerYudkowsky</a>',
    '<a href="AlexeiAndreev.json.html">AlexeiAndreev</a>',
    '<a href="RobBensinger2.json.html">RobBensinger2</a>'
  ],
  childIds: [],
  parentIds: [
    '<a href="ai_alignment.json.html">ai_alignment</a>'
  ],
  commentIds: [
    '<a href="7m.json.html">7m</a>'
  ],
  questionIds: [],
  tagIds: [],
  relatedIds: [],
  markIds: [],
  explanations: [],
  learnMore: [],
  requirements: [],
  subjects: [],
  lenses: [],
  lensParentId: '',
  pathPages: [],
  learnMoreTaughtMap: {},
  learnMoreCoveredMap: {},
  learnMoreRequiredMap: {},
  editHistory: {},
  domainSubmissions: {},
  answers: [],
  answerCount: '0',
  commentCount: '0',
  newCommentCount: '0',
  linkedMarkCount: '0',
  changeLogs: [
    {
      likeableId: '<a href="3542.json.html">3542</a>',
      likeableType: 'changeLog',
      myLikeValue: '0',
      likeCount: '1',
      dislikeCount: '0',
      likeScore: '1',
      individualLikes: [],
      id: '19720',
      pageId: '<a href="relevant_limited_AI.json.html">relevant_limited_AI</a>',
      userId: '<a href="RobBensinger2.json.html">RobBensinger2</a>',
      edit: '7',
      type: 'newEdit',
      createdAt: '2016-09-25 00:04:01',
      auxPageId: '',
      oldSettingsValue: '',
      newSettingsValue: ''
    },
    {
      likeableId: '<a href="0.json.html">0</a>',
      likeableType: 'changeLog',
      myLikeValue: '0',
      likeCount: '0',
      dislikeCount: '0',
      likeScore: '0',
      individualLikes: [],
      id: '3835',
      pageId: '<a href="relevant_limited_AI.json.html">relevant_limited_AI</a>',
      userId: '<a href="AlexeiAndreev.json.html">AlexeiAndreev</a>',
      edit: '0',
      type: 'newAlias',
      createdAt: '2015-12-16 02:45:19',
      auxPageId: '',
      oldSettingsValue: '',
      newSettingsValue: ''
    },
    {
      likeableId: '<a href="0.json.html">0</a>',
      likeableType: 'changeLog',
      myLikeValue: '0',
      likeCount: '0',
      dislikeCount: '0',
      likeScore: '0',
      individualLikes: [],
      id: '3836',
      pageId: '<a href="relevant_limited_AI.json.html">relevant_limited_AI</a>',
      userId: '<a href="AlexeiAndreev.json.html">AlexeiAndreev</a>',
      edit: '6',
      type: 'newEdit',
      createdAt: '2015-12-16 02:45:19',
      auxPageId: '',
      oldSettingsValue: '',
      newSettingsValue: ''
    },
    {
      likeableId: '<a href="0.json.html">0</a>',
      likeableType: 'changeLog',
      myLikeValue: '0',
      likeCount: '0',
      dislikeCount: '0',
      likeScore: '0',
      individualLikes: [],
      id: '365',
      pageId: '<a href="relevant_limited_AI.json.html">relevant_limited_AI</a>',
      userId: '<a href="AlexeiAndreev.json.html">AlexeiAndreev</a>',
      edit: '1',
      type: 'newParent',
      createdAt: '2015-10-28 03:46:51',
      auxPageId: '<a href="ai_alignment.json.html">ai_alignment</a>',
      oldSettingsValue: '',
      newSettingsValue: ''
    },
    {
      likeableId: '<a href="0.json.html">0</a>',
      likeableType: 'changeLog',
      myLikeValue: '0',
      likeCount: '0',
      dislikeCount: '0',
      likeScore: '0',
      individualLikes: [],
      id: '1628',
      pageId: '<a href="relevant_limited_AI.json.html">relevant_limited_AI</a>',
      userId: '<a href="EliezerYudkowsky.json.html">EliezerYudkowsky</a>',
      edit: '5',
      type: 'newEdit',
      createdAt: '2015-03-27 01:55:00',
      auxPageId: '',
      oldSettingsValue: '',
      newSettingsValue: ''
    },
    {
      likeableId: '<a href="0.json.html">0</a>',
      likeableType: 'changeLog',
      myLikeValue: '0',
      likeCount: '0',
      dislikeCount: '0',
      likeScore: '0',
      individualLikes: [],
      id: '1627',
      pageId: '<a href="relevant_limited_AI.json.html">relevant_limited_AI</a>',
      userId: '<a href="EliezerYudkowsky.json.html">EliezerYudkowsky</a>',
      edit: '4',
      type: 'newEdit',
      createdAt: '2015-03-27 01:54:14',
      auxPageId: '',
      oldSettingsValue: '',
      newSettingsValue: ''
    },
    {
      likeableId: '<a href="0.json.html">0</a>',
      likeableType: 'changeLog',
      myLikeValue: '0',
      likeCount: '0',
      dislikeCount: '0',
      likeScore: '0',
      individualLikes: [],
      id: '1626',
      pageId: '<a href="relevant_limited_AI.json.html">relevant_limited_AI</a>',
      userId: '<a href="EliezerYudkowsky.json.html">EliezerYudkowsky</a>',
      edit: '3',
      type: 'newEdit',
      createdAt: '2015-03-27 01:50:13',
      auxPageId: '',
      oldSettingsValue: '',
      newSettingsValue: ''
    },
    {
      likeableId: '<a href="0.json.html">0</a>',
      likeableType: 'changeLog',
      myLikeValue: '0',
      likeCount: '0',
      dislikeCount: '0',
      likeScore: '0',
      individualLikes: [],
      id: '1625',
      pageId: '<a href="relevant_limited_AI.json.html">relevant_limited_AI</a>',
      userId: '<a href="EliezerYudkowsky.json.html">EliezerYudkowsky</a>',
      edit: '2',
      type: 'newEdit',
      createdAt: '2015-03-27 01:38:53',
      auxPageId: '',
      oldSettingsValue: '',
      newSettingsValue: ''
    }
  ],
  feedSubmissions: [],
  searchStrings: {},
  hasChildren: 'false',
  hasParents: 'true',
  redAliases: {},
  improvementTagIds: [],
  nonMetaTagIds: [],
  todos: [],
  slowDownMap: 'null',
  speedUpMap: 'null',
  arcPageIds: 'null',
  contentRequests: {}
}</pre></body></html>