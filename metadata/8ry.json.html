<!DOCTYPE html><html><head><meta charset="utf-8"><title>&quot;Thanks for this analysis and congratulations on...&quot;</title><link rel="stylesheet" type="text/css" href="../common.css"></head><body><pre style="white-space:pre-wrap">{
  localUrl: '<a href="../page/8ry.html">../page/8ry.html</a>',
  arbitalUrl: '<a href="https://arbital.com/p/8ry">https://arbital.com/p/8ry</a>',
  rawJsonUrl: '<a href="../raw/8ry.json">../raw/8ry.json</a>',
  likeableId: '<a href="0.json.html">0</a>',
  likeableType: 'page',
  myLikeValue: '0',
  likeCount: '0',
  dislikeCount: '0',
  likeScore: '0',
  individualLikes: [],
  pageId: '<a href="8ry.json.html">8ry</a>',
  edit: '1',
  editSummary: '',
  prevEdit: '0',
  currentEdit: '1',
  wasPublished: 'true',
  type: 'comment',
  title: '&quot;Thanks for this analysis and congratulations on...&quot;',
  clickbait: '',
  textLength: '1301',
  alias: '8ry',
  externalUrl: '',
  sortChildrenBy: 'recentFirst',
  hasVote: 'false',
  voteType: '',
  votesAnonymous: 'false',
  editCreatorId: '<a href="TobyOrd.json.html">TobyOrd</a>',
  editCreatedAt: '2017-10-26 09:28:51',
  pageCreatorId: '<a href="TobyOrd.json.html">TobyOrd</a>',
  pageCreatedAt: '2017-10-26 09:28:51',
  seeDomainId: '<a href="0.json.html">0</a>',
  editDomainId: '<a href="2896.json.html">2896</a>',
  submitToDomainId: '<a href="0.json.html">0</a>',
  isAutosave: 'false',
  isSnapshot: 'false',
  isLiveEdit: 'true',
  isMinorEdit: 'false',
  indirectTeacher: 'false',
  todoCount: '0',
  isEditorComment: 'false',
  isApprovedComment: 'false',
  isResolved: 'false',
  snapshotText: '',
  anchorContext: '',
  anchorText: '',
  anchorOffset: '0',
  mergedInto: '',
  isDeleted: 'false',
  viewCount: '1128',
  text: 'Thanks for this analysis and congratulations on its clarity. \n\nOne important point is that that when considering:\n\n$\\pi_5$: Shut down and let the humans optimize whatever $V$ they have in the actual world\n\nthe text makes it look like the outcome involves unassisted humans optimising $V$. But the real option is that humans would almost certainly be trying to optimise $V$ with future AI assistance, quite possibly by an improved, restarted version of the AI itself. I think this helps make it a lot more plausible that this is the best policy for the AI to choose (both intuitively and from the AI&#39;s perspective), albeit it introduces some familiar difficulties in analysing whether to trust new versions of yourself to do a better job.\n\nI think that the paragraph about &#39;why wouldn&#39;t the AI just update on the evidence that the human tried to shut it down and then carry on?&#39; is a key point and could be developed.\n\nAnother key point is that we might legitimately want a button that just turns the AI off (and where the AI doesn&#39;t prevent this), rather than a button where the AI decides whether to let us turn it off. On this line, it would be better than nothing to have an AI that typically lets us turn it off, but even better to (also?) just have a way of taking this decision out of its hands.',
  metaText: '',
  isTextLoaded: 'true',
  isSubscribedToDiscussion: 'false',
  isSubscribedToUser: 'false',
  isSubscribedAsMaintainer: 'false',
  discussionSubscriberCount: '1',
  maintainerCount: '1',
  userSubscriberCount: '0',
  lastVisit: '',
  hasDraft: 'false',
  votes: [],
  voteSummary: 'null',
  muVoteSummary: '0',
  voteScaling: '0',
  currentUserVote: '-2',
  voteCount: '0',
  lockedVoteType: '',
  maxEditEver: '0',
  redLinkCount: '0',
  lockedBy: '',
  lockedUntil: '',
  nextPageId: '',
  prevPageId: '',
  usedAsMastery: 'false',
  proposalEditNum: '0',
  permissions: {
    edit: {
      has: 'false',
      reason: 'You don&#39;t have domain permission to edit this page'
    },
    proposeEdit: {
      has: 'true',
      reason: ''
    },
    delete: {
      has: 'false',
      reason: 'You don&#39;t have domain permission to delete this page'
    },
    comment: {
      has: 'false',
      reason: 'You can&#39;t comment in this domain because you are not a member'
    },
    proposeComment: {
      has: 'true',
      reason: ''
    }
  },
  summaries: {},
  creatorIds: [
    '<a href="TobyOrd.json.html">TobyOrd</a>'
  ],
  childIds: [],
  parentIds: [
    '<a href="updated_deference.json.html">updated_deference</a>'
  ],
  commentIds: [],
  questionIds: [],
  tagIds: [],
  relatedIds: [],
  markIds: [],
  explanations: [],
  learnMore: [],
  requirements: [],
  subjects: [],
  lenses: [],
  lensParentId: '',
  pathPages: [],
  learnMoreTaughtMap: {},
  learnMoreCoveredMap: {},
  learnMoreRequiredMap: {},
  editHistory: {},
  domainSubmissions: {},
  answers: [],
  answerCount: '0',
  commentCount: '0',
  newCommentCount: '0',
  linkedMarkCount: '0',
  changeLogs: [
    {
      likeableId: '<a href="0.json.html">0</a>',
      likeableType: 'changeLog',
      myLikeValue: '0',
      likeCount: '0',
      dislikeCount: '0',
      likeScore: '0',
      individualLikes: [],
      id: '22858',
      pageId: '<a href="8ry.json.html">8ry</a>',
      userId: '<a href="TobyOrd.json.html">TobyOrd</a>',
      edit: '1',
      type: 'newEdit',
      createdAt: '2017-10-26 09:28:51',
      auxPageId: '',
      oldSettingsValue: '',
      newSettingsValue: ''
    }
  ],
  feedSubmissions: [],
  searchStrings: {},
  hasChildren: 'false',
  hasParents: 'true',
  redAliases: {},
  improvementTagIds: [],
  nonMetaTagIds: [],
  todos: [],
  slowDownMap: 'null',
  speedUpMap: 'null',
  arcPageIds: 'null',
  contentRequests: {}
}</pre></body></html>