<!DOCTYPE html><html><head><meta charset="utf-8"><title>&quot;&gt; Do we disagree about this...&quot;</title><link rel="stylesheet" type="text/css" href="../common.css"></head><body><pre style="white-space:pre-wrap">{
  localUrl: '<a href="../page/1ht.html">../page/1ht.html</a>',
  arbitalUrl: '<a href="https://arbital.com/p/1ht">https://arbital.com/p/1ht</a>',
  rawJsonUrl: '<a href="../raw/1ht.json">../raw/1ht.json</a>',
  likeableId: '<a href="joint_probability_distribution.json.html">joint_probability_distribution</a>',
  likeableType: 'page',
  myLikeValue: '0',
  likeCount: '0',
  dislikeCount: '0',
  likeScore: '0',
  individualLikes: [],
  pageId: '<a href="1ht.json.html">1ht</a>',
  edit: '2',
  editSummary: '',
  prevEdit: '1',
  currentEdit: '2',
  wasPublished: 'true',
  type: 'comment',
  title: '&quot;&gt; Do we disagree about this...&quot;',
  clickbait: '',
  textLength: '3695',
  alias: '1ht',
  externalUrl: '',
  sortChildrenBy: 'recentFirst',
  hasVote: 'false',
  voteType: '',
  votesAnonymous: 'false',
  editCreatorId: '<a href="EliezerYudkowsky.json.html">EliezerYudkowsky</a>',
  editCreatedAt: '2015-12-31 07:48:49',
  pageCreatorId: '<a href="EliezerYudkowsky.json.html">EliezerYudkowsky</a>',
  pageCreatedAt: '2015-12-31 07:46:59',
  seeDomainId: '<a href="0.json.html">0</a>',
  editDomainId: '<a href="EliezerYudkowsky.json.html">EliezerYudkowsky</a>',
  submitToDomainId: '<a href="0.json.html">0</a>',
  isAutosave: 'false',
  isSnapshot: 'false',
  isLiveEdit: 'true',
  isMinorEdit: 'false',
  indirectTeacher: 'false',
  todoCount: '0',
  isEditorComment: 'false',
  isApprovedComment: 'true',
  isResolved: 'false',
  snapshotText: '',
  anchorContext: '',
  anchorText: '',
  anchorOffset: '0',
  mergedInto: '',
  isDeleted: 'false',
  viewCount: '1038',
  text: '&gt; Do we disagree about this point? That is, do you think that such a pseudo-genie would predict me issuing instructions that lead to me dying? \n\nYes!\n\n&gt; One motivating observation is that human predictions of other humans seem to be complete overkill for running my argument---that is, the kinds of errors you must be concerned about are totally unlike the errors that a sophisticated person might make when reasoning about another person. \n\nFor early genies:  Yes.\n\nFor later genies:  It&#39;s more that I don&#39;t think the approval-based proposal, insofar as it&#39;s been specified so far, has demonstrated that it&#39;s reached the point where anything that kills you is a *prediction error*.  I mean, if you can write out an AI design (or Python program that runs on a hypercomputer) which does useful [6y pivotal] things *and* never kills you unless it makes an epistemic error, that&#39;s a full in-principle solution to Friendly AI!  Which I don&#39;t yet consider you to have presented!  It&#39;s a very big thing to assume you can do!\n\nLike, the way I expect this scenario cashes out in practice is that you write down an approval-directed design, I say, &quot;Well, doesn&#39;t that seek out *this* point where it would correctly predict that you&#39;d say &#39;yes&#39; to this proposal, but this proposal actually kills you, because other optimization pressures sought out a case where you&#39;d approve something extreme by mistake?&quot; and you say &quot;Oh of course *that&#39;s* not what I meant, I didn&#39;t mention this extra weird recursion here that prevents that&quot; and this goes back and forth a bit.  I expect that if you ever you present me with something that has *all* the loose variables nailed down (a la AIXI) and whose consequences can be understood, I&#39;ll think it kills the operator, and you&#39;ll disagree in a way that isn&#39;t based purely on math and doesn&#39;t let you convince me.  That&#39;s what the world looks like in possible worlds where powerful optimization processes end up killing you unless you solve some hard problems and approval-based agents turn out not to deal with those problems.\n\n&gt; Assuming that we agree on that point, then we can perhaps agree on a simpler claim: for a strictly superhuman AI, there would be no reason to have actual human involvement. Human involvement is needed only in domains where humans actually have capabilities, especially for reasoning about other humans, that our early AI lacks.\n\nOr where humans have the preferable settings on their reflectively consistent degrees of freedom, where &quot;reflectively consistent degrees of freedom&quot; include Humean degrees of freedom in values, an intuitive decision theory that&#39;s reluctant to give everything away to blackmail or a Pascal&#39;s Mugging, etcetera.  This is the reason to have human involvement with things that are superhumanly competent at computing the answers to well-specified problems, but aren&#39;t *pointing in a sufficiently preferred direction* with that competence if they were looped in on themselves and had to originate all their own directives.\n\nThis is making me wonder if there mustn&#39;t be a basic miscommunication on *some* end because it really sounds like you&#39;re assuming the problem of Friendly AI - reducing &quot;does useful pivotal things and does not kill you&quot; to &quot;have a sufficiently good answer to some well-specified question whose interpretation doesn&#39;t depend on any further reflectively consistent degrees of freedom&quot; - has been fully solved as just one step in your argument.  Or like you&#39;re assuming that approval-directed agency and predicting human acts or answers can be used to solve that Big Question, but if so, *this is exactly the great big key point* and it&#39;s not something you can just ask me to take for granted!',
  metaText: '',
  isTextLoaded: 'true',
  isSubscribedToDiscussion: 'false',
  isSubscribedToUser: 'false',
  isSubscribedAsMaintainer: 'false',
  discussionSubscriberCount: '0',
  maintainerCount: '0',
  userSubscriberCount: '0',
  lastVisit: '2016-02-25 04:36:01',
  hasDraft: 'false',
  votes: [],
  voteSummary: 'null',
  muVoteSummary: '0',
  voteScaling: '0',
  currentUserVote: '-2',
  voteCount: '0',
  lockedVoteType: '',
  maxEditEver: '0',
  redLinkCount: '0',
  lockedBy: '',
  lockedUntil: '',
  nextPageId: '',
  prevPageId: '',
  usedAsMastery: 'false',
  proposalEditNum: '0',
  permissions: {
    edit: {
      has: 'false',
      reason: 'You don&#39;t have domain permission to edit this page'
    },
    proposeEdit: {
      has: 'true',
      reason: ''
    },
    delete: {
      has: 'false',
      reason: 'You don&#39;t have domain permission to delete this page'
    },
    comment: {
      has: 'false',
      reason: 'You can&#39;t comment in this domain because you are not a member'
    },
    proposeComment: {
      has: 'true',
      reason: ''
    }
  },
  summaries: {},
  creatorIds: [
    '<a href="EliezerYudkowsky.json.html">EliezerYudkowsky</a>'
  ],
  childIds: [],
  parentIds: [
    '<a href="1gj.json.html">1gj</a>',
    '<a href="task_agi.json.html">task_agi</a>'
  ],
  commentIds: [],
  questionIds: [],
  tagIds: [],
  relatedIds: [],
  markIds: [],
  explanations: [],
  learnMore: [],
  requirements: [],
  subjects: [],
  lenses: [],
  lensParentId: '',
  pathPages: [],
  learnMoreTaughtMap: {},
  learnMoreCoveredMap: {},
  learnMoreRequiredMap: {},
  editHistory: {},
  domainSubmissions: {},
  answers: [],
  answerCount: '0',
  commentCount: '0',
  newCommentCount: '0',
  linkedMarkCount: '0',
  changeLogs: [
    {
      likeableId: '<a href="0.json.html">0</a>',
      likeableType: 'changeLog',
      myLikeValue: '0',
      likeCount: '0',
      dislikeCount: '0',
      likeScore: '0',
      individualLikes: [],
      id: '4857',
      pageId: '<a href="1ht.json.html">1ht</a>',
      userId: '<a href="EliezerYudkowsky.json.html">EliezerYudkowsky</a>',
      edit: '2',
      type: 'newEdit',
      createdAt: '2015-12-31 07:48:49',
      auxPageId: '',
      oldSettingsValue: '',
      newSettingsValue: ''
    },
    {
      likeableId: '<a href="0.json.html">0</a>',
      likeableType: 'changeLog',
      myLikeValue: '0',
      likeCount: '0',
      dislikeCount: '0',
      likeScore: '0',
      individualLikes: [],
      id: '4856',
      pageId: '<a href="1ht.json.html">1ht</a>',
      userId: '<a href="EliezerYudkowsky.json.html">EliezerYudkowsky</a>',
      edit: '1',
      type: 'newEdit',
      createdAt: '2015-12-31 07:46:59',
      auxPageId: '',
      oldSettingsValue: '',
      newSettingsValue: ''
    },
    {
      likeableId: '<a href="0.json.html">0</a>',
      likeableType: 'changeLog',
      myLikeValue: '0',
      likeCount: '0',
      dislikeCount: '0',
      likeScore: '0',
      individualLikes: [],
      id: '4853',
      pageId: '<a href="1ht.json.html">1ht</a>',
      userId: '<a href="EliezerYudkowsky.json.html">EliezerYudkowsky</a>',
      edit: '0',
      type: 'newParent',
      createdAt: '2015-12-31 07:30:39',
      auxPageId: '<a href="task_agi.json.html">task_agi</a>',
      oldSettingsValue: '',
      newSettingsValue: ''
    },
    {
      likeableId: '<a href="0.json.html">0</a>',
      likeableType: 'changeLog',
      myLikeValue: '0',
      likeCount: '0',
      dislikeCount: '0',
      likeScore: '0',
      individualLikes: [],
      id: '4855',
      pageId: '<a href="1ht.json.html">1ht</a>',
      userId: '<a href="EliezerYudkowsky.json.html">EliezerYudkowsky</a>',
      edit: '0',
      type: 'newParent',
      createdAt: '2015-12-31 07:30:39',
      auxPageId: '<a href="1gj.json.html">1gj</a>',
      oldSettingsValue: '',
      newSettingsValue: ''
    }
  ],
  feedSubmissions: [],
  searchStrings: {},
  hasChildren: 'false',
  hasParents: 'true',
  redAliases: {},
  improvementTagIds: [],
  nonMetaTagIds: [],
  todos: [],
  slowDownMap: 'null',
  speedUpMap: 'null',
  arcPageIds: 'null',
  contentRequests: {}
}</pre></body></html>