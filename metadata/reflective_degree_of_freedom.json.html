<!DOCTYPE html><html><head><meta charset="utf-8"><title>Reflectively consistent degree of freedom</title><link rel="stylesheet" type="text/css" href="../common.css"></head><body><pre style="white-space:pre-wrap">{
  localUrl: '<a href="../page/reflective_degree_of_freedom.html">../page/reflective_degree_of_freedom.html</a>',
  arbitalUrl: '<a href="https://arbital.com/p/reflective_degree_of_freedom">https://arbital.com/p/reflective_degree_of_freedom</a>',
  rawJsonUrl: '<a href="../raw/2fr.json">../raw/2fr.json</a>',
  likeableId: '<a href="1370.json.html">1370</a>',
  likeableType: 'page',
  myLikeValue: '0',
  likeCount: '0',
  dislikeCount: '0',
  likeScore: '0',
  individualLikes: [],
  pageId: '<a href="reflective_degree_of_freedom.json.html">reflective_degree_of_freedom</a>',
  edit: '4',
  editSummary: '',
  prevEdit: '3',
  currentEdit: '4',
  wasPublished: 'true',
  type: 'wiki',
  title: 'Reflectively consistent degree of freedom',
  clickbait: 'When an instrumentally efficient, self-modifying AI can be like X or like X&#39; in such a way that X wants to be X and X&#39; wants to be X&#39;, that&#39;s a reflectively consistent degree of freedom.',
  textLength: '3798',
  alias: 'reflective_degree_of_freedom',
  externalUrl: '',
  sortChildrenBy: 'likes',
  hasVote: 'false',
  voteType: '',
  votesAnonymous: 'false',
  editCreatorId: '<a href="EliezerYudkowsky.json.html">EliezerYudkowsky</a>',
  editCreatedAt: '2016-03-09 03:19:20',
  pageCreatorId: '<a href="EliezerYudkowsky.json.html">EliezerYudkowsky</a>',
  pageCreatedAt: '2016-03-09 03:08:49',
  seeDomainId: '<a href="0.json.html">0</a>',
  editDomainId: '<a href="EliezerYudkowsky.json.html">EliezerYudkowsky</a>',
  submitToDomainId: '<a href="0.json.html">0</a>',
  isAutosave: 'false',
  isSnapshot: 'false',
  isLiveEdit: 'true',
  isMinorEdit: 'false',
  indirectTeacher: 'false',
  todoCount: '4',
  isEditorComment: 'false',
  isApprovedComment: 'true',
  isResolved: 'false',
  snapshotText: '',
  anchorContext: '',
  anchorText: '',
  anchorOffset: '0',
  mergedInto: '',
  isDeleted: 'false',
  viewCount: '184',
  text: 'A &quot;reflectively consistent degree of freedom&quot; is when a self-modifying AI can have multiple possible properties $X_i \\in X$ such that an AI with property $X_1$ wants to go on being an AI with property $X_1,$ and an AI with $X_2$ will ceteris paribus only choose to self-modify into designs that are also $X_2,$ etcetera.\n\nThe archetypal reflectively consistent degree of freedom is a [humean_freedom Humean degree of freedom], the refective consistency of many different possible [1fw utility functions].  If Gandhi doesn&#39;t want to kill you, and you offer Gandhi a pill that makes him want to kill people, then [gandhi_stability_argument Gandhi will refuse the pill], because he knows that if he takes the pill then pill-taking-future-Gandhi will kill people, and the current Gandhi rates this outcome low in his preference function.  Similarly, a [10h paperclip maximizer] wants to remain a paperclip maximizer.  Since these two possible preference frameworks are both [71 consistent under reflection], they constitute a &quot;reflectively consistent degree of freedom&quot; or &quot;reflective degree of freedom&quot;.\n\nFrom a design perspective, or the standpoint of an [1cv], the key fact about a reflectively consistent degree of freedom is that it doesn&#39;t automatically self-correct as a result of the AI trying to improve itself.  The problem &quot;Has trouble understanding General Relativity&quot; or &quot;Cannot beat a human at poker&quot; or &quot;Crashes on seeing a picture of a dolphin&quot; is something that you might expect to correct automatically and without specifically directed effort, assuming you otherwise improved the AI&#39;s general ability to understand the world and that it was self-improving.  &quot;Wants paperclips instead of eudaimonia&quot; is *not* self-correcting.\n\nAnother way of looking at it is that reflective degrees of freedom describe information that is not automatically extracted or learned given a sufficiently smart AI, the way it would automatically learn General Relativity.  If you have a concept whose borders (membership condition) relies on knowing about General Relativity, then when the AI is sufficiently smart it will see a simple definition of that concept.  If the concept&#39;s borders instead rely on [ value-laden] judgments, there may be no algorithmically simple description of that concept, even given lots of knowledge of the environment, because the [humean_freedom Humean degrees of freedom] need to be independently specified.\n\nOther properties besides the preference function look like they should be reflectively consistent in similar ways.  For example, [ son of CDT] and [ UDT] both seem to be reflectively consistent in different ways.  So an AI that has, from our perspective, a &#39;bad&#39; decision theory (one that leads to behaviors we don&#39;t want), isn&#39;t &#39;bugged&#39; in a way we can rely on to self-correct.  (This is one reason why MIRI studies decision theory and not computer vision.  There&#39;s a sense in which mistakes in computer vision automatically fix themselves, given a sufficiently advanced AI, and mistakes in decision theory don&#39;t fix themselves.)\n\nSimilarly, [27p Bayesian priors] are by default consistent under reflection - if you&#39;re a Bayesian with a prior, you want to create copies of yourself that have the same prior or [1ly Bayes-updated] versions of the prior.  So &#39;bugs&#39; (from a human standpoint) like being [Pascal&#39;s Muggable](https://wiki.lesswrong.com/wiki/Pascal&#39;s_mugging) might not automatically fix themselves in a way that correlated with sufficient growth in other knowledge and general capability, in the way we might expect a specific mistaken belief about gravity to correct itself in a way that correlated to sufficient general growth in capability.  (This is why MIRI thinks about [ naturalistic induction] and similar questions about prior probabilities.)',
  metaText: '',
  isTextLoaded: 'true',
  isSubscribedToDiscussion: 'false',
  isSubscribedToUser: 'false',
  isSubscribedAsMaintainer: 'false',
  discussionSubscriberCount: '1',
  maintainerCount: '1',
  userSubscriberCount: '0',
  lastVisit: '',
  hasDraft: 'false',
  votes: [],
  voteSummary: 'null',
  muVoteSummary: '0',
  voteScaling: '0',
  currentUserVote: '-2',
  voteCount: '0',
  lockedVoteType: '',
  maxEditEver: '0',
  redLinkCount: '0',
  lockedBy: '',
  lockedUntil: '',
  nextPageId: '',
  prevPageId: '',
  usedAsMastery: 'false',
  proposalEditNum: '0',
  permissions: {
    edit: {
      has: 'false',
      reason: 'You don&#39;t have domain permission to edit this page'
    },
    proposeEdit: {
      has: 'true',
      reason: ''
    },
    delete: {
      has: 'false',
      reason: 'You don&#39;t have domain permission to delete this page'
    },
    comment: {
      has: 'false',
      reason: 'You can&#39;t comment in this domain because you are not a member'
    },
    proposeComment: {
      has: 'true',
      reason: ''
    }
  },
  summaries: {},
  creatorIds: [
    '<a href="EliezerYudkowsky.json.html">EliezerYudkowsky</a>'
  ],
  childIds: [
    '<a href="humean_free_boundary.json.html">humean_free_boundary</a>',
    '<a href="value_laden.json.html">value_laden</a>'
  ],
  parentIds: [
    '<a href="reflective_stability.json.html">reflective_stability</a>'
  ],
  commentIds: [
    '<a href="2gh.json.html">2gh</a>'
  ],
  questionIds: [],
  tagIds: [],
  relatedIds: [],
  markIds: [],
  explanations: [],
  learnMore: [],
  requirements: [],
  subjects: [],
  lenses: [],
  lensParentId: '',
  pathPages: [],
  learnMoreTaughtMap: {},
  learnMoreCoveredMap: {},
  learnMoreRequiredMap: {},
  editHistory: {},
  domainSubmissions: {},
  answers: [],
  answerCount: '0',
  commentCount: '0',
  newCommentCount: '0',
  linkedMarkCount: '0',
  changeLogs: [
    {
      likeableId: '<a href="0.json.html">0</a>',
      likeableType: 'changeLog',
      myLikeValue: '0',
      likeCount: '0',
      dislikeCount: '0',
      likeScore: '0',
      individualLikes: [],
      id: '8423',
      pageId: '<a href="reflective_degree_of_freedom.json.html">reflective_degree_of_freedom</a>',
      userId: '<a href="EliezerYudkowsky.json.html">EliezerYudkowsky</a>',
      edit: '4',
      type: 'newChild',
      createdAt: '2016-03-09 03:22:08',
      auxPageId: '<a href="humean_free_boundary.json.html">humean_free_boundary</a>',
      oldSettingsValue: '',
      newSettingsValue: ''
    },
    {
      likeableId: '<a href="0.json.html">0</a>',
      likeableType: 'changeLog',
      myLikeValue: '0',
      likeCount: '0',
      dislikeCount: '0',
      likeScore: '0',
      individualLikes: [],
      id: '8422',
      pageId: '<a href="reflective_degree_of_freedom.json.html">reflective_degree_of_freedom</a>',
      userId: '<a href="EliezerYudkowsky.json.html">EliezerYudkowsky</a>',
      edit: '4',
      type: 'newEdit',
      createdAt: '2016-03-09 03:19:20',
      auxPageId: '',
      oldSettingsValue: '',
      newSettingsValue: ''
    },
    {
      likeableId: '<a href="0.json.html">0</a>',
      likeableType: 'changeLog',
      myLikeValue: '0',
      likeCount: '0',
      dislikeCount: '0',
      likeScore: '0',
      individualLikes: [],
      id: '8419',
      pageId: '<a href="reflective_degree_of_freedom.json.html">reflective_degree_of_freedom</a>',
      userId: '<a href="EliezerYudkowsky.json.html">EliezerYudkowsky</a>',
      edit: '3',
      type: 'newEdit',
      createdAt: '2016-03-09 03:11:53',
      auxPageId: '',
      oldSettingsValue: '',
      newSettingsValue: ''
    },
    {
      likeableId: '<a href="0.json.html">0</a>',
      likeableType: 'changeLog',
      myLikeValue: '0',
      likeCount: '0',
      dislikeCount: '0',
      likeScore: '0',
      individualLikes: [],
      id: '8418',
      pageId: '<a href="reflective_degree_of_freedom.json.html">reflective_degree_of_freedom</a>',
      userId: '<a href="EliezerYudkowsky.json.html">EliezerYudkowsky</a>',
      edit: '2',
      type: 'newEdit',
      createdAt: '2016-03-09 03:11:14',
      auxPageId: '',
      oldSettingsValue: '',
      newSettingsValue: ''
    },
    {
      likeableId: '<a href="0.json.html">0</a>',
      likeableType: 'changeLog',
      myLikeValue: '0',
      likeCount: '0',
      dislikeCount: '0',
      likeScore: '0',
      individualLikes: [],
      id: '8417',
      pageId: '<a href="reflective_degree_of_freedom.json.html">reflective_degree_of_freedom</a>',
      userId: '<a href="EliezerYudkowsky.json.html">EliezerYudkowsky</a>',
      edit: '1',
      type: 'newEdit',
      createdAt: '2016-03-09 03:08:49',
      auxPageId: '',
      oldSettingsValue: '',
      newSettingsValue: ''
    },
    {
      likeableId: '<a href="0.json.html">0</a>',
      likeableType: 'changeLog',
      myLikeValue: '0',
      likeCount: '0',
      dislikeCount: '0',
      likeScore: '0',
      individualLikes: [],
      id: '8416',
      pageId: '<a href="reflective_degree_of_freedom.json.html">reflective_degree_of_freedom</a>',
      userId: '<a href="EliezerYudkowsky.json.html">EliezerYudkowsky</a>',
      edit: '0',
      type: 'newParent',
      createdAt: '2016-03-09 02:39:44',
      auxPageId: '<a href="reflective_stability.json.html">reflective_stability</a>',
      oldSettingsValue: '',
      newSettingsValue: ''
    }
  ],
  feedSubmissions: [],
  searchStrings: {},
  hasChildren: 'true',
  hasParents: 'true',
  redAliases: {},
  improvementTagIds: [],
  nonMetaTagIds: [],
  todos: [],
  slowDownMap: 'null',
  speedUpMap: 'null',
  arcPageIds: 'null',
  contentRequests: {}
}</pre></body></html>