<!DOCTYPE html><html><head><meta charset="utf-8"><title>An early stage project generation model</title><link rel="stylesheet" type="text/css" href="../common.css"><link rel="stylesheet" type="text/css" href="../page-style.css"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  SVG: {EqnChunk: 50, EqnChunkFactor: 1.5, EqChunkDelay: 10, useFontCache: false, linebreaks: {automatic: true}},
  tex2jax: {
    inlineMath: [['$~$', '$~$']],
    displayMath: [['$$~$', '$~$$']],
    processEscapes: true,
    preview: 'none',
  },
  showProcessingMessages: false,
  messageStyle: 'none',
  // http://docs.mathjax.org/en/latest/config-files.html#the-tex-ams-svg-configuration-file
  jax: ["input/TeX","output/SVG", "output/PreviewHTML"],
  extensions: ["tex2jax.js","MathMenu.js","MathZoom.js", "fast-preview.js", "AssistiveMML.js", "a11y/accessibility-menu.js"],
  TeX: { extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"] }
});</script><script type="text/javascript" src="../MathJax-2.7.5/MathJax.js"></script><script type="text/javascript" src="../arbital-demo-bundle.js"></script><script type="text/javascript">window.addEventListener('DOMContentLoaded', e=>window.loadAllDemos())
</script></head><body><header><h1 class="title">An early stage project generation model</h1><div class="page-info"><p class="metadata-link"><a href="../metadata/early_stage_project_generation.json.html">early_stage_project_generation.json</a></p><p class="arbital-url"><a href="https://arbital.com/p/early_stage_project_generation">https://arbital.com/p/early_stage_project_generation</a></p><p class="creator">by
 <a class="page-link" href="../page/BenPace.html">Ben Pace</a> Mar 16 2017 
updated
 Mar 16 2017</p></div><p class="clickbait">How do you figure out which projects to even consider, when you're getting started?</p><nav class="breadcrumbs"><nav class="breadcrumb" aria-label="Breadcrumb"><ul><li><a href="../index.html">Index</a></li><li>An early stage project generation model</li></ul></nav><nav class="breadcrumb" aria-label="Breadcrumb"><ul><li><a href="early_life_planning.html">Early-life planning</a></li><li>â€¦</li></ul></nav></nav></header><hr><main><p>[summary: Most people don't test their hypotheses about what the most important problems are. Do this, do it regularly, and also get a lot of input from experts when you're early in your plans.]</p>
<p>[summary(Brief): Blog post with examples of ways that people generate their life plans, and suggested improvements.]</p>
<p>Here is how I see many people connected to Effective Altruism making their life plan decisions:</p>
<blockquote>
  <p>Well, I don't know what the most important problem is. For example, reducing factory farming seems important, as does alleviating global poverty, but then certain global catastrophes seem potentially awful too, such as novel bioweapons or advanced artificial intelligence. I'm hardly an expert in the ways of the world, so I'll conservatively choose a career that seems like it could be useful in lots of places - maybe a grant writer, or a data scientist - and then I'll be able to pivot later when I've read all of the research output by the FHI, OpenPhil and CEA's research departments and thought about it some more on my own.</p>
</blockquote>
<p>Alternatively, I see some people making a similar mistake, but with slightly more of the virtue of not-being-risk-averse:</p>
<blockquote>
  <p>I'm currently very uncertain about the most important problem, but I think I associate 5% more probability on the importance of AI relative to each other possibility. I know that what you normally do in this career path next is PhD in Artificial Intelligence, so I'll do that. Sure, in 10 years when I'm (fingers crossed) an associate professor, I might turn out to have been wrong, but if everyone trusts their beliefs, then we'll have some people turning out to be experts in the truly important fields.</p>
</blockquote>
<p>The first way to improve on these, is to make plans that give you information about your best guess with much faster feedback loops.</p>
<p>Consider this alternative plan for figuring out what to do next:</p>
<blockquote>
  <p>Gun to my head, I think global catastrophes through novel tech are probably the most likely events that will be pivotal in the long run future of humanity. I don't want to have to wait a decade to find out if I was right about this though, so I'm going to do two things. </p>
  <p>Firstly, I'll talk to a few experts in synthetic biology, nuclear warfare and artificial intelligence, to ask if my basic assumptions are broadly correct. Secondly, if that hans't changed my assessment of the world too much, I'll ask them what ambitious projects in the field someone in my position could work on that last 6-24 months that I could do, that would teach me a bunch about how the field works. I'll pick the best sounding project I come up with, and then see how much I've learned once the project is over.</p>
</blockquote>
<p>The most important thing, is that the project is much shorter, and you're able to re-assess the situation once it's complete, with new information you learned along the way.</p>
<p>To be clearer about the improvements:</p>
<ol>
<li>Explicitly testing your hypotheses about what's the most important thing to work on is the biggest difference here. Do not shy away from this due to a desire to 'do your due diligence' (note: Add in link to appropriate Eliezer post on humility/modesty). Actually testing your beliefs is the only way to get better at forming models.</li>
<li>Ahead of looking at specific projects, ask yourself/experts what sorts of information/learning would change your plans, and then optimise for projects that give you the most information in expectation.</li>
<li>Picking projects that look like what someone <em>in the field</em> would do is a good way to let empiricism guide the skills you learn. If you don't know much about the field, your best guess about what skills to learn are probably not as useful as figuring out how to produce a standard deliverable in that field. Reading all the wikipedia pages corresponding to various programming languages is not as useful a first step as writing code for Battleships.</li>
<li>There is a related but important reason that one should think of things in terms of 'projects'. <a href="https://lesswrong.com/lw/2p5/humans_are_not_automatically_strategic/">Humans are not automatically strategic</a>, and working with a concrete deliverable and clear criteria for failure/success are a strong step for avoiding many of the errors most people run into most of the time.</li>
<li>Talking to experts first will stop you from embarking on a project to learn information that was available to you from the beginning (this is an excellent method for shooting yourself in the foot before taking a step, and I have nearly done this).</li>
<li>Asking experts for 'ambitious' projects is often a good way to point yourself towards projects that will have good signalling value.</li>
</ol>
<p>As a final note, the plan above is a very explicit plan for coming up with projects. There is an important complement, which is to <em>surround yourself with interesting people</em> who come up with ideas themselves. Especially early on, it is too much to expect yourself to produce all the good ideas, and finding people who produce good ideas and occasionally working with them on theirs is a key approach; and this increases the importance of equipping yourself with high quality signals of your abilities. But I digress.</p></main><hr><footer></footer></body></html>