<!DOCTYPE html><html><head><meta charset="utf-8"><title>Shutdown problem</title><link rel="stylesheet" type="text/css" href="../common.css"><link rel="stylesheet" type="text/css" href="../page-style.css"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  SVG: {EqnChunk: 50, EqnChunkFactor: 1.5, EqChunkDelay: 10, useFontCache: false, linebreaks: {automatic: true}},
  tex2jax: {
    inlineMath: [['$~$', '$~$']],
    displayMath: [['$$~$', '$~$$']],
    processEscapes: true,
    preview: 'none',
  },
  showProcessingMessages: false,
  messageStyle: 'none',
  // http://docs.mathjax.org/en/latest/config-files.html#the-tex-ams-svg-configuration-file
  jax: ["input/TeX","output/SVG", "output/PreviewHTML"],
  extensions: ["tex2jax.js","MathMenu.js","MathZoom.js", "fast-preview.js", "AssistiveMML.js", "a11y/accessibility-menu.js"],
  TeX: { extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"] }
});</script><script type="text/javascript" src="../MathJax-2.7.5/MathJax.js"></script><script type="text/javascript" src="../arbital-demo-bundle.js"></script><script type="text/javascript">window.addEventListener('DOMContentLoaded', e=>window.loadAllDemos())
</script></head><body><header><h1 class="title">Shutdown problem</h1><div class="page-info"><p class="metadata-link"><a href="../metadata/shutdown_problem.json.html">shutdown_problem.json</a></p><p class="arbital-url"><a href="https://arbital.com/p/shutdown_problem">https://arbital.com/p/shutdown_problem</a></p><p class="creator">by
 <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a> Mar 28 2016 
updated
 Feb 13 2017</p></div><p class="clickbait">How to build an AGI that lets you shut it down, despite the obvious fact that this will interfere with whatever the AGI's goals are.</p><nav class="breadcrumbs"><nav class="breadcrumb" aria-label="Breadcrumb"><ul><li><a href="../index.html">Index</a></li><li>Shutdown problem</li></ul></nav><nav class="breadcrumb" aria-label="Breadcrumb"><ul><li><a href="ai_alignment.html">AI alignment</a></li><li><a href="corrigibility.html">Corrigibility</a></li><li>â€¦</li></ul></nav></nav></header><hr><main><p>[summary:  The 'shutdown problem' is creating a <a href="sufficiently_advanced_ai.html">sufficiently advanced Artificial Intelligence</a> which will, on the press of a button, suspend itself safely to disk; such that the AI <a href="nonadversarial.html">does not object</a> to the existence of this button, nor <a href="corrigibility.html">try to prevent it</a> from being pressed.</p>
<p>This is difficult because avoiding your own halt or suspension is a <a href="instrumental_convergence.html">convergent instrumental strategy</a>; even <a href="no_coffee_if_dead.html">a goal like &quot;bring the user coffee&quot; implies avoiding shutdown</a>.</p>
<p>This problem is sometimes decomposed into (1) the <a href="shutdown_utility_function.html">problem</a> of finding a utility function that <a href="unforeseen_maximum.html">really actually means</a> "Suspend yourself safely to disk", and (2) the <a href="utility_indifference.html">problem</a> of building an agent that <em>wants</em> to switch to optimizing a different utility function if a button is pressed, but that <em>doesn't</em> want to press that button or prevent its being pressed.</p>
<p>See also <a href="utility_indifference.html">Utility indifference</a>, <a href="shutdown_utility_function.html">Shutdown utility function</a>, <a href="corrigibility.html">Corrigibility</a>, and <a href="interruptibility.html">Interruptibility</a>.]</p>
<p>The 'shutdown problem' is creating a <a href="sufficiently_advanced_ai.html">sufficiently advanced Artificial Intelligence</a> which will, on the press of a button, suspend itself safely to disk; such that the AI <a href="nonadversarial.html">does not object</a> to the existence of this button, nor <a href="corrigibility.html">try to prevent it</a> from being pressed.</p>
<p>This is difficult because avoiding your own halt or suspension is a <a href="instrumental_convergence.html">convergent instrumental strategy</a>; even <a href="no_coffee_if_dead.html">a goal like &quot;bring the user coffee&quot; implies avoiding shutdown</a>.</p>
<p>One possible research avenue is to:</p>
<ol>
<li>Describe a '<a href="shutdown_utility_function.html">shutdown utility function</a>' whose <a href="attainable_optimum.html">attainable optimum</a> is "Suspend safely to disk (and possibly <a href="abortable.html">safely abort</a> plans in progress), without building a hidden environmental copy of yourself to carry out your previous goal and then assimilate all matter in the universe to make absolutely sure the original AI stays shut down".</li>
<li>Find <a href="utility_indifference.html">a sensible way to compose</a> the shutdown utility function with the agent's regular utility function, such that:<ul>
<li>(2a) Which utility function the agent optimizes depends on whether a switch was pressed.</li>
<li>(2b) The AI experiences no incentive to cause the switch to be pressed or prevent the switch from being pressed.</li></ul></li>
</ol>
<p>Harder versions of this problem would add specifications such as:</p>
<ul>
<li>The AI <a href="nonadversarial_safety.html">positively wants</a> the switch to exist and will restore the switch's existence if it is damaged.</li>
<li>The AI will <a href="abortable.html">safely abort</a> any plans in progress (and design plans such that they can be safely aborted).</li>
</ul>
<p>See also <a href="utility_indifference.html">Utility indifference</a>, <a href="shutdown_utility_function.html">Shutdown utility function</a>, <a href="corrigibility.html">Corrigibility</a>, <a href="interruptibility.html">Interruptibility</a>, <a href="low_impact.html">Low impact</a>, and <a href="abortable.html">Abortable plans</a>.</p></main><hr><footer><p class="tagged"><h2>Tagged</h2><span class="page-comma-list"><a class="page-link" href="../page/value_alignment_open_problem.html">AI alignment open problem</a>,
 <a class="page-link" href="../page/taskagi_open_problems.html">Open subproblems in aligning a Task-based AGI</a>,
 <a class="page-link" href="../page/shutdown_utility_function.html">Shutdown utility function</a>,
 <a class="page-link" href="../page/stub_meta_tag.html">Stub</a>,
 <a class="page-link" href="../page/utility_indifference.html">Utility indifference</a></span></p><p class="likes"><h2>Likes</h2><span class="page-comma-list"><a class="page-link" href="../page/NateSoares.html">Nate Soares</a>,
 <a class="page-link" href="../page/NickShesterin.html">Nick Shesterin</a></span></p><p class="related"><h2>Related</h2><ul class="page-list"><li><a class="page-link" href="../page/updated_deference.html">Problem of fully updated deference</a> <q>Why moral uncertainty doesn't stop an AI from defending its off-switch.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li></ul></p><p class="reverse-related"><h2>Reverse Related</h2><ul class="page-list"><li><a class="page-link" href="../page/value_alignment_open_problem.html">AI alignment open problem</a> <q>Tag for open problems under AI alignment.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/taskagi_open_problems.html">Open subproblems in aligning a Task-based AGI</a> <q>Open research problems, especially ones we can model today, in building an AGI that can &quot;paint all cars pink&quot; without turning its future light cone into pink-painted cars.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/shutdown_utility_function.html">Shutdown utility function</a> <q>A special case of a low-impact utility function where you just want the AGI to switch itself off harmlessly (and not create subagents to make absolutely sure it stays off, etcetera).</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/stub_meta_tag.html">Stub</a> <q>This page only gives a very brief overview of the topic. If you're able to, please help expand or improve it!</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/utility_indifference.html">Utility indifference</a> <q>How can we make an AI indifferent to whether we press a button that changes its goals?</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li></ul></p><p class="children"><h2>Children</h2><ul class="page-tree"><li><a class="page-link" href="../page/no_coffee_if_dead.html">You can't get the coffee if you're dead</a> <q>An AI given the goal of 'get the coffee' can't achieve that goal if it has been turned off; so even an AI whose goal is just to fetch the coffee may try to avert a shutdown button being pressed.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li></ul></p></footer></body></html>