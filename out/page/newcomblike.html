<!DOCTYPE html><html><head><meta charset="utf-8"><title>Newcomblike decision problems</title><link rel="stylesheet" type="text/css" href="../common.css"><link rel="stylesheet" type="text/css" href="../page-style.css"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  SVG: {EqnChunk: 50, EqnChunkFactor: 1.5, EqChunkDelay: 10, useFontCache: false, linebreaks: {automatic: true}},
  tex2jax: {
    inlineMath: [['$~$', '$~$']],
    displayMath: [['$$~$', '$~$$']],
    processEscapes: true,
    preview: 'none',
  },
  showProcessingMessages: false,
  messageStyle: 'none',
  // http://docs.mathjax.org/en/latest/config-files.html#the-tex-ams-svg-configuration-file
  jax: ["input/TeX","output/SVG", "output/PreviewHTML"],
  extensions: ["tex2jax.js","MathMenu.js","MathZoom.js", "fast-preview.js", "AssistiveMML.js", "a11y/accessibility-menu.js"],
  TeX: { extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"] }
});</script><script type="text/javascript" src="../MathJax-2.7.5/MathJax.js"></script><script type="text/javascript" src="../arbital-demo-bundle.js"></script><script type="text/javascript">window.addEventListener('DOMContentLoaded', e=>window.loadAllDemos())
</script></head><body><header><h1 class="title">Newcomblike decision problems</h1><div class="page-info"><p class="metadata-link"><a href="../metadata/newcomblike.json.html">newcomblike.json</a></p><p class="arbital-url"><a href="https://arbital.com/p/newcomblike">https://arbital.com/p/newcomblike</a></p><p class="creator">by
 <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a> Jul 31 2016 
updated
 Aug 3 2016</p></div><p class="clickbait">Decision problems in which your choice correlates with something other than its physical consequences (say, because somebody has predicted you very well) can do weird things to some decision theories.</p><nav class="breadcrumbs"><nav class="breadcrumb" aria-label="Breadcrumb"><ul><li><a href="../index.html">Index</a></li><li>Newcomblike decision problems</li></ul></nav><nav class="breadcrumb" aria-label="Breadcrumb"><ul><li><a href="decision_theory.html">Decision theory</a></li><li><a href="logical_dt.html">Logical decision theories</a></li><li>â€¦</li></ul></nav></nav></header><hr><main><p>"Newcomblike decision problems" are scenarios in which your choice correlates with the outcome by some pathway other than the direct physical consequences of your act--for example, because somebody else is predicting you; or you're interacting with someone very similar to yourself; or people with different genes behave differently.  Newcomblike problems force splits between <a href="evidential_dt.html">different</a> <a href="causal_dt.html">decision</a> <a href="logical_dt.html">theories</a> that would give the same answer in more "normal" problems.  Perhaps the most realistic Newcomblike problem is <a href="rationality_of_voting.html">voting in elections</a>, where a large population of voters probably contains at least some people similar to you.  The class of scenarios as a whole is named after the (less realistic) <a href="newcombs_problem.html">Newcomb&#39;s Problem</a>.</p></main><hr><footer><p class="tagged"><h2>Tagged</h2><span class="page-comma-list"><a class="page-link" href="../page/stub_meta_tag.html">Stub</a></span></p><p class="likes"><h2>Likes</h2><span class="page-comma-list"><a class="page-link" href="../page/EricBruylant.html">Eric Bruylant</a></span></p><p class="reverse-related"><h2>Reverse Related</h2><ul class="page-list"><li><a class="page-link" href="../page/stub_meta_tag.html">Stub</a> <q>This page only gives a very brief overview of the topic. If you're able to, please help expand or improve it!</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li></ul></p><p class="children"><h2>Children</h2><ul class="page-tree"><li><a class="page-link" href="../page/rationality_of_voting.html">'Rationality' of voting in elections</a> <q>&quot;A single vote is very unlikely to swing the election, so your vote is unlikely to have an effect&quot; versus &quot;Many people similar to you are making a similar decision about whether to vote.&quot;</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/pd_tournament_99ldt_1cdt.html">99LDT x 1CDT oneshot PD tournament as arguable counterexample to LDT doing better than CDT</a> <q>Arguendo, if 99 LDT agents and 1 CDT agent are facing off in a one-shot Prisoner's Dilemma tournament, the CDT agent does better on a problem that CDT considers 'fair'.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/absentminded_driver.html">Absent-Minded Driver dilemma</a> <q>A road contains two identical intersections.  An absent-minded driver wants to turn right at the second intersection.  &quot;With what probability should the driver turn right?&quot; argue decision theorists.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/death_in_damascus.html">Death in Damascus</a> <q>Death tells you that It is coming for you tomorrow.  You can stay in Damascus or flee to Aleppo.  Whichever decision you actually make is the wrong one.  This gives some decision theories trouble.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/newcombs_problem.html">Newcomb's Problem</a> <q>There are two boxes in front of you, Box A and Box B.  You can take both boxes, or only Box B.  Box A contains $1000.  Box B contains $1,000,000 if and only if Omega predicted you'd take only Box B.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/parfits_hitchhiker.html">Parfit's Hitchhiker</a> <q>You are dying in the desert.  A truck-driver who is very good at reading faces finds you, and offers to drive you into the city if you promise to pay $1,000 on arrival.  You are a selfish rationalist.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/prisoners_dilemma.html">Prisoner's Dilemma</a> <q>You and an accomplice have been arrested.  Both of you must decide, in isolation, whether to testify against the other prisoner--which subtracts one year from your sentence, and adds two to theirs.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a><ul class="page-tree"><li><a class="page-link" href="../page/true_prisoners_dilemma.html">True Prisoner's Dilemma</a> <q>A scenario that would reproduce the ideal payoff matrix of the Prisoner's Dilemma about human beings who care about their public reputation and each other.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li></ul></li><li><a class="page-link" href="../page/toxoplasmosis_dilemma.html">Toxoplasmosis dilemma</a> <q>A parasitic infection, carried by cats, may make humans enjoy petting cats more.  A kitten, now in front of you, isn't infected.  But if you *want* to pet it, you may already be infected.  Do you?</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/transparent_newcombs_problem.html">Transparent Newcomb's Problem</a> <q>Omega has left behind a transparent Box A containing $1000, and a transparent Box B containing $1,000,000 or nothing.  Box B is full iff Omega thinks you one-box on seeing a full Box B.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/ultimatum_game.html">Ultimatum Game</a> <q>A Proposer decides how to split $10 between themselves and the Responder.  The Responder can take what is offered, or refuse, in which case both parties get nothing.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li></ul></p></footer></body></html>