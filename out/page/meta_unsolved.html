<!DOCTYPE html><html><head><meta charset="utf-8"><title>Meta-rules for (narrow) value learning are still unsolved</title><link rel="stylesheet" type="text/css" href="../common.css"><link rel="stylesheet" type="text/css" href="../page-style.css"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  SVG: {EqnChunk: 50, EqnChunkFactor: 1.5, EqChunkDelay: 10, useFontCache: false, linebreaks: {automatic: true}},
  tex2jax: {
    inlineMath: [['$~$', '$~$']],
    displayMath: [['$$~$', '$~$$']],
    processEscapes: true,
    preview: 'none',
  },
  showProcessingMessages: false,
  messageStyle: 'none',
  // http://docs.mathjax.org/en/latest/config-files.html#the-tex-ams-svg-configuration-file
  jax: ["input/TeX","output/SVG", "output/PreviewHTML"],
  extensions: ["tex2jax.js","MathMenu.js","MathZoom.js", "fast-preview.js", "AssistiveMML.js", "a11y/accessibility-menu.js"],
  TeX: { extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"] }
});</script><script type="text/javascript" src="../MathJax-2.7.5/MathJax.js"></script><script type="text/javascript" src="../arbital-demo-bundle.js"></script><script type="text/javascript">window.addEventListener('DOMContentLoaded', e=>window.loadAllDemos())
</script></head><body><header><h1 class="title">Meta-rules for (narrow) value learning are still unsolved</h1><div class="page-info"><p class="metadata-link"><a href="../metadata/meta_unsolved.json.html">meta_unsolved.json</a></p><p class="arbital-url"><a href="https://arbital.com/p/meta_unsolved">https://arbital.com/p/meta_unsolved</a></p><p class="creator">by
 <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a> Feb 21 2017 
updated
 Feb 22 2017</p></div><p class="clickbait">We don't currently know a simple meta-utility function that would take in observation of humans and spit out our true values, or even a good target for a Task AGI.</p><nav class="breadcrumbs"><nav class="breadcrumb" aria-label="Breadcrumb"><ul><li><a href="../index.html">Index</a></li><li>Meta-rules for (narrow) value learning are still unsolved</li></ul></nav><nav class="breadcrumb" aria-label="Breadcrumb"><ul><li><a href="ai_alignment.html">AI alignment</a></li><li><a href="complexity_of_value.html">Complexity of value</a></li><li>â€¦</li></ul></nav></nav></header><hr><main><p>[summary:  This proposition is true if nobody has yet proposed a satisfactory algorithm that takes as input a material description of the universe, and/or channel of sensory observation, and spits out <a href="value_alignment_value.html">ideal values</a> or a <a href="task_identification.html">task identification</a>.</p>
<p>In principle, there can be a simple <em>meta-level program</em> that would operate to <a href="value_identification.html">identify a goal</a> given the right complex inputs, even though <a href="complexity_of_value.html">the object-level goal has high algorithmic complexity</a>.  However, nobody has proposed realistic pseudocode for a realistic algorithm that takes in a full description of the material universe including humans, or a sensory channel currently controlled by fragile and unreliable humans, and spits out a decision function for any kind of goal we could realistically intend.  There are arguably fundamental reasons why this is hard.]</p>
<h1 id="definition">Definition</h1>
<p>This proposition is true according to you if you believe that:  "Nobody has yet proposed a satisfactory fixed/simple algorithm that takes as input a material description of the universe, and/or channels of sensory observation, and spits out <a href="value_alignment_value.html">ideal values</a> or a <a href="task_identification.html">task identification</a>."</p>
<h1 id="arguments">Arguments</h1>
<p>The <a href="complexity_of_value.html">Complexity of value</a> thesis says that, on the object-level, any specification of <a href="value_cosmopolitan.html">what we&#39;d really want from the future</a> has high <a href="Kolmogorov_complexity.html">Algorithmic complexity</a>.</p>
<p>In <em>some</em> sense, all the complexity required to specify value must be contained inside human brains; even as an object of conversation, we can't talk about anything our brains do not point to.  This is why <a href="complexity_of_value.html">Complexity of value</a> distinguishes the object-level complexity of value from <em>meta</em>-level complexity--the minimum program required to get a <a href="sufficiently_advanced_ai.html">Sufficiently advanced Artificial Intelligence</a> to learn values.  It would be a separate question to consider the minimum complexity of a function that takes as input a full description of the material universe including humans, and outputs "<a href="value_alignment_value.html">value</a>".</p>
<p>This question also has a  <a href="ambitious_vs_narrow_value_learning.html">narrow rather than ambitious form</a>: given sensory observations an AGI could reasonably receive in cooperation with its programmers, or a predictive model of humans that AGI could reasonably form and refine, is there a simple rule that will take this data as input, and safely and reliably <a href="task_identification.html">identify</a> Tasks on the order of "develop molecular nanotechnology, use the nanotechnology to synthesize one strawberry, and then stop, with a minimum of side effects"?</p>
<p>In this case we have no strong reason to think that the functions are high-complexity in an absolute sense.</p>
<p>However, nobody has yet proposed a satisfactory piece of pseudocode that solves any variant of this problem even in principle.</p>
<h2 id="obstaclestosimplemetarules">Obstacles to simple meta-rules</h2>
<p>Consider a simple <a href="meta_utility.html">Meta-utility function</a> that specifies a sense-input-dependent formulation of <a href="moral_uncertainty.html">Moral uncertainty</a>:  An object-level outcome $~$o$~$ has a utility $~$U(o)$~$ that is $~$U_1(o)$~$ if a future sense signal $~$s$~$ is 1 and $~$U_2(o)$~$ if $~$s$~$ is 2.  Given this setup, the AI has an incentive to tamper with $~$s$~$ and cause it to be 1 if $~$U_1$~$ is easier to optimize than $~$U_2,$~$ and vice versa.</p>
<p>More generally, sensory signals from humans will usually not be <em>reliably</em> and <em>unalterably</em> correlated with our <a href="intended_goal.html">intended</a> goal identification.  We can't treat human-generated signals as an [no_ground_truth ideally reliable ground truth] about any referent, because (a) <a href="environmental_goals.html">some AI actions interfere with the signal</a>; and (b) humans make mistakes, especially when you ask them something complicated.  You can't have a scheme along the lines of "the humans press a button if something goes wrong", because some policies go wrong in ways humans don't notice until it's too late, and some AI policies destroy the button (or modify the human).</p>
<p>Even leaving that aside, nobody has yet suggested any fully specified pseudocode that takes in a human-controlled sensory channel $~$R$~$ and a description of the universe $~$O$~$ and spits out a utility function that (actually realistically) identifies our <a href="intended_goal.html">intended</a> task over $~$O$~$ (including <em>not</em> tiling the universe with subagents and so on).</p>
<p>Indeed, nobody has yet suggested a realistic scheme for identifying any kind of goal whatsoever <a href="ontology_identification.html">with respect to an AI ontology flexible enough</a> to actually describe the material universe. %note: Except in the rather non-meta sense of inspecting the AI's ontology once it's advanced enough to describe what you think you want the AI to do, and manually programming the AI's consequentialist preferences with respect to what you think that ontology means.%</p>
<h2 id="metametarules">Meta-meta rules</h2>
<p>For similar reasons as above, nobody has yet proposed (even in principle) effective pseudocode for a <em>meta-meta program</em> over some space of meta-rules, which would let the AI <em>learn</em> a value-identifying meta-rule.  Two main problems here are:</p>
<p>One, nobody even has the seed of any proposal whatsoever for how that could, work short of "define a correctness-signaling channel and throw program induction at it" (which seems unlikely to work directly, given [no_ground_truth fallible, fragile humans controlling the signal]).</p>
<p>Two, if the learned meta-rule doesn't have a stable, extremely compact human-transparent representation, it's not clear how we could arrive at any confidence whatsoever <a href="context_disaster.html">that good behavior in a development phase would correspond to good behavior in a test phase</a>.  E.g., consider all the example meta-rules we could imagine which would work well on a small scale but fail to scale, like "something good just happened if the humans smiled".</p></main><hr><footer><p class="tagged"><h2>Tagged</h2><span class="page-comma-list"><a class="page-link" href="../page/meta_utility.html">Meta-utility function</a>,
 <a class="page-link" href="../page/start_meta_tag.html">Start</a></span></p><p class="reverse-related"><h2>Reverse Related</h2><ul class="page-list"><li><a class="page-link" href="../page/meta_utility.html">Meta-utility function</a> <q>Preference frameworks built out of simple utility functions, but where, e.g., the 'correct' utility function for a possible world depends on whether a button is pressed.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/start_meta_tag.html">Start</a> <q>This page gives a basic overview of the topic, but may be missing important information or have stylistic issues. If you're able to, please help expand or improve it!</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li></ul></p></footer></body></html>