<!DOCTYPE html><html><head><meta charset="utf-8"><title>Cognitive uncontainability</title><link rel="stylesheet" type="text/css" href="../common.css"><link rel="stylesheet" type="text/css" href="../page-style.css"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  SVG: {EqnChunk: 50, EqnChunkFactor: 1.5, EqChunkDelay: 10, useFontCache: false, linebreaks: {automatic: true}},
  tex2jax: {
    inlineMath: [['$~$', '$~$']],
    displayMath: [['$$~$', '$~$$']],
    processEscapes: true,
    preview: 'none',
  },
  showProcessingMessages: false,
  messageStyle: 'none',
  // http://docs.mathjax.org/en/latest/config-files.html#the-tex-ams-svg-configuration-file
  jax: ["input/TeX","output/SVG", "output/PreviewHTML"],
  extensions: ["tex2jax.js","MathMenu.js","MathZoom.js", "fast-preview.js", "AssistiveMML.js", "a11y/accessibility-menu.js"],
  TeX: { extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"] }
});</script><script type="text/javascript" src="../MathJax-2.7.5/MathJax.js"></script><script type="text/javascript" src="../arbital-demo-bundle.js"></script><script type="text/javascript">window.addEventListener('DOMContentLoaded', e=>window.loadAllDemos())
</script></head><body><header><h1 class="title">Cognitive uncontainability</h1><div class="page-info"><p class="metadata-link"><a href="../metadata/uncontainability.json.html">uncontainability.json</a></p><p class="arbital-url"><a href="https://arbital.com/p/uncontainability">https://arbital.com/p/uncontainability</a></p><p class="creator">by
 <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a> Jul 1 2015 
updated
 Dec 16 2015</p></div><p class="clickbait">'Cognitive uncontainability' is when we can't hold all of an agent's possibilities inside our own minds.</p><nav class="breadcrumbs"><nav class="breadcrumb" aria-label="Breadcrumb"><ul><li><a href="../index.html">Index</a></li><li>Cognitive uncontainability</li></ul></nav><nav class="breadcrumb" aria-label="Breadcrumb"><ul><li><a href="ai_alignment.html">AI alignment</a></li><li><a href="advanced_agent_theory.html">Theory of (advanced) agents</a></li><li><a href="advanced_agent.html">Advanced agent properties</a></li><li>…</li></ul></nav></nav></header><hr><main><p>[summary:  An <a href="relevant_powerful_agent.html">agent</a> is cognitively uncontainable in a [ domain], relative to us, when we can't hold all of the agent's possible strategies inside our own minds, and we can't make sharp predictions about what it can and can't do.  For example, the most powerful modern chess programs, playing a human novice, would be cognitively uncontainable on the chess board (the novice can't imagine everything the chess program might do), but easily cognitively containable in the context of the larger world (the novice knows the chess program won't suddenly reach out and upset the board).  One of the most critical <a href="advanced_agent.html">advanced agent properties</a> is if an agent is cognitively uncontainable in the real world.]</p>
<p><a href="Vingean_uncertainty.html">Vingean unpredictability</a> is when an agent is cognitively uncontainable because it is smarter than us: if you could predict in advance exactly where [ Deep Blue] would move, you could play chess at least as well as Deep Blue yourself by doing whatever you predicted Deep Blue would do in your shoes.</p>
<p>Although Vingean unpredictability is the classic way in which cognitive uncontainability can arise, other possibilities are <a href="conceivability.html">imaginable</a>.  For instance, the AI could be operating in a <a href="rich_domain.html">rich domain</a> and searching a different part of the search space that humans have difficulty handling, while still being dumber or less competent overall than a human.  In this case the AI's strategies might still be unpredictable to us, even while it was less effective or competent overall.  Most [ anecdotes about AI algorithms doing surprising things] can be viewed from this angle.</p>
<p>An extremely narrow, exhaustibly searchable domain may yield cognitive containability even for intelligence locally superior to a human's.  Even a perfect Tic-Tac-Toe player can only draw against a human who knows the basic strategies, because humans can also play perfect Tic-Tac-Toe.  Of course this is only true so long as the agent can't modulate some transistors to form a wireless radio, escape onto the Internet, and offer a nearby bystander twenty thousand dollars to punch the human in the face - in which case the agent's strategic options would have included, in retrospect, things that affected the real world; and the real world is a much more complicated domain than Tic-Tac-Toe.  There's some sense in which <a href="rich_domain.html">richer domains</a> seem likely to feed into increased cognitive uncontainability, but it's worth remembering that every game and every computer is embedded into the extremely complicated real world.</p>
<p><a href="strong_uncontainability.html">Strong cognitive uncontainability</a> is when the agent knows some facts we don't, that it can use to formulate strategies that we wouldn't be able to recognize in advance as successful.  From the perspective of e.g. the 11th century C.E. trying to cool their house, bringing in cool water from the nearby river to run over some nearby surfaces might be an understandable solution; but if you showed them the sketch of an air conditioner, without running the air conditioner or explaining how it worked, they wouldn't recognize this sketch as a smart solution because they wouldn't know the further facts required to see why it would work.  When an agent can win using options that we didn't imagine, couldn't invent, and wouldn't understand even if we caught a glimpse of them in advance, it is strongly cognitively uncontainable in the same way that the 21st century is strongly uncontainable from the standpoint of the 11th century.</p></main><hr><footer><p class="all-creators"><h2>All Creators</h2><span class="page-comma-list"><a class="page-link" href="../page/AlexeiAndreev.html">Alexei Andreev</a>,
 <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></span></p><p class="likes"><h2>Likes</h2><span class="page-comma-list"><a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></span></p><p class="children"><h2>Children</h2><ul class="page-tree"><li><a class="page-link" href="../page/rich_domain.html">Rich domain</a> <q>A domain is 'rich', relative to our own intelligence, to the extent that (1) its [ search space] is …</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a><ul class="page-tree"><li><a class="page-link" href="../page/real_is_rich.html">Almost all real-world domains are rich</a> <q>Anything you're trying to accomplish in the real world can potentially be accomplished in a *lot* of different ways.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/logical_game.html">Logical game</a> <q>Game's mathematical structure at its purest form.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li></ul></li></ul></p></footer></body></html>