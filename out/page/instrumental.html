<!DOCTYPE html><html><head><meta charset="utf-8"><title>Instrumental</title><link rel="stylesheet" type="text/css" href="../common.css"><link rel="stylesheet" type="text/css" href="../page-style.css"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  SVG: {EqnChunk: 50, EqnChunkFactor: 1.5, EqChunkDelay: 10, useFontCache: false, linebreaks: {automatic: true}},
  tex2jax: {
    inlineMath: [['$~$', '$~$']],
    displayMath: [['$$~$', '$~$$']],
    processEscapes: true,
    preview: 'none',
  },
  showProcessingMessages: false,
  messageStyle: 'none',
  // http://docs.mathjax.org/en/latest/config-files.html#the-tex-ams-svg-configuration-file
  jax: ["input/TeX","output/SVG", "output/PreviewHTML"],
  extensions: ["tex2jax.js","MathMenu.js","MathZoom.js", "fast-preview.js", "AssistiveMML.js", "a11y/accessibility-menu.js"],
  TeX: { extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"] }
});</script><script type="text/javascript" src="../MathJax-2.7.5/MathJax.js"></script><script type="text/javascript" src="../arbital-demo-bundle.js"></script><script type="text/javascript">window.addEventListener('DOMContentLoaded', e=>window.loadAllDemos())
</script></head><body><header><h1 class="title">Instrumental</h1><div class="page-info"><p class="metadata-link"><a href="../metadata/instrumental.json.html">instrumental.json</a></p><p class="arbital-url"><a href="https://arbital.com/p/instrumental">https://arbital.com/p/instrumental</a></p><p class="creator">by
 <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a> Jul 16 2015 
updated
 Dec 17 2015</p></div><p class="clickbait">What is &quot;instrumental&quot; in the context of Value Alignment Theory?</p><nav class="breadcrumbs"><nav class="breadcrumb" aria-label="Breadcrumb"><ul><li><a href="../index.html">Index</a></li><li>Instrumental</li></ul></nav><nav class="breadcrumb" aria-label="Breadcrumb"><ul><li><a href="ai_alignment.html">AI alignment</a></li><li><a href="advanced_agent_theory.html">Theory of (advanced) agents</a></li><li><a href="instrumental_convergence.html">Instrumental convergence</a></li><li>â€¦</li></ul></nav></nav></header><hr><main><p>An 'instrumental strategy', 'instrumental goal', or 'subgoal' is an event E that an agent tries to bring about in order to bring about some other goal G.  If you want to drink milk, then you need to drive to the store; in order to drive to the store, you need to be inside your car; in order to be inside your car, you need to open your car door.  Thus 'be inside my car' and 'open my car door' are instrumental goals or instrumental strategies.</p>
<p>In conventional philosophy, an event is said to have "instrumental value" if it is useful for accomplishing some implied other set of goals, as distinguished from "terminal value" which is unconditional on future events.  Since in <a href="ai_alignment.html">VAT</a> we have reserved the word <a href="value_alignment_value.html">&#39;value&#39;</a>, we can't use that terminology here.</p>
<p>[todo: - Use a Bayes net formalism to explain why some formulations of expected utility make the key idea more or less transparent.]</p></main><hr><footer><p class="tagged"><h2>Tagged</h2><span class="page-comma-list"><a class="page-link" href="../page/definition_meta_tag.html">Definition</a>,
 <a class="page-link" href="../page/value_alignment_glossary.html">Glossary (Value Alignment Theory)</a>,
 <a class="page-link" href="../page/stub_meta_tag.html">Stub</a></span></p><p class="all-creators"><h2>All Creators</h2><span class="page-comma-list"><a class="page-link" href="../page/AlexeiAndreev.html">Alexei Andreev</a>,
 <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></span></p><p class="likes"><h2>Likes</h2><span class="page-comma-list"><a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></span></p><p class="reverse-related"><h2>Reverse Related</h2><ul class="page-list"><li><a class="page-link" href="../page/definition_meta_tag.html">Definition</a> <q>Meta tag used to mark pages that strictly define a particular term or phrase.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/value_alignment_glossary.html">Glossary (Value Alignment Theory)</a> <q>Words that have a special meaning in the context of creating nice AIs.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/stub_meta_tag.html">Stub</a> <q>This page only gives a very brief overview of the topic. If you're able to, please help expand or improve it!</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li></ul></p></footer></body></html>