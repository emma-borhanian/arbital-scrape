<!DOCTYPE html><html><head><meta charset="utf-8"><title>Value</title><link rel="stylesheet" type="text/css" href="../common.css"><link rel="stylesheet" type="text/css" href="../page-style.css"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  SVG: {EqnChunk: 50, EqnChunkFactor: 1.5, EqChunkDelay: 10, useFontCache: false, linebreaks: {automatic: true}},
  tex2jax: {
    inlineMath: [['$~$', '$~$']],
    displayMath: [['$$~$', '$~$$']],
    processEscapes: true,
    preview: 'none',
  },
  showProcessingMessages: false,
  messageStyle: 'none',
  // http://docs.mathjax.org/en/latest/config-files.html#the-tex-ams-svg-configuration-file
  jax: ["input/TeX","output/SVG", "output/PreviewHTML"],
  extensions: ["tex2jax.js","MathMenu.js","MathZoom.js", "fast-preview.js", "AssistiveMML.js", "a11y/accessibility-menu.js"],
  TeX: { extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"] }
});</script><script type="text/javascript" src="../MathJax-2.7.5/MathJax.js"></script><script type="text/javascript" src="../arbital-demo-bundle.js"></script><script type="text/javascript">window.addEventListener('DOMContentLoaded', e=>window.loadAllDemos())
</script></head><body><header><h1 class="title">Value</h1><div class="page-info"><p class="metadata-link"><a href="../metadata/value_alignment_value.json.html">value_alignment_value.json</a></p><p class="arbital-url"><a href="https://arbital.com/p/value_alignment_value">https://arbital.com/p/value_alignment_value</a></p><p class="creator">by
 <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a> Apr 24 2015 
updated
 Jun 1 2016</p></div><p class="clickbait">The word 'value' in the phrase 'value alignment' is a metasyntactic variable that indicates the speaker's future goals for intelligent life.</p><nav class="breadcrumbs"><nav class="breadcrumb" aria-label="Breadcrumb"><ul><li><a href="../index.html">Index</a></li><li>Value</li></ul></nav><nav class="breadcrumb" aria-label="Breadcrumb"><ul><li><a href="ai_alignment.html">AI alignment</a></li><li>â€¦</li></ul></nav></nav></header><hr><main><p>[summary:  Different people advocate different views on what we should want for the outcome of an '[value aligned]' AI (desiderata like human flourishing, or a [ fun-theoretic eudaimonia], or <a href="cev.html">coherent extrapolated volition</a>, or an AI that mostly leaves us alone but protects us from other AIs).  These differences might not be [ irreconcilable]; people are sometimes persuaded to change their views of what we should want.  Either way, there's (arguably) a tremendous overlap in the technical issues for aligning an AI with any of these goals.  So in the technical discussion, 'value' is really a metasyntactic variable that stands in for the speaker's current view, or for what an AI project might later adopt as a reasonable target after further discussion.]</p>
<h3 id="introduction">Introduction</h3>
<p>In the context of <a href="ai_alignment.html">value alignment</a> as a subject, the word 'value' is a speaker-dependent variable that indicates our ultimate goal - the property or meta-property that the speaker wants or 'should want' to see in the final outcome of Earth-originating intelligent life.  E.g: [ human flourishing], [ fun], <a href="cev.html">coherent extrapolated volition</a>, [ normativity].</p>
<p>Different viewpoints are still being debated on this topic; people [ sometimes change their minds about their views].  We don't yet have full knowledge of which views are 'reasonable' in the sense that people with good cognitive skills might retain them <a href="normative_extrapolated_volition.html">even in the limit of ongoing discussion</a>.  Some subtypes of potentially internally coherent views may not be sufficiently [ interpersonalizable] for even very small AI projects to cooperate on them; if e.g. Alice wants to own the whole world and will go on believing that in the limit of continuing contemplation, this is not a desideratum on which Alice, Bob, and Carol can all cooperate.  Thus, using 'value' as a potentially speaker-dependent variable isn't meant to imply that everyone has their own 'value' and that no further debate or cooperation is possible; people can and do talk each other out of positions which are then regarded as having been mistaken, and completely incommunicable stances seem unlikely to be reified even into a very small AI project.  But since this debate is ongoing, there is not yet any one definition of 'value' that can be regarded as settled.</p>
<p>Nonetheless, on many of the current views being advocated, it seems like very similar technical problems of value alignment seem to arise in many of them.  We would need to figure out how to <a href="value_identification.html">identify</a> the objects of value to the AI, robustly assure that the AI's preferences are <a href="reflective_stability.html">stable</a> as the AI self-modifies, or create <a href="corrigibility.html">corrigible</a> ways of recovering from errors in the way we tried to identify and specify the objects of value.</p>
<p>To centralize the very similar discussions of these technical problems while the outer debate about reasonable end goals is ongoing, the word 'value' acts as a metasyntactic placeholder for different views about the target of value alignment.</p>
<p>Similarly, in the larger <a href="value_achievement_dilemma.html">value achievement dilemma</a>, the question of what the end goals should be, and policy difficulties of getting 'good' goals to be adopted in name by the builders or creators of AI, are factored out as the [value_selection value selection problem].  The output of this process is taken to be an input into the value loading problem, and 'value' is a name referring to this output.</p>
<p>'Value' is <em>not</em> assumed to be what the AI is given as its utility function or <a href="preference_framework.html">preference framework</a>.  On many views implying that <a href="complexity_of_value.html">value is complex</a> or otherwise difficult to convey to an AI, the AI may be, e.g., a <a href="task_agi.html">Genie</a> where some stress is taken off the proposition that the AI exactly understands value and put onto human ability to use the Genie well.</p>
<p>Consider a Genie with an explicit preference framework targeted on a [ Do What I Know I Mean system] for making [ checked wishes].  The word 'value' in any discussion thereof should still only be used to refer to whatever the AI creators are targeting for real-world outcomes.  We would say the 'value alignment problem' had been successfully solved to the extent that running the Genie produced high-value outcomes in the sense of the humans' viewpoint on 'value', not to the extent that the outcome matched the Genie's preference framework for how to follow orders.</p>
<h3 id="specificviewsonvalue">Specific views on value</h3>
<p>Obviously, a listing like this will only summarize long debates.  But that summary at least lets us point to some examples of views that have been advocated, and not indefinitely defer the question of what 'value' could possibly refer to.</p>
<p>Again, keep in mind that by technical definition, 'value' is what we are using or should use to rate the ultimate real-world consequences of running the AI, <em>not</em> the explicit goals we are giving the AI.</p>
<p>Some of the major views that have been advocated by more than one person are as follows:</p>
<ul>
<li><strong>Reflective equilibrium.</strong>  We can talk about 'what I <em>should</em> want' as a concept distinct from 'what I want right now' by construing some limit of how our present desires would directionally change given more factual knowledge, time to consider more knowledge, better self-awareness, and better self-control.  Modeling this process is <strong>extrapolation</strong>, a reserved term to mean this process in the context of discussing preferences.  Value would consist in, e.g., whatever properties a supermajority of humans would agree, in the limit of reflective equilibrium, are desirable.  See also [ coherent extrapolated volition].</li>
<li><strong>Standard desires.</strong>  An object-level view that identifies value with qualities that we currently find very desirable, enjoyable, fun, and preferable, such as [ Frankena's list of desiderata] (including truth, happiness, aesthetics, love, challenge and achievement, etc.) On the closely related view of <strong>Fun Theory</strong>, such desires may be further extrapolated, without changing their essential character, into forms suitable for transhuman minds.  Advocates may agree that these object-level desires will be subject to unknown normative corrections by reflective-equilibrium-type considerations, but still believe that some form of Fun or standardly desirable outcome is a likely result.  Therefore (on this view) it is reasonable to speak of value as probably mostly consisting in turning most of the reachable universe into superintelligent life enjoying itself, creating transhuman forms of art, etcetera.</li>
<li><strong>[ImmediateGoods Immediate goods].</strong>  E.g., "Cure cancer" or "Don't transform the world into paperclips."  Such replies arguably have problems as ultimate criteria of value from a human standpoint (see linked discussion), but for obvious reasons, lists of immediate goods are a common early thought when first considering the subject.</li>
<li><strong>Deflationary moral error theory.</strong>  There is no good way to construe a normative concept apart from what particular people want.  AI programmers are just doing what they want, and confused talk of 'fairness' or 'rightness' cannot be rescued.  The speaker would nonetheless personally prefer not to be turned into paperclips.  (This mostly ends up at an 'immediate goods' theory in practice, plus some beliefs relevant to the [value_selection value selection] debate.)</li>
<li><strong>Simple purpose.</strong>  Value can easily be identified with X, for some X.  X is the main thing we should be concerned about passing on to AIs.  Seemingly desirable things besides X are either (a) improper to care about, (b) relatively unimportant, or (c) instrumentally implied by pursuing X, qua X.</li>
</ul>
<p>The following versions of desiderata for AI outcomes would tend to imply that the value alignment / value loading problem is an entirely wrong way of looking at the issue, which might make it disingenuous to claim that 'value' in 'value alignment' can cover them as a metasyntactic variable as well:</p>
<ul>
<li><strong>Moral internalist value.</strong>  The normative is inherently compelling to all, or almost all cognitively powerful agents.  Whatever is not thus compelling cannot be normative or a proper object of human desire.</li>
<li><strong>AI rights.</strong>  The primary thing is to ensure that the AI's natural and intrinsic desires are respected.  The ideal is to end up in a diverse civilization that respects the rights of all sentient beings, including AIs.  (Generally linked are the views that no special selection of AI design is required to achieve this, or that special selection of AI design to shape particular motivations would itself violate AI rights.)</li>
</ul>
<h2 id="modularityofvalue">Modularity of 'value'</h2>
<h3 id="alignablevalues">Alignable values</h3>
<p>Many issues in value alignment seem to generalize very well across the Reflective Equilibrium, Fun Theory, Intuitive Desiderata, and Deflationary Error Theory viewpoints.  In all cases we would have to consider stability of self-modification, the <a href="edge_instantiation.html">Edge Instantiation</a> problem in <a href="value_identification.html">value identification</a>, and most of the rest of 'standard' value alignment theory.  This seemingly good generalization of the resulting technical problems across such wide-ranging viewpoints, and especially that it (arguably) covers the case of intuitive desiderata, is what justifies treating 'value' as a metasyntactic variable in 'value loading problem'.</p>
<p>A neutral term for referring to all the values in this class might be 'alignable values'.</p>
<h3 id="simplepurpose">Simple purpose</h3>
<p>In the [ simple purpose] case, the key difference from an Immediate Goods scenario is that the desideratum is usually advocated to be simple enough to negate <a href="complexity_of_value.html">Complexity of Value</a> and make <a href="value_identification.html">value identification</a> easy.</p>
<p>E.g., Juergen Schmidhuber stated at the 20XX Singularity Summit that he thought the only proper and normative goal of any agent was to increase compression of sensory information [todo: find exact quote, exact Summit].  Conditioned on this being the sum of all normativity, 'value' is algorithmically simple.  Then the problems of <a href="edge_instantiation.html">Edge Instantiation</a>, <a href="unforeseen_maximum.html">Unforeseen Maximums</a>, and Nearest Unblocked Neighbor are all moot.  (Except perhaps as there is an Ontology Identification problem for defining exactly what constitutes 'sensory information' for a [ self-modifying agent].)</p>
<p>Even in the [ simple purpose] case, the [ value loading problem] would still exist (it would still be necessary to make an AI that cared about the simple purpose rather than paperclips) along with associated problems of <a href="71.html">reflective stability</a> (it would be necessary to make an AI that went on caring about X through self-modification).  Nonetheless, the overall problem difficulty and immediate technical priorities would be different enough that the Simple Purpose case seems importantly distinct from e.g. Fun Theory on a policy level.</p>
<h3 id="moralinternalism">Moral internalism</h3>
<p>Some viewpoints on 'value' deliberately reject <a href="orthogonality.html">Orthogonality</a>.  Strong versions of the [ moral internalist position in metaethics] claim as an empirical prediction that every sufficiently powerful cognitive agent will come to pursue the same end, which end is to be identified with normativity, and is the only proper object of human desire.  If true, this would imply that the entire value alignment problem is moot for advanced agents.</p>
<p>Many people who advocate 'simple purposes' also claim these purposes are universally compelling.  In a policy sense, this seems functionally similar to the Moral Internalist case regardless of the simplicity or complexity of the universally compelling value.  Hence an alleged simple universally compelling purpose is categorized for these purposes as Moral Internalist rather than Simple Purpose.</p>
<p>The special case of a Simple Purpose claimed to be universally <a href="instrumental_convergence.html">instrumentally convergent</a> also seems functionally identical to Moral Internalism from a policy standpoint.)</p>
<h3 id="airights">AI Rights</h3>
<p>Someone might believe as a proposition of fact that all (accessible) AI designs would have 'innate' desires, believe as a proposition of fact that no AI would gain enough advantage to wipe out humanity or prevent the creation of other AIs, and assert as a matter of morality that a good outcome consists of everyone being free to pursue their own value and trade.  In this case the value alignment problem is implied to be an entirely wrong way to look at the problem, with all associated technical issues moot.  Thus, it again might be disingenuous to have 'value' as a metasyntactic variable try to cover this case.</p></main><hr><section class="comments"><h2>Comments</h2><div class="comment"><p><a class="page-link" href="../page/BrandonReinhart.html">Brandon Reinhart</a></p><p><p>It may be worth commenting on the rights of computations-as-people here (<a href="some_computations_are_people.html">Some computations are people</a>). We would seek to respect the rights of AIs, but we also seek to respect the rights of the computations within the AI (and other complex systems) that are themselves sentient. This would also apply in cases of self-modification, where modified biological brains become sophisticated enough to create complex models that are also objects of ethical value.</p></p></div><div class="comment"><p><a class="page-link" href="../page/BenjyForstadt.html">Benjy Forstadt</a></p><p><p>Due partly to the choice of using 'value' as a speaker dependent variable, some of the terminology used in this article doesn't align with how the terms are used by professional metaethicists. I would strongly suggest one of:</p>
<p>1) replacing the phrase "moral internalism" with a new phrase that better individuates the concept.</p>
<p>2) including a note that the phrase is being used extremely non-standardly.</p>
<p>3) adding a section explaining the layout of metaethical possibilities, using moral internalism in the sense intended by professional metaethicists.</p>
<p>In metaethics, moral internalism, roughly, is the disjunction:</p>
<p>'Value' is speaker independent and universally compelling OR 'Value' is speaker dependent and is only used to indicate properties the speaker finds compelling</p>
<p>This seems very un-joint-carvy from a perspective of value allignment, but most philosophers see internalism as a semantic thesis that captures the relation between moral judgements  and motivation. The idea is: If someone says something has value, she values that thing. This is very very different from how the term is used in this article.</p>
<p>I can provide numerous sources to back this up, if needed.</p></p></div></section><footer><p class="tagged"><h2>Tagged</h2><span class="page-comma-list"><a class="page-link" href="../page/definition_meta_tag.html">Definition</a>,
 <a class="page-link" href="../page/value_alignment_glossary.html">Glossary (Value Alignment Theory)</a></span></p><p class="all-creators"><h2>All Creators</h2><span class="page-comma-list"><a class="page-link" href="../page/AlexeiAndreev.html">Alexei Andreev</a>,
 <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></span></p><p class="likes"><h2>Likes</h2><span class="page-comma-list"><a class="page-link" href="../page/NateSoares.html">Nate Soares</a></span></p><p class="reverse-related"><h2>Reverse Related</h2><ul class="page-list"><li><a class="page-link" href="../page/definition_meta_tag.html">Definition</a> <q>Meta tag used to mark pages that strictly define a particular term or phrase.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/value_alignment_glossary.html">Glossary (Value Alignment Theory)</a> <q>Words that have a special meaning in the context of creating nice AIs.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li></ul></p><p class="children"><h2>Children</h2><ul class="page-tree"><li><a class="page-link" href="../page/beneficial.html">'Beneficial'</a> <q>Really actually good.  A metasyntactic variable to mean &quot;favoring whatever the speaker wants ideally to accomplish&quot;, although different speakers have different morals and metaethics.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/detrimental.html">'Detrimental'</a> <q>The opposite of beneficial.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/cev.html">Coherent extrapolated volition (alignment target)</a> <q>A proposed direction for an extremely well-aligned autonomous superintelligence - do what humans would want, if we knew what the AI knew, thought that fast, and understood ourselves.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/value_cosmopolitan.html">Cosmopolitan value</a> <q>Intuitively: Value as seen from a broad, embracing standpoint that is aware of how other entities may not always be like us or easily understandable to us, yet still worthwhile.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/normative_extrapolated_volition.html">Extrapolated volition (normative moral theory)</a> <q>If someone asks you for orange juice, and you know that the refrigerator contains no orange juice, should you bring them lemonade?</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a><ul class="page-tree"><li><a class="page-link" href="../page/rescue_utility.html">Rescuing the utility function</a> <q>If your utility function values 'heat', and then you discover to your horror that there's no ontologically basic heat, switch to valuing disordered kinetic energy. Likewise 'free will' or 'people'.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li></ul></li><li><a class="page-link" href="../page/immediate_goods.html">Immediate goods</a> <q>One of the potential views on 'value' in the value alignment problem is that what we should want froâ€¦</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/frankena_goods.html">William Frankena's list of terminal values</a> <q>Life, consciousness, and activity; health and strength; pleasures and satisfactions of all or certain kinds; happiness, beatitude, contentment, etc.; truth; knowledge and true opinions...</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li></ul></p></footer></body></html>