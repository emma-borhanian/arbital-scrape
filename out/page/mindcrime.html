<!DOCTYPE html><html><head><meta charset="utf-8"><title>Mindcrime</title><link rel="stylesheet" type="text/css" href="../common.css"><link rel="stylesheet" type="text/css" href="../page-style.css"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  SVG: {EqnChunk: 50, EqnChunkFactor: 1.5, EqChunkDelay: 10, useFontCache: false, linebreaks: {automatic: true}},
  tex2jax: {
    inlineMath: [['$~$', '$~$']],
    displayMath: [['$$~$', '$~$$']],
    processEscapes: true,
    preview: 'none',
  },
  showProcessingMessages: false,
  messageStyle: 'none',
  // http://docs.mathjax.org/en/latest/config-files.html#the-tex-ams-svg-configuration-file
  jax: ["input/TeX","output/SVG", "output/PreviewHTML"],
  extensions: ["tex2jax.js","MathMenu.js","MathZoom.js", "fast-preview.js", "AssistiveMML.js", "a11y/accessibility-menu.js"],
  TeX: { extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"] }
});</script><script type="text/javascript" src="../MathJax-2.7.5/MathJax.js"></script><script type="text/javascript" src="../arbital-demo-bundle.js"></script><script type="text/javascript">window.addEventListener('DOMContentLoaded', e=>window.loadAllDemos())
</script></head><body><header><h1 class="title">Mindcrime</h1><div class="page-info"><p class="metadata-link"><a href="../metadata/mindcrime.json.html">mindcrime.json</a></p><p class="arbital-url"><a href="https://arbital.com/p/mindcrime">https://arbital.com/p/mindcrime</a></p><p class="creator">by
 <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a> Jun 9 2015 
updated
 Dec 29 2016</p></div><p class="clickbait">Might a machine intelligence contain vast numbers of unhappy conscious subprocesses?</p><nav class="breadcrumbs"><nav class="breadcrumb" aria-label="Breadcrumb"><ul><li><a href="../index.html">Index</a></li><li>Mindcrime</li></ul></nav><nav class="breadcrumb" aria-label="Breadcrumb"><ul><li><a href="ai_alignment.html">AI alignment</a></li><li>…</li></ul></nav></nav></header><hr><main><p>[summary(Gloss):  A huge amount of harm could occur if a <a href="advanced_agent.html">machine intelligence</a> turns out to contain lots of <a href="some_computations_are_people.html">conscious subprograms</a> enduring poor living conditions.  One worry is that this might happen if an AI models humans in too much detail.]</p>
<p>[summary:  'Mindcrime' is <a href="NickBostrom.html">Nick Bostrom</a>'s suggested term for the moral catastrophe that occurs if a <a href="advanced_agent.html">machine intelligence</a> contains enormous numbers of <a href="some_computations_are_people.html">conscious beings trapped inside its code</a>.</p>
<p>This could happen as a result of self-awareness being a natural property of computationally efficient subprocesses.  Perhaps more worryingly, the best model of a person may be a person itself, even if they're not the same person.  This means that AIs trying to model humans might be unusually likely to create hypotheses and simulations that are themselves conscious.]</p>
<p>[summary(Technical):  'Mindcrime' is <a href="NickBostrom.html">Nick Bostrom</a>'s term for mind designs producing moral harm by their internal operation, particularly through embedding sentient subprocesses.</p>
<p>One worry is that mindcrime might arise in the course of an agent trying to predict or manipulate the humans in its environment, since this implies a pressure to model the humans in faithful detail.  This is especially concerning since several value alignment proposals would explicitly call for modeling humans in detail, e.g. <a href="cev.html">extrapolated volition</a> and <a href="imitation_agent.html">imitation-based agents</a>.</p>
<p>Another problem scenario is if the natural design for an efficient subprocess involves independent consciousness (though it is a separate question if this optimal design involves pain or suffering).</p>
<p>Computationally powerful agents might contain vast numbers of trapped conscious subprocesses, qualifying this as a [ global catastrophic risk].]</p>
<p>"Mindcrime" is <a href="NickBostrom.html">Nick Bostrom</a>'s suggested term for scenarios in which an AI's cognitive processes are intrinsically doing moral harm, for example because the AI contains trillions of suffering conscious beings inside it.</p>
<p>Ways in which this might happen:</p>
<ul>
<li>Problem of sapient models (of humans):  Occurs naturally if the best predictive model for humans in the environment involves models that are detailed enough to be people themselves.</li>
<li>Problem of sapient models (of civilizations):  Occurs naturally if the agent tries to simulate, e.g., alien civilizations that might be simulating it, in enough detail to include conscious simulations of the aliens.</li>
<li>Problem of sapient subsystems:  Occurs naturally if the most efficient design for some cognitive subsystems involves creating subagents that are self-reflective, or have some other property leading to consciousness or personhood.</li>
<li>Problem of sapient self-models:  If the AI is conscious or possible future versions of the AI are conscious, it might run and terminate a large number of conscious-self models in the course of considering possible self-modifications.</li>
</ul>
<h1 id="problemofsapientmodelsofhumans">Problem of sapient models (of humans):</h1>
<p>An <a href="instrumental_pressure.html">instrumental pressure</a> to produce high-fidelity predictions of human beings (or to predict [ decision counterfactuals] about them, or to [ search] for events that lead to particular consequences, etcetera) may lead the AI to run computations that are unusually likely to possess personhood.</p>
<p>An <a href="unbounded_analysis.html">unrealistic</a> example of this would be <a href="solomonoff_induction.html">Solomonoff induction</a>, where predictions are made by means that include running many possible simulations of the environment and seeing which ones best correspond to reality.  Among current machine learning algorithms, particle filters and Monte Carlo algorithms similarly involve running many possible simulated versions of a system.</p>
<p>It's possible that a sufficiently advanced AI to have successfully arrived at detailed models of human intelligence, would usually also be advanced enough that it never tried to use a predictable/searchable model that engaged in brute-force simulations of those models.  (Consider, e.g., that there will usually be many possible settings of a variable inside a model, and an efficient model might manipulate data representing a probability distribution over those settings, rather than ever considering one exact, specific human in toto.)</p>
<p>This, however, doesn't make it certain that no mindcrime will occur.  It may not take exact, faithful simulation of specific humans to create a conscious model.  An efficient model of a (spread of possibilities for a) human may still contain <em>enough</em> computations that resemble a person <em>enough</em> to create consciousness, or whatever other properties may be deserving of personhood.  Consider, in particular, an agent trying to use </p>
<p>Just as it almost certainly isn't necessary to go all the way down to the neural level to create a sapient being, it may be that even with some parts of a mind considered abstractly, the remainder would be computed in enough detail to imply consciousness, sapience, personhood, etcetera.</p>
<p>The problem of sapient models is not to be confused with [ Simulation Hypothesis] issues.  An efficient model of a human need not have subjective experience indistinguishable from that of the human (although it will be a model <em>of</em> a person who doesn't believe themselves to be a model).  The problem occurs if the model <em>is a person</em>, not if the model is <em>the same person</em> as its subject, and the latter possibility plays no role in the implication of moral harm.</p>
<p>Besides problems that are directly or obviously about modeling people, many other practical problems and questions can benefit from modeling other minds - e.g., reading the directions on a toaster oven in order to discern the intent of the mind that was trying to communicate how to use a toaster.  Thus, mindcrime might result from a sufficiently powerful AI trying to solve very mundane problems.</p>
<h1 id="problemofsapientmodelsofcivilizations">Problem of sapient models (of civilizations)</h1>
<p>A separate route to mindcrime comes from an advanced agent considering, in sufficient detail, the possible origins and futures of intelligent life on other worlds.  (Imagine that you were suddenly told that this version of you was actually embedded in a superintelligence that was imagining how life might evolve on a place like Earth, and that your subprocess was not producing sufficiently valuable information and was about to be shut down.  You would probably be annoyed!  We should try not to annoy other people in this way.)</p>
<p>Three possible origins of a <a href="instrumental_convergence.html">convergent instrumental pressure</a> to consider intelligent civilizations in great detail:</p>
<ul>
<li>Assigning sufficient probability to the existence of non-obvious extraterrestrial intelligences in Earth's vicinity, perhaps due to considering the [ Fermi Paradox].</li>
<li>[ Naturalistic induction], combined with the AI considering the hypothesis that it is in a simulated environment.</li>
<li>[ Logical decision theories] and utility functions that care about the consequences of the AI's decisions via instances of the AI's reference class that could be embedded inside alien simulations.</li>
</ul>
<p>With respect to the latter two possibilities, note that the AI does not need to be considering possibilities in which the whole Earth as we know it is a simulation.  The AI only needs to consider that, among the possible explanations of the AI's current sense data and internal data, there are scenarios in which the AI is embedded in some world other than the most 'obvious' one implied by the sense data.  See also <a href="probable_environment_hacking.html">Distant superintelligences can coerce the most probable environment of your AI</a> for a related hazard of the AI considering possibilities in which it is being simulated.</p>
<p>(<a href="EliezerYudkowsky.html">Eliezer Yudkowsky</a> has advocated that we shouldn't let any AI short of <em>extreme</em> levels of safety and robustness assurance consider distant civilizations in lots of detail in any case, since this means our AI might embed (a model of) a hostile superintelligence.)</p>
<h1 id="problemofsapientsubsystems">Problem of sapient subsystems:</h1>
<p>It's possible that the most efficient system for, say, allocating memory on a local cluster, constitutes a complete reflective agent with a self-model.  Or that some of the most efficient designs for subprocesses of an AI, in general, happen to have whatever properties lead up to consciousness or whatever other properties are important to personhood.</p>
<p>This might possibly constitute a relatively less severe moral catastrophe, if the subsystems are sentient but [ lack a reinforcement-based pleasure/pain architecture] (since the latter is not obviously a property of the most efficient subagents).  In this case, there might be large numbers of conscious beings embedded inside the AI and occasionally dying as they are replaced, but they would not be suffering.  It is nonetheless the sort of scenario that many of us would prefer to avoid.</p>
<h1 id="problemofsapientselfmodels">Problem of sapient self-models:</h1>
<p>The AI's models of <em>itself</em>, or of other AIs it could possibly build, might happen to be conscious or have other properties deserving of personhood.  This is worth considering as a separate possibility from building a conscious or personhood-deserving AI ourselves, when [ we didn't mean to do so], because of these two additional properties:</p>
<ul>
<li>Even if the AI's current design is not conscious or personhood-deserving, the current AI might consider possible future versions or subagent designs that would be conscious, and those considerations might themselves be conscious.</li>
<li>This means that even if the AI's current version doesn't seem like it has key personhood properties on its own - that we've successfully created the AI itself as a nonperson - we still need to worry about other conscious AIs being embedded into it.</li>
<li>The AI might create, run, and terminate very large numbers of potential self-models.</li>
<li>Even if we consider tolerable the potential moral harm of creating <em>one</em> conscious AI (e.g. the AI lacks all of the conditions that a responsible parent would want to ensure when creating a new intelligent species, but it's just one sapient being so it's okay to do that in order to save the world), we might not want to take on the moral harm of creating <em>trillions</em> of evanescent, swiftly erased conscious beings.</li>
</ul>
<h1 id="difficulties">Difficulties</h1>
<p>Trying to consider these issues is complicated by:</p>
<ul>
<li>[ Philosophical uncertainty] about what properties are constitutive of consciousness and which computer programs have them;</li>
<li>[ Moral uncertainty] about what ([ idealized] versions of) (any particular person's) morality would consider to be the key properties of personhood;</li>
<li>Our present-day uncertainty about what efficient models in advanced agents would look like.</li>
</ul>
<p>It'd help if we knew the answers to these questions, but the fact that we don't know doesn't mean we can thereby conclude that any particular model is not a person.  (This would be some mix of [ argumentum ad ignorantiem], and [ availability bias] making us think that a scenario is unlikely when it is hard to visualize.)  In the limit of infinite computing power, the epistemically best models of humans would almost certainly involve simulating many possible versions of them; superintelligences would have [ very large amounts of computing power] and we don't know at what point we come close enough to this [ limiting property] to cross the threshold.</p>
<h2 id="scopeofpotentialdisaster">Scope of potential disaster</h2>
<p>The prospect of mindcrime is an especially alarming possibility because sufficiently advanced agents, <em>especially</em> if they are using computationally efficient models, might consider <em>very large numbers</em> of hypothetical possibilities that would count as people.  There's no limit that says that if there are seven billion people, an agent will run at most seven billion models; the agent might be considering many possibilities per individual human.  This would not be an [ astronomical disaster] since it would not (by hypothesis) wipe out our posterity and our intergalactic future, but it could be a disaster orders of magnitude larger than the Holocaust, the Mongol Conquest, the Middle Ages, or all human tragedy to date.</p>
<h2 id="developmentorderissue">Development-order issue</h2>
<p>If we ask an AI to predict what we would say if we had a thousand years to think about the problem of defining personhood or think about which causal processes are 'conscious', this seems unusually likely to cause the AI to commit mindcrime in the course of answering the question.  Even asking the AI to think abstractly about the problem of consciousness, or predict by abstract reasoning what humans might say about it, seems unusually likely to result in mindcrime.  There thus exists a [ development order issue] preventing us from asking a Friendly AI to solve the problem for us, since to file this request safely and without committing mindcrime, we would need the request to already have been completed.</p>
<p>The prospect of enormous-scale disaster mitigates against 'temporarily' tolerating mindcrime inside a system, while, e.g., an [ extrapolated-volition] or [ approval-based] agent tries to compute the code or design of a non-mindcriminal agent.  Depending on the agent's efficiency, and secondarily on its computational limits, a tremendous amount of moral harm might be done during the 'temporary' process of computing an answer.</p>
<h2 id="weirdness">Weirdness</h2>
<p>Literally nobody outside of MIRI or FHI ever talks about this problem.</p>
<h1 id="nonpersonpredicates">Nonperson predicates</h1>
<p>A <a href="nonperson_predicate.html">nonperson predicate</a> is an [ effective] test that we, or an AI, can use to determine that some computer program is definitely <em>not</em> a person.  In principle, a nonperson predicate needs only two possible outputs, "Don't know" and "Definitely not a person".  It's acceptable for many actually-nonperson programs to be labeled "don't know", so long as no people are labeled "definitely not a person".</p>
<p>If the above was the only requirement, one simple nonperson predicate would be to label everything "don't know".  The implicit difficulty is that the nonperson predicate must also pass some programs of high complexity that do things like "acceptably model humans" or "acceptably model future versions of the AI".</p>
<p>Besides addressing mindcrime scenarios, Yudkowsky's <a href="http://lesswrong.com/lw/x4/nonperson_predicates/">original proposal</a> was also aimed at knowing that the AI design itself was not conscious, or not a person.</p>
<p>It seems likely to be very hard to find a good nonperson predicate:</p>
<ul>
<li>Not all philosophical confusions and computational difficulties are averted by asking for a partial list of unconscious programs instead of a total list of conscious programs.  Even if we don't know which properties are sufficient, we'd need to know something solid about properties that are necessary for consciousness or sufficient for nonpersonhood.</li>
<li>We can't pass once-and-for-all any class of programs that's Turing-complete. We can't say once and for all that it's safe to model gravitational interactions in a solar system, if enormous gravitational systems could encode computers that encode people.</li>
<li>The <a href="nearest_unblocked.html">Nearest unblocked strategy</a> problem seems particularly worrisome here.  If we block off some options for modeling humans directly, the <em>next best</em> option is unusually likely to be conscious.  Even if we rely on a whitelist rather than a blacklist, this may lead to a whitelisted "gravitational model" that secretly encodes a human, and so on.</li>
</ul>
<h1 id="researchavenues">Research avenues</h1>
<ul>
<li><p><a href="behaviorist.html">Behaviorism</a>:  Try to create a <a href="limited_agi.html">limited AI</a> that does not model other minds or possibly even itself, except using some narrow class of agent models that we are pretty sure will not be sentient.  This avenue is potentially motivated for other reasons as well, such as avoiding <a href="probable_environment_hacking.html">probable environment hacking</a> and averting [ programmer manipulation].</p></li>
<li><p>Try to define a nonperson predicate that whitelists enough programs to carry out some <a href="pivotal.html">pivotal achievement</a>.</p></li>
<li><p>Try for an AI that can bootstrap our understanding of consciousness and tell us about what we would define as a person, while committing a relatively small amount of mindcrime, with all computed possible-people being stored rather than discarded, and the modeled agents being entirely happy, mostly happy, or non-suffering.  E.g., put a happy person at the center of the approval-directed agent, and try to oversee the AI's algorithms and ask it not to use Monte Carlo simulations if possible.</p></li>
<li><p>Ignore the problem in all pre-interstellar stages because it's still relatively small compared to astronomical stakes and therefore not worth significant losses in success probability.  (This may [ backfire] under some versions of the Simulation Hypothesis.)</p></li>
<li><p>Try to <a href="executable_philosophy.html">finish</a> the philosophical problem of understanding which causal processes experience sapience (or are otherwise objects of ethical value), in the next couple of decades, to sufficient detail that it can be crisply stated to an AI, with sufficiently complete coverage that it's not subject to the <a href="nearest_unblocked.html">Nearest unblocked strategy</a> problem.</p></li>
</ul></main><hr><section class="comments"><h2>Comments</h2><div class="comment"><p><a class="page-link" href="../page/DavidKrueger2.html">David Krueger</a></p><p><p>"Weirdness: Literally nobody outside of MIRI or FHI ever talks about this problem"</p>
<p>…but it does seem to be a popular topic of contemporary SciFi (WestWorld, Black Mirror, etc.)</p></p></div><div class="comment"><p><a class="page-link" href="../page/AlexeiAndreev.html">Alexei Andreev</a></p><p><p>I have an intuition that says that if you run any sufficiently large computation (even if it's as simple as multiplication, e.g. (3^^^3)*(3^^^3), you'll likely accidentally create sentient life within it. Checking for that seems prohibitively expensive, or may be even impossible, since checking itself might run into the same problem.</p></p></div><div class="comment"><p><a class="page-link" href="../page/RyanCarey2.html">Ryan Carey</a></p><p><blockquote class="comment-context">Literally nobody outside of MIRI or FHI ever talks about this problem\.</blockquote>
<p>This is discussed under some name or other, by at least the utilitarians and by Paul Christiano.</p></p></div><div class="comment"><p><a class="page-link" href="../page/BogdanButnaru.html">Bogdan Butnaru</a></p><p><blockquote>
  <p>In principle, a nonperson predicate needs only two possible outputs, "Don't know" and "Definitely not a person". It's acceptable for many actually-nonperson programs to be labeled "don't know", so long as no people are labeled "definitely not a person". […] The implicit difficulty is that the nonperson predicate must also pass some programs of high complexity that do things like "acceptably model humans" or "acceptably model future versions of the AI".</p>
</blockquote>
<p>There's another difficulty: the nonperson predicate must not itself commit mindcrime while evaluating the programs. This sounds obvious enough in retrospect that it doesn't feel worth mentioning, but it took me a while to notice it.</p>
<p>Obviously, if you're running the program to determine if it's a person by analyzing its behavior (e.g. by asking it if it feels like it's conscious), you already commited mindcrime by the time you return "Don't know".</p>
<p>But if the tested program and the predicate are complex enough, lots of analysis other than straight running the program could accidentally instantiate persons as sub-processes, potentially ones distinct from those that might be instantiated by the tested program itself.</p>
<p>In other words: Assume Π is the set of all programs that potentially contain a person, i.e. for any program π, π in Π iff running π could instantiate a person.</p>
<p>We want a computable safety predicate S such that {S(π): π is a program} implies π ∉ Π, i.e. S(π) means π is safe. (Though !S(π) does <em>not</em> necessarily imply π ∈ Π.)</p>
<p>The problem is that S(π) is also a program, and we need to make sure that S(π) ∉ Π before running it. We can't use S(S(π)) to check, because we'd need to check first that S(S(π)) ∉ Π…</p>
<p>(Note that a program that implements a sufficietly complex safety predicate S, when executed with another program π as input, might instantiate a person even if just running π directly would not!)</p></p></div><div class="comment"><p><a class="page-link" href="../page/NathanFish.html">Nathan Fish</a></p><p><blockquote class="comment-context">This, however, doesn't make it certain that no mindcrime will occur\.  It may not take exact, faithful simulation of specific humans to create a conscious model\.  An efficient model of a \(spread of possibilities for a\) human may still contain enough computations that resemble a person enough to create consciousness, or whatever other properties may be deserving of personhood\.  Consider, in particular, an agent trying to<mark> use </mark></blockquote>
<p>Trying to use what?</p></p></div><div class="comment"><p><a class="page-link" href="../page/PhilGoetz.html">Phil Goetz</a></p><p><p>Eliezer goes back and forth between "sapient" and "sentient", which are not synonyms.  Neither is obviously a justification for claiming moral status as an agent.</p>
<p>It is important either to state clearly <em>what</em> one presumes gives an agent moral status (and hence what constitutes mindcrime), or to change each occurence of "sapient", "sentient", or "personhood" to all use the same word.  I recommend stating the general case using personhood(X), a function to be supplied by the user and not defined here.  Addressing the problem depends critically on what that function is--but the statement of the general case shouldn't be bound up with the choice of personhood predicate.</p>
<p>Choosing either "sapient" or "sentient" is problematic: "sentient" because it includes at least all mammals, and "sapient" because it really just means "intelligent", and the AI is going to be equally intelligent (defined as problem-solving or optimizing ability) whether it simulates <em>humans</em> or not.  If intelligence grants moral standing (as it seems to here), and mindcrime means trapping an agent with moral standing in the AI's world, then the construction of any AI is inherently mindcrime.</p></p></div></section><footer><p class="tagged"><h2>Tagged</h2><span class="page-comma-list"><a class="page-link" href="../page/nearest_unblocked.html">Nearest unblocked strategy</a></span></p><p class="all-creators"><h2>All Creators</h2><span class="page-comma-list"><a class="page-link" href="../page/AlexeiAndreev.html">Alexei Andreev</a>,
 <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a>,
 <a class="page-link" href="../page/JeremyPerret.html">Jeremy Perret</a>,
 <a class="page-link" href="../page/NateSoares.html">Nate Soares</a></span></p><p class="likes"><h2>Likes</h2><span class="page-comma-list"><a class="page-link" href="../page/AlexeiAndreev.html">Alexei Andreev</a>,
 <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a>,
 <a class="page-link" href="../page/MarianAndrecki.html">Marian Andrecki</a></span></p><p class="related"><h2>Related</h2><ul class="page-list"><li><a class="page-link" href="../page/behaviorist.html">Behaviorist genie</a> <q>An advanced agent that's forbidden to model minds in too much detail.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li></ul></p><p class="reverse-related"><h2>Reverse Related</h2><ul class="page-list"><li><a class="page-link" href="../page/nearest_unblocked.html">Nearest unblocked strategy</a> <q>If you patch an agent's preference framework to avoid an undesirable solution, what can you expect to happen?</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li></ul></p><p class="children"><h2>Children</h2><ul class="page-tree"><li><a class="page-link" href="../page/mindcrime_introduction.html">Mindcrime: Introduction</a> <q>The more predictive accuracy we want from a model, the more detailed the model becomes.  A very roug…</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/nonperson_predicate.html">Nonperson predicate</a> <q>If we knew which computations were definitely not people, we could tell AIs which programs they were definitely allowed to compute.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li></ul></p></footer></body></html>