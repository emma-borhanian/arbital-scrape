<!DOCTYPE html><html><head><meta charset="utf-8"><title>Nick Bostrom</title><link rel="stylesheet" type="text/css" href="../common.css"><link rel="stylesheet" type="text/css" href="../page-style.css"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  SVG: {EqnChunk: 50, EqnChunkFactor: 1.5, EqChunkDelay: 10, useFontCache: false, linebreaks: {automatic: true}},
  tex2jax: {
    inlineMath: [['$~$', '$~$']],
    displayMath: [['$$~$', '$~$$']],
    processEscapes: true,
    preview: 'none',
  },
  showProcessingMessages: false,
  messageStyle: 'none',
  // http://docs.mathjax.org/en/latest/config-files.html#the-tex-ams-svg-configuration-file
  jax: ["input/TeX","output/SVG", "output/PreviewHTML"],
  extensions: ["tex2jax.js","MathMenu.js","MathZoom.js", "fast-preview.js", "AssistiveMML.js", "a11y/accessibility-menu.js"],
  TeX: { extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"] }
});</script><script type="text/javascript" src="../MathJax-2.7.5/MathJax.js"></script><script type="text/javascript" src="../arbital-demo-bundle.js"></script><script type="text/javascript">window.addEventListener('DOMContentLoaded', e=>window.loadAllDemos())
</script></head><body><header><h1 class="title">Nick Bostrom</h1><div class="page-info"><p class="metadata-link"><a href="../metadata/NickBostrom.json.html">NickBostrom.json</a></p><p class="arbital-url"><a href="https://arbital.com/p/NickBostrom">https://arbital.com/p/NickBostrom</a></p><p class="creator">by
 <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a> Dec 1 2015 
updated
 Dec 1 2015</p></div><p class="clickbait">Nick Bostrom, secretly the inventor of Friendly AI</p><nav class="breadcrumbs"><nav class="breadcrumb" aria-label="Breadcrumb"><ul><li><a href="../index.html">Index</a></li><li>Nick Bostrom</li></ul></nav><nav class="breadcrumb" aria-label="Breadcrumb"><ul><li><a href="ai_alignment.html">AI alignment</a></li><li><a href="value_alignment_researchers.html">Researchers in value alignment theory</a></li><li>…</li></ul></nav><nav class="breadcrumb" aria-label="Breadcrumb"><ul><li><a href="people.html">People</a></li><li>…</li></ul></nav></nav></header><hr><main><p>Nick Bostrom is, with <a href="EliezerYudkowsky.html">Eliezer Yudkowsky</a>, one of the two cofounders of the current field of <a href="ai_alignment.html">value alignment theory</a>.  Bostrom published a paper singling out the problem of superintelligent values as critical in 1999, two years before Yudkowsky entered the field, which has sometimes led Yudkowsky to say that Bostrom should receive credit for inventing the Friendly AI concept.  Bostrom is founder and director of the [ Oxford Future of Humanity Institute].  He is the author of the popular book [Superintelligence_book Superintelligence] that currently forms the best book-length introduction to the field.  Bostrom's academic background is as an analytic philosopher formerly specializing in [ anthropic probability theory] and [ transhumanist ethics].  Relative to Yudkowsky, Bostrom is relatively more interested in Oracle models of value alignment and in potential exotic methods of obtaining aligned goals.</p></main><hr><footer><p class="likes"><h2>Likes</h2><span class="page-comma-list"><a class="page-link" href="../page/AlexRay.html">Alex Ray</a></span></p><p class="related"><h2>Related</h2><ul class="page-list"><li><a class="page-link" href="../page/bostrom_superintelligence.html">Nick Bostrom's book Superintelligence</a> <q>The current best book-form introduction to AI alignment theory.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li></ul></p></footer></body></html>