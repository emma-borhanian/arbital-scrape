<!DOCTYPE html><html><head><meta charset="utf-8"><title>AI alignment</title><link rel="stylesheet" type="text/css" href="../common.css"><link rel="stylesheet" type="text/css" href="../page-style.css"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  SVG: {EqnChunk: 50, EqnChunkFactor: 1.5, EqChunkDelay: 10, useFontCache: false, linebreaks: {automatic: true}},
  tex2jax: {
    inlineMath: [['$~$', '$~$']],
    displayMath: [['$$~$', '$~$$']],
    processEscapes: true,
    preview: 'none',
  },
  showProcessingMessages: false,
  messageStyle: 'none',
  // http://docs.mathjax.org/en/latest/config-files.html#the-tex-ams-svg-configuration-file
  jax: ["input/TeX","output/SVG", "output/PreviewHTML"],
  extensions: ["tex2jax.js","MathMenu.js","MathZoom.js", "fast-preview.js", "AssistiveMML.js", "a11y/accessibility-menu.js"],
  TeX: { extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"] }
});</script><script type="text/javascript" src="../MathJax-2.7.5/MathJax.js"></script><script type="text/javascript" src="../arbital-demo-bundle.js"></script><script type="text/javascript">window.addEventListener('DOMContentLoaded', e=>window.loadAllDemos())
</script></head><body><header><h1 class="title">AI alignment</h1><div class="page-info"><p class="metadata-link"><a href="../metadata/ai_alignment.json.html">ai_alignment.json</a></p><p class="arbital-url"><a href="https://arbital.com/p/ai_alignment">https://arbital.com/p/ai_alignment</a></p><p class="creator">by
 <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a> Mar 26 2015 
updated
 Jan 27 2017</p></div><p class="clickbait">The great civilizational problem of creating artificially intelligent computer systems such that running them is a good idea.</p><nav class="breadcrumbs"><nav class="breadcrumb" aria-label="Breadcrumb"><ul><li><a href="../index.html">Index</a></li><li>AI alignment</li></ul></nav></nav></header><hr><main><p>[summary:  AI alignment or "the alignment problem for <a href="advanced_agent.html">advanced agents</a>" is the overarching research topic of how to develop  <a href="sufficiently_advanced_ai.html">sufficiently advanced machine intelligences</a> such that running them produces <a href="beneficial.html">good</a> outcomes in the real world.  Other terms that have been used for this research field include "Friendly AI" or <a href="AI_safety_mindset.html">robust</a> and <a href="beneficial.html">beneficial</a> AI.</p>
<p>"AI alignment theory" is meant as an overarching term to cover all the ideas and research associated with this problem.  Including, e.g., debates over [ how rapidly an AI might gain in capability] once it goes over a particular threshold.]</p>
<p>The "alignment problem for <a href="advanced_agent.html">advanced agents</a>" or "AI alignment" is the overarching research topic of how to develop <a href="sufficiently_advanced_ai.html">sufficiently advanced machine intelligences</a> such that running them produces <a href="beneficial.html">good</a> outcomes in the real world.</p>
<p>Both '<a href="advanced_agent.html">advanced agent</a>' and '<a href="value_alignment_value.html">good</a>' should be understood as metasyntactic placeholders for complicated ideas still under debate.  The term 'alignment' is intended to convey the idea of pointing an AI in a direction--just like, once you build a rocket, it has to be pointed in a particular direction.</p>
<p>"AI alignment theory" is meant as an overarching term to cover the whole research field associated with this problem, including, e.g., the much-debated attempt to estimate [ how rapidly an AI might gain in capability] once it goes over various particular thresholds.</p>
<p>Other terms that have been used to describe this research problem include  "<a href="AI_safety_mindset.html">robust</a> and <a href="beneficial.html">beneficial</a> AI" and "Friendly AI".  The term "<a href="value_alignment_problem.html">value alignment problem</a>" was coined by [ Stuart Russell] to refer to the primary subproblem of aligning AI preferences with (potentially <a href="cev.html">idealized</a>) human preferences.</p>
<p>Some alternative terms for this general field of study, such as 'control problem', can sound <a href="nonadversarial.html">adversarial</a>--like the rocket is already pointed in a bad direction and you need to wrestle with it.  Other terms, like 'AI safety', understate the advocated degree to which alignment ought to be an intrinsic part of building advanced agents.  E.g., there isn't a separate theory of "bridge safety" for how to build bridges that don't fall down.  Pointing the agent in a particular direction ought to be seen as part of the standard problem of building an advanced machine agent.  The problem does not divide into "building an advanced AI" and then separately "somehow causing that AI to produce good outcomes", the problem is "getting good outcomes via building a cognitive agent that brings about those good outcomes".</p>
<p>A good introductory article or survey paper for this field does not presently exist.  If you have no idea what this problem is about, consider reading <a href="bostrom_superintelligence.html">Nick Bostrom&#39;s popular book Superintelligence</a>.</p>
<p>You can explore this Arbital domain by following <a href="../explore.html#ai_alignment">this link</a>.  See also the <a href="value_alignment_subject_list.html">List of Value Alignment Topics on Arbital</a> although this is not up-to-date.</p>
<pre>%%%comment:

# Overview

## Supporting knowledge

If you&#39;re willing to spend time on learning this field and are not previously familiar with the basics of decision theory and probability theory, it&#39;s worth reading the Arbital introductions to those first.  In particular, it may be useful to become familiar with the notion of [1zq priors and belief revision], and with the [7hh coherence arguments for expected utility].

If you have time to read a textbook to gain general familiarity with AI, &quot;[Artificial Intelligence: A Modern Approach](https://amazon.com/Artificial-Intelligence-Modern-Approach-3rd/dp/0136042597)&quot; is highly recommended.

## Key terms

- Advanced agent
- Value / goodness / beneficial
- Artificial General Intelligence
- Superintelligence

## Key factual propositions

- Orthogonality thesis
- Instrumental convergence thesis
- Capability gain
- Complexity and fragility of cosmopolitan value
- Alignment difficulty

## Advocated basic principles

- Nonadversarial principle
- Minimality principle
- Intrinsic safety
- Unreliable monitoring %note: Treat human monitoring as expensive, unreliable, and fragile.%

## Advocated methodology

- Generalized security mindset
- Foreseeable difficulties

## Advocated design principles

- Value alignment
- Corrigibility
- Mild optimization / Taskishness
- Conservatism
- Transparency
- Whitelisting
- Redzoning

## Current research areas

- Cooperative inverse reinforcement learning
- Interruptibility
- Utility switching
- Stable self-modification &amp; Vingean reflection
- Mirror models
- Robust machine learning

# Open problems

- Environmental goals
- Other-izer problem
- Impact penalty
- Shutdown utility function &amp; abortability
- Fully updated deference
- Epistemic exclusion

# Partially solved problems

- Logical uncertainty for doubly-exponential reflective agents.
- Interruptibility for non-reflective non-consequentialist Q-learners.
- Strategy-determined Newcomblike problems.
- Leverage prior

# Future work

- Ontology identification problem
- Behaviorism
- Averting instrumental strategies
- Reproducibility

%%%</pre></main><hr><section class="comments"><h2>Comments</h2><div class="comment"><p><a class="page-link" href="../page/AdnllL.html">Adnll</a></p><p><p>Hi,</p>
<p>If we have <em>some</em> idea of what AI/Alignment is about, should we start with <em>The AI-Foom Debate</em>, or with Nick Bostrom's <em>Superintelligence</em>?</p>
<p>Thanks</p></p></div></section><footer><p class="all-creators"><h2>All Creators</h2><span class="page-comma-list"><a class="page-link" href="../page/AlexeiAndreev.html">Alexei Andreev</a>,
 <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a>,
 <a class="page-link" href="../page/NateSoares.html">Nate Soares</a></span></p><p class="likes"><h2>Likes</h2><span class="page-comma-list"><a class="page-link" href="../page/AlexRay.html">Alex Ray</a>,
 <a class="page-link" href="../page/AlexeiAndreev.html">Alexei Andreev</a>,
 <a class="page-link" href="../page/AndrewMcKnight.html">Andrew McKnight</a>,
 <a class="page-link" href="../page/BrandonReinhart.html">Brandon Reinhart</a>,
 <a class="page-link" href="../page/ChaseRoycroft.html">Chase Roycroft</a>,
 <a class="page-link" href="../page/ChristopherSiewert.html">Christopher Siewert</a>,
 <a class="page-link" href="../page/EliasBaixas.html">Elias Baixas</a>,
 <a class="page-link" href="../page/FedorAndreev.html">Fedor Andreev</a>,
 <a class="page-link" href="../page/JoeChooChoy.html">Joe Choo-Choy</a>,
 <a class="page-link" href="../page/MackHidalgo.html">Mack Hidalgo</a>,
 <a class="page-link" href="../page/MatthewMattila.html">Matthew Mattila</a>,
 <a class="page-link" href="../page/NateSoares.html">Nate Soares</a>,
 <a class="page-link" href="../page/SergeGaiotti.html">Serge Gaiotti</a>,
 <a class="page-link" href="../page/StephanieZolayvar.html">Stephanie Zolayvar</a>,
 <a class="page-link" href="../page/StevenWang.html">Steven Wang</a>,
 <a class="page-link" href="../page/kaiweynberg.html">kai weynberg</a></span></p><p class="related"><h2>Related</h2><ul class="page-list"><li><a class="page-link" href="../page/paul_ai_control.html">Paul Christiano's AI control blog</a> <q>Speculations on the design of safe, efficient AI systems.</q> - <a class="page-link" href="../page/PaulChristiano.html">Paul Christiano</a></li></ul></p><p class="children"><h2>Children</h2><ul class="page-tree"><li><a class="page-link" href="../page/value_alignment_open_problem.html">AI alignment open problem</a> <q>Tag for open problems under AI alignment.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/ai_arms_race.html">AI arms races</a> <q>AI arms races are bad</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/advanced_safety.html">Advanced safety</a> <q>An agent is *really* safe when it has the capacity to do anything, but chooses to do what the programmer wants.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a><ul class="page-tree"><li><a class="page-link" href="../page/AI_safety_mindset.html">AI safety mindset</a> <q>Asking how AI designs could go wrong, instead of imagining them going right.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a><ul class="page-tree"><li><a class="page-link" href="../page/hack.html">Ad-hoc hack (alignment theory)</a> <q>A &quot;hack&quot; is when you alter the behavior of your AI in a way that defies, or doesn't correspond to, a principled approach for that problem.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/direct_limit_oppose.html">Directing, vs. limiting, vs. opposing</a> <q>Getting the AI to compute the right action in a domain; versus getting the AI to not compute at all in an unsafe domain; versus trying to prevent the AI from acting successfully.  (Prefer 1 &amp; 2.)</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/dont_solve_whole_problem.html">Don't try to solve the entire alignment problem</a> <q>New to AI alignment theory?  Want to work in this area?  Already been working in it for years?  Don't try to solve the entire alignment problem with your next good idea!</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/load_bearing_premises.html">Flag the load-bearing premises</a> <q>If somebody says, &quot;This AI safety plan is going to fail, because X&quot; and you reply, &quot;Oh, that's fine because of Y and Z&quot;, then you'd better clearly flag Y and Z as &quot;load-bearing&quot; parts of your plan.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/show_broken.html">Show me what you've broken</a> <q>To demonstrate competence at computer security, or AI alignment, think in terms of breaking proposals and finding technically demonstrable flaws in them.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/complacency_valley.html">Valley of Dangerous Complacency</a> <q>When the AGI works often enough that you let down your guard, but it still has bugs.  Imagine a robotic car that almost always steers perfectly, but sometimes heads off a cliff.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li></ul></li><li><a class="page-link" href="../page/actual_effectiveness.html">Actual effectiveness</a> <q>If you want the AI's so-called 'utility function' to actually be steering the AI, you need to think about how it meshes up with beliefs, or what gets output to actions.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/context_disaster.html">Context disaster</a> <q>Some possible designs cause your AI to behave nicely while developing, and behave a lot less nicely when it's smarter.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/distinguish_advancement.html">Distinguish which advanced-agent properties lead to the foreseeable difficulty</a> <q>Say what kind of AI, or threshold level of intelligence, or key type of advancement, first produces the difficulty or challenge you're talking about.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/goodharts_curse.html">Goodhart's Curse</a> <q>The Optimizer's Curse meets Goodhart's Law.  For example, if our values are V, and an AI's utility function U is a proxy for V, optimizing for high U seeks out 'errors'--that is, high values of U - V.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/goodness_estimate_bias.html">Goodness estimate biaser</a> <q>Some of the main problems in AI alignment can be seen as scenarios where actual goodness is likely to be systematically lower than a broken way of estimating goodness.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/foreseeable_difficulties.html">Methodology of foreseeable difficulties</a> <q>Building a nice AI is likely to be hard enough, and contain enough gotchas that won't show up in the AI's early days, that we need to foresee problems coming in advance.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/unbounded_analysis.html">Methodology of unbounded analysis</a> <q>What we do and don't understand how to do, using unlimited computing power, is a critical distinction and important frontier.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a><ul class="page-tree"><li><a class="page-link" href="../page/AIXI.html">AIXI</a> <q>How to build an (evil) superintelligent AI using unlimited computing power and one page of Python code.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a><ul class="page-tree"><li><a class="page-link" href="../page/aixitl.html">AIXI-tl</a> <q>A time-bounded version of the ideal agent AIXI that uses an impossibly large finite computer instead of a hypercomputer.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li></ul></li><li><a class="page-link" href="../page/cartesian_agent.html">Cartesian agent</a> <q>Agents separated from their environments by impermeable barriers through which only sensory information can enter and motor output can exit.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a><ul class="page-tree"><li><a class="page-link" href="../page/cartesian_boundary.html">Cartesian agent-environment boundary</a> <q>If your agent is separated from the environment by an absolute border that can only be crossed by sensory information and motor outputs, it might just be a Cartesian agent.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li></ul></li><li><a class="page-link" href="../page/hypercomputer.html">Hypercomputer</a> <q>Some formalisms demand computers larger than the limit of all finite computers</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/mechanical_turk.html">Mechanical Turk (example)</a> <q>The 19th-century chess-playing automaton known as the Mechanical Turk actually had a human operator inside. People at the time had interesting thoughts about the possibility of mechanical chess.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/nofreelunch_irrelevant.html">No-Free-Lunch theorems are often irrelevant</a> <q>There's often a theorem proving that some problem has no optimal answer across every possible world.  But this may not matter, since the real world is a special case.  (E.g., a low-entropy universe.)</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/solomonoff_induction.html">Solomonoff induction</a> <q>A simple way to superintelligently predict sequences of data, given unlimited computing power.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a><ul class="page-tree"><li><a class="page-link" href="../page/1hh.html">Solomonoff induction: Intro Dialogue (Math 2)</a> <q>An introduction to Solomonoff induction for the unfamiliar reader who isn't bad at math</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li></ul></li><li><a class="page-link" href="../page/large_computer.html">Unphysically large finite computer</a> <q>The imaginary box required to run programs that require impossibly large, but finite, amounts of computing power.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li></ul></li><li><a class="page-link" href="../page/nearest_unblocked.html">Nearest unblocked strategy</a> <q>If you patch an agent's preference framework to avoid an undesirable solution, what can you expect to happen?</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/daemons.html">Optimization daemons</a> <q>When you optimize something so hard that it crystalizes into an optimizer, like the way natural selection optimized apes so hard they turned into human-level intelligences</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/safe_useless.html">Safe but useless</a> <q>Sometimes, at the end of locking down your AI so that it seems extremely safe, you'll end up with an AI that can't be used to do anything interesting.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li></ul></li><li><a class="page-link" href="../page/complexity_of_value.html">Complexity of value</a> <q>There's no simple way to describe the goals we want Artificial Intelligences to want.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a><ul class="page-tree"><li><a class="page-link" href="../page/meta_unsolved.html">Meta-rules for (narrow) value learning are still unsolved</a> <q>We don't currently know a simple meta-utility function that would take in observation of humans and spit out our true values, or even a good target for a Task AGI.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/underestimate_value_complexity_perceputal_property.html">Underestimating complexity of value because goodness feels like a simple property</a> <q>When you just want to yell at the AI, &quot;Just do normal high-value X, dammit, not weird low-value X!&quot; and that 'high versus low value' boundary is way more complicated than your brain wants to think.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li></ul></li><li><a class="page-link" href="../page/4j.html">Coordinative AI development hypothetical</a> <q>What would safe AI development look like if we didn't have to worry about anything else?</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/correlated_coverage.html">Correlated coverage</a> <q>In which parts of AI alignment can we hope that getting many things right, will mean the AI gets everything right?</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/corrigibility.html">Corrigibility</a> <q>&quot;I can't let you do that, Dave.&quot;</q> - <a class="page-link" href="../page/NateSoares.html">Nate Soares</a><ul class="page-tree"><li><a class="page-link" href="../page/avert_instrumental_pressure.html">Averting instrumental pressures</a> <q>Almost-any utility function for an AI, whether the target is diamonds or paperclips or eudaimonia, implies subgoals like rapidly self-improving and refusing to shut down.  Can we make that not happen?</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/avert_self_improvement.html">Averting the convergent instrumental strategy of self-improvement</a> <q>We probably want the first AGI to *not* improve as fast as possible, but improving as fast as possible is a convergent strategy for accomplishing most things.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/hard_corrigibility.html">Hard problem of corrigibility</a> <q>Can you build an agent that reasons as if it knows itself to be incomplete and sympathizes with your wanting to rebuild or correct it?</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/interruptibility.html">Interruptibility</a> <q>A subproblem of corrigibility under the machine learning paradigm: when the agent is interrupted, it must not learn to prevent future interruptions.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/updated_deference.html">Problem of fully updated deference</a> <q>Why moral uncertainty doesn't stop an AI from defending its off-switch.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/programmer_deception.html">Programmer deception</a> <q>Programmer deception is when the AI's decision process leads it to optimize for an instrumental goal…</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a><ul class="page-tree"><li><a class="page-link" href="../page/cognitive_steganography.html">Cognitive steganography</a> <q>Disaligned AIs that are modeling human psychology and trying to deceive their programmers will want to hide their internal thought processes from their programmers.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li></ul></li><li><a class="page-link" href="../page/shutdown_problem.html">Shutdown problem</a> <q>How to build an AGI that lets you shut it down, despite the obvious fact that this will interfere with whatever the AGI's goals are.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a><ul class="page-tree"><li><a class="page-link" href="../page/no_coffee_if_dead.html">You can't get the coffee if you're dead</a> <q>An AI given the goal of 'get the coffee' can't achieve that goal if it has been turned off; so even an AI whose goal is just to fetch the coffee may try to avert a shutdown button being pressed.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li></ul></li><li><a class="page-link" href="../page/user_manipulation.html">User manipulation</a> <q>If not otherwise averted, many of an AGI's desired outcomes are likely to interact with users and hence imply an incentive to manipulate users.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a><ul class="page-tree"><li><a class="page-link" href="../page/30b.html">User maximization</a> <q>A sub-principle of avoiding user manipulation - if you see an argmax over X or 'optimize X' instruction and X includes a user interaction, you've just told the AI to optimize the user.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li></ul></li><li><a class="page-link" href="../page/utility_indifference.html">Utility indifference</a> <q>How can we make an AI indifferent to whether we press a button that changes its goals?</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li></ul></li><li><a class="page-link" href="../page/development_phase_unpredictable.html">Development phase unpredictable</a> <q>Several proposed problems in advanced safety are alleged to be difficult because they depend on some…</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a><ul class="page-tree"><li><a class="page-link" href="../page/unforeseen_maximum.html">Unforeseen maximum</a> <q>When you tell AI to produce world peace and it kills everyone.  (Okay, some SF writers saw that one coming.)</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a><ul class="page-tree"><li><a class="page-link" href="../page/missing_weird.html">Missing the weird alternative</a> <q>People might systematically overlook &quot;make tiny molecular smileyfaces&quot; as a way of &quot;producing smiles&quot;, because our brains automatically search for high-utility-to-us ways of &quot;producing smiles&quot;.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li></ul></li></ul></li><li><a class="page-link" href="../page/alignment_difficulty.html">Difficulty of AI alignment</a> <q>How hard is it exactly to point an Artificial General Intelligence in an intuitively okay direction?</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/executable_philosophy.html">Executable philosophy</a> <q>Philosophical discourse aimed at producing a trustworthy answer or meta-answer, in limited time, which can used in constructing an Artificial Intelligence.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/value_alignment_glossary.html">Glossary (Value Alignment Theory)</a> <q>Words that have a special meaning in the context of creating nice AIs.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a><ul class="page-tree"><li><a class="page-link" href="../page/ai_concept.html">'Concept'</a> <q>In the context of Artificial Intelligence, a 'concept' is a category, something that identifies thingies as being inside or outside the concept.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/cognitive_domain.html">Cognitive domain</a> <q>An allegedly compact unit of knowledge, such that ideas inside the unit interact mainly with each other and less with ideas in other domains.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a><ul class="page-tree"><li><a class="page-link" href="../page/domain_distance.html">Distances between cognitive domains</a> <q>Often in AI alignment we want to ask, &quot;How close is 'being able to do X' to 'being able to do Y'?&quot;</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li></ul></li><li><a class="page-link" href="../page/FAI.html">Friendly AI</a> <q>Old terminology for an AI whose preferences have been successfully aligned with idealized human values.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li></ul></li><li><a class="page-link" href="../page/inductive_ambiguity.html">Identifying ambiguous inductions</a> <q>What do a &quot;red strawberry&quot;, a &quot;red apple&quot;, and a &quot;red cherry&quot; have in common that a &quot;yellow carrot&quot; doesn't?  Are they &quot;red fruits&quot; or &quot;red objects&quot;?</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/informed_oversight.html">Informed oversight</a> <q>Incentivize a reinforcement learner that's less smart than you to accomplish some task</q> - <a class="page-link" href="../page/JessicaTaylor.html">Jessica Taylor</a></li><li><a class="page-link" href="../page/intended_goal.html">Intended goal</a> <q>Definition.  An &quot;intended goal&quot; refers to the intuitive intention in the mind of a human programmer …</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/5b.html">Linguistic conventions in value alignment</a> <q>How and why to use precise language and words with special meaning when talking about value alignment.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a><ul class="page-tree"><li><a class="page-link" href="../page/value_alignment_utility.html">Utility</a> <q>What is &quot;utility&quot; in the context of Value Alignment Theory?</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li></ul></li><li><a class="page-link" href="../page/value_alignment_subject_list.html">List: value-alignment subjects</a> <q>Bullet point list of core VAT subjects.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/mindcrime.html">Mindcrime</a> <q>Might a machine intelligence contain vast numbers of unhappy conscious subprocesses?</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a><ul class="page-tree"><li><a class="page-link" href="../page/mindcrime_introduction.html">Mindcrime: Introduction</a> <q>The more predictive accuracy we want from a model, the more detailed the model becomes.  A very roug…</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/nonperson_predicate.html">Nonperson predicate</a> <q>If we knew which computations were definitely not people, we could tell AIs which programs they were definitely allowed to compute.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li></ul></li><li><a class="page-link" href="../page/distant_SIs.html">Modeling distant superintelligences</a> <q>The several large problems that might occur if an AI starts to think about alien superintelligences.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a><ul class="page-tree"><li><a class="page-link" href="../page/probable_environment_hacking.html">Distant superintelligences can coerce the most probable environment of your AI</a> <q>Distant superintelligences may be able to hack your local AI, if your AI's preference framework depends on its most probable environment.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li></ul></li><li><a class="page-link" href="../page/4s.html">Natural language understanding of &quot;right&quot; will yield normativity</a> <q>What will happen if you tell an advanced agent to do the &quot;right&quot; thing?</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/bostrom_superintelligence.html">Nick Bostrom's book Superintelligence</a> <q>The current best book-form introduction to AI alignment theory.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/object_level_goal.html">Object-level vs. indirect goals</a> <q>Difference between &quot;give Alice the apple&quot; and &quot;give Alice what she wants&quot;.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/patch_resistant.html">Patch resistance</a> <q>One does not simply solve the value alignment problem.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a><ul class="page-tree"><li><a class="page-link" href="../page/unforeseen_maximum.html">Unforeseen maximum</a> <q>When you tell AI to produce world peace and it kills everyone.  (Okay, some SF writers saw that one coming.)</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a><ul class="page-tree"><li><a class="page-link" href="../page/missing_weird.html">Missing the weird alternative</a> <q>People might systematically overlook &quot;make tiny molecular smileyfaces&quot; as a way of &quot;producing smiles&quot;, because our brains automatically search for high-utility-to-us ways of &quot;producing smiles&quot;.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li></ul></li></ul></li><li><a class="page-link" href="../page/alignment_principle.html">Principles in AI alignment</a> <q>A 'principle' of AI alignment is a very general design goal like 'understand what the heck is going on inside the AI' that has informed a wide set of specific design proposals.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a><ul class="page-tree"><li><a class="page-link" href="../page/minimality_principle.html">Minimality principle</a> <q>The first AGI ever built should save the world in a way that requires the least amount of the least dangerous cognition.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/nonadversarial.html">Non-adversarial principle</a> <q>At no point in constructing an Artificial General Intelligence should we construct a computation that tries to hurt us, and then try to stop it from hurting us.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a><ul class="page-tree"><li><a class="page-link" href="../page/direct_limit_oppose.html">Directing, vs. limiting, vs. opposing</a> <q>Getting the AI to compute the right action in a domain; versus getting the AI to not compute at all in an unsafe domain; versus trying to prevent the AI from acting successfully.  (Prefer 1 &amp; 2.)</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/cognitive_alignment.html">Generalized principle of cognitive alignment</a> <q>When we're asking how we want the AI to think about an alignment problem, one source of inspiration is trying to have the AI mirror our own thoughts about that problem.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/niceness_defense.html">Niceness is the first line of defense</a> <q>The *first* line of defense in dealing with any partially superhuman AI system advanced enough to possibly be dangerous is that it does not *want* to hurt you or defeat your safety measures.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/omni_test.html">Omnipotence test for AI safety</a> <q>Would your AI produce disastrous outcomes if it suddenly gained omnipotence and omniscience? If so, why did you program something that *wants* to hurt you and is held back only by lacking the power?</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/nonadversarial_safety.html">The AI must tolerate your safety measures</a> <q>A corollary of the nonadversarial principle is that &quot;The AI must tolerate your safety measures.&quot;</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li></ul></li><li><a class="page-link" href="../page/hyperexistential_separation.html">Separation from hyperexistential risk</a> <q>The AI should be widely separated in the design space from any AI that would constitute a &quot;hyperexistential risk&quot; (anything worse than death).</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/understandability_principle.html">Understandability principle</a> <q>The more you understand what the heck is going on inside your AI, the safer you are.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a><ul class="page-tree"><li><a class="page-link" href="../page/effability.html">Effability principle</a> <q>You are safer the more you understand the inner structure of how your AI thinks; the better you can describe the relation of smaller pieces of the AI's thought process.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li></ul></li></ul></li><li><a class="page-link" href="../page/value_alignment_programmer.html">Programmer</a> <q>Who is building these advanced agents?</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/relevant_limited_AI.html">Relevant limited AI</a> <q>Can we have a limited AI, that's nonetheless relevant?</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/relevant_powerful_agent.html">Relevant powerful agent</a> <q>An agent is relevant if it completely changes the course of history.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/powerful_agent_highly_optimized.html">Relevant powerful agents will be highly optimized</a> <q>The probability that an agent that is cognitively powerful enough to be relevant to existential outc…</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/reliable_prediction.html">Reliable prediction</a> <q>How can we train predictors that reliably predict observable phenomena such as human behavior?</q> - <a class="page-link" href="../page/JessicaTaylor.html">Jessica Taylor</a></li><li><a class="page-link" href="../page/value_alignment_researchers.html">Researchers in value alignment theory</a> <q>Who's working full-time in value alignment theory?</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a><ul class="page-tree"><li><a class="page-link" href="../page/NickBostrom.html">Nick Bostrom</a> <q>Nick Bostrom, secretly the inventor of Friendly AI</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li></ul></li><li><a class="page-link" href="../page/4l.html">Safe impact measure</a> <q>What can we measure to make sure an agent is acting in a safe manner?</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/safe_training_for_imitators.html">Safe training procedures for human-imitators</a> <q>How does one train a reinforcement learner to act like a human?</q> - <a class="page-link" href="../page/JessicaTaylor.html">Jessica Taylor</a></li><li><a class="page-link" href="../page/selective_similarity_metric.html">Selective similarity metrics for imitation</a> <q>Can we make human-imitators more efficient by scoring them more heavily on imitating the aspects of human behavior we care about more?</q> - <a class="page-link" href="../page/JessicaTaylor.html">Jessica Taylor</a></li><li><a class="page-link" href="../page/some_computations_are_people.html">Some computations are people</a> <q>It's possible to have a conscious person being simulated inside a computer or other substrate.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/AGI_typology.html">Strategic AGI typology</a> <q>What broad types of advanced AIs, corresponding to which strategic scenarios, might it be possible or wise to create?</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a><ul class="page-tree"><li><a class="page-link" href="../page/Sovereign.html">Autonomous AGI</a> <q>The hardest possible class of Friendly AI to build, with the least moral hazard; an AI intended to neither require nor accept further direction.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/KANSI.html">Known-algorithm non-self-improving agent</a> <q>Possible advanced AIs that aren't self-modifying, aren't self-improving, and where we know and understand all the component algorithms.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/oracle.html">Oracle</a> <q>System designed to safely answer questions.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a><ul class="page-tree"><li><a class="page-link" href="../page/ZF_provability_oracle.html">Zermelo-Fraenkel provability oracle</a> <q>We might be able to build a system that can safely inform us that a theorem has a proof in set theory, but we can't see how to use that capability to save the world.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li></ul></li><li><a class="page-link" href="../page/task_agi.html">Task-directed AGI</a> <q>An advanced AI that's meant to pursue a series of limited-scope goals given it by the user.  In Bostrom's terminology, a Genie.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a><ul class="page-tree"><li><a class="page-link" href="../page/behaviorist.html">Behaviorist genie</a> <q>An advanced agent that's forbidden to model minds in too much detail.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/AI_boxing.html">Boxed AI</a> <q>Idea: what if we limit how AI can interact with the world. That'll make it safe, right??</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a><ul class="page-tree"><li><a class="page-link" href="../page/ZF_provability_oracle.html">Zermelo-Fraenkel provability oracle</a> <q>We might be able to build a system that can safely inform us that a theorem has a proof in set theory, but we can't see how to use that capability to save the world.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li></ul></li><li><a class="page-link" href="../page/conservative_concept.html">Conservative concept boundary</a> <q>Given N example burritos, draw a boundary around what is a 'burrito' that is relatively simple and allows as few positive instances as possible.  Helps make sure the next thing generated is a burrito.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/epistemic_exclusion.html">Epistemic exclusion</a> <q>How would you build an AI that, no matter what else it learned about the world, never knew or wanted to know what was inside your basement?</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/faithful_simulation.html">Faithful simulation</a> <q>How would you identify, to a Task AGI (aka Genie), the problem of scanning a human brain, and then running a sufficiently accurate simulation of it for the simulation to not be crazy or psychotic?</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/limited_agi.html">Limited AGI</a> <q>Task-based AGIs don't need unlimited cognitive and material powers to carry out their Tasks; which means their powers can potentially be limited.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/low_impact.html">Low impact</a> <q>The open problem of having an AI carry out tasks in ways that cause minimum side effects and change as little of the rest of the universe as possible.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a><ul class="page-tree"><li><a class="page-link" href="../page/abortable.html">Abortable plans</a> <q>Plans that can be undone, or switched to having low further impact.  If the AI builds abortable nanomachines, they'll have a quiet self-destruct option that includes any replicated nanomachines.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/shutdown_utility_function.html">Shutdown utility function</a> <q>A special case of a low-impact utility function where you just want the AGI to switch itself off harmlessly (and not create subagents to make absolutely sure it stays off, etcetera).</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li></ul></li><li><a class="page-link" href="../page/soft_optimizer.html">Mild optimization</a> <q>An AGI which, if you ask it to paint one car pink, just paints one car pink and doesn't tile the universe with pink-painted cars, because it's not trying *that* hard to max out its car-painting score.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/taskagi_open_problems.html">Open subproblems in aligning a Task-based AGI</a> <q>Open research problems, especially ones we can model today, in building an AGI that can &quot;paint all cars pink&quot; without turning its future light cone into pink-painted cars.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/oracle.html">Oracle</a> <q>System designed to safely answer questions.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a><ul class="page-tree"><li><a class="page-link" href="../page/ZF_provability_oracle.html">Zermelo-Fraenkel provability oracle</a> <q>We might be able to build a system that can safely inform us that a theorem has a proof in set theory, but we can't see how to use that capability to save the world.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li></ul></li><li><a class="page-link" href="../page/user_querying.html">Querying the AGI user</a> <q>Postulating that an advanced agent will check something with its user, probably comes with some standard issues and gotchas (e.g., prioritizing what to query, not manipulating the user, etc etc).</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/safe_plan_identification.html">Safe plan identification and verification</a> <q>On a particular task or problem, the issue of how to communicate to the AGI what you want it to do and all the things you don't want it to do.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a><ul class="page-tree"><li><a class="page-link" href="../page/dwim.html">Do-What-I-Mean hierarchy</a> <q>Successive levels of &quot;Do What I Mean&quot; or AGIs that understand their users increasingly well</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li></ul></li><li><a class="page-link" href="../page/task_goal.html">Task (AI goal)</a> <q>When building the first AGIs, it may be wiser to assign them only goals that are bounded in space and time, and can be satisfied by bounded efforts.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/task_identification.html">Task identification problem</a> <q>If you have a task-based AGI (Genie) then how do you pinpoint exactly what you want it to do (and not do)?</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a><ul class="page-tree"><li><a class="page-link" href="../page/pointing_finger.html">Look where I'm pointing, not at my finger</a> <q>When trying to communicate the concept &quot;glove&quot;, getting the AGI to focus on &quot;gloves&quot; rather than &quot;my user's decision to label something a glove&quot; or &quot;anything that depresses the glove-labeling button&quot;.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li></ul></li></ul></li></ul></li><li><a class="page-link" href="../page/strong_uncontainability.html">Strong cognitive uncontainability</a> <q>An advanced agent can win in ways humans can't understand in advance.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/optimized_agent_appears_coherent.html">Sufficiently optimized agents appear coherent</a> <q>If you could think as well as a superintelligence, you'd be at least that smart yourself.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/task_agi.html">Task-directed AGI</a> <q>An advanced AI that's meant to pursue a series of limited-scope goals given it by the user.  In Bostrom's terminology, a Genie.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a><ul class="page-tree"><li><a class="page-link" href="../page/behaviorist.html">Behaviorist genie</a> <q>An advanced agent that's forbidden to model minds in too much detail.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/AI_boxing.html">Boxed AI</a> <q>Idea: what if we limit how AI can interact with the world. That'll make it safe, right??</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a><ul class="page-tree"><li><a class="page-link" href="../page/ZF_provability_oracle.html">Zermelo-Fraenkel provability oracle</a> <q>We might be able to build a system that can safely inform us that a theorem has a proof in set theory, but we can't see how to use that capability to save the world.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li></ul></li><li><a class="page-link" href="../page/conservative_concept.html">Conservative concept boundary</a> <q>Given N example burritos, draw a boundary around what is a 'burrito' that is relatively simple and allows as few positive instances as possible.  Helps make sure the next thing generated is a burrito.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/epistemic_exclusion.html">Epistemic exclusion</a> <q>How would you build an AI that, no matter what else it learned about the world, never knew or wanted to know what was inside your basement?</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/faithful_simulation.html">Faithful simulation</a> <q>How would you identify, to a Task AGI (aka Genie), the problem of scanning a human brain, and then running a sufficiently accurate simulation of it for the simulation to not be crazy or psychotic?</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/limited_agi.html">Limited AGI</a> <q>Task-based AGIs don't need unlimited cognitive and material powers to carry out their Tasks; which means their powers can potentially be limited.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/low_impact.html">Low impact</a> <q>The open problem of having an AI carry out tasks in ways that cause minimum side effects and change as little of the rest of the universe as possible.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a><ul class="page-tree"><li><a class="page-link" href="../page/abortable.html">Abortable plans</a> <q>Plans that can be undone, or switched to having low further impact.  If the AI builds abortable nanomachines, they'll have a quiet self-destruct option that includes any replicated nanomachines.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/shutdown_utility_function.html">Shutdown utility function</a> <q>A special case of a low-impact utility function where you just want the AGI to switch itself off harmlessly (and not create subagents to make absolutely sure it stays off, etcetera).</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li></ul></li><li><a class="page-link" href="../page/soft_optimizer.html">Mild optimization</a> <q>An AGI which, if you ask it to paint one car pink, just paints one car pink and doesn't tile the universe with pink-painted cars, because it's not trying *that* hard to max out its car-painting score.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/taskagi_open_problems.html">Open subproblems in aligning a Task-based AGI</a> <q>Open research problems, especially ones we can model today, in building an AGI that can &quot;paint all cars pink&quot; without turning its future light cone into pink-painted cars.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/oracle.html">Oracle</a> <q>System designed to safely answer questions.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a><ul class="page-tree"><li><a class="page-link" href="../page/ZF_provability_oracle.html">Zermelo-Fraenkel provability oracle</a> <q>We might be able to build a system that can safely inform us that a theorem has a proof in set theory, but we can't see how to use that capability to save the world.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li></ul></li><li><a class="page-link" href="../page/user_querying.html">Querying the AGI user</a> <q>Postulating that an advanced agent will check something with its user, probably comes with some standard issues and gotchas (e.g., prioritizing what to query, not manipulating the user, etc etc).</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/safe_plan_identification.html">Safe plan identification and verification</a> <q>On a particular task or problem, the issue of how to communicate to the AGI what you want it to do and all the things you don't want it to do.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a><ul class="page-tree"><li><a class="page-link" href="../page/dwim.html">Do-What-I-Mean hierarchy</a> <q>Successive levels of &quot;Do What I Mean&quot; or AGIs that understand their users increasingly well</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li></ul></li><li><a class="page-link" href="../page/task_goal.html">Task (AI goal)</a> <q>When building the first AGIs, it may be wiser to assign them only goals that are bounded in space and time, and can be satisfied by bounded efforts.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/task_identification.html">Task identification problem</a> <q>If you have a task-based AGI (Genie) then how do you pinpoint exactly what you want it to do (and not do)?</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a><ul class="page-tree"><li><a class="page-link" href="../page/pointing_finger.html">Look where I'm pointing, not at my finger</a> <q>When trying to communicate the concept &quot;glove&quot;, getting the AGI to focus on &quot;gloves&quot; rather than &quot;my user's decision to label something a glove&quot; or &quot;anything that depresses the glove-labeling button&quot;.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li></ul></li></ul></li><li><a class="page-link" href="../page/rocket_alignment_metaphor.html">The rocket alignment problem</a> <q>If people talked about the problem of space travel the way they talked about AI...</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/advanced_agent_theory.html">Theory of (advanced) agents</a> <q>One of the research subproblems of building powerful nice AIs, is the theory of (sufficiently advanced) minds in general.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a><ul class="page-tree"><li><a class="page-link" href="../page/advanced_agent.html">Advanced agent properties</a> <q>How smart does a machine intelligence need to be, for its niceness to become an issue?  &quot;Advanced&quot; is a broad term to cover cognitive abilities such that we'd need to start considering AI alignment.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a><ul class="page-tree"><li><a class="page-link" href="../page/advanced_nonagent.html">Advanced nonagent</a> <q>Hypothetically, cognitively powerful programs that don't follow the loop of &quot;observe, learn, model the consequences, act, observe results&quot; that a standard &quot;agent&quot; would.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/agi.html">Artificial General Intelligence</a> <q>An AI which has the same kind of &quot;significantly more general&quot; intelligence that humans have compared to chimpanzees; it can learn new domains, like we can.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/big_picture_awareness.html">Big-picture strategic awareness</a> <q>We start encountering new AI alignment issues at the point where a machine intelligence recognizes the existence of a real world, the existence of programmers, and how these relate to its goals.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/uncontainability.html">Cognitive uncontainability</a> <q>'Cognitive uncontainability' is when we can't hold all of an agent's possibilities inside our own minds.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a><ul class="page-tree"><li><a class="page-link" href="../page/rich_domain.html">Rich domain</a> <q>A domain is 'rich', relative to our own intelligence, to the extent that (1) its [ search space] is …</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a><ul class="page-tree"><li><a class="page-link" href="../page/real_is_rich.html">Almost all real-world domains are rich</a> <q>Anything you're trying to accomplish in the real world can potentially be accomplished in a *lot* of different ways.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/logical_game.html">Logical game</a> <q>Game's mathematical structure at its purest form.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li></ul></li></ul></li><li><a class="page-link" href="../page/consequentialist.html">Consequentialist cognition</a> <q>The cognitive ability to foresee the consequences of actions, prefer some outcomes to others, and output actions leading to the preferred outcomes.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/corps_vs_si.html">Corporations vs. superintelligences</a> <q>Corporations have relatively few of the advanced-agent properties that would allow one mistake in aligning a corporation to immediately kill all humans and turn the future light cone into paperclips.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/efficiency.html">Epistemic and instrumental efficiency</a> <q>An efficient agent never makes a mistake you can predict.  You can never successfully predict a directional bias in its estimates.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a><ul class="page-tree"><li><a class="page-link" href="../page/timemachine_efficiency_metaphor.html">Time-machine metaphor for efficient agents</a> <q>Don't imagine a paperclip maximizer as a mind.  Imagine it as a time machine that always spits out the output leading to the greatest number of future paperclips.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li></ul></li><li><a class="page-link" href="../page/general_intelligence.html">General intelligence</a> <q>Compared to chimpanzees, humans seem to be able to learn a much wider variety of domains.  We have 'significantly more generally applicable' cognitive abilities, aka 'more general intelligence'.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/relative_ability.html">Infrahuman, par-human, superhuman, efficient, optimal</a> <q>A categorization of AI ability levels relative to human, with some gotchas in the ordering.  E.g., in simple domains where humans can play optimally, optimal play is not superhuman.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/intelligence_explosion.html">Intelligence explosion</a> <q>What happens if a self-improving AI gets to the point where each amount x of self-improvement triggers &gt;x further self-improvement, and it stays that way for a while.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/real_world.html">Real-world domain</a> <q>Some AIs play chess, some AIs play Go, some AIs drive cars.  These different 'domains' present different options.  All of reality, in all its messy entanglement, is the 'real-world domain'.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/standard_agent.html">Standard agent properties</a> <q>What's a Standard Agent, and what can it do?</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a><ul class="page-tree"><li><a class="page-link" href="../page/bounded_agent.html">Bounded agent</a> <q>An agent that operates in the real world, using realistic amounts of computing power, that is uncertain of its environment, etcetera.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li></ul></li><li><a class="page-link" href="../page/sufficiently_advanced_ai.html">Sufficiently advanced Artificial Intelligence</a> <q>'Sufficiently advanced Artificial Intelligences' are AIs with enough 'advanced agent properties' that we start needing to do 'AI alignment' to them.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/superintelligent.html">Superintelligent</a> <q>A &quot;superintelligence&quot; is strongly superhuman (strictly higher-performing than any and all humans) on every cognitive problem.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/Vingean_uncertainty.html">Vingean uncertainty</a> <q>You can't predict the exact actions of an agent smarter than you - so is there anything you _can_ say about them?</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a><ul class="page-tree"><li><a class="page-link" href="../page/deep_blue.html">Deep Blue</a> <q>The chess-playing program, built by IBM, that first won the world chess championship from Garry Kasparov in 1996.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/Vinge_law.html">Vinge's Law</a> <q>You can't predict exactly what someone smarter than you would do, because if you could, you'd be that smart yourself.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li></ul></li></ul></li><li><a class="page-link" href="../page/instrumental_convergence.html">Instrumental convergence</a> <q>Some strategies can help achieve most possible simple goals.  E.g., acquiring more computing power or more material resources.  By default, unless averted, we can expect advanced AIs to do that.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a><ul class="page-tree"><li><a class="page-link" href="../page/convergent_strategies.html">Convergent instrumental strategies</a> <q>Paperclip maximizers can make more paperclips by improving their cognitive abilities or controlling more resources.  What other strategies would almost-any AI try to use?</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a><ul class="page-tree"><li><a class="page-link" href="../page/preference_stability.html">Consequentialist preferences are reflectively stable by default</a> <q>Gandhi wouldn't take a pill that made him want to kill people, because he knows in that case more people will be murdered.  A paperclip maximizer doesn't want to stop maximizing paperclips.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/convergent_self_modification.html">Convergent strategies of self-modification</a> <q>The strategies we'd expect to be employed by an AI that understands the relevance of its code and hardware to achieving its goals, which therefore has subgoals about its code and hardware.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li></ul></li><li><a class="page-link" href="../page/instrumental.html">Instrumental</a> <q>What is &quot;instrumental&quot; in the context of Value Alignment Theory?</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/instrumental_pressure.html">Instrumental pressure</a> <q>A consequentialist agent will want to bring about certain instrumental events that will help to fulfill its goals.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/paperclip_maximizer.html">Paperclip maximizer</a> <q>This agent will not stop until the entire universe is filled with paperclips.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a><ul class="page-tree"><li><a class="page-link" href="../page/paperclip.html">Paperclip</a> <q>A configuration of matter that we'd see as being worthless even from a very cosmopolitan perspective.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/random_utility_function.html">Random utility function</a> <q>A 'random' utility function is one chosen at random according to some simple probability measure (e.g. weight by Kolmorogov complexity) on a logical space of formal utility functions.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li></ul></li><li><a class="page-link" href="../page/not_more_paperclips.html">You can't get more paperclips that way</a> <q>Most arguments that &quot;A paperclip maximizer could get more paperclips by (doing nice things)&quot; are flawed.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li></ul></li><li><a class="page-link" href="../page/orthogonality.html">Orthogonality Thesis</a> <q>Will smart AIs automatically become benevolent, or automatically become hostile?  Or do different AI designs imply different goals?</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a><ul class="page-tree"><li><a class="page-link" href="../page/instrumental_goals_equally_tractable.html">Instrumental goals are almost-equally as tractable as terminal goals</a> <q>Getting the milk from the refrigerator because you want to drink it, is not vastly harder than getting the milk from the refrigerator because you inherently desire it.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/mind_design_space_wide.html">Mind design space is wide</a> <q>Imagine all human beings as one tiny dot inside a much vaster sphere of possibilities for &quot;The space of minds in general.&quot;  It is wiser to make claims about *some* minds than *all* minds.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/paperclip_maximizer.html">Paperclip maximizer</a> <q>This agent will not stop until the entire universe is filled with paperclips.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a><ul class="page-tree"><li><a class="page-link" href="../page/paperclip.html">Paperclip</a> <q>A configuration of matter that we'd see as being worthless even from a very cosmopolitan perspective.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/random_utility_function.html">Random utility function</a> <q>A 'random' utility function is one chosen at random according to some simple probability measure (e.g. weight by Kolmorogov complexity) on a logical space of formal utility functions.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li></ul></li></ul></li></ul></li><li><a class="page-link" href="../page/unforeseen_maximum.html">Unforeseen maximum</a> <q>When you tell AI to produce world peace and it kills everyone.  (Okay, some SF writers saw that one coming.)</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a><ul class="page-tree"><li><a class="page-link" href="../page/missing_weird.html">Missing the weird alternative</a> <q>People might systematically overlook &quot;make tiny molecular smileyfaces&quot; as a way of &quot;producing smiles&quot;, because our brains automatically search for high-utility-to-us ways of &quot;producing smiles&quot;.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li></ul></li><li><a class="page-link" href="../page/3ck.html">VAT playpen</a> <q>Playpen page for VAT domain.</q> - <a class="page-link" href="../page/AlexeiAndreev.html">Alexei Andreev</a></li><li><a class="page-link" href="../page/value_alignment_value.html">Value</a> <q>The word 'value' in the phrase 'value alignment' is a metasyntactic variable that indicates the speaker's future goals for intelligent life.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a><ul class="page-tree"><li><a class="page-link" href="../page/beneficial.html">'Beneficial'</a> <q>Really actually good.  A metasyntactic variable to mean &quot;favoring whatever the speaker wants ideally to accomplish&quot;, although different speakers have different morals and metaethics.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/detrimental.html">'Detrimental'</a> <q>The opposite of beneficial.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/cev.html">Coherent extrapolated volition (alignment target)</a> <q>A proposed direction for an extremely well-aligned autonomous superintelligence - do what humans would want, if we knew what the AI knew, thought that fast, and understood ourselves.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/value_cosmopolitan.html">Cosmopolitan value</a> <q>Intuitively: Value as seen from a broad, embracing standpoint that is aware of how other entities may not always be like us or easily understandable to us, yet still worthwhile.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/normative_extrapolated_volition.html">Extrapolated volition (normative moral theory)</a> <q>If someone asks you for orange juice, and you know that the refrigerator contains no orange juice, should you bring them lemonade?</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a><ul class="page-tree"><li><a class="page-link" href="../page/rescue_utility.html">Rescuing the utility function</a> <q>If your utility function values 'heat', and then you discover to your horror that there's no ontologically basic heat, switch to valuing disordered kinetic energy. Likewise 'free will' or 'people'.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li></ul></li><li><a class="page-link" href="../page/immediate_goods.html">Immediate goods</a> <q>One of the potential views on 'value' in the value alignment problem is that what we should want fro…</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/frankena_goods.html">William Frankena's list of terminal values</a> <q>Life, consciousness, and activity; health and strength; pleasures and satisfactions of all or certain kinds; happiness, beatitude, contentment, etc.; truth; knowledge and true opinions...</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li></ul></li><li><a class="page-link" href="../page/value_achievement_dilemma.html">Value achievement dilemma</a> <q>How can Earth-originating intelligent life achieve most of its potential value, whether by AI or otherwise?</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a><ul class="page-tree"><li><a class="page-link" href="../page/aligning_adds_time.html">Aligning an AGI adds significant development time</a> <q>Aligning an advanced AI foreseeably involves extra code and extra testing and not being able to do everything the fastest way, so it takes longer.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/4j.html">Coordinative AI development hypothetical</a> <q>What would safe AI development look like if we didn't have to worry about anything else?</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/cosmic_endowment.html">Cosmic endowment</a> <q>The 'cosmic endowment' consists of all the stars that could be reached from probes originating on Earth; the sum of all matter and energy potentially available to be transformed into life and fun.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/moral_hazard.html">Moral hazards in AGI development</a> <q>&quot;Moral hazard&quot; is when owners of an advanced AGI give in to the temptation to do things with it that the rest of us would regard as 'bad', like, say, declaring themselves God-Emperor.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/pivotal.html">Pivotal event</a> <q>Which types of AIs, if they work, can do things that drastically change the nature of the further game?</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li></ul></li><li><a class="page-link" href="../page/value_alignment_problem.html">Value alignment problem</a> <q>You want to build an advanced AI with the right values... but how?</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a><ul class="page-tree"><li><a class="page-link" href="../page/preference_framework.html">Preference framework</a> <q>What's the thing an agent uses to compare its preferences?</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a><ul class="page-tree"><li><a class="page-link" href="../page/attainable_optimum.html">Attainable optimum</a> <q>The 'attainable optimum' of an agent's preferences is the best that agent can actually do given its finite intelligence and resources (as opposed to the global maximum of those preferences).</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/meta_utility.html">Meta-utility function</a> <q>Preference frameworks built out of simple utility functions, but where, e.g., the 'correct' utility function for a possible world depends on whether a button is pressed.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/moral_uncertainty.html">Moral uncertainty</a> <q>A meta-utility function in which the utility function as usually considered, takes on different values in different possible worlds, potentially distinguishable by evidence.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a><ul class="page-tree"><li><a class="page-link" href="../page/ideal_target.html">Ideal target</a> <q>The 'ideal target' of a meta-utility function is the value the ground-level utility function would take on if the agent updated on all possible evidence; the 'true' utilities under moral uncertainty.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li></ul></li></ul></li><li><a class="page-link" href="../page/total_alignment.html">Total alignment</a> <q>We say that an advanced AI is &quot;totally aligned&quot; when it knows *exactly* which outcomes and plans are beneficial, with no further user input.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li></ul></li><li><a class="page-link" href="../page/value_identification.html">Value identification problem</a> <q>The subproblem category of value alignment which deals with pinpointing valuable outcomes to an adva…</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a><ul class="page-tree"><li><a class="page-link" href="../page/edge_instantiation.html">Edge instantiation</a> <q>When you ask the AI to make people happy, and it tiles the universe with the smallest objects that can be happy.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/environmental_goals.html">Environmental goals</a> <q>The problem of having an AI want outcomes that are out in the world, not just want direct sense events.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/identify_goal_concept.html">Goal-concept identification</a> <q>Figuring out how to say &quot;strawberry&quot; to an AI that you want to bring you strawberries (and not fake plastic strawberries, either).</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/happiness_maximizer.html">Happiness maximizer</a> <q>It is sometimes proposed that we build an AI intended to maximize human happiness.  (One early propo…</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/identify_causal_goals.html">Identifying causal goal concepts from sensory data</a> <q>If the intended goal is &quot;cure cancer&quot; and you show the AI healthy patients, it sees, say, a pattern of pixels on a webcam.  How do you get to a goal concept *about* the real patients?</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/ontology_identification.html">Ontology identification problem</a> <q>How do we link an agent's utility function to its model of the world, when we don't know what that model will look like?</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a><ul class="page-tree"><li><a class="page-link" href="../page/diamond_maximizer.html">Diamond maximizer</a> <q>How would you build an agent that made as much diamond material as possible, given vast computing power but an otherwise rich and complicated environment?</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/ontology_identification_technical_tutorial.html">Ontology identification problem: Technical tutorial</a> <q>Technical tutorial for ontology identification problem.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li></ul></li></ul></li><li><a class="page-link" href="../page/Vingean_reflection.html">Vingean reflection</a> <q>The problem of thinking about your future self when it's smarter than you.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a><ul class="page-tree"><li><a class="page-link" href="../page/reflective_consistency.html">Reflective consistency</a> <q>A decision system is reflectively consistent if it can approve of itself, or approve the construction of similar decision systems (as well as perhaps approving other decision systems too).</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/reflective_stability.html">Reflective stability</a> <q>Wanting to think the way you currently think, building other agents and self-modifications that think the same way.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a><ul class="page-tree"><li><a class="page-link" href="../page/preference_stability.html">Consequentialist preferences are reflectively stable by default</a> <q>Gandhi wouldn't take a pill that made him want to kill people, because he knows in that case more people will be murdered.  A paperclip maximizer doesn't want to stop maximizing paperclips.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/otherizer.html">Other-izing (wanted: new optimization idiom)</a> <q>Maximization isn't possible for bounded agents, and satisficing doesn't seem like enough.  What other kind of 'izing' might be good for realistic, bounded agents?</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/reflective_degree_of_freedom.html">Reflectively consistent degree of freedom</a> <q>When an instrumentally efficient, self-modifying AI can be like X or like X' in such a way that X wants to be X and X' wants to be X', that's a reflectively consistent degree of freedom.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a><ul class="page-tree"><li><a class="page-link" href="../page/humean_free_boundary.html">Humean degree of freedom</a> <q>A concept includes 'Humean degrees of freedom' when the intuitive borders of the human version of that concept depend on our values, making that concept less natural for AIs to learn.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/value_laden.html">Value-laden</a> <q>Cure cancer, but avoid any bad side effects?  Categorizing &quot;bad side effects&quot; requires knowing what's &quot;bad&quot;.  If an agent needs to load complex human goals to evaluate something, it's &quot;value-laden&quot;.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li></ul></li></ul></li><li><a class="page-link" href="../page/tiling_agents.html">Tiling agents theory</a> <q>The theory of self-modifying agents that build successors that are very similar to themselves, like repeating tiles on a tesselated plane.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/Vinge_principle.html">Vinge's Principle</a> <q>An agent building another agent must usually approve its design without knowing the agent's exact policy choices.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li></ul></li></ul></p></footer></body></html>