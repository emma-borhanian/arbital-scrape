<!DOCTYPE html><html><head><meta charset="utf-8"><title>For mitigating AI x-risk, an off-Earth colony would be about as useful as a warm scarf</title><link rel="stylesheet" type="text/css" href="../common.css"><link rel="stylesheet" type="text/css" href="../page-style.css"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  SVG: {EqnChunk: 50, EqnChunkFactor: 1.5, EqChunkDelay: 10, useFontCache: false, linebreaks: {automatic: true}},
  tex2jax: {
    inlineMath: [['$~$', '$~$']],
    displayMath: [['$$~$', '$~$$']],
    processEscapes: true,
    preview: 'none',
  },
  showProcessingMessages: false,
  messageStyle: 'none',
  // http://docs.mathjax.org/en/latest/config-files.html#the-tex-ams-svg-configuration-file
  jax: ["input/TeX","output/SVG", "output/PreviewHTML"],
  extensions: ["tex2jax.js","MathMenu.js","MathZoom.js", "fast-preview.js", "AssistiveMML.js", "a11y/accessibility-menu.js"],
  TeX: { extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"] }
});</script><script type="text/javascript" src="../MathJax-2.7.5/MathJax.js"></script><script type="text/javascript" src="../arbital-demo-bundle.js"></script><script type="text/javascript">window.addEventListener('DOMContentLoaded', e=>window.loadAllDemos())
</script></head><body><header><h1 class="title">For mitigating AI x-risk, an off-Earth colony would be about as useful as a warm scarf</h1><div class="page-info"><p class="metadata-link"><a href="../metadata/off_earth_warm_scarf.json.html">off_earth_warm_scarf.json</a></p><p class="arbital-url"><a href="https://arbital.com/p/off_earth_warm_scarf">https://arbital.com/p/off_earth_warm_scarf</a></p><p class="creator">by
 <a class="page-link" href="../page/EricRogstad.html">Eric Rogstad</a> Dec 22 2016 
updated
 Dec 22 2016</p></div><nav class="breadcrumbs"><nav class="breadcrumb" aria-label="Breadcrumb"><ul><li><a href="../index.html">Index</a></li><li>For mitigating AI x-risk, an off-Earth colony would be about as useful as a warm scarf</li></ul></nav></nav></header><hr><main><p>H/T to <a href="EliezerYudkowsky.html">Eliezer Yudkowsky</a> for <a href="https://www.facebook.com/robert.wiblin/posts/757111267835?comment_id=757158518145&reply_comment_id=757175214685">"warm scarf"</a></p></main><hr><section class="comments"><h2>Comments</h2><div class="comment"><p><a class="page-link" href="../page/EricBruylant.html">Eric Bruylant</a></p><p><p>Neat, I'm a contrarian. I guess I should explain why my credence is about 80% different from everyone else's :)</p>
<p>Obviously, being off earth would provide essentially no protection from a uFAI. It may, however. shift the odds of us getting an aligned AI in the first place.</p>
<p>Maybe this is because I'm taking this to mean more than most, I only think it helps if well-established and significant, but by my models both the rate of technological progress and ability to coordinate seems to be proportional to something like density of awesome people with a non-terrible incentive structure. Filtering by "paid half a million dollars to get to mars" and designing the incentive structure from scratch seems like an unusually good way to create a dense pocket of awesome people focused on important problems, in a way which is very hard to dilute.</p>
<p>I claim that if we have long enough timelines for a self-sustaining off-earth colony to be created, the first recursively self-improving AGI has a good chance of being built there. And that a strongly filtered group immersed in other hard challenges with and setting up decision-making infrastructure intentionally rather than working with all the normal civilization cruft are more likely to coordinate on safety than earth-based teams.</p>
<p>I do not expect timelines to be long enough that this is an option, so do not endorse this as a sane use of funding. But having an off-earth colony seems way, way more useful than a warm scarf.</p>
<p>I would agree with:</p>
<ul>
<li>There are currently much better ways to reduce AI x-risk than funding off-earth colonies. (~96%)</li>
<li>It is unlikely that off-earth colonies will be sufficiently established in time to mitigate AI x-risk. (~77%)</li>
</ul></p></div><div class="comment"><p><a class="page-link" href="../page/PaulChristiano.html">Paul Christiano</a></p><p><p>I don't think the existence of such a colony would directly mitigate AI risk, but it could help in the same way that e.g. improved governance or public discourse could help. I think that over the long term, off-Earth colonies have a significant positive expected effect on institution quality (analogously with European colonization of North America). And "warm scarf" sets the bar low.</p></p></div></section><footer><p class="tagged"><h2>Tagged</h2><span class="page-comma-list"><a class="page-link" href="../page/x_risk.html">Existential risk</a></span></p><p class="likes"><h2>Likes</h2><span class="page-comma-list"><a class="page-link" href="../page/AlexPear.html">Alex Pear</a>,
 <a class="page-link" href="../page/EricBruylant.html">Eric Bruylant</a>,
 <a class="page-link" href="../page/MiddleKek.html">Middle Kek</a></span></p><p class="reverse-related"><h2>Reverse Related</h2><ul class="page-list"><li><a class="page-link" href="../page/x_risk.html">Existential risk</a> <q>&gt; Because of accelerating technological progress, humankind may be rapidly approaching a critical phâ€¦</q> - <a class="page-link" href="../page/EricRogstad.html">Eric Rogstad</a></li></ul></p></footer></body></html>