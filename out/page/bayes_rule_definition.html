<!DOCTYPE html><html><head><meta charset="utf-8"><title>Bayes' rule: Definition</title><link rel="stylesheet" type="text/css" href="../common.css"><link rel="stylesheet" type="text/css" href="../page-style.css"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  SVG: {EqnChunk: 50, EqnChunkFactor: 1.5, EqChunkDelay: 10, useFontCache: false, linebreaks: {automatic: true}},
  tex2jax: {
    inlineMath: [['$~$', '$~$']],
    displayMath: [['$$~$', '$~$$']],
    processEscapes: true,
    preview: 'none',
  },
  showProcessingMessages: false,
  messageStyle: 'none',
  // http://docs.mathjax.org/en/latest/config-files.html#the-tex-ams-svg-configuration-file
  jax: ["input/TeX","output/SVG", "output/PreviewHTML"],
  extensions: ["tex2jax.js","MathMenu.js","MathZoom.js", "fast-preview.js", "AssistiveMML.js", "a11y/accessibility-menu.js"],
  TeX: { extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"] }
});</script><script type="text/javascript" src="../MathJax-2.7.5/MathJax.js"></script><script type="text/javascript" src="../arbital-demo-bundle.js"></script><script type="text/javascript">window.addEventListener('DOMContentLoaded', e=>window.loadAllDemos())
</script></head><body><header><h1 class="title">Bayes' rule: Definition</h1><div class="page-info"><p class="metadata-link"><a href="../metadata/bayes_rule_definition.json.html">bayes_rule_definition.json</a></p><p class="arbital-url"><a href="https://arbital.com/p/bayes_rule_definition">https://arbital.com/p/bayes_rule_definition</a></p><p class="creator">by
 <a class="page-link" href="../page/NateSoares.html">Nate Soares</a> Jul 6 2016 
updated
 Oct 4 2016</p></div><nav class="breadcrumbs"><nav class="breadcrumb" aria-label="Breadcrumb"><ul><li><a href="../index.html">Index</a></li><li>Bayes' rule: Definition</li></ul></nav><nav class="breadcrumb" aria-label="Breadcrumb"><ul><li><a href="math.html">Mathematics</a></li><li><a href="probability_theory.html">Probability theory</a></li><li><a href="bayes_reasoning.html">Bayesian reasoning</a></li><li><a href="bayes_rule.html">Bayes' rule</a></li><li>…</li></ul></nav><nav class="breadcrumb" aria-label="Breadcrumb"><ul><li><a href="rationality.html">Rationality</a></li><li><a href="probability_theory.html">Probability theory</a></li><li><a href="bayes_reasoning.html">Bayesian reasoning</a></li><li><a href="bayes_rule.html">Bayes' rule</a></li><li>…</li></ul></nav></nav></header><hr><main><p>Bayes' rule is the mathematics of <a href="probability.html">probability theory</a> governing how to update your beliefs in the light of new evidence.</p>
<p>[toc:]</p>
<h2 id="ahrefbayes_probability_notationhtmlnotationa"><a href="bayes_probability_notation.html">Notation</a></h2>
<p>In much of what follows, we'll use the following <a href="bayes_probability_notation.html">notation</a>:</p>
<ul>
<li>Let the hypotheses being considered be $~$H_1$~$ and $~$H_2$~$.</li>
<li>Let the evidence observed be $~$e_0.$~$</li>
<li>Let $~$\mathbb P(H_i)$~$ denote the <a href="prior_probability.html">prior probability</a> of $~$H_i$~$ before observing the evidence.</li>
<li>Let the <a href="conditional_probability.html">conditional probability</a> $~$\mathbb P(e_0\mid H_i)$~$ denote the <a href="relative_likelihood.html">likelihood</a> of observing evidence $~$e_0$~$ assuming $~$H_i$~$ to be true.</li>
<li>Let the <a href="conditional_probability.html">conditional probability</a> $~$\mathbb P(H_i\mid e_0)$~$ denote the <a href="posterior_probability.html">posterior probability</a> of $~$H_i$~$ after observing $~$e_0.$~$</li>
</ul>
<h2 id="ahrefbayes_rule_oddshtmloddsaahrefbayes_rule_proportionalhtmlproportionalaform"><a href="bayes_rule_odds.html">Odds</a>/<a href="bayes_rule_proportional.html">proportional</a> form</h2>
<p>Bayes' rule in the <a href="bayes_rule_odds.html">odds form</a> or <a href="bayes_rule_proportional.html">proportional form</a> states:</p>
<p>$$~$\dfrac{\mathbb P(H_1)}{\mathbb P(H_2)} \times \dfrac{\mathbb P(e_0\mid H_1)}{\mathbb P(e_0\mid H_2)} = \dfrac{\mathbb P(H_1\mid e_0)}{\mathbb P(H_2\mid e_0)}$~$$</p>
<p>In other words, the <a href="prior_probability.html">prior</a> <a href="odds.html">odds</a> times the <a href="relative_likelihood.html">likelihood ratio</a> yield the <a href="posterior_probability.html">posterior</a> odds.  <a href="normalize_probabilities.html">Normalizing</a> these odds will then yield the posterior probabilities.</p>
<p>In <a href="bayes_rule_proportional.html">other other words</a>:  If you initially think $~$h_i$~$ is $~$\alpha$~$ times as probable as $~$h_k$~$, and then see evidence that you're $~$\beta$~$ times as likely to see if $~$h_i$~$ is true as if $~$h_k$~$ is true, you should update to thinking that $~$h_i$~$ is $~$\alpha \cdot \beta$~$ times as probable as $~$h_k.$~$</p>
<p>Suppose that Professor Plum and Miss Scarlet are two suspects in a murder, and that we start out thinking that Professor Plum is twice as likely to have committed the murder as Miss Scarlet (<a href="prior_probability.html">prior</a> <a href="odds.html">odds</a> of 2 : 1).  We then discover that the victim was poisoned.  We think that Professor Plum is around one-fourth as likely to use poison as Miss Scarlet (<a href="relative_likelihood.html">likelihood ratio</a> of 1 : 4).  Then after observing the victim was poisoned, we should think Plum is around half as likely to have committed the murder as Scarlet: $~$2 \times \dfrac{1}{4} = \dfrac{1}{2}.$~$  This reflects <a href="posterior_probability.html">posterior</a> odds of 1 : 2, or a posterior probability of 1/3, that Professor Plum did the deed.</p>
<h2 id="ahrefbayes_rule_proofhtmlproofa"><a href="bayes_rule_proof.html">Proof</a></h2>
<p>The <a href="bayes_rule_proof.html">proof of Bayes&#39; rule</a> is by the definition of <a href="conditional_probability.html">conditional probability</a> $~$\mathbb P(X\wedge Y) = \mathbb P(X\mid Y) \cdot \mathbb P(Y):$~$</p>
<p>$$~$
\dfrac{\mathbb P(H_i)}{\mathbb P(H_j)} \times  \dfrac{\mathbb P(e\mid H_i)}{\mathbb P(e\mid H_j)}
= \dfrac{\mathbb P(e \wedge H_i)}{\mathbb P(e \wedge H_j)}
= \dfrac{\mathbb P(e \wedge H_i) / \mathbb P(e)}{\mathbb P(e \wedge H_j) / \mathbb P(e)}
= \dfrac{\mathbb P(H_i\mid e)}{\mathbb P(H_j\mid e)}
$~$$</p>
<h2 id="ahrefbayes_log_oddshtmllogoddsforma"><a href="bayes_log_odds.html">Log odds form</a></h2>
<p>The <a href="bayes_log_odds.html">log odds form of Bayes&#39; rule</a> states:</p>
<p>$$~$\log \left ( \dfrac
   {\mathbb P(H_i)}
   {\mathbb P(H_j)}
\right )
+
\log \left ( \dfrac
   {\mathbb P(e\mid H_i)}
   {\mathbb P(e\mid H_j)}
\right ) 
 =
\log \left ( \dfrac
   {\mathbb P(H_i\mid e)}
   {\mathbb P(H_j\mid e)}
\right )
$~$$</p>
<p>E.g.:  "A study of Chinese blood donors found that roughly 1 in 100,000 of them had HIV (as determined by a very reliable gold-standard test). The non-gold-standard test used for initial screening had a sensitivity of 99.7% and a specificity of 99.8%, meaning that it was 500 times as likely to return positive for infected as non-infected patients."  Then our prior belief is -5 orders of magnitude against HIV, and if we then observe a positive test result, this is evidence of strength +2.7 orders of magnitude for HIV.  Our posterior belief is -2.3 orders of magnitude, or odds of less than 1 to a 100, against HIV.</p>
<p>In log odds form, the same <a href="bayes_strength_of_evidence.html">strength of evidence</a> (log <a href="relative_likelihood.html">likelihood ratio</a>) always <a href="bayes_log_odds.html">moves us the same additive distance</a> along a line representing strength of belief (also in log odds).  If we measured distance in probabilities, then the same 2 : 1 likelihood ratio might move us a different distance along the probability line depending on whether we started with prior 10% probability or 50% probability.</p>
<h2 id="visualizations">Visualizations</h2>
<p>Graphical of visualizing Bayes' rule include <a href="bayes_waterfall_diagram.html">frequency diagrams, the waterfall visualization</a>, the <a href="bayes_rule_proportional.html">spotlight visualization</a>, the <a href="bayes_log_odds.html">magnet visualization</a>, and the <a href="bayes_rule_proof.html">Venn diagram for the proof</a>.</p>
<h2 id="examples">Examples</h2>
<p>Examples of Bayes' rule may be found <a href="bayes_rule_examples.html">here</a>.</p>
<h2 id="ahrefbayes_rule_multiplehtmlmultiplehypothesesandupdatesa"><a href="bayes_rule_multiple.html">Multiple hypotheses and updates</a></h2>
<p>The <a href="bayes_rule_odds.html">odds form of Bayes&#39; rule</a> works for odds ratios between more than two hypotheses, and applying multiple pieces of evidence.  Suppose there's a bathtub full of coins.  1/2 of the coins are "fair" and have a 50% probability of producing heads on each coinflip; 1/3 of the coins produce 25% heads; and 1/6 produce 75% heads.  You pull out a coin at random, flip it 3 times, and get the result HTH.  You may legitimately calculate:</p>
<p>$$~$\begin{array}{rll}
(1/2 : 1/3 : 1/6) \cong &amp; (3 : 2 : 1) &amp; \\
\times &amp; (2 : 1 : 3) &amp; \\
\times &amp; (2 : 3 : 1) &amp; \\
\times &amp; (2 : 1 : 3) &amp; \\
= &amp; (24 : 6 : 9) &amp; \cong (8 : 2 : 3)
\end{array}$~$$</p>
<p>Since multiple pieces of evidence may not be [conditional_independence conditionally independent] from one another, it is important to be aware of the [naive_bayes_assumption Naive Bayes assumption] and whether you are making it.</p>
<h2 id="ahrefbayes_rule_probabilityhtmlprobabilityforma"><a href="bayes_rule_probability.html">Probability form</a></h2>
<p>As a formula for a single probability $~$\mathbb P(H_i\mid e),$~$ Bayes' rule states:</p>
<p>$$~$\mathbb P(H_i\mid e) = \dfrac{\mathbb P(e\mid H_i) \cdot \mathbb P(H_i)}{\sum_k \mathbb P(e\mid H_k) \cdot \mathbb P(H_k)}$~$$</p>
<h2 id="ahrefbayes_rule_functionalhtmlfunctionalforma"><a href="bayes_rule_functional.html">Functional form</a></h2>
<p>In <a href="bayes_rule_functional.html">functional form</a>, Bayes' rule states:</p>
<p>$$~$\mathbb P(\mathbf{H}\mid e) \propto \mathbb P(e\mid \mathbf{H}) \cdot \mathbb P(\mathbf{H}).$~$$</p>
<p>The posterior probability function over hypotheses given the evidence, is <em>proportional</em> to the likelihood function from the evidence to those hypotheses, times the prior probability function over those hypotheses.</p>
<p>Since posterior probabilities over <a href="exclusive_exhaustive.html">mutually exclusive and exhaustive</a> possibilities must sum to $~$1,$~$ <a href="normalize_probabilities.html">normalizing</a> the product of the likelihood function and prior probability function will yield the exact posterior probability function.</p></main><hr><section class="comments"><h2>Comments</h2><div class="comment"><p><a class="page-link" href="../page/EricRogstad.html">Eric Rogstad</a></p><p><blockquote class="comment-context">This is one of the key insights of Bayes' rule: Given what you knew, and what you saw, the maximally accurate state of belief for you to be in is completely pinned down, and while <mark>it's</mark> quite difficult to find in principle, we know how to find it in practice\. Humanity has uncovered some of the laws of reasoning, and they are captured by Bayes' rule\. If you want your beliefs to become more as you observe the world, Bayes' rule gives some hints about what you need to do\.</blockquote>
<p>It's not totally clear what the antecedent of this "it's" is. (Because "it's" often means "it is the case that")</p></p></div><div class="comment"><p><a class="page-link" href="../page/EricRogstad.html">Eric Rogstad</a></p><p><blockquote class="comment-context">This is one of the key insights of Bayes' rule: Given what you knew, and what you saw, the maximally accurate state of belief for you to be in is completely pinned down, and while it's quite difficult to find in principle, we know how to find it in practice\. Humanity has uncovered some of the laws of reasoning, and they are captured by Bayes' rule\. If you want your beliefs to become more as you observe the world, Bayes' rule gives some hints about what you need to do\.</blockquote>
<p>Too Eliezer-voice. What would Sal Khan say?</p></p></div></section><footer><p class="tagged"><h2>Tagged</h2><span class="page-comma-list"><a class="page-link" href="../page/c_class_meta_tag.html">C-Class</a></span></p><p class="all-creators"><h2>All Creators</h2><span class="page-comma-list"><a class="page-link" href="../page/AlexeiAndreev.html">Alexei Andreev</a>,
 <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a>,
 <a class="page-link" href="../page/EricBruylant.html">Eric Bruylant</a>,
 <a class="page-link" href="../page/NateSoares.html">Nate Soares</a></span></p><p class="likes"><h2>Likes</h2><span class="page-comma-list"><a class="page-link" href="../page/AndreiAlexandru.html">Andrei Alexandru</a>,
 <a class="page-link" href="../page/EricBruylant.html">Eric Bruylant</a>,
 <a class="page-link" href="../page/JOSEPHCASSILLY.html">JOSEPH CASSILLY</a>,
 <a class="page-link" href="../page/MichaelKillinger.html">Michael Killinger</a>,
 <a class="page-link" href="../page/NateSoares.html">Nate Soares</a></span></p><p class="reverse-related"><h2>Reverse Related</h2><ul class="page-list"><li><a class="page-link" href="../page/c_class_meta_tag.html">C-Class</a> <q>This page has substantial content, but may not thoroughly cover the topic, may not meet style and prose standards, or may not explain the concept in a way the target audience will reliably understand.</q> - <a class="page-link" href="../page/EricBruylant.html">Eric Bruylant</a></li></ul></p></footer></body></html>