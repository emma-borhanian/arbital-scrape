<!DOCTYPE html><html><head><meta charset="utf-8"><title>Evolution Strategies and Reinforcement Learning</title><link rel="stylesheet" type="text/css" href="../common.css"><link rel="stylesheet" type="text/css" href="../page-style.css"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  SVG: {EqnChunk: 50, EqnChunkFactor: 1.5, EqChunkDelay: 10, useFontCache: false, linebreaks: {automatic: true}},
  tex2jax: {
    inlineMath: [['$~$', '$~$']],
    displayMath: [['$$~$', '$~$$']],
    processEscapes: true,
    preview: 'none',
  },
  showProcessingMessages: false,
  messageStyle: 'none',
  // http://docs.mathjax.org/en/latest/config-files.html#the-tex-ams-svg-configuration-file
  jax: ["input/TeX","output/SVG", "output/PreviewHTML"],
  extensions: ["tex2jax.js","MathMenu.js","MathZoom.js", "fast-preview.js", "AssistiveMML.js", "a11y/accessibility-menu.js"],
  TeX: { extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"] }
});</script><script type="text/javascript" src="../MathJax-2.7.5/MathJax.js"></script><script type="text/javascript" src="../arbital-demo-bundle.js"></script><script type="text/javascript">window.addEventListener('DOMContentLoaded', e=>window.loadAllDemos())
</script></head><body><header><h1 class="title">Evolution Strategies and Reinforcement Learning</h1><div class="page-info"><p class="metadata-link"><a href="../metadata/evolution_strategies.json.html">evolution_strategies.json</a></p><p class="arbital-url"><a href="https://arbital.com/p/evolution_strategies">https://arbital.com/p/evolution_strategies</a></p><p class="creator">by
 <a class="page-link" href="../page/AshtonHellwig.html">Ashton Hellwig</a> Apr 19 2017</p></div><p class="clickbait">Evolution strategies as a simplified implementation of reinforcement learning</p><nav class="breadcrumbs"><nav class="breadcrumb" aria-label="Breadcrumb"><ul><li><a href="../index.html">Index</a></li><li>Evolution Strategies and Reinforcement Learning</li></ul></nav></nav></header><hr><main><p><a href="1qm.html">Reinforcement learning</a> is a machine learning task that involves an agent interacting with a dynamic environment to find the correct parameters (typically out of 1,000,000) in a policy function that best links the input with the output, with the key difference against supervised learning that the agent gains feedback as either punishments (for less-than-optimal behavior) and rewards (which increase based on efficacy). The output suggests a way to improve the agent, rather than a set of correct initial <a href="vector_arithmetic.html">vector variables</a>, which is then iterated into a new process to collect new episodes of interaction and further update the agent <a href="https://blog.openai.com/evolution-strategies/">(Salimans et al.)</a>. According to <a href="the_plan_orgs_and_people.html">OpenAI</a>, the older form of what is now known as RL, called evolution strategies (ES), perhaps rivals the performance of standard RL considering there is no agent or environment, in addition to being more easily scalable in a distributed system <a href="https://blog.openai.com/evolution-strategies/">(Salimans et al.)</a>. The ES <a href="Kolmogorov_complexity.html">algorithm</a> is most closely related to a "guess and check" environment. Reinforcement learning is being applied to cases such as self-driving cars and video game programming.</p></main><hr><footer></footer></body></html>