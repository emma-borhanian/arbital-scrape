<!DOCTYPE html><html><head><meta charset="utf-8"><title>Rob Bensinger</title><link rel="stylesheet" type="text/css" href="../common.css"><link rel="stylesheet" type="text/css" href="../page-style.css"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  SVG: {EqnChunk: 50, EqnChunkFactor: 1.5, EqChunkDelay: 10, useFontCache: false, linebreaks: {automatic: true}},
  tex2jax: {
    inlineMath: [['$~$', '$~$']],
    displayMath: [['$$~$', '$~$$']],
    processEscapes: true,
    preview: 'none',
  },
  showProcessingMessages: false,
  messageStyle: 'none',
  // http://docs.mathjax.org/en/latest/config-files.html#the-tex-ams-svg-configuration-file
  jax: ["input/TeX","output/SVG", "output/PreviewHTML"],
  extensions: ["tex2jax.js","MathMenu.js","MathZoom.js", "fast-preview.js", "AssistiveMML.js", "a11y/accessibility-menu.js"],
  TeX: { extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"] }
});</script><script type="text/javascript" src="../MathJax-2.7.5/MathJax.js"></script><script type="text/javascript" src="../arbital-demo-bundle.js"></script><script type="text/javascript">window.addEventListener('DOMContentLoaded', e=>window.loadAllDemos())
</script></head><body><header><h1 class="title">Rob Bensinger</h1><div class="page-info"><p class="metadata-link"><a href="../metadata/RobBensinger2.json.html">RobBensinger2.json</a></p><p class="arbital-url"><a href="https://arbital.com/p/RobBensinger2">https://arbital.com/p/RobBensinger2</a></p><p class="creator">by
 <a class="page-link" href="../page/RobBensinger2.html">Rob Bensinger</a> Apr 7 2016</p></div><p class="clickbait">Automatically generated group for Rob Bensinger</p><nav class="breadcrumbs"><nav class="breadcrumb" aria-label="Breadcrumb"><ul><li><a href="../index.html">Index</a></li><li>Rob Bensinger</li></ul></nav></nav></header><hr><main><p>[summary: Nothing here yet.]</p>
<p>Automatically generated page for "Rob Bensinger" user.
If you are the owner, click <a href="http://arbital.com/edit/345">here to edit</a>.</p></main><hr><footer><hr><p class="created"><h2>Created</h2><h3 id="createdgroup">group</h3><ul class="page-list"><li><a class="page-link" href="../page/RobBensinger2.html">Rob Bensinger</a></li></ul><h3 id="createdcomment">comment</h3><ul class="page-list"><li><a class="page-link" href="../page/6y9.html"><q>I think this is a factor in whether I feel motivated to donate, but because of unconscious associati…</q></a></li><li><a class="page-link" href="../page/6yb.html"><q>I think this is a good idea for specific 'kickstarter' projects by organizations. I don't think nonp…</q></a></li><li><a class="page-link" href="../page/6yq.html"><q>Yes. In particular, the first milestone or two should probably be small (assuming there are no assoc…</q></a></li></ul></p><p class="edited"><h2>Edited</h2><ul class="page-list"><li><a class="page-link" href="../page/AI_safety_mindset.html">AI safety mindset</a> <q>Asking how AI designs could go wrong, instead of imagining them going right.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/goodharts_curse.html">Goodhart's Curse</a> <q>The Optimizer's Curse meets Goodhart's Law.  For example, if our values are V, and an AI's utility function U is a proxy for V, optimizing for high U seeks out 'errors'--that is, high values of U - V.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/updated_deference.html">Problem of fully updated deference</a> <q>Why moral uncertainty doesn't stop an AI from defending its off-switch.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/relevant_limited_AI.html">Relevant limited AI</a> <q>Can we have a limited AI, that's nonetheless relevant?</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/understandability_principle.html">Understandability principle</a> <q>The more you understand what the heck is going on inside your AI, the safer you are.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/waiting_to_donate.html">Why waiting to donate harms charities</a> <q>A blog post explaining the potential reasons why someone would choose to wait to donate and how that leads to suboptimal outcomes for the charity.</q> - <a class="page-link" href="../page/AlexeiAndreev.html">Alexei Andreev</a></li><li><a class="page-link" href="../page/ZF_provability_oracle.html">Zermelo-Fraenkel provability oracle</a> <q>We might be able to build a system that can safely inform us that a theorem has a proof in set theory, but we can't see how to use that capability to save the world.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li></ul></p></footer></body></html>