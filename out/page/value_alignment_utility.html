<!DOCTYPE html><html><head><meta charset="utf-8"><title>Utility</title><link rel="stylesheet" type="text/css" href="../common.css"><link rel="stylesheet" type="text/css" href="../page-style.css"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  SVG: {EqnChunk: 50, EqnChunkFactor: 1.5, EqChunkDelay: 10, useFontCache: false, linebreaks: {automatic: true}},
  tex2jax: {
    inlineMath: [['$~$', '$~$']],
    displayMath: [['$$~$', '$~$$']],
    processEscapes: true,
    preview: 'none',
  },
  showProcessingMessages: false,
  messageStyle: 'none',
  // http://docs.mathjax.org/en/latest/config-files.html#the-tex-ams-svg-configuration-file
  jax: ["input/TeX","output/SVG", "output/PreviewHTML"],
  extensions: ["tex2jax.js","MathMenu.js","MathZoom.js", "fast-preview.js", "AssistiveMML.js", "a11y/accessibility-menu.js"],
  TeX: { extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"] }
});</script><script type="text/javascript" src="../MathJax-2.7.5/MathJax.js"></script><script type="text/javascript" src="../arbital-demo-bundle.js"></script><script type="text/javascript">window.addEventListener('DOMContentLoaded', e=>window.loadAllDemos())
</script></head><body><header><h1 class="title">Utility</h1><div class="page-info"><p class="metadata-link"><a href="../metadata/value_alignment_utility.json.html">value_alignment_utility.json</a></p><p class="arbital-url"><a href="https://arbital.com/p/value_alignment_utility">https://arbital.com/p/value_alignment_utility</a></p><p class="creator">by
 <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a> Jul 15 2015 
updated
 Dec 17 2015</p></div><p class="clickbait">What is &quot;utility&quot; in the context of Value Alignment Theory?</p><nav class="breadcrumbs"><nav class="breadcrumb" aria-label="Breadcrumb"><ul><li><a href="../index.html">Index</a></li><li>Utility</li></ul></nav><nav class="breadcrumb" aria-label="Breadcrumb"><ul><li><a href="ai_alignment.html">AI alignment</a></li><li><a href="5b.html">Linguistic conventions in value alignment</a></li><li>â€¦</li></ul></nav></nav></header><hr><main><p>In the context of value alignment theory, 'Utility' always refers to a goal held by an artificial agent.  It further implies that the agent is a <a href="consequentialist.html">consequentialist</a>; that the agent has [ probabilistic] beliefs about the consequences of its actions; that the agent has a quantitative notion of "how much better" one outcome is than another and the relative size of different intervals of betterness; and that the agent can therefore, e.g., trade off large probabilities of a small utility gain against small probabilities of a large utility loss.</p>
<p>True coherence in the sense of a [ von-Neumann Morgenstern utility function] may be out of reach for [ bounded agents], but the term 'utility' may also be used for the [ bounded analogues] of such decision-making, provided that quantitative relative intervals of preferability are being combined with quantitative degrees of belief to yield decisions.</p>
<p>Utility is explicitly not assumed to be normative.  E.g., if speaking of a <a href="paperclip_maximizer.html">paperclip maximizer</a>, we will say that an outcome has higher utility iff it contains more paperclips.</p>
<p>Humans should not be said (without further justification) to have utilities over complicated outcomes.  On the mainstream view from psychology, humans are inconsistent enough that it would take additional assumptions to translate our psychology into a coherent utility function.  E.g., we may differently value the interval between two outcomes depending on whether the interval is framed as a 'gain' or a 'loss'. For the things humans do or should want, see the special use of the word <a href="value_alignment_value.html">&#39;value&#39;</a>.  For a general disambiguation page on words used to talk about human and AI wants, see <a href="5b.html">Linguistic conventions in value alignment</a>.</p>
<p>On some <a href="value_alignment_value.html">construals of value</a>, e.g. <a href="71.html">reflective equilibrium</a>, this construal may imply that the true values form a coherent utility function.  Nonetheless, by convention, we will not speak of value as a utility unless it has been spelled out that, e.g., the value in question has been assumed to be a reflective equilibrium.</p>
<p>Multiple agents with different utility functions should not be said (without further exposition) to have a collective utility function over outcomes, since at present, there is no accepted [ canonical way to aggregate utility functions][todo: link loudness problem].</p></main><hr><footer><p class="tagged"><h2>Tagged</h2><span class="page-comma-list"><a class="page-link" href="../page/definition_meta_tag.html">Definition</a>,
 <a class="page-link" href="../page/value_alignment_glossary.html">Glossary (Value Alignment Theory)</a></span></p><p class="all-creators"><h2>All Creators</h2><span class="page-comma-list"><a class="page-link" href="../page/AlexeiAndreev.html">Alexei Andreev</a>,
 <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></span></p><p class="likes"><h2>Likes</h2><span class="page-comma-list"><a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a>,
 <a class="page-link" href="../page/EricRogstad.html">Eric Rogstad</a></span></p><p class="reverse-related"><h2>Reverse Related</h2><ul class="page-list"><li><a class="page-link" href="../page/definition_meta_tag.html">Definition</a> <q>Meta tag used to mark pages that strictly define a particular term or phrase.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/value_alignment_glossary.html">Glossary (Value Alignment Theory)</a> <q>Words that have a special meaning in the context of creating nice AIs.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li></ul></p></footer></body></html>