<!DOCTYPE html><html><head><meta charset="utf-8"><title>Principles in AI alignment</title><link rel="stylesheet" type="text/css" href="../common.css"><link rel="stylesheet" type="text/css" href="../page-style.css"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  SVG: {EqnChunk: 50, EqnChunkFactor: 1.5, EqChunkDelay: 10, useFontCache: false, linebreaks: {automatic: true}},
  tex2jax: {
    inlineMath: [['$~$', '$~$']],
    displayMath: [['$$~$', '$~$$']],
    processEscapes: true,
    preview: 'none',
  },
  showProcessingMessages: false,
  messageStyle: 'none',
  // http://docs.mathjax.org/en/latest/config-files.html#the-tex-ams-svg-configuration-file
  jax: ["input/TeX","output/SVG", "output/PreviewHTML"],
  extensions: ["tex2jax.js","MathMenu.js","MathZoom.js", "fast-preview.js", "AssistiveMML.js", "a11y/accessibility-menu.js"],
  TeX: { extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"] }
});</script><script type="text/javascript" src="../MathJax-2.7.5/MathJax.js"></script><script type="text/javascript" src="../arbital-demo-bundle.js"></script><script type="text/javascript">window.addEventListener('DOMContentLoaded', e=>window.loadAllDemos())
</script></head><body><header><h1 class="title">Principles in AI alignment</h1><div class="page-info"><p class="metadata-link"><a href="../metadata/alignment_principle.json.html">alignment_principle.json</a></p><p class="arbital-url"><a href="https://arbital.com/p/alignment_principle">https://arbital.com/p/alignment_principle</a></p><p class="creator">by
 <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a> Feb 16 2017 
updated
 Feb 16 2017</p></div><p class="clickbait">A 'principle' of AI alignment is a very general design goal like 'understand what the heck is going on inside the AI' that has informed a wide set of specific design proposals.</p><nav class="breadcrumbs"><nav class="breadcrumb" aria-label="Breadcrumb"><ul><li><a href="../index.html">Index</a></li><li>Principles in AI alignment</li></ul></nav><nav class="breadcrumb" aria-label="Breadcrumb"><ul><li><a href="ai_alignment.html">AI alignment</a></li><li>â€¦</li></ul></nav></nav></header><hr><main><p>[summary: A 'principle' of <a href="ai_alignment.html">AI alignment</a> is something we want in a broad sense for the whole AI, which has informed narrower design proposals for particular parts or aspects of the AI.</p>
<p>Examples:</p>
<ul>
<li>The <strong><a href="nonadversarial.html">Non-adversarial principle</a></strong> says that the AI should never be searching for a way to defeat our safety measures or do something else we don't want, even if we <em>think</em> this search will come up empty; it's just the wrong thing for us to program computing power to do.</li>
<li>This informs the proposal of <a href="corrigibility.html">Corrigibility</a>, subproposal <a href="utility_indifference.html">Utility indifference</a>: if we build a <a href="shutdown_problem.html">suspend button</a> into the AI, we need to make sure the AI experiences no <a href="instrumental_pressure.html">instrumental pressure</a> to <a href="no_coffee_if_dead.html">disable the suspend button</a>.</li>
<li>The <strong><a href="minimality_principle.html">Minimality principle</a></strong> says that when we are building the first AGI, we should try to do as little as possible, using the least dangerous cognitive computations possible, in order to prevent the default outcome of the world otherwise being destroyed by the second AGI.</li>
<li>This informs the proposal of <a href="soft_optimizer.html">Mild optimization</a> and <a href="task_goal.html">taskishness</a>:  We are safer if all goals and subgoals of the AI are formulated in such a way that they can be achieved as greatly as preferable using a bounded amount of effort, and the AI only exerts enough effort to do that.]</li>
</ul>
<p>A 'principle' of <a href="ai_alignment.html">AI alignment</a> is something we want in a broad sense for the whole AI, which has informed narrower design proposals for particular parts or aspects of the AI.</p>
<p>For example:</p>
<ul>
<li>The <strong><a href="nonadversarial.html">Non-adversarial principle</a></strong> says that the AI should never be searching for a way to defeat our safety measures or do something else we don't want, even if we <em>think</em> this search will come up empty; it's just the wrong thing for us to program computing power to do.</li>
<li>This informs the proposal of <a href="value_alignment_problem.html">Value alignment problem</a>: we ought to build an AI that wants to attain the class of outcomes we want to see.</li>
<li>This informs the proposal of <a href="corrigibility.html">Corrigibility</a>, subproposal <a href="utility_indifference.html">Utility indifference</a>: if we build a <a href="shutdown_problem.html">suspend button</a> into the AI, we need to make sure the AI experiences no <a href="instrumental_pressure.html">instrumental pressure</a> to <a href="no_coffee_if_dead.html">disable the suspend button</a>.</li>
<li>The <strong><a href="minimality_principle.html">Minimality principle</a></strong> says that when we are building the first aligned AGI, we should try to do as little as possible, using the least dangerous cognitive computations possible, that is necessary in order to prevent the default outcome of the world being destroyed by the first unaligned AGI.</li>
<li>This informs the proposal of <a href="soft_optimizer.html">Mild optimization</a> and <a href="task_goal.html">Taskishness</a>:  We are safer if all goals and subgoals of the AI are formulated in such a way that they can be achieved as greatly as preferable using a bounded amount of effort, and the AI only exerts enough effort to do that.</li>
<li>This informs the proposal of <a href="behaviorist.html">Behaviorism</a>:  It seems like there are some <a href="pivotal.html">pivotal-act</a> proposals that don't require the AI to understand and predict humans in great detail, just to master engineering; and it seems like we can head off multiple thorny problems by not having the AI trying to model humans or other minds in as much detail as possible.</li>
</ul>
<p>Please be <a href="guarded_definition.html">guarded</a> about declaring things to be 'principles' unless they have already informed more than one specific design proposal and more than one person thinks they are a good idea.  You could call them 'proposed principles' and post them under your own domain if you personally think they are a good idea.  There are a <em>lot</em> of possible 'broad design wishes', or things that people think are 'broad design wishes', and the principles that have actually already informed specific design proposals would otherwise get lost in the crowd.</p></main><hr><footer><p class="children"><h2>Children</h2><ul class="page-tree"><li><a class="page-link" href="../page/minimality_principle.html">Minimality principle</a> <q>The first AGI ever built should save the world in a way that requires the least amount of the least dangerous cognition.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/nonadversarial.html">Non-adversarial principle</a> <q>At no point in constructing an Artificial General Intelligence should we construct a computation that tries to hurt us, and then try to stop it from hurting us.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a><ul class="page-tree"><li><a class="page-link" href="../page/direct_limit_oppose.html">Directing, vs. limiting, vs. opposing</a> <q>Getting the AI to compute the right action in a domain; versus getting the AI to not compute at all in an unsafe domain; versus trying to prevent the AI from acting successfully.  (Prefer 1 &amp; 2.)</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/cognitive_alignment.html">Generalized principle of cognitive alignment</a> <q>When we're asking how we want the AI to think about an alignment problem, one source of inspiration is trying to have the AI mirror our own thoughts about that problem.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/niceness_defense.html">Niceness is the first line of defense</a> <q>The *first* line of defense in dealing with any partially superhuman AI system advanced enough to possibly be dangerous is that it does not *want* to hurt you or defeat your safety measures.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/omni_test.html">Omnipotence test for AI safety</a> <q>Would your AI produce disastrous outcomes if it suddenly gained omnipotence and omniscience? If so, why did you program something that *wants* to hurt you and is held back only by lacking the power?</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/nonadversarial_safety.html">The AI must tolerate your safety measures</a> <q>A corollary of the nonadversarial principle is that &quot;The AI must tolerate your safety measures.&quot;</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li></ul></li><li><a class="page-link" href="../page/hyperexistential_separation.html">Separation from hyperexistential risk</a> <q>The AI should be widely separated in the design space from any AI that would constitute a &quot;hyperexistential risk&quot; (anything worse than death).</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/understandability_principle.html">Understandability principle</a> <q>The more you understand what the heck is going on inside your AI, the safer you are.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a><ul class="page-tree"><li><a class="page-link" href="../page/effability.html">Effability principle</a> <q>You are safer the more you understand the inner structure of how your AI thinks; the better you can describe the relation of smaller pieces of the AI's thought process.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li></ul></li></ul></p></footer></body></html>