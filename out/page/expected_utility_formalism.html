<!DOCTYPE html><html><head><meta charset="utf-8"><title>Expected utility formalism</title><link rel="stylesheet" type="text/css" href="../common.css"><link rel="stylesheet" type="text/css" href="../page-style.css"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  SVG: {EqnChunk: 50, EqnChunkFactor: 1.5, EqChunkDelay: 10, useFontCache: false, linebreaks: {automatic: true}},
  tex2jax: {
    inlineMath: [['$~$', '$~$']],
    displayMath: [['$$~$', '$~$$']],
    processEscapes: true,
    preview: 'none',
  },
  showProcessingMessages: false,
  messageStyle: 'none',
  // http://docs.mathjax.org/en/latest/config-files.html#the-tex-ams-svg-configuration-file
  jax: ["input/TeX","output/SVG", "output/PreviewHTML"],
  extensions: ["tex2jax.js","MathMenu.js","MathZoom.js", "fast-preview.js", "AssistiveMML.js", "a11y/accessibility-menu.js"],
  TeX: { extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"] }
});</script><script type="text/javascript" src="../MathJax-2.7.5/MathJax.js"></script><script type="text/javascript" src="../arbital-demo-bundle.js"></script><script type="text/javascript">window.addEventListener('DOMContentLoaded', e=>window.loadAllDemos())
</script></head><body><header><h1 class="title">Expected utility formalism</h1><div class="page-info"><p class="metadata-link"><a href="../metadata/expected_utility_formalism.json.html">expected_utility_formalism.json</a></p><p class="arbital-url"><a href="https://arbital.com/p/expected_utility_formalism">https://arbital.com/p/expected_utility_formalism</a></p><p class="creator">by
 <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a> Dec 2 2015 
updated
 Feb 16 2017</p></div><p class="clickbait">Expected utility is the central idea in the quantitative implementation of consequentialism</p><nav class="breadcrumbs"><nav class="breadcrumb" aria-label="Breadcrumb"><ul><li><a href="../index.html">Index</a></li><li>Expected utility formalism</li></ul></nav><nav class="breadcrumb" aria-label="Breadcrumb"><ul><li><a href="decision_theory.html">Decision theory</a></li><li>â€¦</li></ul></nav></nav></header><hr><main><p>The expected utility formalism combines a probability distribution relating actions to their probable consequences, with a utility function over consequences of actions, in order to assign expected utilities to actions.  A key issue is the form and derivation of the probability distribution relating actions to consequences, usually held to be a [ counterfactual probability distribution].</p></main><hr><footer><p class="tagged"><h2>Tagged</h2><span class="page-comma-list"><a class="page-link" href="../page/stub_meta_tag.html">Stub</a></span></p><p class="reverse-related"><h2>Reverse Related</h2><ul class="page-list"><li><a class="page-link" href="../page/stub_meta_tag.html">Stub</a> <q>This page only gives a very brief overview of the topic. If you're able to, please help expand or improve it!</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li></ul></p><p class="children"><h2>Children</h2><ul class="page-tree"><li><a class="page-link" href="../page/coherence_theorems.html">Coherence theorems</a> <q>A 'coherence theorem' shows that something bad happens to an agent if its decisions can't be viewed as 'coherent' in some sense. E.g., an inconsistent preference ordering leads to going in circles.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/intro_utility_coherence.html">Coherent decisions imply consistent utilities</a> <q>Why do we all use the 'expected utility' formalism?  Because any behavior that can't be viewed from that perspective, must be qualitatively self-defeating (in various mathy ways).</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/expected_utility.html">Expected utility</a> <q>Scoring actions based on the average score of their probable consequences.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/expected_utility_agent.html">Expected utility agent</a> <q>If you're not some kind of expected utility agent, you're going in circles.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/utility_function.html">Utility function</a> <q>The only coherent way of wanting things is to assign consistent relative scores to outcomes.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li></ul></p></footer></body></html>