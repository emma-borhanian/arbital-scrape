<!DOCTYPE html><html><head><meta charset="utf-8"><title>Solomonoff induction</title><link rel="stylesheet" type="text/css" href="../common.css"><link rel="stylesheet" type="text/css" href="../page-style.css"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  SVG: {EqnChunk: 50, EqnChunkFactor: 1.5, EqChunkDelay: 10, useFontCache: false, linebreaks: {automatic: true}},
  tex2jax: {
    inlineMath: [['$~$', '$~$']],
    displayMath: [['$$~$', '$~$$']],
    processEscapes: true,
    preview: 'none',
  },
  showProcessingMessages: false,
  messageStyle: 'none',
  // http://docs.mathjax.org/en/latest/config-files.html#the-tex-ams-svg-configuration-file
  jax: ["input/TeX","output/SVG", "output/PreviewHTML"],
  extensions: ["tex2jax.js","MathMenu.js","MathZoom.js", "fast-preview.js", "AssistiveMML.js", "a11y/accessibility-menu.js"],
  TeX: { extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"] }
});</script><script type="text/javascript" src="../MathJax-2.7.5/MathJax.js"></script><script type="text/javascript" src="../arbital-demo-bundle.js"></script><script type="text/javascript">window.addEventListener('DOMContentLoaded', e=>window.loadAllDemos())
</script></head><body><header><h1 class="title">Solomonoff induction</h1><div class="page-info"><p class="metadata-link"><a href="../metadata/solomonoff_induction.json.html">solomonoff_induction.json</a></p><p class="arbital-url"><a href="https://arbital.com/p/solomonoff_induction">https://arbital.com/p/solomonoff_induction</a></p><p class="creator">by
 <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a> Aug 5 2015 
updated
 Dec 30 2015</p></div><p class="clickbait">A simple way to superintelligently predict sequences of data, given unlimited computing power.</p><nav class="breadcrumbs"><nav class="breadcrumb" aria-label="Breadcrumb"><ul><li><a href="../index.html">Index</a></li><li>Solomonoff induction</li></ul></nav><nav class="breadcrumb" aria-label="Breadcrumb"><ul><li><a href="ai_alignment.html">AI alignment</a></li><li><a href="advanced_safety.html">Advanced safety</a></li><li><a href="unbounded_analysis.html">Methodology of unbounded analysis</a></li><li>…</li></ul></nav><nav class="breadcrumb" aria-label="Breadcrumb"><ul><li><a href="math.html">Mathematics</a></li><li><a href="probability_theory.html">Probability theory</a></li><li><a href="bayes_reasoning.html">Bayesian reasoning</a></li><li><a href="ignorance_prior.html">Ignorance prior</a></li><li><a href="inductive_prior.html">Inductive prior</a></li><li>…</li></ul></nav><nav class="breadcrumb" aria-label="Breadcrumb"><ul><li><a href="rationality.html">Rationality</a></li><li><a href="probability_theory.html">Probability theory</a></li><li><a href="bayes_reasoning.html">Bayesian reasoning</a></li><li><a href="ignorance_prior.html">Ignorance prior</a></li><li><a href="inductive_prior.html">Inductive prior</a></li><li>…</li></ul></nav></nav></header><hr><main><p>Solomonoff induction is an ideal answer to questions like "What probably comes next in the sequence 1, 1, 2, 3, 5, 8?" or "Given the last three years of visual data from this webcam, what will this robot probably see next?" or "Will the sun rise tomorrow?"  Solomonoff induction requires infinite computing power, and is defined by taking every computable algorithm for giving a probability distribution over future data given past data, weighted by their <a href="Kolmogorov_complexity.html">algorithmic simplicity</a>, and <a href="BayesRule-1">updating those weights</a> by comparison to the actual data.</p>
<p>E.g., somewhere in the ideal Solomonoff distribution is an exact copy of <em>you, right now</em>, staring at a string of 1s and 0s and trying to predict what comes next - though this copy of you starts out with a very low weight in the mixture owing to its complexity.  Since a copy of you is present in this mixture of computable predictors, we can prove a theorem about how well Solomonoff induction does compared to an exact copy of you; namely, Solomonoff induction commits only a bounded amount of error relative to you, or any other computable way of making predictions.  Solomonoff induction is thus a kind of perfect or rational ideal for probabilistically predicting sequences, although it cannot be implemented in reality due to requiring infinite computing power.  Still, considering Solomonoff induction can give us important insights into how non-ideal reasoning should operate in the real world.</p>
<p>Additional reading:</p>
<ul>
<li><a href="https://en.wikipedia.org/wiki/Solomonoff%27s_theory_of_inductive_inference">https://en.wikipedia.org/wiki/Solomonoff%27s_theory_of_inductive_inference</a></li>
<li><a href="http://lesswrong.com/lw/dhg/an_intuitive_explanation_of_solomonoff_induction/">http://lesswrong.com/lw/dhg/an_intuitive_explanation_of_solomonoff_induction/</a></li>
<li><a href="http://wiki.lesswrong.com/wiki/Solomonoff_induction">http://wiki.lesswrong.com/wiki/Solomonoff_induction</a></li>
</ul></main><hr><section class="comments"><h2>Comments</h2><div class="comment"><p><a class="page-link" href="../page/MYass.html">M Yass</a></p><p><p>Do we have(or need) any empirical evidence that algorithmic simplicity (space) is the ideal and ultimate absolute prior? If I reread the article carefully I see it doesn't <em>quite</em> seem to advocate this, but I think it's very easy for a learner to pick up that misconception from the way AIXI is generally discussed, the assumption that we somehow know that space complexity is the final answer, and I wonder if it should be ruled out or qualified here.</p>
<p>(I believe it's easy to pick up that misconception because it happened to me. I later came to realize I probably wouldn't bet at good odds that Space AIXI wont ever be dominated by a variant AIXI made with some other razor, the <em>speed</em> of generating environment turing machines, for instance, instead of space. Or how about inverse cyclomatic complexity or some more general measure of algorithm quality that humans thus far have lacked the breadth of mind to find, test or work with mathematically? Or maybe even just some other space complexity of some other machine model? Space complexity of TMs seems like extremely low-hanging fruit.)</p>
<p>I'm hoping to hear someone's done the work of compiling some ridiculously extensive dataset of algorithms and their competitors and demonstrating that space complexity seemed more predictive of domination than any of the other general metrics we could think of. If this result had been found, nobody ever seems to cite it. Though I suppose we might not hear about it when so many people find it completely intuitive.</p></p></div><div class="comment"><p><a class="page-link" href="../page/GurkenglasGurkenglas.html">Gurkenglas Gurkenglas</a></p><p><p>The Solomonoff prior is not computed from the space each hypothesis needs to compute its answer, it's its program length, the space needed to pass the hypothesis to our UTM!</p>
<p>Any other form of induction one could try, like the space or time complexities of evaluating a hypothesis you described, or other machine models that Turing machines can describe, or human intuition, is included by Solomonoff induction as a hypothesis, and will replace it after finite error if it outperforms.</p>
<p>How would you even compute each program's space/time complexity? Rice's theorem says it cannot be done in general.</p></p></div></section><footer><p class="tagged"><h2>Tagged</h2><span class="page-comma-list"><a class="page-link" href="../page/start_meta_tag.html">Start</a></span></p><p class="all-creators"><h2>All Creators</h2><span class="page-comma-list"><a class="page-link" href="../page/AlexeiAndreev.html">Alexei Andreev</a>,
 <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></span></p><p class="likes"><h2>Likes</h2><span class="page-comma-list"><a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a>,
 <a class="page-link" href="../page/EricBruylant.html">Eric Bruylant</a>,
 <a class="page-link" href="../page/RokResnik.html">Rok Resnik</a></span></p><p class="reverse-related"><h2>Reverse Related</h2><ul class="page-list"><li><a class="page-link" href="../page/start_meta_tag.html">Start</a> <q>This page gives a basic overview of the topic, but may be missing important information or have stylistic issues. If you're able to, please help expand or improve it!</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li></ul></p><p class="children"><h2>Children</h2><ul class="page-tree"><li><a class="page-link" href="../page/1hh.html">Solomonoff induction: Intro Dialogue (Math 2)</a> <q>An introduction to Solomonoff induction for the unfamiliar reader who isn't bad at math</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li></ul></p></footer></body></html>