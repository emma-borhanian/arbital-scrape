<!DOCTYPE html><html><head><meta charset="utf-8"><title>Work in progress</title><link rel="stylesheet" type="text/css" href="../common.css"><link rel="stylesheet" type="text/css" href="../page-style.css"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  SVG: {EqnChunk: 50, EqnChunkFactor: 1.5, EqChunkDelay: 10, useFontCache: false, linebreaks: {automatic: true}},
  tex2jax: {
    inlineMath: [['$~$', '$~$']],
    displayMath: [['$$~$', '$~$$']],
    processEscapes: true,
    preview: 'none',
  },
  showProcessingMessages: false,
  messageStyle: 'none',
  // http://docs.mathjax.org/en/latest/config-files.html#the-tex-ams-svg-configuration-file
  jax: ["input/TeX","output/SVG", "output/PreviewHTML"],
  extensions: ["tex2jax.js","MathMenu.js","MathZoom.js", "fast-preview.js", "AssistiveMML.js", "a11y/accessibility-menu.js"],
  TeX: { extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"] }
});</script><script type="text/javascript" src="../MathJax-2.7.5/MathJax.js"></script><script type="text/javascript" src="../arbital-demo-bundle.js"></script><script type="text/javascript">window.addEventListener('DOMContentLoaded', e=>window.loadAllDemos())
</script></head><body><header><h1 class="title">Work in progress</h1><div class="page-info"><p class="metadata-link"><a href="../metadata/work_in_progress_meta_tag.json.html">work_in_progress_meta_tag.json</a></p><p class="arbital-url"><a href="https://arbital.com/p/work_in_progress_meta_tag">https://arbital.com/p/work_in_progress_meta_tag</a></p><p class="creator">by
 <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a> Apr 17 2015 
updated
 Jul 5 2016</p></div><p class="clickbait">This page is being actively worked on by an editor. Check with them before making major changes.</p><nav class="breadcrumbs"><nav class="breadcrumb" aria-label="Breadcrumb"><ul><li><a href="../index.html">Index</a></li><li>Work in progress</li></ul></nav><nav class="breadcrumb" aria-label="Breadcrumb"><ul><li><a href="Arbital.html">Arbital</a></li><li><a href="arbital_meta_tag.html">Meta tags</a></li><li><a href="3zb.html">Meta tags which suppress a page from being featured</a></li><li>…</li></ul></nav><nav class="breadcrumb" aria-label="Breadcrumb"><ul><li><a href="Arbital.html">Arbital</a></li><li><a href="arbital_meta_tag.html">Meta tags</a></li><li>…</li></ul></nav></nav></header><hr><main><p>A meta tag for pages unfinished pages which an author is still making major changes to.</p>
<p>[todo: Define/explain guidelines for use.]</p></main><hr><footer><p class="tagged"><h2>Tagged</h2><span class="page-comma-list"><a class="page-link" href="../page/stub_meta_tag.html">Stub</a></span></p><p class="all-creators"><h2>All Creators</h2><span class="page-comma-list"><a class="page-link" href="../page/AlexeiAndreev.html">Alexei Andreev</a>,
 <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a>,
 <a class="page-link" href="../page/EricBruylant.html">Eric Bruylant</a></span></p><p class="likes"><h2>Likes</h2><span class="page-comma-list"><a class="page-link" href="../page/AlexeiAndreev.html">Alexei Andreev</a>,
 <a class="page-link" href="../page/RafaelCosman.html">Rafael Cosman</a></span></p><p class="related"><h2>Related</h2><ul class="page-list"><li><a class="page-link" href="../page/advanced_agent.html">Advanced agent properties</a> <q>How smart does a machine intelligence need to be, for its niceness to become an issue?  &quot;Advanced&quot; is a broad term to cover cognitive abilities such that we'd need to start considering AI alignment.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/Kolmogorov_complexity.html">Algorithmic complexity</a> <q>When you compress the information, what you are left with determines the complexity.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/real_is_rich.html">Almost all real-world domains are rich</a> <q>Anything you're trying to accomplish in the real world can potentially be accomplished in a *lot* of different ways.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/5kv.html">An Introduction to Logical Decision Theory for Everyone Else</a> <q>So like what the heck is 'logical decision theory' in terms a normal person can understand?</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/3tk.html">Arbital subscriptions: Maintenance</a> <q>Subscribing to a page with intention of maintaining it.</q> - <a class="page-link" href="../page/AlexeiAndreev.html">Alexei Andreev</a></li><li><a class="page-link" href="../page/arbital_do_what_works.html">Arbital: Do what works</a> <q>When deciding things on Arbital, think about the real goals, and move towards them.</q> - <a class="page-link" href="../page/EricBruylant.html">Eric Bruylant</a></li><li><a class="page-link" href="../page/arguments.html">Arguments</a> <q>An argument is a formal reasoning, valid or not.</q> - <a class="page-link" href="../page/JeremyPerret.html">Jeremy Perret</a></li><li><a class="page-link" href="../page/78l.html">Asymptotic Notation</a> <q>Asymptotic notation seeks to capture the behavior of functions as its input(s) become extreme.  It is most widely used in Computer Science and Numerical Approximation.</q> - <a class="page-link" href="../page/MorganRedding.html">Morgan Redding</a></li><li><a class="page-link" href="../page/Arbital_author_feedback.html">Author's guide to processing feedback</a> <q>Requisite used for teaching authors about Arbital feedback features.</q> - <a class="page-link" href="../page/AlexeiAndreev.html">Alexei Andreev</a></li><li><a class="page-link" href="../page/5f3.html">Bayes' rule: Beginner's guide</a> <q>Beginner's guide to learning about Bayes' rule.</q> - <a class="page-link" href="../page/AlexeiAndreev.html">Alexei Andreev</a></li><li><a class="page-link" href="../page/behaviorist.html">Behaviorist genie</a> <q>An advanced agent that's forbidden to model minds in too much detail.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/bijective_function_intro_math_0.html">Bijective Function: Intro (Math 0)</a> <q>Two boxes are bijective if they contain the same number of items.</q> - <a class="page-link" href="../page/MarkChimes.html">Mark Chimes</a></li><li><a class="page-link" href="../page/data_bit.html">Bit (of data)</a> <q>A bit of data is the amount of data required to single out one message from a set of two. Equivalen…</q> - <a class="page-link" href="../page/NateSoares.html">Nate Soares</a></li><li><a class="page-link" href="../page/bit_examples.html">Bit (of data): Examples</a> <q>In the game &quot;20 questions&quot;, one player (the &quot;leader&quot;) thinks of a concept, and the other players ask…</q> - <a class="page-link" href="../page/NateSoares.html">Nate Soares</a></li><li><a class="page-link" href="../page/AI_boxing.html">Boxed AI</a> <q>Idea: what if we limit how AI can interact with the world. That'll make it safe, right??</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/category_mathematics.html">Category (mathematics)</a> <q>A description of how a collection of mathematical objects are related to one another.</q> - <a class="page-link" href="../page/MarkChimes.html">Mark Chimes</a></li><li><a class="page-link" href="../page/category_theory.html">Category theory</a> <q>How mathematical objects are related to others in the same category.</q> - <a class="page-link" href="../page/MarkChimes.html">Mark Chimes</a></li><li><a class="page-link" href="../page/causal_dt.html">Causal decision theories</a> <q>On CDT, to choose rationally, you should imagine the world where your physical act changes, then imagine running that world forward in time.  (Therefore, it's irrational to vote in elections.)</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/value_alignment_central_examples.html">Central examples</a> <q>List of central examples in Value Alignment Theory domain.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/245.html">Civilization scale energy</a> <q>What are the main options for powering civilization, and how do they compare?</q> - <a class="page-link" href="../page/EricBruylant.html">Eric Bruylant</a></li><li><a class="page-link" href="../page/cev.html">Coherent extrapolated volition (alignment target)</a> <q>A proposed direction for an extremely well-aligned autonomous superintelligence - do what humans would want, if we knew what the AI knew, thought that fast, and understood ourselves.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/magician_message.html">Communication: magician example</a> <q>Imagine that you and I are both magicians, performing a trick where I think of a card from a deck of…</q> - <a class="page-link" href="../page/NateSoares.html">Nate Soares</a></li><li><a class="page-link" href="../page/math_order_complete_lattice.html">Complete lattice</a> <q>A poset that is closed under arbitrary joins and meets.</q> - <a class="page-link" href="../page/KevinClancy.html">Kevin Clancy</a></li><li><a class="page-link" href="../page/complex_number.html">Complex number</a> <q>A complex number is a number of the form $z = a + b\textrm{i}$, where $\textrm{i}$ is the imaginary …</q> - <a class="page-link" href="../page/ElianaRuby.html">Eliana Ruby</a></li><li><a class="page-link" href="../page/complexity_of_value.html">Complexity of value</a> <q>There's no simple way to describe the goals we want Artificial Intelligences to want.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/multiple_compression.html">Compressing multiple messages</a> <q>How many bits of data does it take to encode an $n$-message? Naively, the answer is $\lceil \log_2(n…</q> - <a class="page-link" href="../page/NateSoares.html">Nate Soares</a></li><li><a class="page-link" href="../page/2wk.html">Conjunctions and disjunctions</a> <q>The fancy name for the &quot;and&quot; and &quot;or&quot; connectives.</q> - <a class="page-link" href="../page/JeremyPerret.html">Jeremy Perret</a></li><li><a class="page-link" href="../page/context_disaster.html">Context disaster</a> <q>Some possible designs cause your AI to behave nicely while developing, and behave a lot less nicely when it's smarter.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/alignment_difficulty.html">Difficulty of AI alignment</a> <q>How hard is it exactly to point an Artificial General Intelligence in an intuitively okay direction?</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/probable_environment_hacking.html">Distant superintelligences can coerce the most probable environment of your AI</a> <q>Distant superintelligences may be able to hack your local AI, if your AI's preference framework depends on its most probable environment.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/trits_with_galcom_bits.html">Encoding trits with GalCom bits</a> <q>There are $\log_2(3) \approx 1.585$ bits to a Trit. Why is it that particular value? Consider the Ga…</q> - <a class="page-link" href="../page/NateSoares.html">Nate Soares</a></li><li><a class="page-link" href="../page/epistemic_exclusion.html">Epistemic exclusion</a> <q>How would you build an AI that, no matter what else it learned about the world, never knew or wanted to know what was inside your basement?</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/expected_utility_agent.html">Expected utility agent</a> <q>If you're not some kind of expected utility agent, you're going in circles.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/faithful_simulation.html">Faithful simulation</a> <q>How would you identify, to a Task AGI (aka Genie), the problem of scanning a human brain, and then running a sufficiently accurate simulation of it for the simulation to not be crazy or psychotic?</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/fixed_point_theorem_provability_logic.html">Fixed point theorem of provability logic</a> <q>Deal with those pesky self-referential sentences!</q> - <a class="page-link" href="../page/JaimeSevillaMolina.html">Jaime Sevilla Molina</a></li><li><a class="page-link" href="../page/2h8.html">Formal Logic</a> <q>Formal logic studies the form of correct arguments through rigorous and precise mathematical theories.</q> - <a class="page-link" href="../page/ErikIstre.html">Erik Istre</a></li><li><a class="page-link" href="../page/function.html">Function</a> <q>Intuitively, a function $f$  is a procedure (or machine) that takes an input and performs some opera…</q> - <a class="page-link" href="../page/NateSoares.html">Nate Soares</a></li><li><a class="page-link" href="../page/247.html">Grid scale storage</a> <q>Scalable energy storage is required if civilization's switches to primarily renewables in order to keep the grid powered at night. What are the options and how do they compare?</q> - <a class="page-link" href="../page/EricBruylant.html">Eric Bruylant</a></li><li><a class="page-link" href="../page/bits_in_a_trit.html">How many bits to a trit?</a> <q>$\log_2(3) \approx 1.585.$ This can be interpreted a few different ways:

1. If you multiply the nu…</q> - <a class="page-link" href="../page/NateSoares.html">Nate Soares</a></li><li><a class="page-link" href="../page/how_to_arbital.html">How to author on Arbital!</a> <q>Want to contribute pages to Arbital?  Here's our current version of the ad-hoc guide to being an author!</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/inductive_ambiguity.html">Identifying ambiguous inductions</a> <q>What do a &quot;red strawberry&quot;, a &quot;red apple&quot;, and a &quot;red cherry&quot; have in common that a &quot;yellow carrot&quot; doesn't?  Are they &quot;red fruits&quot; or &quot;red objects&quot;?</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/immediate_goods.html">Immediate goods</a> <q>One of the potential views on 'value' in the value alignment problem is that what we should want fro…</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/information.html">Information</a> <q>Information is a measure of how much a message grants an observer the ability to predict the world.…</q> - <a class="page-link" href="../page/NateSoares.html">Nate Soares</a></li><li><a class="page-link" href="../page/instrumental_convergence.html">Instrumental convergence</a> <q>Some strategies can help achieve most possible simple goals.  E.g., acquiring more computing power or more material resources.  By default, unless averted, we can expect advanced AIs to do that.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/468.html">Joint probability distribution: (Motivation) coherent probabilities</a> <q>If you don't use joint probability distributions, none of your probabilities will make any sense. So, yeah, use joint probability distributions.</q> - <a class="page-link" href="../page/TsviBT.html">Tsvi BT</a></li><li><a class="page-link" href="../page/KANSI.html">Known-algorithm non-self-improving agent</a> <q>Possible advanced AIs that aren't self-modifying, aren't self-improving, and where we know and understand all the component algorithms.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/law_of_syllogism.html">Law of syllogism</a> <q>Deriving something from the conclusion of another thing.</q> - <a class="page-link" href="../page/JeremyPerret.html">Jeremy Perret</a></li><li><a class="page-link" href="../page/likelihood_vs_pvalue.html">Likelihood functions, p-values, and the replication crisis</a> <q>What's the whole Bayesian-vs.-frequentist debate about?</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/log_tutorial_overview.html">Logarithm tutorial overview</a> <q>The logarithm tutorial covers the following six subjects:

1. What are logarithms?
2. Logarithms as…</q> - <a class="page-link" href="../page/NateSoares.html">Nate Soares</a></li><li><a class="page-link" href="../page/foreseeable_difficulties.html">Methodology of foreseeable difficulties</a> <q>Building a nice AI is likely to be hard enough, and contain enough gotchas that won't show up in the AI's early days, that we need to foresee problems coming in advance.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/unbounded_analysis.html">Methodology of unbounded analysis</a> <q>What we do and don't understand how to do, using unlimited computing power, is a critical distinction and important frontier.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/modus_tollens.html">Modus tollens</a> <q>Deriving a negation from another negation</q> - <a class="page-link" href="../page/JeremyPerret.html">Jeremy Perret</a></li><li><a class="page-link" href="../page/morphism.html">Morphism</a> <q>A morphism is the abstract representation of a relation between mathematical objects.

Usually, it i…</q> - <a class="page-link" href="../page/JaimeSevillaMolina.html">Jaime Sevilla Molina</a></li><li><a class="page-link" href="../page/4s.html">Natural language understanding of &quot;right&quot; will yield normativity</a> <q>What will happen if you tell an advanced agent to do the &quot;right&quot; thing?</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/natural_number_numbersets.html">Natural numbers: Intro to Number Sets</a> <q>Natural numbers are the numbers we use to count in everyday life.</q> - <a class="page-link" href="../page/JoeZeng.html">Joe Zeng</a></li><li><a class="page-link" href="../page/nearest_unblocked.html">Nearest unblocked strategy</a> <q>If you patch an agent's preference framework to avoid an undesirable solution, what can you expect to happen?</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/logical_negation.html">Negation of propositions</a> <q>The proposition that is false if another one is true and vice-versa.</q> - <a class="page-link" href="../page/JeremyPerret.html">Jeremy Perret</a></li><li><a class="page-link" href="../page/ontology_identification.html">Ontology identification problem</a> <q>How do we link an agent's utility function to its model of the world, when we don't know what that model will look like?</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/taskagi_open_problems.html">Open subproblems in aligning a Task-based AGI</a> <q>Open research problems, especially ones we can model today, in building an AGI that can &quot;paint all cars pink&quot; without turning its future light cone into pink-painted cars.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/daemons.html">Optimization daemons</a> <q>When you optimize something so hard that it crystalizes into an optimizer, like the way natural selection optimized apes so hard they turned into human-level intelligences</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/oracle.html">Oracle</a> <q>System designed to safely answer questions.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/order_theory.html">Order theory</a> <q>The study of binary relations that are reflexive, transitive, and antisymmetic.</q> - <a class="page-link" href="../page/KevinClancy.html">Kevin Clancy</a></li><li><a class="page-link" href="../page/orthogonality.html">Orthogonality Thesis</a> <q>Will smart AIs automatically become benevolent, or automatically become hostile?  Or do different AI designs imply different goals?</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/paperclip_maximizer.html">Paperclip maximizer</a> <q>This agent will not stop until the entire universe is filled with paperclips.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/programmer_deception.html">Programmer deception</a> <q>Programmer deception is when the AI's decision process leads it to optimize for an instrumental goal…</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/prog_dep_typ.html">Programming in  Dependent Type Theory</a> <q>Working with simple types in Lean</q> - <a class="page-link" href="../page/JackGallagher.html">Jack Gallagher</a></li><li><a class="page-link" href="../page/propositions.html">Propositions</a> <q>Propositions are statements with a truth value.</q> - <a class="page-link" href="../page/JeremyPerret.html">Jeremy Perret</a></li><li><a class="page-link" href="../page/246.html">Resources and the future</a> <q>Resource constraints are a widely held concern. Which are most likely to be limiting factors, and what can we do to relax those limits?</q> - <a class="page-link" href="../page/EricBruylant.html">Eric Bruylant</a></li><li><a class="page-link" href="../page/rice_and_halt.html">Rice's theorem and the Halting problem</a> <q>We will show that Rice's theorem and the the halting problem are equivalent.

#The Halting theorem i…</q> - <a class="page-link" href="../page/JaimeSevillaMolina.html">Jaime Sevilla Molina</a></li><li><a class="page-link" href="../page/rich_domain.html">Rich domain</a> <q>A domain is 'rich', relative to our own intelligence, to the extent that (1) its [ search space] is …</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/algebraic_ring.html">Ring</a> <q>A ring is a kind of Algebraic structure which we obtain by considering groups as being &quot;things with…</q> - <a class="page-link" href="../page/NateSoares.html">Nate Soares</a></li><li><a class="page-link" href="../page/shannon.html">Shannon</a> <q>The shannon (Sh) is a unit of Information. One shannon is the difference in [info\_entropy entropy] …</q> - <a class="page-link" href="../page/NateSoares.html">Nate Soares</a></li><li><a class="page-link" href="../page/arithmetical_adequacy_GL.html">Solovay's theorems of arithmetical adequacy for GL</a> <q>Using GL to reason about PA, and viceversa</q> - <a class="page-link" href="../page/JaimeSevillaMolina.html">Jaime Sevilla Molina</a></li><li><a class="page-link" href="../page/standard_agent.html">Standard agent properties</a> <q>What's a Standard Agent, and what can it do?</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/task_agi.html">Task-directed AGI</a> <q>An advanced AI that's meant to pursue a series of limited-scope goals given it by the user.  In Bostrom's terminology, a Genie.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/reals_as_dedekind_cuts_form_a_field.html">The reals (constructed as Dedekind cuts) form a field</a> <q>The reals are an archetypal example of a field, but if we are to construct them from simpler objects, we need to show that our construction does indeed have the right properties.</q> - <a class="page-link" href="../page/PatrickStevens.html">Patrick Stevens</a></li><li><a class="page-link" href="../page/only_one_log.html">There is only one logarithm</a> <q>All logarithm functions are the same, up to a multiplicative constant.</q> - <a class="page-link" href="../page/NateSoares.html">Nate Soares</a></li><li><a class="page-link" href="../page/type_theory.html">Type theory</a> <q>Modern foundations for formal mathematics.</q> - <a class="page-link" href="../page/JackGallagher.html">Jack Gallagher</a></li><li><a class="page-link" href="../page/value_achievement_dilemma.html">Value achievement dilemma</a> <q>How can Earth-originating intelligent life achieve most of its potential value, whether by AI or otherwise?</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/value_identification.html">Value identification problem</a> <q>The subproblem category of value alignment which deals with pinpointing valuable outcomes to an adva…</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/value_laden.html">Value-laden</a> <q>Cure cancer, but avoid any bad side effects?  Categorizing &quot;bad side effects&quot; requires knowing what's &quot;bad&quot;.  If an agent needs to load complex human goals to evaluate something, it's &quot;value-laden&quot;.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/Vingean_uncertainty.html">Vingean uncertainty</a> <q>You can't predict the exact actions of an agent smarter than you - so is there anything you _can_ say about them?</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li><li><a class="page-link" href="../page/ZF_provability_oracle.html">Zermelo-Fraenkel provability oracle</a> <q>We might be able to build a system that can safely inform us that a theorem has a proof in set theory, but we can't see how to use that capability to save the world.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li></ul></p><p class="reverse-related"><h2>Reverse Related</h2><ul class="page-list"><li><a class="page-link" href="../page/stub_meta_tag.html">Stub</a> <q>This page only gives a very brief overview of the topic. If you're able to, please help expand or improve it!</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li></ul></p></footer></body></html>