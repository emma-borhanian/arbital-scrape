<!DOCTYPE html><html><head><meta charset="utf-8"><title>Relative likelihood</title><link rel="stylesheet" type="text/css" href="../common.css"><link rel="stylesheet" type="text/css" href="../page-style.css"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  SVG: {EqnChunk: 50, EqnChunkFactor: 1.5, EqChunkDelay: 10, useFontCache: false, linebreaks: {automatic: true}},
  tex2jax: {
    inlineMath: [['$~$', '$~$']],
    displayMath: [['$$~$', '$~$$']],
    processEscapes: true,
    preview: 'none',
  },
  showProcessingMessages: false,
  messageStyle: 'none',
  // http://docs.mathjax.org/en/latest/config-files.html#the-tex-ams-svg-configuration-file
  jax: ["input/TeX","output/SVG", "output/PreviewHTML"],
  extensions: ["tex2jax.js","MathMenu.js","MathZoom.js", "fast-preview.js", "AssistiveMML.js", "a11y/accessibility-menu.js"],
  TeX: { extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"] }
});</script><script type="text/javascript" src="../MathJax-2.7.5/MathJax.js"></script><script type="text/javascript" src="../arbital-demo-bundle.js"></script><script type="text/javascript">window.addEventListener('DOMContentLoaded', e=>window.loadAllDemos())
</script></head><body><header><h1 class="title">Relative likelihood</h1><div class="page-info"><p class="metadata-link"><a href="../metadata/relative_likelihood.json.html">relative_likelihood.json</a></p><p class="arbital-url"><a href="https://arbital.com/p/relative_likelihood">https://arbital.com/p/relative_likelihood</a></p><p class="creator">by
 <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a> Jan 27 2016 
updated
 Aug 4 2016</p></div><p class="clickbait">How relatively likely an observation is, given two or more hypotheses, determines the strength and direction of evidence.</p><nav class="breadcrumbs"><nav class="breadcrumb" aria-label="Breadcrumb"><ul><li><a href="../index.html">Index</a></li><li>Relative likelihood</li></ul></nav><nav class="breadcrumb" aria-label="Breadcrumb"><ul><li><a href="math.html">Mathematics</a></li><li><a href="probability_theory.html">Probability theory</a></li><li><a href="bayes_reasoning.html">Bayesian reasoning</a></li><li><a href="bayesian_likelihood.html">Likelihood</a></li><li>…</li></ul></nav><nav class="breadcrumb" aria-label="Breadcrumb"><ul><li><a href="rationality.html">Rationality</a></li><li><a href="probability_theory.html">Probability theory</a></li><li><a href="bayes_reasoning.html">Bayesian reasoning</a></li><li><a href="bayesian_likelihood.html">Likelihood</a></li><li>…</li></ul></nav></nav></header><hr><main><p>Relative likelihoods express how <em>relatively</em> more likely an observation is, comparing one hypothesis to another.  For example, suppose we're investigating the murder of Mr. Boddy, and we find that he was killed by poison. The suspects are Miss Scarlett and Colonel Mustard. Now, suppose that the <a href="probability.html">probability</a> that Miss Scarlett would use poison, if she <em>were</em> the murderer, is 20%. And suppose that the probability that Colonel Mustard would use poison, if he were the murderer, is 10%. Then, Miss Scarlett is <em>twice as likely</em> to use poison as a murder weapon as Colonel Mustard. Thus, the "Mr. Boddy was poisoned" evidence supports the "Scarlett" hypothesis twice as much as the "Mustard" hypothesis, for relative likelihoods of $~$(2 : 1).$~$</p>
<p>These likelihoods are called "relative" because it wouldn't matter if the respective probabilities were 4% and 2%, or 40% and 20% &mdash; what matters is the <em>relative proportion</em>.</p>
<p>Relative likelihoods may be given between many different hypotheses at once. Given the evidence $~$e_p$~$ = "Mr. Boddy was poisoned", it might be the case that Miss Scarlett, Colonel Mustard, and Mrs. White have the respective probabilities 20%, 10%, and 1% of using poison any time they commit a murder. In this case, we have three hypotheses &mdash; $~$H_S$~$ = "Scarlett did it", $~$H_M$~$ = "Mustard did it", and $~$H_W$~$ = "White did it". The relative likelihoods between them may be written $~$(20 : 10 : 1).$~$</p>
<p>In general, given a list of hypotheses $~$H_1, H_2, \ldots, H_n,$~$ the relative likelihoods on the evidence $~$e$~$ can be written as a [ scale-invariant list] of the likelihoods $~$\mathbb P(e \mid H_i)$~$ for each $~$i$~$ from 1 to $~$n.$~$ In other words, the relative likelihoods are</p>
<p>$$~$ \alpha \mathbb P(e \mid H_1) : \alpha \mathbb P(e \mid H_2) : \ldots : \alpha \mathbb P(e \mid H_n) $~$$</p>
<p>where the choice of $~$\alpha &gt; 0$~$ does not change the value denoted by the list (i.e., the list is [scale_invariant_list scale-invariant]). For example, the relative likelihood list $~$(20 : 10 : 1)$~$ above denotes the same thing as the relative likelihood list $~$(4 : 2 : 0.20)$~$ denotes the same thing as the relative likelihood list $~$(60 : 30 : 3).$~$ This is why we call them "relative likelihoods" &mdash; all that matters is the ratio between each term, not the absolute values.</p>
<p>Any two terms in a list of relative likelihoods can be used to generate a <a href="likelihood_ratio.html">Likelihood ratio</a> between two hypotheses. For example, above, the likelihood ratio $~$H_S$~$ to $~$H_M$~$ is 2/1, and the likelihood ratio of $~$H_S$~$ to $~$H_W$~$ is 20/1. This means that the evidence $~$e_p$~$ supports the "Scarlett" hypothesis 2x more than it supports the "Mustard" hypothesis, and 20x more than it supports the "White" hypothesis.</p>
<p>Relative likelihoods summarize the <a href="bayes_strength_of_evidence.html">strength of the evidence</a> represented by the observation that Mr. Boddy was poisoned &mdash; under <a href="bayes_rule.html">Bayes&#39; rule</a>, the evidence points to Miss Scarlett to the same degree whether the absolute probabilities are 20% vs. 10%, or 4% vs. 2%.</p>
<p>By Bayes' rule, the way to update your beliefs in the face of evidence is to take your <a href="prior_probability.html">prior</a> <a href="odds.html">Odds</a> and simply multiply them by the corresponding relative likelihood list, to obtain your <a href="posterior_probability.html">posterior</a> odds. See also <a href="bayes_rule_odds.html">Bayes&#39; rule: Odds form</a>.</p></main><hr><footer><p class="tagged"><h2>Tagged</h2><span class="page-comma-list"><a class="page-link" href="../page/start_meta_tag.html">Start</a></span></p><p class="all-creators"><h2>All Creators</h2><span class="page-comma-list"><a class="page-link" href="../page/AlPrihodko.html">Al Prihodko</a>,
 <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a>,
 <a class="page-link" href="../page/EricBruylant.html">Eric Bruylant</a>,
 <a class="page-link" href="../page/EricRogstad.html">Eric Rogstad</a>,
 <a class="page-link" href="../page/NateSoares.html">Nate Soares</a></span></p><p class="likes"><h2>Likes</h2><span class="page-comma-list"><a class="page-link" href="../page/BuiLinh.html">Linh Mai</a>,
 <a class="page-link" href="../page/SzymonWilczyski.html">Szymon Wilczyński</a></span></p><p class="reverse-related"><h2>Reverse Related</h2><ul class="page-list"><li><a class="page-link" href="../page/start_meta_tag.html">Start</a> <q>This page gives a basic overview of the topic, but may be missing important information or have stylistic issues. If you're able to, please help expand or improve it!</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li></ul></p></footer></body></html>