<!DOCTYPE html><html><head><meta charset="utf-8"><title>Total alignment</title><link rel="stylesheet" type="text/css" href="../common.css"><link rel="stylesheet" type="text/css" href="../page-style.css"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  SVG: {EqnChunk: 50, EqnChunkFactor: 1.5, EqChunkDelay: 10, useFontCache: false, linebreaks: {automatic: true}},
  tex2jax: {
    inlineMath: [['$~$', '$~$']],
    displayMath: [['$$~$', '$~$$']],
    processEscapes: true,
    preview: 'none',
  },
  showProcessingMessages: false,
  messageStyle: 'none',
  // http://docs.mathjax.org/en/latest/config-files.html#the-tex-ams-svg-configuration-file
  jax: ["input/TeX","output/SVG", "output/PreviewHTML"],
  extensions: ["tex2jax.js","MathMenu.js","MathZoom.js", "fast-preview.js", "AssistiveMML.js", "a11y/accessibility-menu.js"],
  TeX: { extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"] }
});</script><script type="text/javascript" src="../MathJax-2.7.5/MathJax.js"></script><script type="text/javascript" src="../arbital-demo-bundle.js"></script><script type="text/javascript">window.addEventListener('DOMContentLoaded', e=>window.loadAllDemos())
</script></head><body><header><h1 class="title">Total alignment</h1><div class="page-info"><p class="metadata-link"><a href="../metadata/total_alignment.json.html">total_alignment.json</a></p><p class="arbital-url"><a href="https://arbital.com/p/total_alignment">https://arbital.com/p/total_alignment</a></p><p class="creator">by
 <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a> Jun 6 2016</p></div><p class="clickbait">We say that an advanced AI is &quot;totally aligned&quot; when it knows *exactly* which outcomes and plans are beneficial, with no further user input.</p><nav class="breadcrumbs"><nav class="breadcrumb" aria-label="Breadcrumb"><ul><li><a href="../index.html">Index</a></li><li>Total alignment</li></ul></nav><nav class="breadcrumb" aria-label="Breadcrumb"><ul><li><a href="ai_alignment.html">AI alignment</a></li><li><a href="value_alignment_problem.html">Value alignment problem</a></li><li>â€¦</li></ul></nav></nav></header><hr><main><p>An advanced agent can be said to be "totally aligned" when it can assess the <em>exact</em> <a href="value_alignment_value.html">Value</a> of well-described outcomes and hence the <em>exact</em> subjective value of actions, policies, and plans; where <a href="value_alignment_value.html">Value</a> has its overridden meaning of a metasyntactic variable standing in for "whatever we really do or really should value in the world or want from an Artificial Intelligence" (this is the same as "normative" if the speaker believes in normativity).  That is:  It's an advanced agent that captures <em>all</em> the distinctions we would make or should make within which outcomes are good or bad; it has "full coverage" of the true or intended goals; it correctly resolves every <a href="reflective_degree_of_freedom.html">Reflectively consistent degree of freedom</a>.</p>
<p>We don't need to try and give such an AI simplified orders like, e.g., "try to have a <a href="low_impact.html">lower impact</a>" because we're worried about, e.g., a <a href="nearest_unblocked.html">Nearest unblocked strategy</a> problem on trying to draw exact boundaries around what constitutes a bad impact.  The AI knows <em>everything</em> worth knowing about which impacts are bad, and even if it thinks of a really weird exotic plan, it will still be able to figure out which aspects of this plan match our intended notion of  <a href="value_alignment_value.html">Value</a> or a normative notion of  <a href="value_alignment_value.html">Value</a>.</p>
<p>If this agent does not systematically underestimate the probability of bad outcomes / overestimate the probability of good outcomes, and its maximization over policies is not subject to adverse subjection, its estimates of expected <a href="value_alignment_value.html">Value</a> will be well-calibrated even from our own outside standpoint.</p></main><hr><footer><p class="tagged"><h2>Tagged</h2><span class="page-comma-list"><a class="page-link" href="../page/stub_meta_tag.html">Stub</a></span></p><p class="reverse-related"><h2>Reverse Related</h2><ul class="page-list"><li><a class="page-link" href="../page/stub_meta_tag.html">Stub</a> <q>This page only gives a very brief overview of the topic. If you're able to, please help expand or improve it!</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li></ul></p></footer></body></html>