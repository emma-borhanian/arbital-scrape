<!DOCTYPE html><html><head><meta charset="utf-8"><title>&quot;&gt; on my view it seems extre...&quot;</title><link rel="stylesheet" type="text/css" href="../common.css"><link rel="stylesheet" type="text/css" href="../page-style.css"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  SVG: {EqnChunk: 50, EqnChunkFactor: 1.5, EqChunkDelay: 10, useFontCache: false, linebreaks: {automatic: true}},
  tex2jax: {
    inlineMath: [['$~$', '$~$']],
    displayMath: [['$$~$', '$~$$']],
    processEscapes: true,
    preview: 'none',
  },
  showProcessingMessages: false,
  messageStyle: 'none',
  // http://docs.mathjax.org/en/latest/config-files.html#the-tex-ams-svg-configuration-file
  jax: ["input/TeX","output/SVG", "output/PreviewHTML"],
  extensions: ["tex2jax.js","MathMenu.js","MathZoom.js", "fast-preview.js", "AssistiveMML.js", "a11y/accessibility-menu.js"],
  TeX: { extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"] }
});</script><script type="text/javascript" src="../MathJax-2.7.5/MathJax.js"></script><script type="text/javascript" src="../arbital-demo-bundle.js"></script><script type="text/javascript">window.addEventListener('DOMContentLoaded', e=>window.loadAllDemos())
</script></head><body><header><h1 class="title">&quot;&gt; on my view it seems extre...&quot;</h1><div class="page-info"><p class="metadata-link"><a href="../metadata/1jl.json.html">1jl.json</a></p><p class="arbital-url"><a href="https://arbital.com/p/1jl">https://arbital.com/p/1jl</a></p><p class="creator">by
 <a class="page-link" href="../page/PaulChristiano.html">Paul Christiano</a> Jan 4 2016</p></div><nav class="breadcrumbs"><nav class="breadcrumb" aria-label="Breadcrumb"><ul><li><a href="../index.html">Index</a></li><li>&quot;&gt; on my view it seems extre...&quot;</li></ul></nav><nav class="breadcrumb" aria-label="Breadcrumb"><ul><li><a href="ai_alignment.html">AI alignment</a></li><li><a href="AGI_typology.html">Strategic AGI typology</a></li><li><a href="KANSI.html">Known-algorithm non-self-improving agent</a></li><li><a href="1gp.html">&quot;Eliezer seems to have, and ...&quot;</a></li><li>…</li></ul></nav><nav class="breadcrumb" aria-label="Breadcrumb"><ul><li><a href="ai_alignment.html">AI alignment</a></li><li><a href="AGI_typology.html">Strategic AGI typology</a></li><li><a href="KANSI.html">Known-algorithm non-self-improving agent</a></li><li>…</li></ul></nav></nav></header><hr><main><blockquote>
  <p>on my view it seems extremely probable that, whatever we have in the way of AI algorithms short of full FAI creating other AI algorithms, they'll be helping out not at all with alignment and control</p>
</blockquote>
<p>You often say this, but I'm obviously not yet convinced.</p>
<p>As I see it the biggest likely gap is that you can empirically validate work in AI, but maybe cannot validate work on alignment/control except by consulting a human. This is problematic if either human feedback ends up being a major cost/obstacle (e.g. because AI systems are extremely cheap/fast, or because they are too far beyond humans for humans to provide meaningful oversight), or if task definitions that involve human feedback end up being harder by virtue of being mushier goals that don't line up as well with the actual structure of reality.</p>
<p>These objections are more plausible for establishing that control work is a comparative advantage of humans. In that context I would accept them as plausible arguments, though I think there is a pretty good chance of working around them.</p>
<p>But those considerations don't seem to imply that AI will help out "not at all." It seems pretty plausible that you are drawing on some other intuitions that I haven't considered.</p>
<p>Another possible gap is that control may just be harder than capabilities. But in that case the development of AI wouldn't really change the game, it would just make the game go faster, so this doesn't seem relevant to the present discussion. (If humans can solve the control problem anyway, humans+AI systems would have a comparable chance.)</p>
<p>Another possible gap is that there are many more iterations of AI design, and a failure at any time cascades into future iterations. I've pointed out that there can't be many big productivity improvements before any earlier thinking about AI is thoroughly obsolete, but I'm certainly willing to grant that forcing control to keep up for a while does make the problem materially harder (moreso the more that our solutions to the control problem are closely tied to details of the AI systems we are building). I agree that sticking with the same AI designs for longer can in some respects make the control problem easier. But it seems like you are talking about a difference-in-kind for safety work, rather than another way to slightly improve safety at the expense of efficacy.</p>
<p>Note: I'm saying that if you can solve the AI control/alignment problem for the AI systems in year N, then the involvement of those AI systems in subsequent AI design doesn't exert a significant additional pressure that makes it harder to solve the control/alignment problem in year N+1. It seems like this is the relevant question in the context of the OP.</p></main><hr><footer></footer></body></html>