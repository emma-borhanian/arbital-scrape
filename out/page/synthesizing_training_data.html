<!DOCTYPE html><html><head><meta charset="utf-8"><title>Synthesizing training data</title><link rel="stylesheet" type="text/css" href="../common.css"><link rel="stylesheet" type="text/css" href="../page-style.css"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  SVG: {EqnChunk: 50, EqnChunkFactor: 1.5, EqChunkDelay: 10, useFontCache: false, linebreaks: {automatic: true}},
  tex2jax: {
    inlineMath: [['$~$', '$~$']],
    displayMath: [['$$~$', '$~$$']],
    processEscapes: true,
    preview: 'none',
  },
  showProcessingMessages: false,
  messageStyle: 'none',
  // http://docs.mathjax.org/en/latest/config-files.html#the-tex-ams-svg-configuration-file
  jax: ["input/TeX","output/SVG", "output/PreviewHTML"],
  extensions: ["tex2jax.js","MathMenu.js","MathZoom.js", "fast-preview.js", "AssistiveMML.js", "a11y/accessibility-menu.js"],
  TeX: { extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"] }
});</script><script type="text/javascript" src="../MathJax-2.7.5/MathJax.js"></script><script type="text/javascript" src="../arbital-demo-bundle.js"></script><script type="text/javascript">window.addEventListener('DOMContentLoaded', e=>window.loadAllDemos())
</script></head><body><header><h1 class="title">Synthesizing training data</h1><div class="page-info"><p class="metadata-link"><a href="../metadata/synthesizing_training_data.json.html">synthesizing_training_data.json</a></p><p class="arbital-url"><a href="https://arbital.com/p/synthesizing_training_data">https://arbital.com/p/synthesizing_training_data</a></p><p class="creator">by
 <a class="page-link" href="../page/PaulChristiano.html">Paul Christiano</a> Feb 4 2016 
updated
 Feb 24 2016</p></div><nav class="breadcrumbs"><nav class="breadcrumb" aria-label="Breadcrumb"><ul><li><a href="../index.html">Index</a></li><li>Synthesizing training data</li></ul></nav><nav class="breadcrumb" aria-label="Breadcrumb"><ul><li><a href="paul_ai_control.html">Paul Christiano's AI control blog</a></li><li>…</li></ul></nav></nav></header><hr><main><p><a href="human_counterfactual_loop.html?title=human-in-counterfactual-loop">Counterfactual oversight</a>&nbsp;requires the overseer to stick around while the system is operating. This&nbsp;<a href="https://ordinaryideas.wordpress.com/2015/11/30/driving-fast-in-the-counterfactual-loop/">causes trouble if the overseer is no longer able to provide effective oversight</a>.</p>
<p>This post will describe a scheme for resolving this problem. (Translating a key idea from&nbsp;<a href="implementing_considered_judgement.html?title=implementing-our-considered-judgment">this post</a>&nbsp;to the setting of realistic predictors.) The scheme is not very good. But it is good enough to suggest that there is no impossibility result — no theoretically fundamental difficulty, just a practical problem. So I’m optimistic it is possible to come up with something better.</p>
<p>Another limitation of counterfactual oversight is that we need to provide frequent oversight if we want to respond quickly to context changes. I won’t discuss this issue in this post, but I think that the same techniques can be used to address it by synthesizing pessimistic data.</p>
<h3 id="synthesizingdata">Synthesizing data</h3>
<p>The traditional approach to supervised learning is to train learning systems in advance, and deploy them after verifying that they work well.</p>
<p>This procedure implicitly defines the intended behavior of the system as “do what the overseer would have done/approved of, if this situation had appeared in training.”</p>
<p>From a control perspective, this is unsatisfying because the counterfactual (“if this situation had appeared in training”) may be extremely improbable.</p>
<p>Counterfactual oversight is one way to get around this problem. But a more natural way is to simply try our best to make the training data “look like” the test data. This post lays out a sequence of proposals along these lines.</p>
<h3 id="model">Model</h3>
<p>I’ll assume that we have a constant supply of tasks, but that the distribution of tasks may change over time. The most interesting changes are those that arise from the introduction of machine learning systems — these seem especially hard to deal with using traditional techniques. These changes are also unavoidable for the bootstrapping protocols I have been considering recently.</p>
<p>I’ll also assume that we are willing to invest continuously in training (this is especially plausible if the same training data is deployed across many systems).</p>
<p>I’ll talk about training occurring on discrete days. Obviously this is an unrealistic simplifying simplification and the time frame is arbitrary, it should be clear how to make the setting more realistic.</p>
<p>For notation, I’ll assume that the training occurs in the morning of each day, and all actual decisions occur later. If we need to use AI assistants to help us with training, they will use the same model that we deployed in the field on the previous day.</p>
<p>I’ll call the data that we actually care about the “test” data.</p>
<p>I’ll imagine we are training a learner, which I’ll call the “actor,” to predict which action a human would recommend in response to its most recent observation. I’ll assume that the “observation” includes the current state of the system and that the action includes the next state of the system.</p>
<h1 id="approaches">Approaches</h1>
<h3 id="1thebasics">1. The basics</h3>
<p>On the morning of day N we train the actor using observations from previous days. We’ll label each of these observations with “day N.”</p>
<p>During day N, we tag each observation with “day N” before giving it to the actor. So the actor is trying to predict what the overseer&nbsp;<em>would have said</em>, if that observation had occurred on a previous day and been labelled that morning.</p>
<p>In principle this avoids the problem discussed in&nbsp;<a href="https://ordinaryideas.wordpress.com/2015/11/30/driving-fast-in-the-counterfactual-loop/">this post</a>. After training on the morning of day N, we have produced a system that can operate autonomously without further oversight.</p>
<p>But this approach introduces the problem mentioned above: some observations from day N may obviously be from day N. We never train our system on this kind of labelled data — the data we actually use on the morning of day N is always from preceding days — and so its not clear how it will respond to it.</p>
<h3 id="2sanitychecking">2. Sanity-checking</h3>
<p>A first response is to at least&nbsp;<em>notice</em>&nbsp;the context change and abort or provide a warning. To this end we train an additional learner:</p>
<ul>
<li>The&nbsp;<strong>distinguisher</strong>&nbsp;is given a tagged observation and needs to output a probability distribution over “training” or “test.” On day N it is trained using the training data and test data from all previous days (re-weighted so that the two categories are equiprobable), with the same labels they were given at the time. It is aiming to distinguish the two kinds of data.</li>
</ul>
<p>When training the distinguisher, we ensure that each observation appears only once to avoid memorization. For example, we could randomly partition the test data into “data to be used to train the system on a future day” and “data to be used to train the distinguisher.”</p>
<p>To use our system on day N, we tag observations with “day N” and then feed them to the distinguisher. If the distinguisher is very confident (say &gt;99%) that this is test data, then we abort and notify the user that the observation is too far from the training distribution. If the distinguisher thinks that the data may be training data, then we pass it on to the actor.</p>
<p>We also modify the actor: when running the actor on day N, we use only the training data through the morning of day N-1. (We leave out the data from that morning.)</p>
<p>In order for this scheme to work well the distinguisher should be as good at distinguishing as the actor — otherwise the actor might behave strangely on an input because it could not be in the training distribution, while the distinguisher might not notice. (This is why we delayed the actor’s training by a day — to keep it synchronized with the distinguisher.)</p>
<p>We could try to enforce this property by combining the two models, e.g. by allowing the actor to output ⊥ and providing payoffs that incentivize this output whenever the actor is 99% sure that an observation is test data.</p>
<h3 id="theproblem">The problem</h3>
<p>The previous scheme seems to behave well when it is willing to make a prediction. But it may reject data too often. For example, any observation with a calendar would be rejected automatically.</p>
<p>Obviously these aren’t important details, and it&nbsp;<em>ought</em>&nbsp;to be easy to transfer knowledge across this kind of “context change.” But it’s not a trivial problem to deal with (and it has much more pernicious cousins).</p>
<p>To see the problem, consider the basic actor encountering an observation labelled “November 29.” Suppose that it can tell from the position of the sun that it is really November 30. What will it do? It knows that that this data was collected on November 30. But it also knows that it’s predicting the label that the human would produce on November 29. I’m not even sure what it&nbsp;<em>should</em>&nbsp;do with this data. Does predict how the human would label the data on December 1? Does it predict what would happen in the November 29 human saw data from November 30? Does it evaluate the data <em>as if</em>&nbsp;it were collected before November 29, despite the position of the sun? Does it infer that it is confused about the weather?</p>
<p>So we’d like some way to transfer knowledge across this kind of simple change. I think this is an especially reasonable thing to ask of unsupervised learning, but I’d still&nbsp;<a href="unsupervised_learning_ai_control.html">prefer to avoid it</a>.</p>
<p><strong>Note</strong>: I’m using the date itself as a feature that reliably differs between the test data and the training data. I think this is a helpful, simple example to keep in mind. But obviously we care about more substantive changes, especially those that are caused by deploying increasingly sophisticated machine learning systems or by changes that affect the operators themselves. For example, on November 30 the humans are interacting with (very marginally) more sophisticated machine learning systems than were available on November 29.</p>
<p>These differences are especially noticeable for test observations that occur during the mornings, when one AI system is helping the human train another. They also become more noticeable as these AI systems play a larger role in the world and especially in their local environments.</p>
<h3 id="3synthesizingdata">3. Synthesizing data</h3>
<p>To address this problem, we’ll introduce another learning system:</p>
<ul>
<li>The&nbsp;<strong>synthesizer</strong>&nbsp;is trained to produce samples that the distinguisher will tag as “test.” On the morning of day N the synthesizer produces observations that will be labelled with “day N.” The results are scored by the distinguisher and P(“test”) is the payoff of the synthesizer. The resulting data points are used as observations to train the actor and distinguisher on the morning of day N.</li>
</ul>
<p>Note that the data from the synthesizer is being included in the training data, and so if the distinguisher is able to tell that a data point is produced by the synthesizer, then it will label it as “training.” So the synthesizer must produce realistic-looking data.</p>
<p>If the synthesizer can actually learn to sample from the data distribution (well enough that the distinguisher can’t tell the difference), then this seem like an extremely effective procedure. But sampling from the data distribution may be quite difficult.</p>
<p>We can make the synthesizer’s task a bit easier by setting aside some data from previous days as raw material. Rather than synthesizing data&nbsp;<em>de novo</em>the synthesizer can potentially use features from this data and simply make the modifications that are necessary to make the data look like it is from a future day.</p>
<p>This is my last principled proposal. The rest of the post is about an ad hoc technique for making the synthesizer’s job easier.</p>
<h3 id="4basicfiltering">4. Basic filtering</h3>
<p>We can take additional steps to make the the synthesis easier. One idea is to allow the synthesizer to leave out parts of the data (and filter these parts of the data when it appears). In this section I’ll describe a very basic version of this proposal, and in the next section I’ll describe a slightly less terrible version.</p>
<p>We allow the synthesizer to synthesize observations in which each bit is either 0, 1, or *, which I’ll call “filtered” observations. (If the observations aren’t encoded in binary, use whatever arbitrary filtering you want.) We don’t change the payoffs for the synthesizer — it still wants to convince the distinguisher that this filtered data appeared during the test period (though I haven’t described how that could possibly happen). We also don’t change the actor or distinguisher, who keep doing exactly the same thing but now operate on filtered data.</p>
<p>We introduce an additional system:</p>
<ul>
<li>The&nbsp;<strong>censor</strong>&nbsp;processes test data before it is given to the distinguisher. The censor chooses a subset of bits in the data to replace with *. The censor needs to find a set of bits such that the distinguisher assigns at least a 1% probability to the data having come from the training set. Subject to that constraint, the censor’s loss function is the number of bits that are removed.</li>
</ul>
<p>As a simple example, suppose that each morning the pope publishes a message, which includes the day’s date and which is cryptographically signed with the pope’s public key. It would be very difficult for the synthesizer to produce any credible observations of the pope’s message. But by filtering a small part of the observation, the censor can make it impossible to verify that the pope really signed the morning’s message, and the synthesizer may be able to synthesize this filtered message in advance (e.g. by using a previous day’s message, modifying it, and then censoring the signature).</p>
<h3 id="5generalfiltering">5. General filtering</h3>
<p>Removing individual bits of an observation is pretty brittle; in reality we would like to allow a much broader class of transformations.</p>
<p>(<em>Note: this part of the proposal is completely outlandish given existing learning techniques.</em>)</p>
<p>We can generalize the filtering process by having the censor specify any stochastic program&nbsp;<em>P</em>&nbsp;and providing&nbsp;<em>P</em>&nbsp;along with a random output&nbsp;<em>P</em>(<em>x</em>), where&nbsp;<em>x</em>&nbsp;is the observation-to-be-filtered. This significantly generalizes the process of filtering out particular bits. For example,&nbsp;<em>P</em>&nbsp;can extract high-level features, censor large parts of the data but summarize the removed data, or censor parts of the data which don’t neatly line up as bits.</p>
<p>These operations can obscure data that changes over time, allowing the synthesizer to make more use of historical data. But they can also obscure data that is hard for the synthesizer to synthesize, potentially making its job easier.</p>
<p>This more general filtering significantly complicates the definition of the censor’s task. We want to encourage the censor to censor as little as possible, i.e. to leave a data point which is as useful as possible for the underlying task. But what does that objective actually mean?</p>
<p>Note that we can’t get human feedback on pairs of the form (<em>filtered data</em>,<em>real data</em>), because the whole problem is that we can’t get any human feedback on data that looks like the real data. (Until the next morning.)</p>
<p>But we&nbsp;<em>can</em>&nbsp;train directly on the filtered data, and estimate a measure of how useful it is. After all, the filtered data (by hypothesis) looks a lot like the data available during the morning’s training. So we introduce yet another learner:</p>
<ul>
<li>The&nbsp;<strong>evaluator</strong>&nbsp;is trained on filtered observations, and predicts what score in [0, 1] a human would assign to that data. The human assigns the score based on how useful the data seems to be, i.e. on how little important information the filtering seems to have removed. The human judgments are elicited during the morning, using data produced by the synthesizer.</li>
<li>The&nbsp;<strong>censor</strong>&nbsp;now tries to produce filtered data that the human would rate highly.</li>
</ul>
<p>This seems kind of workable, doing essentially the same thing as the previous proposal while allowing a much broader class of preprocessing.</p>
<p>But it seems very difficult for the human to assign scores in the abstract (what kind of data deserves a score of 0.73? there is no natural scale to compare to, but the human needs to make consistent judgments in order for the proposal to work). And the user may not know if the filtered data was critical or unimportant, which may affect their ranking. So I think&nbsp;<a href="optimizing_with_comparisons.html">the following procedure</a>&nbsp;would work significantly better:</p>
<ul>
<li>The&nbsp;<strong>evaluator</strong>&nbsp;is trained on pairs of filtered observations. It predicts which of those observations the human would prefer. This data is gathered in the same way as before.</li>
<li>The&nbsp;<strong>censor</strong>&nbsp;still optimizes to produce filtered data that the human would prefer. But instead of being defined by maximizing a rating, this is now defined by&nbsp;<a href="optimizing_with_comparisons.html">comparing favorably to other filtered versions that it might have produced</a>. The censor is still required to convince the distinguisher that the filtered observations have at least a 1% probability of being training data.</li>
<li>The&nbsp;<strong>synthesizer</strong>&nbsp;now produces pairs of observations, which are purportedly two different filtered versions of the same underlying data.</li>
<li>The&nbsp;<strong>distinguisher</strong>&nbsp;now operates on pairs of two different filtered versions of the same underlying data, and predicts whether they were produced by the synthesizer or they were actually two different filtered versions of a test observation.</li>
</ul>
<p>I think this modified protocol places more realistic demands on the human evaluator, though the roles of the synthesizer and censor are still extraordinarily complex.</p>
<p>The key question is: can the synthesizer actually anticipate the next day’s data? Or will they be forced to filter extremely aggressively, destroying information that is essential to actually solving the problem?</p>
<p>Between expressive filtering and the ability to reuse parts of past data, it seems like the synthesizer’s task is at least&nbsp;<em>possibly possible.</em> And if successful, the interaction between the synthesizer and censor cleanly defines the intended behavior of the actor even on unprecedented data whose details are impossible-to-anticipate.</p>
<h1 id="postscript">Postscript</h1>
<h3 id="state">State</h3>
<p>An algorithm operating in the real world will build up a representation of its environment. The distinguisher and synthesizer will necessarily operate on observations that include a representation of the environment, and we should probably allow the censor to filter parts of that representation.</p>
<p>Training this kind of system, on individual data points that include state rather than on interactions, requires a&nbsp;<a href="Learning_representations.html">somewhat unusual approach to representation learning</a>. This foregoes some of the techniques that are currently essential, and it remains to be seen whether it is practical.</p>
<p>I think that compatibility with this kind of synthesis is actually a nice concrete test of whether a proposed approach to representation learning is suitable for using directly in&nbsp;<a href="act_based_agents.html">act-based agents</a>&nbsp;(as I currently understand them).</p>
<h3 id="adversarialsynthesis">Adversarial synthesis</h3>
<p>If this kind of synthesis can work really well, it has the potential to add a lot of robustness.</p>
<p>Suppose that our system would predictably require human input to perform well on some kind of as-yet-unseen data — maybe its an object that sits on an important and unexplored boundary between categories, maybe its a situation that depends on unknown preferences of the user, maybe it is <a href="http://www.datascienceassn.org/sites/default/files/Intriguing%20Properties%20of%20Neural%20Networks_0.pdf">something</a>&nbsp;<a href="simulations_inductive_definitions.htmlsimulations_inductive_definitions">strange</a>.</p>
<p>It may be that on these cases our model is uncertain, and so knows that it expects to receive an unusually low payoff. It may be that an auxiliary model can recognize that this is the kind of case where the model would behave badly. At any rate, let’s suppose that this kind of case can be seen in advance to cause trouble.</p>
<p>If these cases appear in the training data, then it would be natural to emphasize them in order to improve the model’s robustness, as in boosting or hard negative mining.</p>
<p>If we can synthesize data (and have extremely good models of what might go wrong) then we can take this a step further, synthesizing problem cases that don’t appear in the training set. (And of course this can be combined with censoring or with other techniques to make the synthesizer’s job easier.) If this worked well, it could potentially make our models much more robust. It would guarantee not only that our models behave reasonably on the data we have seen so far, but on all of the data that we can imagine.</p>
<p>A simple way to implement this is to change the rewards for the synthesizer. It can now achieve payoffs in one of two ways:</p>
<ul>
<li>As before, produce data that the distinguisher thinks must be test data.</li>
<li>Produce data which (1) the distinguisher thinks&nbsp;<em>might</em>&nbsp;be test data, and (2) that seem to be problem spots for the current actor.</li>
</ul>
<p>It’s hard to define what “trip up the current actor” means. For example, we don’t want to produce cases that are simply&nbsp;<em>hard</em>&nbsp;(or outside of the domain of applicability of our current techniques). We want to produce cases that are hard but that the model could be trained to perform well on. We also want to train on cases that are relatively realistic, or at least have a high product of [importance of mistake] × [1rf of situation].</p>
<p>We could simply ask the user “does it look like the actor should be able to do better?” We could also compare to the future behavior of the actor after training on some other similar examples, and consider a data point to be a current problem spot if the actor is able to improve rapidly with further training. Or we could use other techniques (e.g. looking at gradients, looking for explicit uncertainty, or looking at the behavior of similar models). Of course if we ask the user they are free to make use of these kinds of techniques.</p>
<p>(As usual,&nbsp;<a href="optimizing_with_comparisons.html">it’s sufficient</a>&nbsp;for us to rank which of two mistakes is graver/more-fixable, rather than to score a mistake in isolation.)</p>
<p>This is obviously a very hard task for the synthesizer. But it doesn’t require the synthesizer to be much smarter than the actor. So as machine learning improves, this kind of technique might become a viable way to significantly improve robustness. Of course this process need not be done exclusively by machines — humans and machines can work together to try to synthesize problematic cases.</p></main><hr><footer></footer></body></html>