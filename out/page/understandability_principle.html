<!DOCTYPE html><html><head><meta charset="utf-8"><title>Understandability principle</title><link rel="stylesheet" type="text/css" href="../common.css"><link rel="stylesheet" type="text/css" href="../page-style.css"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  SVG: {EqnChunk: 50, EqnChunkFactor: 1.5, EqChunkDelay: 10, useFontCache: false, linebreaks: {automatic: true}},
  tex2jax: {
    inlineMath: [['$~$', '$~$']],
    displayMath: [['$$~$', '$~$$']],
    processEscapes: true,
    preview: 'none',
  },
  showProcessingMessages: false,
  messageStyle: 'none',
  // http://docs.mathjax.org/en/latest/config-files.html#the-tex-ams-svg-configuration-file
  jax: ["input/TeX","output/SVG", "output/PreviewHTML"],
  extensions: ["tex2jax.js","MathMenu.js","MathZoom.js", "fast-preview.js", "AssistiveMML.js", "a11y/accessibility-menu.js"],
  TeX: { extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"] }
});</script><script type="text/javascript" src="../MathJax-2.7.5/MathJax.js"></script><script type="text/javascript" src="../arbital-demo-bundle.js"></script><script type="text/javascript">window.addEventListener('DOMContentLoaded', e=>window.loadAllDemos())
</script></head><body><header><h1 class="title">Understandability principle</h1><div class="page-info"><p class="metadata-link"><a href="../metadata/understandability_principle.json.html">understandability_principle.json</a></p><p class="arbital-url"><a href="https://arbital.com/p/understandability_principle">https://arbital.com/p/understandability_principle</a></p><p class="creator">by
 <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a> Feb 16 2017 
updated
 Mar 7 2017</p></div><p class="clickbait">The more you understand what the heck is going on inside your AI, the safer you are.</p><nav class="breadcrumbs"><nav class="breadcrumb" aria-label="Breadcrumb"><ul><li><a href="../index.html">Index</a></li><li>Understandability principle</li></ul></nav><nav class="breadcrumb" aria-label="Breadcrumb"><ul><li><a href="ai_alignment.html">AI alignment</a></li><li><a href="alignment_principle.html">Principles in AI alignment</a></li><li>â€¦</li></ul></nav></nav></header><hr><main><p>An obvious <a href="alignment_principle.html">design principle</a> of <a href="ai_alignment.html">AI alignment</a> that nonetheless deserves to be stated explicitly:  The more you understand what the heck is going on inside your AI, the more likely you are to succeed at aligning it.</p>
<p>This principle participates in motivating design subgoals like [passive_transparency passive transparency]; or the AI having explicitly represented preferences; or, taken more broadly, pretty much every aspect of the AI design where we think we understand how any part works or what any part is doing.</p>
<p>The Understandability Principle in its broadest sense is <em>so</em> widely applicable that it may verge on being an [applause_light applause light].  So far as is presently known to the author(s) of this page, counterarguments against the importance of understanding at least <em>some</em> parts of the AI's thought processes, have been offered only by people who reject at least one of the <a href="orthogonality.html">Orthogonality Thesis</a> or the [fragility Fragility of Cosmopolitan Value thesis].  That is, the Understandability Principle in this very broad sense is rejected only by people who reject in general the importance of deliberate design efforts to align AI.</p>
<p>A more controversial subthesis is Yudkowsky's proposed <a href="effability.html">Effability principle</a>.</p></main><hr><footer><p class="tagged"><h2>Tagged</h2><span class="page-comma-list"><a class="page-link" href="../page/stub_meta_tag.html">Stub</a></span></p><p class="all-creators"><h2>All Creators</h2><span class="page-comma-list"><a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a>,
 <a class="page-link" href="../page/RobBensinger2.html">Rob Bensinger</a></span></p><p class="reverse-related"><h2>Reverse Related</h2><ul class="page-list"><li><a class="page-link" href="../page/stub_meta_tag.html">Stub</a> <q>This page only gives a very brief overview of the topic. If you're able to, please help expand or improve it!</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li></ul></p><p class="children"><h2>Children</h2><ul class="page-tree"><li><a class="page-link" href="../page/effability.html">Effability principle</a> <q>You are safer the more you understand the inner structure of how your AI thinks; the better you can describe the relation of smaller pieces of the AI's thought process.</q> - <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a></li></ul></p></footer></body></html>