<!DOCTYPE html><html><head><meta charset="utf-8"><title> CFAR should explicitly focus on AI safety</title><link rel="stylesheet" type="text/css" href="../common.css"><link rel="stylesheet" type="text/css" href="../page-style.css"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  SVG: {EqnChunk: 50, EqnChunkFactor: 1.5, EqChunkDelay: 10, useFontCache: false, linebreaks: {automatic: true}},
  tex2jax: {
    inlineMath: [['$~$', '$~$']],
    displayMath: [['$$~$', '$~$$']],
    processEscapes: true,
    preview: 'none',
  },
  showProcessingMessages: false,
  messageStyle: 'none',
  // http://docs.mathjax.org/en/latest/config-files.html#the-tex-ams-svg-configuration-file
  jax: ["input/TeX","output/SVG", "output/PreviewHTML"],
  extensions: ["tex2jax.js","MathMenu.js","MathZoom.js", "fast-preview.js", "AssistiveMML.js", "a11y/accessibility-menu.js"],
  TeX: { extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"] }
});</script><script type="text/javascript" src="../MathJax-2.7.5/MathJax.js"></script><script type="text/javascript" src="../arbital-demo-bundle.js"></script><script type="text/javascript">window.addEventListener('DOMContentLoaded', e=>window.loadAllDemos())
</script></head><body><header><h1 class="title"> CFAR should explicitly focus on AI safety</h1><div class="page-info"><p class="metadata-link"><a href="../metadata/6wx.json.html">6wx.json</a></p><p class="arbital-url"><a href="https://arbital.com/p/6wx">https://arbital.com/p/6wx</a></p><p class="creator">by
 <a class="page-link" href="../page/StephanieZolayvar.html">Stephanie Zolayvar</a> Dec 16 2016</p></div><nav class="breadcrumbs"><nav class="breadcrumb" aria-label="Breadcrumb"><ul><li><a href="../index.html">Index</a></li><li> CFAR should explicitly focus on AI safety</li></ul></nav></nav></header><hr><main><p>The Center for Applied Rationality has historically had a "cause-neutral" mission but has recently revised its mission to partly be focused on AI safety efforts in particular.</p></main><hr><section class="comments"><h2>Comments</h2><div class="comment"><p><a class="page-link" href="../page/AnnaSalamon.html">Anna Salamon</a></p><p><p>I want a wrong question button!! :/</p></p></div><div class="comment"><p><a class="page-link" href="../page/TimothyChu.html">Timothy Chu</a></p><p><p>Addressing the post, a focus on AI risk feels like something worth experimenting with. </p>
<p><a href="70q.html">My lame model suggests that the main downside is that it risks the brand</a>. If so, experimenting with AI risk in the CFAR context seems like a potentially high value avenue of exploration, and brand damage can be mitigated. </p>
<p><a href="70r.html">For example, if it turned out to be toxic for the CFAR brand, the same group of people could spin off a new program called something else, and people may not remember or care that it was the old CFAR folks</a>.</p></p></div><div class="comment"><p><a class="page-link" href="../page/ConnorFlexman3.html">Connor Flexman</a></p><p><p>Along with "Growing EA is net-positive", anything with a large search space + value judgment seems like it's going to have this issue. </p></p></div></section><footer><p class="likes"><h2>Likes</h2><span class="page-comma-list"><a class="page-link" href="../page/AlexeiAndreev.html">Alexei Andreev</a></span></p></footer></body></html>