<!DOCTYPE html><html><head><meta charset="utf-8"><title>Absent-Minded Driver dilemma</title><link rel="stylesheet" type="text/css" href="../common.css"><link rel="stylesheet" type="text/css" href="../page-style.css"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  SVG: {EqnChunk: 50, EqnChunkFactor: 1.5, EqChunkDelay: 10, useFontCache: false, linebreaks: {automatic: true}},
  tex2jax: {
    inlineMath: [['$~$', '$~$']],
    displayMath: [['$$~$', '$~$$']],
    processEscapes: true,
    preview: 'none',
  },
  showProcessingMessages: false,
  messageStyle: 'none',
  // http://docs.mathjax.org/en/latest/config-files.html#the-tex-ams-svg-configuration-file
  jax: ["input/TeX","output/SVG", "output/PreviewHTML"],
  extensions: ["tex2jax.js","MathMenu.js","MathZoom.js", "fast-preview.js", "AssistiveMML.js", "a11y/accessibility-menu.js"],
  TeX: { extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"] }
});</script><script type="text/javascript" src="../MathJax-2.7.5/MathJax.js"></script><script type="text/javascript" src="../arbital-demo-bundle.js"></script><script type="text/javascript">window.addEventListener('DOMContentLoaded', e=>window.loadAllDemos())
</script></head><body><header><h1 class="title">Absent-Minded Driver dilemma</h1><div class="page-info"><p class="metadata-link"><a href="../metadata/absentminded_driver.json.html">absentminded_driver.json</a></p><p class="arbital-url"><a href="https://arbital.com/p/absentminded_driver">https://arbital.com/p/absentminded_driver</a></p><p class="creator">by
 <a class="page-link" href="../page/EliezerYudkowsky.html">Eliezer Yudkowsky</a> Aug 1 2016 
updated
 Aug 2 2016</p></div><p class="clickbait">A road contains two identical intersections.  An absent-minded driver wants to turn right at the second intersection.  &quot;With what probability should the driver turn right?&quot; argue decision theorists.</p><nav class="breadcrumbs"><nav class="breadcrumb" aria-label="Breadcrumb"><ul><li><a href="../index.html">Index</a></li><li>Absent-Minded Driver dilemma</li></ul></nav><nav class="breadcrumb" aria-label="Breadcrumb"><ul><li><a href="decision_theory.html">Decision theory</a></li><li><a href="logical_dt.html">Logical decision theories</a></li><li><a href="newcomblike.html">Newcomblike decision problems</a></li><li>â€¦</li></ul></nav></nav></header><hr><main><p>[summary:  An absent-minded driver is traveling down a road with two identical-looking intersections.  They want to exit at the second intersection, but can't remember if they've passed the first intersection already.</p>
<p>The utility exiting at the first intersection is \$0, the utility of exiting at the second intersection is \$4, and the utility of continuing past both intersections is \$1.</p>
<p>Since the driver has to implement the same policy at both intersections, with what probability $~$p$~$ should they continue at each intersection to maximize expected utility?</p>
<p>The correct answer is 2/3.  However, this optimal policy is complicated to arrive at under <a href="causal_dt.html">Causal decision theories</a>, and in some formulations is never output at all.  Because, depending on what policy we choose, the probability that we're already at the second intersection is different; and according to CDT, if you're already at the second intersection, this remains true no matter what policy you choose now, etcetera.  Thus the Absent-Minded Driver is widely considered to be a difficult or complicated <a href="newcomblike.html">Newcomblike dilemma</a> under CDT.]</p>
<p>A road contains two-identical looking intersections.  An absent-minded driver wants to exit at the second intersection, but can't remember if they've passed the first intersection already.</p>
<p>The utility of exiting at the first intersection is \$0, the utility of exiting at the second intersection is \$4, and the utility of continuing straight past both intersections is \$1. %note: <a href="utility_function.html">Utility functions</a> describe the <em>relative</em> desirability intervals between outcomes.  So this payoff matrix says that the added inconvenience of "going past both intersections" compared to "turning right at 2nd" is 1/4 of the added inconvenience of "turning right at 1st" compared to "turning right at 2nd".  Perhaps turning right at 1st involves a much longer detour by the time the driver realizes their mistake, or a traffic-jammed stoplight to get back on the road.%</p>
<p>With what probability should the driver continue vs. exit at a generic-looking intersection, in order to maximize their expected utility?</p>
<h1 id="analyses">Analyses</h1>
<p>From the standpoint of <a href="newcomblike.html">Newcomblike problems</a>, the Absent-Minded Driver is noteworthy because the logical correlation of the two decisions arises just from the agent's imperfect memory (anterograde amnesia or limited storage space).  There is no outside <a href="omega_troll.html">Omega</a> making predictions about the agent; any problem that the agent encounters is strictly of its own making.</p>
<h2 id="intuitivepretheoretic">Intuitive/pretheoretic</h2>
<p>The driver doesn't know each time whether they're at the first or second intersection, so will continue with the same probability $~$p$~$ at each intersection.  The expected payoff of adopting $~$p$~$ as a policy is the sum of:</p>
<ul>
<li>\$0 times the probability $~$1 - p$~$ of exiting at 1st;</li>
<li>\$4 times a $~$p$~$ probability of continuing past first multiplied by a $~$1 - p$~$ probability of exiting at the second intersection;</li>
<li>\$1 times a $~$p^2$~$ probability of continuing past both intersections.</li>
</ul>
<p>To find the maximum of the function $~$0(1-p) + 4(1-p)p + 1p^2$~$ we set the <a href="http://www.wolframalpha.com/input/?i=d%2Fdp+%5B0(1-p)+%2B+4(1-p">derivative</a>p+%2B+1p%5E2%5D) $~$4 -6p$~$ equal to 0 <a href="http://www.wolframalpha.com/input/?i=maximize+%5B0(1-p)+%2B+4(1-p">yielding</a>p+%2B+1p%5E2%5D) $~$p = \frac{2}{3}$~$.</p>
<p>So the driver should continue with 2/3 probability and exit with 1/3 probability at each intersection, <a href="http://www.wolframalpha.com/input/?i=p%3D2%2F3,+%5B0(1-p)+%2B+4(1-p">yielding</a>p+%2B+1p%5E2%5D) an expected payoff of $~$\$0\cdot\frac{1}{3} + \$4\cdot\frac{2}{3}\frac{1}{3} + \$1\cdot\frac{2}{3}\frac{2}{3} = \$\frac{4}{3} \approx \$1.33.$~$</p>
<h2 id="causaldecisiontheory">Causal decision theory</h2>
<p>The analysis of this problem under <a href="causal_dt.html">causal decision theory</a> has traditionally been considered difficult; e.g., Volume 20 of the journal <em>Games and Economic Behavior</em> was devoted entirely to the Absent-Minded Driver game.</p>
<p>Suppose that before you set out on your journey, you intended to adopt a policy of continuing with probability 2/3.  Then when you actually encounter an intersection, you believe you are at the second intersection with probability 3/5.  (There is a 100% or 3/3 chance of encountering the first intersection, and a 2/3 chance of encountering the second intersection.  So the <a href="odds.html">odds</a> are 3 : 2 for being in the first intersection versus the second intersection.)</p>
<p>Now since you are <em>not</em> a <a href="logical_dt.html">logical decision theorist</a>, you believe that if you happen to <em>already</em> be at the second intersection, you can change your policy $~$p$~$ without retroactively affecting the probability that you're already at the second intersection - either you're already at the second intersection or not, after all!</p>
<p>The first analysis of this problem was given by Piccione and Rubinstein (1997):</p>
<p>Suppose we start out believing we are continuing with probability $~$q.$~$  Then our odds of being at the first vs. second intersection would be $~$1 : q,$~$ so the probability of being at each intersection would be $~$\frac{1}{1+q}$~$ and $~$\frac{q}{1+q}$~$ respectively.</p>
<p>If we're at the first intersection and we choose a policy $~$p,$~$ we should expect a future payoff of $~$4p(1-p) + 1p^2.$~$   If we're already at the second intersection, we should expect a policy $~$p$~$'s future payoff to be $~$4(1-p) + 1p.$~$</p>
<p>In total our expected payoff is then $~$\frac{1}{1+q}(4p(1-p) + p^2) + \frac{q}{1+q}(4(1-p) + p)$~$ whose <a href="http://www.wolframalpha.com/input/?i=d%2Fdp+%5B4p(1-p)+%2B+p%5E2+%2B+q(4(1-p">derivative</a>+%2B+p%29%5D%2F(q%2B1)) $~$\frac{-6p - 3q + 4}{q+1}$~$ equals 0 at $~$p=\frac{4-3q}{6}.$~$</p>
<p>Our decision at $~$q$~$ will be stable only if the resulting maximum of $~$p$~$ is equal to $~$q,$~$ and this is true when $~$p=q=\frac{4}{9}.$~$  The expected payoff from this policy is $~$\$4\cdot\frac{4}{9}\frac{5}{9} + \$1\cdot\frac{4}{9}\frac{4}{9} \approx \$1.19.$~$</p>
<p>However, the immediately following paper by <a href="http://www.ma.huji.ac.il/hart/papers/driver.pdf">Robert Aumann et. al. (1997)</a> offered an alternative analysis in which, starting out believing our policy to be $~$q$~$, if we are at the first intersection, then our decision $~$p$~$ also cannot affect our decision $~$q$~$ that will be made at the second intersection. %note: From an <a href="logical_dt.html">LDT</a> perspective, at least the <a href="causal_dt.html">CDT</a> agent is being consistent about ignoring logical correlations!% So:</p>
<ul>
<li>If we had in fact implemented the policy $~$q,$~$ our <a href="odds.html">odds</a> for being at the first vs. second intersection would be $~$1 : q \cong \frac{1}{1+q} : \frac{q}{1+q}$~$ respectively.</li>
<li><em>If</em> we're at the first intersection, then the payoff of choosing a policy $~$p,$~$ given that our future self will go on implementing $~$q$~$ regardless, is $~$4p(1-q) + 1pq.$~$</li>
<li><em>If</em> we're already at the second intersection, then the payoff of continuing with probability $~$p$~$ is $~$4(1-p) + 1p.$~$</li>
</ul>
<p>So if our policy is $~$q,$~$ the expected payoff of the policy $~$p$~$ under CDT is:</p>
<p>$$~$\frac{1}{1+q}(4p(1-q) + pq) + \frac{q}{1+q}(4(1-p) + p)$~$$</p>
<p>Differentiating with respect to p yields $~$\frac{4 - 6q}{1+q}$~$ which has no dependency on $~$p.$~$  This makes a kind of sense, since if your decision now has no impact on your past or future decision at the other intersection, most settings of $~$q$~$ will just yield an answer of "definitely turn right" or "definitely turn left".  However, there is a setting of $~$q$~$ which makes any policy $~$p$~$ seem equally desirable, the point at which $~$4-6q = 0 \implies q=\frac{2}{3}.$~$  Aumann et al. take this to imply that a CDT agent should output a $~$p$~$ of 2/3.</p>
<p>One might ask how this result of 2/3 is actually rendered into an output, since on the analysis of Aumann et. al., if your policy $~$q$~$ in the past or future is to continue with 2/3 probability, then <em>any</em> policy $~$p$~$ seems to have equal utility.  However, outputting $~$p$~$=2/3 would also correspond to the general procedure proposed to resolve e.g. <a href="death_in_damascus.html">Death in Damascus</a> within <a href="causal_dt.html">CDT</a>.  Allegedly, it is just a general rule of the [principle_rational_choice principle of rational choice] that in this type of problem one should find a policy where, assuming one implements that policy, all policies look equally good, and then do that.</p>
<p>Further analyses have, e.g., <a href="http://www.umsu.de/words/driver.pdf">remarked on the analogy to the Sleeping Beauty Problem</a> and delved into anthropics; or considered the problem as a game between two different agents occupying each intersection, etcetera.  It is considered nice to arrive at an answer of 2/3 at the end, but this is not mandatory.</p>
<h2 id="logicaldecisiontheory">Logical decision theory</h2>
<p><a href="logical_dt.html">Logical decision theorists</a> using, e.g., the <a href="updateless_dt.html">updateless</a> form of [timeless_dt timeless decision theory], will compute an answer of 2/3 using the same procedure and computation as in the intuitive/pretheoretic version.  They will also remark that it is strange to imagine that the reasonable answer could be different from the optimal policy, or even that they should require a different reasoning path to compute; and will note that while simplicity is not the <em>only</em> virtue of a theory of instrumental rationality, it is <em>a</em> virtue.</p></main><hr><footer><p class="tagged"><h2>Tagged</h2><span class="page-comma-list"><a class="page-link" href="../page/b_class_meta_tag.html">B-Class</a></span></p><p class="likes"><h2>Likes</h2><span class="page-comma-list"><a class="page-link" href="../page/EricRogstad.html">Eric Rogstad</a></span></p><p class="reverse-related"><h2>Reverse Related</h2><ul class="page-list"><li><a class="page-link" href="../page/b_class_meta_tag.html">B-Class</a> <q>This page is mostly complete and without major problems, but has not had detailed feedback from the target audience and reviewers.</q> - <a class="page-link" href="../page/EricBruylant.html">Eric Bruylant</a></li></ul></p></footer></body></html>