{
  "resetEverything": false,
  "user": {
    "id": "",
    "firstName": "",
    "lastName": "",
    "lastWebsiteVisit": "",
    "isSubscribed": false,
    "domainMembershipMap": {},
    "fbUserId": "",
    "email": "",
    "isAdmin": false,
    "emailFrequency": "",
    "emailThreshold": 0,
    "ignoreMathjax": false,
    "showAdvancedEditorMode": false,
    "isSlackMember": false,
    "analyticsId": "aid:HjtoRsOSDgb1ytnpgWDqtrVFZ2P1sOebgyJSHvjJ4+0",
    "hasReceivedMaintenanceUpdates": false,
    "hasReceivedNotifications": false,
    "newNotificationCount": 0,
    "newAchievementCount": 0,
    "maintenanceUpdateCount": 0,
    "invitesClaimed": [],
    "mailchimpInterests": {},
    "continueBayesPath": null,
    "continueLogPath": null
  },
  "pages": {
    "1": {
      "likeableId": "1",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "1",
      "edit": 5,
      "editSummary": "",
      "prevEdit": 4,
      "currentEdit": 5,
      "wasPublished": true,
      "type": "group",
      "title": "Alexei Andreev",
      "clickbait": "There is no spoon",
      "textLength": 304,
      "alias": "AlexeiAndreev",
      "externalUrl": "",
      "sortChildrenBy": "alphabetical",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "1",
      "editCreatedAt": "2016-12-13 02:34:00",
      "pageCreatorId": "1",
      "pageCreatedAt": "2015-09-04 16:14:58",
      "seeDomainId": "0",
      "editDomainId": "21",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 0,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 741,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": 0,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "2": {
      "likeableId": "938",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "2",
      "edit": 4,
      "editSummary": "",
      "prevEdit": 3,
      "currentEdit": 4,
      "wasPublished": true,
      "type": "group",
      "title": "Eliezer Yudkowsky",
      "clickbait": "Cofounder, with Nick Bostrom, of the field of value alignment theory.",
      "textLength": 512,
      "alias": "EliezerYudkowsky",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "2",
      "editCreatedAt": "2015-12-19 01:46:45",
      "pageCreatorId": "2",
      "pageCreatedAt": "2015-09-04 16:14:58",
      "seeDomainId": "0",
      "editDomainId": "2",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 5,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 2016,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": 0,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "15": {
      "likeableId": "140",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "15",
      "edit": 2,
      "editSummary": "",
      "prevEdit": 1,
      "currentEdit": 2,
      "wasPublished": true,
      "type": "wiki",
      "title": "Serum 25-Hydroxyvitamin D and Risks of Colon and Rectal Cancer in Finnish Men",
      "clickbait": "",
      "textLength": 323,
      "alias": "15",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "1",
      "editCreatedAt": "2015-03-26 22:21:26",
      "pageCreatorId": "1",
      "pageCreatedAt": "2015-02-27 22:44:15",
      "seeDomainId": "0",
      "editDomainId": "21",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 0,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 26,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": 0,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "178": {
      "likeableId": "202",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "178",
      "edit": 6,
      "editSummary": "",
      "prevEdit": 5,
      "currentEdit": 6,
      "wasPublished": true,
      "type": "wiki",
      "title": "Arbital \"tag\" relationship",
      "clickbait": "Tags are a way to connect pages that share a common topic.",
      "textLength": 2689,
      "alias": "Arbital_tag",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "1",
      "editCreatedAt": "2016-05-11 15:44:58",
      "pageCreatorId": "1",
      "pageCreatedAt": "2015-11-15 15:31:40",
      "seeDomainId": "0",
      "editDomainId": "3",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 0,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 96,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": 0,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "185": {
      "likeableId": "0",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "185",
      "edit": 0,
      "editSummary": "",
      "prevEdit": 0,
      "currentEdit": 0,
      "wasPublished": false,
      "type": "",
      "title": "",
      "clickbait": "",
      "textLength": 0,
      "alias": "",
      "externalUrl": "",
      "sortChildrenBy": "",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "",
      "editCreatedAt": "",
      "pageCreatorId": "",
      "pageCreatedAt": "",
      "seeDomainId": "",
      "editDomainId": "",
      "submitToDomainId": "",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": false,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 0,
      "isEditorComment": false,
      "isApprovedComment": false,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 0,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": 0,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "187": {
      "likeableId": "0",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "187",
      "edit": 0,
      "editSummary": "",
      "prevEdit": 0,
      "currentEdit": 0,
      "wasPublished": false,
      "type": "",
      "title": "",
      "clickbait": "",
      "textLength": 0,
      "alias": "",
      "externalUrl": "",
      "sortChildrenBy": "",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "",
      "editCreatedAt": "",
      "pageCreatorId": "",
      "pageCreatedAt": "",
      "seeDomainId": "",
      "editDomainId": "",
      "submitToDomainId": "",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": false,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 0,
      "isEditorComment": false,
      "isApprovedComment": false,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 0,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": 0,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "198": {
      "likeableId": "266",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "198",
      "edit": 4,
      "editSummary": "",
      "prevEdit": 3,
      "currentEdit": 4,
      "wasPublished": true,
      "type": "wiki",
      "title": "Team Arbital",
      "clickbait": "The people behind Arbital",
      "textLength": 184,
      "alias": "TeamArbital",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "1",
      "editCreatedAt": "2016-06-17 16:55:46",
      "pageCreatorId": "1",
      "pageCreatedAt": "2015-12-13 23:14:48",
      "seeDomainId": "0",
      "editDomainId": "8",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 0,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 1185,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": 0,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "370": {
      "likeableId": "2144",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "370",
      "edit": 4,
      "editSummary": "reflecting the fact that we only have one type of mark now.",
      "prevEdit": 3,
      "currentEdit": 4,
      "wasPublished": true,
      "type": "wiki",
      "title": "Arbital mark",
      "clickbait": "What is a mark on Arbital? When is it created? Why is it important?",
      "textLength": 1724,
      "alias": "arbital_mark",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "1yq",
      "editCreatedAt": "2016-07-22 00:08:32",
      "pageCreatorId": "1",
      "pageCreatedAt": "2016-04-14 23:12:16",
      "seeDomainId": "0",
      "editDomainId": "3",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 0,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 58,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": 0,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "595": {
      "likeableId": "0",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "595",
      "edit": 3,
      "editSummary": "",
      "prevEdit": 2,
      "currentEdit": 3,
      "wasPublished": true,
      "type": "wiki",
      "title": "Arbital page alias",
      "clickbait": "",
      "textLength": 1215,
      "alias": "arbital_alias",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "1yq",
      "editCreatedAt": "2016-07-21 23:06:57",
      "pageCreatorId": "5",
      "pageCreatedAt": "2016-07-10 00:52:28",
      "seeDomainId": "0",
      "editDomainId": "3",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 0,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 46,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": 0,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "596": {
      "likeableId": "0",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "596",
      "edit": 1,
      "editSummary": "",
      "prevEdit": 0,
      "currentEdit": 1,
      "wasPublished": true,
      "type": "wiki",
      "title": "Arbital page title",
      "clickbait": "",
      "textLength": 738,
      "alias": "Arbital_title",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "5",
      "editCreatedAt": "2016-07-10 01:18:37",
      "pageCreatorId": "5",
      "pageCreatedAt": "2016-07-10 01:18:37",
      "seeDomainId": "0",
      "editDomainId": "3",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 0,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 31,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": 0,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "597": {
      "likeableId": "3067",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "597",
      "edit": 2,
      "editSummary": "added clickbait",
      "prevEdit": 1,
      "currentEdit": 2,
      "wasPublished": true,
      "type": "wiki",
      "title": "Arbital page clickbait",
      "clickbait": "The text you are reading right now is clickbait.",
      "textLength": 1128,
      "alias": "Arbital_clickbait",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "1yq",
      "editCreatedAt": "2016-08-05 17:48:01",
      "pageCreatorId": "5",
      "pageCreatedAt": "2016-07-10 01:24:23",
      "seeDomainId": "0",
      "editDomainId": "3",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 0,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 43,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": 0,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "12y": {
      "likeableId": "88",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "12y",
      "edit": 2,
      "editSummary": "",
      "prevEdit": 1,
      "currentEdit": 2,
      "wasPublished": true,
      "type": "group",
      "title": "Patrick LaVictoire",
      "clickbait": "",
      "textLength": 199,
      "alias": "PatrickLaVictoir",
      "externalUrl": "",
      "sortChildrenBy": "alphabetical",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "12y",
      "editCreatedAt": "2016-01-25 22:25:07",
      "pageCreatorId": "12y",
      "pageCreatedAt": "2015-09-04 16:14:58",
      "seeDomainId": "0",
      "editDomainId": "32",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 0,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 64,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": 0,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "17b": {
      "likeableId": "204",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "17b",
      "edit": 16,
      "editSummary": "",
      "prevEdit": 15,
      "currentEdit": 16,
      "wasPublished": true,
      "type": "wiki",
      "title": "Arbital lens",
      "clickbait": "A lens is a page that presents another page's content from a different angle.",
      "textLength": 7216,
      "alias": "Arbital_lens",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "1yq",
      "editCreatedAt": "2016-12-05 13:10:54",
      "pageCreatorId": "1",
      "pageCreatedAt": "2015-11-15 18:01:48",
      "seeDomainId": "0",
      "editDomainId": "3",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 0,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 687,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": 0,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "18v": {
      "likeableId": "252",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "18v",
      "edit": 4,
      "editSummary": "",
      "prevEdit": 2,
      "currentEdit": 4,
      "wasPublished": true,
      "type": "wiki",
      "title": "Expected utility formalism",
      "clickbait": "Expected utility is the central idea in the quantitative implementation of consequentialism",
      "textLength": 390,
      "alias": "expected_utility_formalism",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "2",
      "editCreatedAt": "2017-02-16 19:21:50",
      "pageCreatorId": "2",
      "pageCreatedAt": "2015-12-02 23:59:07",
      "seeDomainId": "0",
      "editDomainId": "15",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 1,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 1982,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0
      ],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": -2,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "1ln": {
      "likeableId": "553",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "1ln",
      "edit": 6,
      "editSummary": "alias. note to self: come back and explain new requisites system.",
      "prevEdit": 5,
      "currentEdit": 6,
      "wasPublished": true,
      "type": "wiki",
      "title": "Arbital requisites",
      "clickbait": "To understand a thing you often need to understand some other things.",
      "textLength": 1210,
      "alias": "arbital_requisite",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "1yq",
      "editCreatedAt": "2016-07-19 23:24:15",
      "pageCreatorId": "1",
      "pageCreatedAt": "2016-01-11 17:09:53",
      "seeDomainId": "0",
      "editDomainId": "3",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 0,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 316,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": 0,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "1lw": {
      "likeableId": "559",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "1lw",
      "edit": 5,
      "editSummary": "added links",
      "prevEdit": 4,
      "currentEdit": 5,
      "wasPublished": true,
      "type": "wiki",
      "title": "Mathematics",
      "clickbait": "Mathematics is the study of numbers and other ideal objects that can be described by axioms.",
      "textLength": 745,
      "alias": "math",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "1yq",
      "editCreatedAt": "2016-06-22 17:49:03",
      "pageCreatorId": "2",
      "pageCreatedAt": "2016-01-15 03:02:51",
      "seeDomainId": "0",
      "editDomainId": "1",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 0,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 2288,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": 0,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "1lx": {
      "likeableId": "560",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "1lx",
      "edit": 4,
      "editSummary": "",
      "prevEdit": 3,
      "currentEdit": 4,
      "wasPublished": true,
      "type": "wiki",
      "title": "Ability to read algebra",
      "clickbait": "Do you have sufficient mathematical ability that you can read a sentence that uses some algebra or invokes a mathematical idea, without slowing down too much?",
      "textLength": 495,
      "alias": "reads_algebra",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "1",
      "editCreatedAt": "2016-07-26 23:55:30",
      "pageCreatorId": "2",
      "pageCreatedAt": "2016-01-15 03:24:28",
      "seeDomainId": "0",
      "editDomainId": "1",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 0,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 280,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": 0,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "1ly": {
      "likeableId": "561",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "1ly",
      "edit": 26,
      "editSummary": "",
      "prevEdit": 25,
      "currentEdit": 26,
      "wasPublished": true,
      "type": "wiki",
      "title": "Bayesian update",
      "clickbait": "Bayesian updating: the ideal way to change probabilistic beliefs based on evidence.",
      "textLength": 1520,
      "alias": "bayes_update",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "2",
      "editCreatedAt": "2017-02-08 18:36:41",
      "pageCreatorId": "2",
      "pageCreatedAt": "2016-01-15 03:45:14",
      "seeDomainId": "0",
      "editDomainId": "1",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 0,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 994,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0
      ],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": -2,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "1rj": {
      "likeableId": "702",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "1rj",
      "edit": 25,
      "editSummary": "",
      "prevEdit": 24,
      "currentEdit": 25,
      "wasPublished": true,
      "type": "wiki",
      "title": "Conditional probability",
      "clickbait": "The notation for writing \"The probability that someone has green eyes, if we know that they have red hair.\"",
      "textLength": 6468,
      "alias": "conditional_probability",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "2",
      "editCreatedAt": "2016-10-08 02:05:05",
      "pageCreatorId": "2",
      "pageCreatedAt": "2016-01-26 23:06:38",
      "seeDomainId": "0",
      "editDomainId": "1",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 0,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 5397,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0
      ],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": -2,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "1rm": {
      "likeableId": "705",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "1rm",
      "edit": 17,
      "editSummary": "",
      "prevEdit": 16,
      "currentEdit": 17,
      "wasPublished": true,
      "type": "wiki",
      "title": "Prior probability",
      "clickbait": "What we believed before seeing the evidence.",
      "textLength": 1659,
      "alias": "prior_probability",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "1yq",
      "editCreatedAt": "2016-08-04 14:27:46",
      "pageCreatorId": "2",
      "pageCreatedAt": "2016-01-27 04:55:27",
      "seeDomainId": "0",
      "editDomainId": "1",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 0,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 334,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0
      ],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": -2,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "1rt": {
      "likeableId": "711",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "1rt",
      "edit": 3,
      "editSummary": "",
      "prevEdit": 2,
      "currentEdit": 3,
      "wasPublished": true,
      "type": "wiki",
      "title": "Arbital path",
      "clickbait": "Arbital path is a linear sequence of pages tailored specifically to teach a given concept to a user.",
      "textLength": 2327,
      "alias": "Arbital_path",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "1",
      "editCreatedAt": "2016-05-11 20:53:18",
      "pageCreatorId": "1",
      "pageCreatedAt": "2016-01-27 16:33:23",
      "seeDomainId": "0",
      "editDomainId": "3",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 0,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 214,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": 0,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "1y6": {
      "likeableId": "881",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "1y6",
      "edit": 21,
      "editSummary": "",
      "prevEdit": 20,
      "currentEdit": 21,
      "wasPublished": true,
      "type": "wiki",
      "title": "Belief revision as probability elimination",
      "clickbait": "Update your beliefs by throwing away large chunks of probability mass.",
      "textLength": 5701,
      "alias": "bayes_rule_elimination",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "2",
      "editCreatedAt": "2016-10-08 18:59:55",
      "pageCreatorId": "2",
      "pageCreatedAt": "2016-02-10 05:11:56",
      "seeDomainId": "0",
      "editDomainId": "1",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 0,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 7476,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0
      ],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": -2,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "1yq": {
      "likeableId": "897",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "1yq",
      "edit": 2,
      "editSummary": "",
      "prevEdit": 1,
      "currentEdit": 2,
      "wasPublished": true,
      "type": "group",
      "title": "Eric Bruylant",
      "clickbait": "Automatically generated group for Eric Bruylant",
      "textLength": 216,
      "alias": "EricBruylant",
      "externalUrl": "",
      "sortChildrenBy": "alphabetical",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "1yq",
      "editCreatedAt": "2016-02-17 17:38:40",
      "pageCreatorId": "1yq",
      "pageCreatedAt": "2016-02-12 16:14:31",
      "seeDomainId": "0",
      "editDomainId": "116",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 0,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 256,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": 0,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "35z": {
      "likeableId": "0",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "35z",
      "edit": 0,
      "editSummary": "",
      "prevEdit": 0,
      "currentEdit": 0,
      "wasPublished": false,
      "type": "",
      "title": "",
      "clickbait": "",
      "textLength": 0,
      "alias": "",
      "externalUrl": "",
      "sortChildrenBy": "",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "",
      "editCreatedAt": "",
      "pageCreatorId": "",
      "pageCreatedAt": "",
      "seeDomainId": "",
      "editDomainId": "",
      "submitToDomainId": "",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": false,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 0,
      "isEditorComment": false,
      "isApprovedComment": false,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 0,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": 0,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "3d": {
      "likeableId": "2273",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "3d",
      "edit": 33,
      "editSummary": "",
      "prevEdit": 32,
      "currentEdit": 33,
      "wasPublished": true,
      "type": "wiki",
      "title": "Arbital",
      "clickbait": "Arbital is the place for crowdsourced, intuitive math explanations.",
      "textLength": 5201,
      "alias": "Arbital",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "1",
      "editCreatedAt": "2016-08-08 16:07:52",
      "pageCreatorId": "1",
      "pageCreatedAt": "2015-03-30 22:19:47",
      "seeDomainId": "0",
      "editDomainId": "8",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 0,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 2635,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": 0,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "3ft": {
      "likeableId": "0",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "3ft",
      "edit": 2,
      "editSummary": "",
      "prevEdit": 1,
      "currentEdit": 2,
      "wasPublished": true,
      "type": "wiki",
      "title": "Peano Arithmetic",
      "clickbait": "A system for proving theorems about arithmetic, which is strong enough to include self-reference.",
      "textLength": 1126,
      "alias": "peano_arithmetic",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "12y",
      "editCreatedAt": "2016-05-06 18:17:18",
      "pageCreatorId": "12y",
      "pageCreatedAt": "2016-05-06 17:57:22",
      "seeDomainId": "0",
      "editDomainId": "32",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 0,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 67,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0
      ],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": -2,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "3hs": {
      "likeableId": "2499",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "3hs",
      "edit": 19,
      "editSummary": "added link to exemplar pages",
      "prevEdit": 18,
      "currentEdit": 19,
      "wasPublished": true,
      "type": "wiki",
      "title": "Author's guide to Arbital",
      "clickbait": "How to write intuitive, flexible content on Arbital.",
      "textLength": 4420,
      "alias": "author_guide_to_arbital",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "1yq",
      "editCreatedAt": "2016-08-08 14:32:40",
      "pageCreatorId": "1",
      "pageCreatedAt": "2016-05-10 17:55:35",
      "seeDomainId": "0",
      "editDomainId": "3",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 0,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 433,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": 0,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "3n": {
      "likeableId": "2281",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "3n",
      "edit": 7,
      "editSummary": "",
      "prevEdit": 6,
      "currentEdit": 7,
      "wasPublished": true,
      "type": "wiki",
      "title": "Arbital \"parent\" relationship",
      "clickbait": "Parent-child relationship between pages implies a strong, inseparable connection.",
      "textLength": 2510,
      "alias": "Arbital_parent_child",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "8pb",
      "editCreatedAt": "2017-09-20 13:30:49",
      "pageCreatorId": "1",
      "pageCreatedAt": "2015-04-01 19:51:44",
      "seeDomainId": "0",
      "editDomainId": "3",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 0,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 199,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": 0,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "3rk": {
      "likeableId": "2859",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 1,
      "dislikeCount": 0,
      "likeScore": 1,
      "individualLikes": [],
      "pageId": "3rk",
      "edit": 9,
      "editSummary": "",
      "prevEdit": 8,
      "currentEdit": 9,
      "wasPublished": true,
      "type": "wiki",
      "title": "Start",
      "clickbait": "This page gives a basic overview of the topic, but may be missing important information or have stylistic issues. If you're able to, please help expand or improve it!",
      "textLength": 890,
      "alias": "start_meta_tag",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "2",
      "editCreatedAt": "2016-08-03 22:19:29",
      "pageCreatorId": "2",
      "pageCreatedAt": "2016-05-22 15:13:16",
      "seeDomainId": "0",
      "editDomainId": "8",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 0,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 136,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": 0,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "4bn": {
      "likeableId": "0",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "4bn",
      "edit": 1,
      "editSummary": "",
      "prevEdit": 0,
      "currentEdit": 1,
      "wasPublished": true,
      "type": "wiki",
      "title": "Needs work",
      "clickbait": "Meta tag for pages which need content improvement.",
      "textLength": 50,
      "alias": "4bn",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "1yq",
      "editCreatedAt": "2016-06-14 22:36:59",
      "pageCreatorId": "1yq",
      "pageCreatedAt": "2016-06-14 22:36:59",
      "seeDomainId": "0",
      "editDomainId": "116",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 0,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 34,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": 0,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "4v": {
      "likeableId": "2318",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 2,
      "dislikeCount": 0,
      "likeScore": 2,
      "individualLikes": [],
      "pageId": "4v",
      "edit": 6,
      "editSummary": "",
      "prevEdit": 5,
      "currentEdit": 6,
      "wasPublished": true,
      "type": "wiki",
      "title": "Work in progress",
      "clickbait": "This page is being actively worked on by an editor. Check with them before making major changes.",
      "textLength": 131,
      "alias": "work_in_progress_meta_tag",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "1yq",
      "editCreatedAt": "2016-07-05 22:48:12",
      "pageCreatorId": "2",
      "pageCreatedAt": "2015-04-17 01:27:41",
      "seeDomainId": "0",
      "editDomainId": "8",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 1,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 98,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": 0,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "4v4": {
      "likeableId": "2858",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 1,
      "dislikeCount": 0,
      "likeScore": 1,
      "individualLikes": [],
      "pageId": "4v4",
      "edit": 1,
      "editSummary": "",
      "prevEdit": 0,
      "currentEdit": 1,
      "wasPublished": true,
      "type": "wiki",
      "title": "Still needs work",
      "clickbait": "The next step up from \"Work in Progress\".  The page can be read as complete, but is a draft that needs further review and fine-tuning.",
      "textLength": 155,
      "alias": "still_needs_work",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "2",
      "editCreatedAt": "2016-06-27 01:42:54",
      "pageCreatorId": "2",
      "pageCreatedAt": "2016-06-27 01:42:54",
      "seeDomainId": "0",
      "editDomainId": "3",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": false,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 0,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": true,
      "viewCount": 19,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": 0,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "55w": {
      "likeableId": "3113",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "55w",
      "edit": 7,
      "editSummary": "",
      "prevEdit": 6,
      "currentEdit": 7,
      "wasPublished": true,
      "type": "wiki",
      "title": "Lb's theorem",
      "clickbait": "Lb's theorem ",
      "textLength": 1644,
      "alias": "lobs_theorem",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "5",
      "editCreatedAt": "2016-07-30 04:03:46",
      "pageCreatorId": "2vh",
      "pageCreatedAt": "2016-07-06 21:10:36",
      "seeDomainId": "0",
      "editDomainId": "1",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 2,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 692,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0
      ],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": -2,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "58b": {
      "likeableId": "3654",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 2,
      "dislikeCount": 0,
      "likeScore": 2,
      "individualLikes": [],
      "pageId": "58b",
      "edit": 12,
      "editSummary": "",
      "prevEdit": 11,
      "currentEdit": 12,
      "wasPublished": true,
      "type": "wiki",
      "title": "Logical decision theories",
      "clickbait": "Root page for topics on logical decision theory, with multiple intros for different audiences.",
      "textLength": 404,
      "alias": "logical_dt",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "2",
      "editCreatedAt": "2018-06-01 18:56:49",
      "pageCreatorId": "2",
      "pageCreatedAt": "2016-07-08 18:06:14",
      "seeDomainId": "0",
      "editDomainId": "15",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 0,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 3916,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": 0,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "58c": {
      "likeableId": "0",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "58c",
      "edit": 1,
      "editSummary": "",
      "prevEdit": 0,
      "currentEdit": 1,
      "wasPublished": true,
      "type": "wiki",
      "title": "Decision Theory",
      "clickbait": "",
      "textLength": 161,
      "alias": "DecisionTheory",
      "externalUrl": "",
      "sortChildrenBy": "alphabetical",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "1",
      "editCreatedAt": "2016-07-08 18:23:14",
      "pageCreatorId": "1",
      "pageCreatedAt": "2016-07-08 18:23:14",
      "seeDomainId": "0",
      "editDomainId": "15",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 0,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 178,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": 0,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "58f": {
      "likeableId": "3054",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "58f",
      "edit": 2,
      "editSummary": "",
      "prevEdit": 1,
      "currentEdit": 14,
      "wasPublished": true,
      "type": "wiki",
      "title": "Intro for Economists",
      "clickbait": "An introduction to logical decision theory for economists",
      "textLength": 42482,
      "alias": "ldt_intro_econ",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "2",
      "editCreatedAt": "2016-07-11 18:42:35",
      "pageCreatorId": "2",
      "pageCreatedAt": "2016-07-09 00:33:13",
      "seeDomainId": "0",
      "editDomainId": "15",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": false,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 19,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 587,
      "text": "Currently, students of economics learn that:\n\n- The expected return on your time from voting in elections is very low, unless your vote *directly causes* many other people to vote.  Most elections are not settled by one vote, and your one vote is very unlikely to change that.\n- In the [ultimatum_game Ultimatum Game], the experimenter sets up a one-shot, two-player scenario for dividing \\$1.  One player, the Proposer, proposes how to split the \\$10 with the other player, the Responder.  If the Responder accepts the proposed split, it goes through.  Otherwise both players get nothing.  A *rational* Proposer should offer \\$1 to the other player and keep the remaining \\$9; a *rational* Responder should accept this bargain.  (Assuming that it's a one-time deal, maybe anonymous so that there's no reputation effects, etcetera.)\n- You and your enemy share a terrible secret.  Your enemy sets up an automatic mechanism so that unless your Bitcoin address pays 10 Bitcoins to a certain other address, a secret will be published, greatly harming you and your enemy.  In this case, paying your enemy the 10 Bitcoins has higher expected utility than not paying, and that is what a rational agent will do--even though your enemy predicted this response by you, and that's why your enemy set up the automatic mechanism.\n\nAll of the currently-standard replies above about *coordination problems,* *bargaining theory,* and *game theory* are derived from the decision theory that's dominant in modern analytic philosophy, namely *causal decision theory* (CDT).  Roughly, causal decision theory says, \"Decide based on the varying *causal* consequences you expect from different *physical acts.*\"  Most economics textbooks may not go into this, but it's where the standard analysis comes from.\n\nOn this standard analysis, if we want a theory of how an agent can resist having their money pumped out of them by blackmail, then we must look beyond theories of 'rationality' and talk about agents that are 'usefully irrational' in certain ways, or maybe 'socially rational' or some other not-very-well-formalized concept.\n\n*Logical* decision theorists argue that causal decision theory suffers from extremely severe problems.  Not just problems along the lines of \"We don't like the answer this gives for the Ultimatum Game\", but problems like \"We can make this decision theory [death_in_damascus go into infinite loops]\" or \"This decision theory can [ calculate] a negative [information_value value of information]\".\n\nArguendo, when we fix these problems, we end up with a new, foundational, formal principle of rational choice:  \"Decide based on the *logical* consequences of different potential outputs for your *decision algorithm*.\"  Although people are still working on variant ways to implement this principle, the general family is known as Logical Decision Theory or LDT.\n\nLogical decision theorists sometimes use the metaphor of computational agents that have definite or probabilistic knowledge about other agents' source code.  If you simulated another agent and saw for certain that it would reject any offer less than \\$5 in the Ultimatum Game, would you still offer it only \\$1?%note:  (What about if your simulation showed that the other agent rejected any offer less than \\$9?)%\n\nIn the real world, taking into account iterated interactions and reputational effects and subjective uncertainties, dilemmas are no longer simple as two agents playing the Ultimatum Game with knowledge of each other's source code.  We can't take the answers of logical decision theory for simple cases, as directly indicating what rational humans should do in much more complex cases.\n\nStill, the simplified games are often held up as an archetypal example or a base case.  Arguendo according to LDT, these base cases are being wrongly analyzed by standard causal decision theory.\n\n%%%knows-requisite([dt_prisonersdilemma]):\nLogical decision theory can be formalized at least as much as competing decision theories.  For example, there is now [running code](https://arxiv.org/abs/1401.5577) for simulating in a general way how multiple agents with knowledge of each other's code, reasoning about each other's actions, can arrive at an equilibrium.  Two similar (but not identical!) maximizing agents of this kind will end up cooperating in the [prisoners_dilemma Prisoner's Dilemma], assuming they have [common_knowledge common knowledge] of each other's code.  Again, you can actually try this out in simulation!\n%%%\n\nAnd yes, LDT seems to strongly suggest that, even in the real world, if you're part of a sufficiently large cohort of people all voting the same way for similar reasons, you should (all) vote in the election.  %%knows-requisite([dt_prisonersdilemma]):(For much the same reason LDT says you ought to Cooperate, if you're playing the Prisoner's Dilemma against a recent cloned copy of yourself.)%%\n\n# Different principles of expected utility\n\nAlmost everyone in the debate agrees that 'rational' agents maximize expected utility *conditional on* their decisions.  The central question of decision theory turns out to be, \"How exactly do we condition on a decision?\"\n\n## Evidential versus counterfactual conditioning\n\n[todo: condition this text on math2 and write a math1 alternate version.]\n\nMost economics papers show the expected utility formula as:\n\n$$\\mathbb E[\\mathcal U|a_x] = \\sum_{o_i \\in \\mathcal O} \\mathcal U(o_i) \\cdot \\mathbb P(o_i|a_x)$$\n\nwhere\n\n- $\\mathbb E[\\mathcal U|a_x]$ is our average expectation of utility, if action $a_x$ is chosen;\n- $\\mathcal O$ is the set of possible outcomes;\n- $\\mathcal U$ is our utility function, mapping outcomes onto real numbers;\n- $\\mathbb P(o_i|a_x)$ is the [1rj conditional probability] of outcome $o_i$ if $a_x$ is chosen.\n\nTechnically speaking, this formula is almost universally agreed to be wrong.\n\nThe problem is the use of standard evidential conditioning in $\\mathbb P(o_i|a_x).$  On this formula we are behaving as if we're asking, \"What would be my [1y6 revised] probability for $\\mathbb P(o_i),$ if I was *told the news* or *observed the evidence* that my action had been $a_x$?\"\n\nCausal decision theory says we should instead use the *counterfactual conditional* $\\ \\mathbb P(a_x \\ \\square \\! \\! \\rightarrow o_i).$\n\nThe difference between evidential and counterfactual conditioning is often explained by contrasting these two sentences:\n\n- If Lee Harvey Oswald didn't shoot John F. Kennedy, somebody else did.\n- If Lee Harvey Oswald hadn't shot John F. Kennedy, somebody else would have.\n\nIn the first sentence, we're being told as news that Oswald didn't shoot Kennedy, and [1ly updating our beliefs] accordingly to match the world we already saw.  In the second world, we're imagining how a counterfactual world would have played out if Oswald had acted differently.\n\nIf $K$ denotes the proposition that somebody else shot Kennedy and $O$ denotes the proposition that Oswald shot him, then the first sentence and second sentence are respectively talking about:\n\n- $\\mathbb P(K| \\neg O)$\n- $\\mathbb P(\\neg O \\ \\square \\!\\! \\rightarrow K)$\n\nCalculating expected utility using evidential conditioning is widely agreed to lead to an irrational policy of 'managing the news'.  For example, suppose that toxoplasmosis, a parasitic infection carried by cats, can cause toxoplasmosis-infected humans to become fonder of cats.%note:  (This was formerly thought to actually be true.  More recently, this result may have failed to replicate.)%\n\nYou are now faced with a cute cat that has been checked by a veterinarian who says this cat definitely does *not* have toxoplasmosis.  If you decide to pet the cat, an impartial observer watching you will conclude that you are 10% more likely to have toxoplasmosis, which can be a fairly detrimental infection.  If you don't pet the cat, you'll miss out on the hedonic enjoyment of petting it.  Do you pet the cat?\n\nMost decision theorist agree that in this case you should pet the cat.  Either you already have toxoplasmosis or you don't.  Petting the cat can't *cause* you to acquire toxoplasmosis.  You'd just be missing out on the pleasant sensation of cat-petting.  Afterwards you may update your beliefs based on observing your own decision, and realize that you had toxoplasmosis all along.  But when you're considering the consequences of actions, you should reason that *if counterfactually* you had not pet the cat, you *still* would have had toxoplasmosis *and* missed out on petting the cat.  (Just like, if Oswald *hadn't* shot Kennedy, nobody else would have.)\n\nThe decision theory which claims that we should condition on our actions via the standard [1rj conditional probability formula], as if we were being told our choices as news or [1ly Bayesian-updating] on our actions as observations, is termed [evidential_decision_theory evidential decision theory].  Evidential decision theory answers the central question \"How do I condition on my choices?\" by replying \"Condition on your choices as if observing them as evidence\" or \"Take the action that you would consider best if you heard it as news.\"\n\n(For a more severe criticism of evidential decision theory showing how more clever agents can pump money out of evidential decision agents, see the [termite_dilemma Termite Dilemma].)\n\n## Causal decision theory\n\nCausal decision theory, the current academic standard, says that the expected utility formula should be written:\n\n$$\\mathbb E[\\mathcal U|a_x] = \\sum_{o_i \\in \\mathcal O} \\mathcal U(o_i) \\cdot \\mathbb P(a_x \\ \\square \\!\\! \\rightarrow o_i)$$\n\nThis leads into the question of how we compute $\\mathbb P(a_x \\ \\square \\!\\! \\rightarrow o_i),$ since it's not a standard [1rj conditional probability].\n\nIn the philosophical literature, it's often assumed that we intuitively know what the counterfactual results must be.  (E.g., we're just taking for granted that you somehow know that if Oswald hadn't shot Kennedy, nobody else would have; this is intuitively obvious.)  This is formalized by having a conditional distribution $\\mathbb P(\\bullet \\ || \\ \\bullet)$ which is treated as heaven-sent and includes the results of all counterfactual conditionals.\n\nPeople working in Artificial Intelligence will probably find this unsatisfactory, and will want to refer to the theory of [causal_model causal models] developed by Judea Pearl et. al.  The theory of causal models formally states how to perform counterfactual surgery on graphical models of causal processes.\n\n[todo: condition the following text on math2, write weaker-math version]\n\nFormally, we have a directed acyclic graph such as:\n\n[todo: put real diagram here]\n\n- $X_1$ -> {$X_2$, $X_3$} -> $X_4$ -> $X_5$\n\n*Click for example:* %note:\n\nOne of Judea Pearl's examples of a causal graph is:\n\n[todo: real diagram here]\n\n- SEASON -> {RAINING, SPRINKLER} -> {SIDEWALK} -> {SLIPPERY}\n\nThis says, e.g.:\n\n- That the current SEASON affects the probability that it's RAINING, and separately affects the probability of the SPRINKLER turning on.  (But RAINING and SPRINKLER don't affect each other; if we know the current SEASON, we don't need to know whether it's RAINING to figure out the probability the SPRINKLER is on.)\n- RAINING and SPRINKLER can both cause the SIDEWALK to become wet.  (So if we did observe that the sidewalk was wet, then even already knowing the SEASON, we would estimate a different probability that it was RAINING depending on whether the SPRINKLER was on.  The SPRINKLER being on would 'explain away' the SIDEWALK's observed wetness without any need to postulate RAIN.)\n- Whether the SIDEWALK is wet is the sole determining factor for whether the SIDEWALK is SLIPPERY.  (So that if we *know* whether the SIDEWALK is wet, we learn nothing more about the probability that the path is SLIPPERY by being told that the SEASON is summer.  But if we didn't already know whether the SIDEWALK was wet, whether the SEASON was summer or fall might be very relevant for guessing whether the path was SLIPPERY!)\n%\n\nA causal model goes beyond the graph by including specific probability functions $\\mathbb P(X_i | \\mathbf{pa}_i)$ for how to calculate the probability of each node $X_i$ taking on the value $x_i$ given the values \\mathbf {pa}_i$ of $x_i$'s immediate ancestors.  It is implicitly assumed that the causal model [ factorizes], so that the probability of any value assignment $\\mathbf x$ to the whole graph can be calculated using the product:\n\n$$\\mathbb P(\\mathbf x) = \\prod_i \\mathbb P(x_i | \\mathbf{pa}_i)$$\n\nThen, rather straightforwardly, the counterfactual conditional $\\mathbb P(\\mathbf x | \\operatorname{do}(X_j=x_j))$ is calculated via:\n\n$$\\mathbb P(\\mathbf x | \\operatorname{do}(X_j=x_j)) = \\prod_{i \\neq j} \\mathbb P(x_i | \\mathbf{pa}_i)$$\n\n(We assume that $\\mathbf x$ has $x_j$ equaling the $\\operatorname{do}$-specified value of $X_j$; otherwise its conditioned probability is defined to be $0$.)\n\nThis formula implies - as one might intuitively expect - that conditioning on $\\operatorname{do}(X_j=x_j)$ can only affect the probabilities of variables $X_i$ that are \"downstream\" of $X_j$ in the directed acyclic graph that is the backbone of the causal model.  In much the same way that (ordinarily) we think our choices today affect how much money we have tomorrow, but not how much money we had yesterday.\n\nThen expected utility should be calculated as:\n\n$$\\mathbb E[\\mathcal U| \\operatorname{do}(a_x)] = \\sum_{o_i \\in \\mathcal O} \\mathcal U(o_i) \\cdot \\mathbb P(o_i | \\operatorname{do}(a_x))\n\nUnder this rule, we won't calculate that we can affect the probability of having toxoplasmosis by petting the cat, since our choice to pet the cat is causally downstream of whether we have toxoplasmosis.\n\n[todo: put diagram here]\n\n%%comment:\nOne class of problems is hinted-at by the point that the above expected utility formula, in the course of making its calculation, gives the wrong expected utility for the action actually implemented!\n\nSuppose the [1rm prior probability] of having toxoplasmosis is 10%, and the posterior probability after being seen to pet the cat is 20%.  Suppose that *not* having toxoplasmosis has \\$100 utility; that having toxoplasmosis has \\$0 utility; and that, given the amount you enjoy petting cats, petting the cat adds \\$1 of utility to your outcome.\n\nThen the above formula for deciding whether to pet the cat suggests that petting leads to an expected utility of \\$91, and not petting leads to an expected utility of \\$90.  This tells us to pet the cat, which is the correct decision, but it also tells us to expect \\$91 of expected utility after petting the cat, where we actually receive \\$81 in expectation.  It seems like the intuitively \"correct\" answer is that we should calculate \\$81 of utility for petting the cat and \\$80 utility for not petting it.\n\nYou might initially be tempted to solve this problem by doing the calculation in phases:\n\n- Phase 1:  Calculate the decision based on prior beliefs.\n- Phase 2:  Update our beliefs based on having observed our first-order decision.\n- Phase 3:  Recalculate the expected utilities based on the posterior beliefs, possibly picking a new action.\n\n...and then wait for this algorithm to settle into a consistent state.\n\nBut besides lacking the computational efficiency of computing our decision in one swoop, it's entirely possible for an agent like this to [death_in_damascus go into an infinite loop].\n%%\n\n## Newcomblike problems\n\nAlthough causal decision theory became widely accepted, there were also widespread suspicions that causal decision theory might not be optimal, or might be missing some key element of rationality.\n\nThe academic debate on this subject revolved mainly around *Newcomblike problems,* a broad class of dilemmas which turned out to include the Prisoner's Dilemma; commons problems and coordination problems (like voting in elections); blackmail and other dilemmas of negotiation; plus other problems of interest.\n\nRoughly, we could describe Newcomblike problems as those where somebody similar to you, or trying to predict you, exists in the environment.  In this case your decision can *correlate* with events outside you, without your action *physically causing* those events.\n\nThe original Newcomb's Problem was somewhat artificial, but it's worth going into for historical reasons:\n\nAn alien named [5b2 Omega] presents you with two boxes, a transparent box A containing \\$1,000, and an opaque Box B.  Omega then flies away, leaving you with the choice of whether to take only Box B ('one-box') or to take Box A plus Box B ('two-box').  Omega has put $1,000,000 in Box B if and only if Omega predicted that you would take only one box; otherwise Box B is empty.\n\nOmega has already departed, so Box B is already empty or already full.\n\nOmega is an excellent predictor and has been observed to be right in the 73 out of 73 cases it has previously run this experiment.\n\nDo you take both boxes, or only Box B?\n\n- Argument 1:  People who take only Box B tend to walk away rich.  People who two-box tend to walk away poor.  It is better to be rich than poor.\n- Argument 2:  Omega has already made its prediction.  Box B is already empty or already full.  It would be irrational to leave behind Box A for no reason.  It's true that Omega has chosen to reward irrationality in this setup, but Box B is now *already empty* for your rational self, and leaving it behind would just counterfactually result in your getting \\$0 instead of \\$1,000.\n\nThis setup went on to generate an incredible amount of debate.  Newcomb's Problem is conventionally seen as an example that splits the verdict of evidential decision theory (\"Taking Box B is good news!  Do that.\") versus causal decision theory (\"Taking both boxes does not *cause* Box B to be empty, it just adds \\$1,000 to the reward\") in a way that initially seems more favorable to evidential decision agents, who walk away rich.\n\n(Naturally, logical decision agents one-box, not because one-boxing is good news, but because the LDT algorithm chooses as if controlling the output of the algorithm; and if the output of the LDT algorithm is 'take one box', this leads to Box B containing a million dollars.)\n\nThe setup in Newcomb's Problem may seem contrived, but consider the following variant, Parfit's Hitchhiker:\n\nYou are lost in the desert, your water bottle almost exhausted, when somebody drives up in a lorry.  This person is (a) entirely selfish, and (b) very good at detecting lies--maybe the driver went through Paul Ekman's training for reading facial microexpressions, or the driver got a lot of experience interrogating potential liars in a context where they actually received feedback on how well they did.\n\nThe driver says that they will drive you into town, but only if you promise to give them \\$1,000 on arrival.\n\nYou are also entirely selfish.\n\nIf you value your life at \\$1,000,000 (pay \\$1,000 to avoid 0.1% risks of death) then this problem is nearly isomorphic to Gary Drescher's *transparent Newcomb's Problem,* in which Box B is transparent, and Omega has put \\$1,000,000 into Box B iff Omega predicts that you one-box when seeing a full Box B.  This makes Parfit's Hitchhiker a *Newcomblike problem,* but one in which, one observes, the driver's behavior seems quite economically sensible, and not at all contrived as in the case of Newcomb's Omega.\n\nParfit's Hitchhiker also bears a strong resemblance to some real-life dilemmas of central banks, e.g., threatening not to bail out too-big-to-fail institutions, or promising not to raise interest rates later.  In both cases, the central bank would benefit today from people believing that \"the central bank will not bail out irresponsible institutions\" or \"the central bank will wait before raising the targeted interest rate\".  But the markets are extremely good at predicting future events, and market actors know that later, the central bank will think that it is much more convenient to bail out the big bank, and that whatever irresponsibility has occurred is now already in the past and unalterable.\n\nSimilarly, on reaching the city in Parfit's Hitchhiker, you might be tempted to reason that the car has already driven you there, and so, when you *now* make the decision in your selfishness, you will reason that you are better off by \\$1,000 *now* if you refuse to pay, since your decision can't alter the past (right?)  Similarly if in Newcomb's Problem you see that Box B is already full; the money is right there and it can't vanish if you take both boxes, right?  But if you are a sort of agent that reasons like this, Parfit's driver asks you a few hard questions and then drives off to let you die in the desert.  Or the markets, which are sufficiently efficient to behave like extremely good predictors in many ways, may go ahead and call your bluff about moral hazard.\n\nBoth causal decision agents and evidential decision agents will two-box on the transparent version of Newcomb's Problem, or be left to die in the desert on Parfit's Hitchhiker.  A causal agent who sees a full Box B reasons \"I cannot cause Box B to become empty by leaving behind Box A\", and even an evidential agent reasons, \"It wouldn't be good news to leave behind Box A; I already *know* Box B is full, so Omega must have made a mistake.\" \n\nA logical decision theorist, on the other hand, cheerfully promises to pay the \\$1,000 to Parfit's driver, and then actually does so; they also cheerfully leave behind Box A in the transparent Newcomb's Problem.  An LDT agent is choosing the best output for their algorithm, and reasoning, \"If my algorithm had output 'don't pay' / 'take both boxes', then this would have implied my dying in the desert / Box B being empty.\"  (This calculation will be made more formal later.)\n\nOf course these simple scenarios are not exactly representative of what happens in the real world with the central bank and interest rates.  In the real world there are reputational effects and iterated games; if you bail out a bank today, this has a penalty in the form of people expecting you to bail out more banks later (albeit by that time you may have retired and left somebody else to run the central bank).  But this doesn't rule out Newcomblike channels as a *component* of the problem; people may have some idea of what kind of algorithm you're running and try to mentally simulate what your algorithm does.  There's also no rule saying that efficient markets *can't* read your expression well enough for what you secretly expect you'll do later %note: 'What you expect you'll do later' is produced by your current self's mental simulation of your future self, which is why the output of your future self's algorithm is appearing at two separate points in the overall process.% to have some effect on what people think of you now. %note: From the perspective of a causal decision agent, we should just regard our facial expressions as causal actions now that affect what other people believe, and control our facial expressions, darn it!  Of course, a logical decision theorist replies that if you could in fact control your facial expressions, and that market actors could mentally simulate you well enough to predict that you would try to control your facial expressions, the market actors would not already regard those facial expressions as evidence.%\n\nIn this sense, it may matter a non-zero amount whether people think that the *rational* course of action in the simplified Parfit's Hitchhiker dilemma is to pay \\$1,000 even after already reaching town, or if it is *rational* to die in the desert.\n\n## Logical decision theory\n\nIn general, a logical agent ought to calculate expected utility as follows:\n\n$$\\mathsf Q(s) = \\big ( \\underset{\\pi_x \\in \\Pi}{argmax} \\ \\sum_{o_i \\in \\mathcal O} \\mathcal U(o_i) \\cdot \\mathbb P(\\ulcorner \\mathsf Q = \\pi_x \\urcorner \\righttriangle o_i | \\mathsf I_0) \\big ) (s)$$\n\nWhere:\n\n- $\\mathsf Q$ is the agent's current decision algorithm - that is, the whole calculation presently running.\n- $s$ is the agent's sense data.\n- $\\pi_x \\in \\Pi$ is the output of $\\mathcal Q$, a *policy* that maps sense data to actions.\n- $\\ulcorner \\mathsf Q = \\pi_x \\urcorner$ is the proposition that, as a logical fact, the output of algorithm $\\mathsf Q$ is $\\pi_x.$\n- $\\mathbb P(X \\righttriangle o_i | \\mathsf I_0)$ is the probability of $o_i$ given the prior information $\\mathsf I_0$ and further *conditioning* on the logical fact $X.$\n\nTechnically, with regards to the key logical conditioning operator $X \\righttriangle Y$, logical decision theorists know two special-case ways to try to calculate it:\n\n- [ As a premise introduced into a standard proof algorithm], working out further logical implications.\n- As a Pearl-style $\\operatorname do()$ on a standard causal model that [ includes some nodes intended to denote unknown logical propositions.]\n\nLogical decision theorists are still trying to figure out a good candidate for a *general* formalization of $X -righttriangle Y,$ but meanwhile:\n\n- \"Treating $X$ as a new premise in a proof system and deriving its implications\" works to formalize [modal_agents], and let us formally simulate agents with common knowledge of each other's code negotiating on the Prisoner's Dilemma and other game-theoretic setups.\n- \"$\\operatorname{do}()$ on causal models that include some logical propositions\" suffices to formalize all the dilemmas, thought experiments, and scenarios (or rather, formalize the scenarios at least as far as causal decision theory formalized them).\n\nWe know these two formalizations aren't complete because:\n\n- The proof formalism we use for modal agents has [ weird edge cases] indicating that it only correctly formalizes the intuitive notion of logical conditioning some of the time.\n- We don't have a general algorithm for *building* causal models that include logical facts, and it's not clear that representation can model any setup more complicated than \"run this exact algorithm in two different places\".\n\n# Some LDT behaviors in economically suggestive scenarios\n\n(For the question of whether all of these are *rational* behaviors, or merely *usefully irrational* behaviors that happen to make agents richer, see the final section \"LDT as the principle of rational choice\" below.)\n\n## Voting\n\nLDT agents vote in elections if they believe they are part of a sufficiently large cohort of people voting for similar reasons that their cohort has a non-negligible probability of swinging the election (such that the expected value of possibly swinging this election outweighs the cost of everyone in their cohort voting).\n\nVoting is one of the most plausible candidates for something like a pure LDT analysis going through.  With so many voters participating in elections, there may plausibly be a large number of agents out there whom you should consider as being logically correlated with you.\n\n## Oneshot Prisoner's Dilemma\n\n%%!knows-requisite([dt_prisonersdilemma]):\n[todo: introduce PD here]\n%%\n\nMuch of the early work in LDT revolved around computational agents playing the Prisoner's Dilemma with [common_knowledge common knowledge] of each other's source code.\n\nSuppose, for instance, that you are a computational agent $\\mathsf {A}$ playing the Prisoner's Dilemma and you are told that the code of the other agent $\\mathsf {Fairbot}$ is as follows:\n\n    def Fairbot(A):\n      if is-provable(\"A(Fairbot) == Cooperate\"):\n        return Cooperate\n      else:\n        return Defect\n\nIn other words, $\\mathsf {Fairbot}$ tries to prove that $\\mathsf {A}$, in this case $\\mathsf {A}$ (you) cooperates with it.  If $\\mathsf {Fairbot}$ proves that $\\mathsf A,$ playing with $\\mathsf {Fairbot},$ cooperates, then $\\mathsf {Fairbot}$ cooperates; otherwise $\\mathsf {Fairbot}$ defects.\n\nWould you, in this case, defect on the Prisoner's Dilemma?  We can suppose even that $\\mathsf {Fairbot}$ has already run and has already made its move one way or another, so that your move cannot possibly have any causal effect on $\\mathsf {Fairbot}.$  The prescription of CDT is thus to Defect, which an LDT agent reasoning informally would regard as unwise.\n\nFairbot does not play optimally on the PD.  $\\mathsf{Fairbot}$ does cooperate with another $\\mathsf {Fairbot}$, even if the code of the other agent is not exactly similar (e.g. they can use different proof systems). %note: (If you want to know why Agent A trying to prove things about Agent B who is simultaneously trying to prove things Agent A (trying to prove things about agent B...) doesn't just collapse into an infinite recursion, the answer is \"[modal_agents Because] of [55w].\"  Sorry, this one takes a detailed explanation.%  If $\\mathsf {Fairbot}$ is reasoning in a [logic_soundness sound] system such as [3ft first-order arithmetic], then $\\mathsf {Fairbot}$ is also inexploitable; it never Cooperates when the opponent Defects.  However, $\\mathsf {Fairbot}$ cooperates with $\\mathsf {CooperateBot},$ the agent which simply always returns 'Cooperate'.  By the [true_prisoners_dilemma premises] of the Prisoner's Dilemma, we ought to at least bother to Defect against a rock with the word \"Cooperate\" written on it.  $\\mathsf {Fairbot}$ fails to exploit $\\mathsf {CooperateBot},$ so $\\mathsf {Fairbot}$'s play is not optimal.\n\nThe milestone paper \"[Robust Cooperation in the Prisoner's Dilemma](https://arxiv.org/abs/1401.5577)\" exhibited a proof (and running code!) that there existed a simple agent $\\mathsf{PrudentBot}$ which:\n\n- Mutually cooperated with $\\mathsf{Fairbot}$\n- Mutually cooperated with another $\\mathsf{PrudentBot}$\n- Defected against $\\mathsf{DefectBot}$\n- Defected against (hence exploited) $\\mathsf{CooperateBot}$\n- Was itself provably unexploitable.\n\nThis idiom of \"modal agents\" was later generalized to more natural and generic agents.\n\nReal-life versions of the Prisoner's Dilemma do not nearly match the setup of two computational agents with common knowledge of each other's source code.  In real life, there are reputation effects, people who care about other people, and a far more tenuous grasp on the other person's reasoning processes.  The Prisoner's Dilemma with probabilistic beliefs has yet to be formally analyzed in LDT.\n\nNonetheless. it seems fair to say that if you are an LDT agent:\n\n- You should not Defect against somebody who you are pretty sure will reason very similarly to you.\n- You should not Defect against a fair-minded agent that you think is pretty good at predicting you.\n- You should not Defect against an agent that is pretty good at predicting you, that you are pretty good at predicting, who you predict has decided to Cooperate iff it predicts you Cooperate (in order to incentivize you to do likewise, given your own predictive abilities).\n- As the relative benefits of Cooperation in a PD-like scenario increase, it seems increasingly plausible that some LDT-like reason for cooperation will end up going through.  E.g. if the payoff matrix is (1,1) vs (0, 101) vs (101, 0) vs (100, 100), even a small probability that the two of you are reasoning similarly might be enough to carry the handshake.\n\nIf you further accept the argument below that LDT is a better candidate than CDT for the principle of rational choice, then it is also fair to say that economists should stop going around proclaiming that rational agents defect in the Prisoner's Dilemma.  LDT is not a polyanna solution and it seems quite possible that two LDT agents might end up Defecting against each other (e.g. because they don't know each other to be LDT agents, or don't know the other knows, etcetera); but \"rational agents in general just can't do anything about the mutual-defection equilibrium in the Prisoner's Dilemma\" is much too harsh.\n\n%%knows-requisite([dt_gametheory]):\n## Game theory\n\nThe first-ever derivation of Nash-equilibrium play *in two agents doing pure expected utility maximization* and modeling each other, *without* each agent starting from the assumption that the other agent is already looking for Nash equilibria, was done by logical decision theorists [using a common-knowledge-of-code setup and reflective oracles](http://www.auai.org/uai2016/proceedings/papers/87.pdf).\n%%\n\n## Ultimatum Game\n\n\n\n## Iterated Prisoner's Dilemma\n\n## Blackmail\n\n# LDT as the principle of rational choice\n\nLogical decision theory claims to embody *the principle of rational decision*--or at least, embody it a lot better than causal decision theory and better than any currently known alternative.  This is primarily the domain of the more analytic-philosophical viewpoint on LDT, but is worth summarizing.\n\nThe first and foremost motivation behind LDT is that LDT agents systematically end up rich.  The current literature contains rich veins of discourse about \"one-boxers\" on Newcomb's Problem asking two-boxers \"Why aincha rich, if you're so rational?\" and various retorts along the lines of \"It's not my fault Omega decided to punish people who'd act rationally before this experiment started; it doesn't change what the rational choice is.\"\n\n(That retort may sound less persuasive if we're thinking about Parfit's Hitchhiker.  The driver is not making a weird arbitrary choice to punish 'rational' agents, the driver is just acting with undisputed economic rationality on their own part.  It makes no (selfish) sense to rescue someone you don't predict will pay up afterwards.)\n\nContrarily, the LDT agent thinks that Omega (and the driver in Parfit's Hitchhiker) are not being particularly 'unfair' to one particular kind of algorithm.  If Omega read the agent's source code and decided to reward only agents with an *algorithm* that output 'one box' *by picking the first choice in alphabetical order,* punishing all agents that behaved in exactly the same way due a different internal computation, then this would indeed be a rigged contest.  But in Newcomb's Problem, Omega only cares about the behavior, and not the kind of algorithm that produced it; and an agent can indeed take on whatever kind of behavior it likes; so, according to LDT, there's no point in saying that Omega is being unfair.  You can make the logical output of your algorithm be whatever you want (though one cannot perhaps want whatever one wants) so there's no point in picking an output that leaves you to die in the desert.\n\nSimilarly, LDT agents never need to resort to *precommitments* (since LDT agents never wish their future selves would act differently from the LDT algorithm) and LDT agents always calculate *a positive value of information* (where an evidential decision agent might beg you to *not* render Box B transparent, since then it will be empty).  From the standpoint of an LDT agent, CDT agents are undergoing [ preference reversals] and being subject to [ money pumps] in a way that the economic literature usually treats as prima facie indicators of economic irrationality.\n\nE.g., a CDT agent will, given the chance, pay a \\$10 fee to have a precommitment assistant stand around with a gun threatening to shoot them if they take both boxes in Newcomb's Problem.  Naturally, given the chance by surprise, the same agent would *later* (after Omega's departure) pay \\$10 to make the gun-toter go away. %note: Even believing that Omega has accurately predicted this, and that Box B is therefore empty!%  From the standpoint of an LDT agent, this is no more excusable than an agent exhibiting non-Bayesian behavior on the [allais_paradox Allais Paradox], paying \\$1 to throw a switch and then paying another \\$1 to throw it back.  A rational agent would not exhibit such incoherence and drive itself around in circles, any more than a rational agent would deign to have [ circular preferences].\n\nOf course, in real life, other people may not be certain of what algorithm we are running; so there will remain an in-practice use for publicly visible precommitments.  But it is worth asking questions like \"To the extent people *can* figure out what algorithm we're using, what should that algorithm be?\"  Or \"If we could publicly pay someone to force us to use a particular algorithm, what should that algorithm be?\"  (If [Distributed Autonomous Organizations](https://en.wikipedia.org/wiki/Decentralized_autonomous_organization) ever happen, there will be economic actors which can have publicly visible source code.)  Similarly, in real life there are reputational effects that serve as local incentives to behave the way we want people to expect us to behave later.  But it remains worth asking, \"What general algorithm do I want to have a reputation for using?\"\n\nThe modern economics literature takes for granted 'precommitments' and 'usefully irrational' behavior at the bargaining table that it may sound odd to claim that [ dynamic consistency] and [ reflective consistency] are desirable properties for the principle of rational choice to have - why, who would think in the first place that rational agents would want their future selves to behave rationally?  There are just two entirely different subjects of study, the study of 'What is rational', and the study of 'What rationalists wish they chose' or rationality as modified by precommitments, useful irrationality, etcetera.\n\nBut it would nonetheless be a remarkable fact if the second field of study happened to point strongly in the direction of a simple, general principle with various philosophically appealing properties which happened to be much more strongly consistent in various ways, whose corresponding agents stand around saying things like \"Why on Earth would a rational agent wistfully wish that their future selves would be irrational?\" or \"What do you mean *useful irrationality?*  If a choice pattern is useful, it's not irrational!  That's like [Spock the Straw Vulcan complaining about losing a chess game](http://intelligenceexplosion.com/en/2011/why-spock-is-not-rational/#fn5x6-bk) to an opponent who played 'illogically'!\"\n\nIf you are a pure causal decision agent, you will wave off all that irrationality with a sigh.  But if we let ourselves blank our minds of our previous thoughts and try to return to an intuitive, pretheoretic standpoint, we might suspect from looking over this situation that we have made a mistake about what to adopt as our explicit theory of rationality.\n\nIn analytic philosophy, the case for causal decision theory rests primarily on the intuition that one-boxing on Newcomb's problem cannot *cause* Box B to be full.  Or on the transparent Newcomb's Problem, with Box B transparently full (or empty), it cannot be reasonable to imagine that by leaving behind Box A and its \\$1,000 you can cause things to be different?  Is there not only one reasonable way to construe the counterfactual conditioning in the expected utility formula?  Is it not in some sense *true,* after Parfit's driver has conveyed the LDT agent to the city in the desert, that in the counterfactual world where the LDT agent does not at that time choose to pay, they remain in the city?  In this sense, must not an LDT agent be deluded about some question of fact, or act as if it is so deluded?\n\nThe LDT agent responds:\n\n- There aren't any actual worlds \"where I didn't pay\" floating out there.  The only *real* world is the one where my decision algorithm had the output it actually had.  To imagine other worlds is an act of imagination, computing a description of a certain world that doesn't exist; there's no corresponding actual world, so my description of the impossible can't be true or false under a correspondence theory of truth.  That being the case, \"Counterfactuals were made for humanity, not humanity for counterfactuals\"; I can decide to condition on the actions my algorithm doesn't take in whatever way produces the greatest wealth.  I am free to say \"In the nonexistent world where I don't pay now, I already died in the desert\".\n- I don't one-box in Newcomb's Problem *because I think it physically causes Box B to be full.*  I one-box in Newcomb's Problem because I have computed this as the optimal output in an entirely different way.  It begs the question to assume that a rational agent must make its decision by carrying out this particular ritual of cognition about which things physically cause which other things, therefore stating that I am \"acting as if\" I irrationally believe that my choice physically causes Box B to be full.\n\nTo this the LDT agent also adds that it *is* desirable for the action-conditionals we compute to draw accurate pictures of the world where we take the action we actually end up taking.  That is, if it's a fact that your decision algorithm outputs action $a_x,$ then your previous imagination of \"The world where I do $a_x$\" should match reality.  [ This is a condition that CDT violates!]  (In a way that can be used to pump money out of CDT agents.)\n\nFor this reason, logical decision theorists think that when we talk about what LDT agents would do, this is a much stronger claim to be speaking about *what rational agents would do* than if we talk about what CDT agents would do, with respect to our pretheoretic intuitions about what sort of thing 'rationality' ought to be.\n\n[todo:  Greenlink to a page where all this gets more extensively discussed.]\n\nRegardless of what one eventually decides about principles of normative choice, it ought to be uncontroversial that a reputation for acting like an LDT agent is a valuable reputation for an agent to have.  Then any analysis of what it is economically rational to do *in public* ought to concern itself with LDT agents rather than CDT agents.  If other people can see you playing the Ultimatum Game, or you are iterating it an unknown number of times, then it ought to be entirely uncontroversial that the rational agent acts according to LDT rather than CDT.  In the case of the Ultimatum Game, the LDT prescription also happens to be a better descriptive theory of how actual humans behave.  So if for some reason we *must* descriptively model humans as causal decision agents, we ought to model them as causal decision agents who've realized that they ought to acquire a reputation for acting like logical decision agents, thereby saving the phenomena.\n\nBut it is our own opinion that putting the outputs of the CDT algorithm into economics textbooks as the 'rational' choice was just an honest mistake, and that academic economics ought to chew over this situation for a year or two and then go back and announce that voting can be rational after all.\n\n# Further exploration\n\n[todo: put stuff here]\n\n",
      "metaText": "",
      "isTextLoaded": true,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 1,
      "maintainerCount": 1,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": 0,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 14,
      "redLinkCount": 0,
      "lockedBy": "2",
      "lockedUntil": "2018-10-30 20:52:33",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": {
        "edit": {
          "has": false,
          "reason": "You don't have domain permission to edit this page"
        },
        "proposeEdit": {
          "has": true,
          "reason": ""
        },
        "delete": {
          "has": false,
          "reason": "You don't have domain permission to delete this page"
        },
        "comment": {
          "has": false,
          "reason": "You can't comment in this domain because you are not a member"
        },
        "proposeComment": {
          "has": true,
          "reason": ""
        }
      },
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [
        "58b"
      ],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [
        "3rk"
      ],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [
        {
          "id": "5937",
          "parentId": "1lx",
          "childId": "58f",
          "type": "requirement",
          "creatorId": "2",
          "createdAt": "2016-08-03 22:43:12",
          "level": 2,
          "isStrong": true,
          "everPublished": true
        }
      ],
      "subjects": [
        {
          "id": "5189",
          "parentId": "58b",
          "childId": "58f",
          "type": "subject",
          "creatorId": "2",
          "createdAt": "2016-07-11 20:25:54",
          "level": 2,
          "isStrong": true,
          "everPublished": true
        },
        {
          "id": "5935",
          "parentId": "5n9",
          "childId": "58f",
          "type": "subject",
          "creatorId": "2",
          "createdAt": "2016-08-03 22:42:33",
          "level": 2,
          "isStrong": true,
          "everPublished": true
        },
        {
          "id": "5936",
          "parentId": "5px",
          "childId": "58f",
          "type": "subject",
          "creatorId": "2",
          "createdAt": "2016-08-03 22:42:57",
          "level": 1,
          "isStrong": true,
          "everPublished": true
        }
      ],
      "lenses": [],
      "lensParentId": "58b",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "23107",
          "pageId": "58f",
          "userId": "2",
          "edit": 14,
          "type": "newEdit",
          "createdAt": "2018-10-30 20:52:33",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "22240",
          "pageId": "58f",
          "userId": "2",
          "edit": 13,
          "type": "newEdit",
          "createdAt": "2017-03-03 18:51:52",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "18308",
          "pageId": "58f",
          "userId": "2",
          "edit": 0,
          "type": "newRequirement",
          "createdAt": "2016-08-03 22:43:13",
          "auxPageId": "1lx",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "18307",
          "pageId": "58f",
          "userId": "2",
          "edit": 0,
          "type": "newSubject",
          "createdAt": "2016-08-03 22:42:58",
          "auxPageId": "5px",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "18305",
          "pageId": "58f",
          "userId": "2",
          "edit": 0,
          "type": "newSubject",
          "createdAt": "2016-08-03 22:42:33",
          "auxPageId": "5n9",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "18303",
          "pageId": "58f",
          "userId": "2",
          "edit": 0,
          "type": "newTag",
          "createdAt": "2016-08-03 22:41:39",
          "auxPageId": "3rk",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "17875",
          "pageId": "58f",
          "userId": "2",
          "edit": 12,
          "type": "newEdit",
          "createdAt": "2016-08-01 00:50:50",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "17469",
          "pageId": "58f",
          "userId": "2",
          "edit": 11,
          "type": "newEdit",
          "createdAt": "2016-07-25 00:45:00",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "16956",
          "pageId": "58f",
          "userId": "2",
          "edit": 10,
          "type": "newEdit",
          "createdAt": "2016-07-16 21:17:16",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "16767",
          "pageId": "58f",
          "userId": "2",
          "edit": 9,
          "type": "newEdit",
          "createdAt": "2016-07-15 02:14:48",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "16710",
          "pageId": "58f",
          "userId": "2",
          "edit": 8,
          "type": "newEdit",
          "createdAt": "2016-07-14 20:13:31",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "16582",
          "pageId": "58f",
          "userId": "2",
          "edit": 7,
          "type": "newEdit",
          "createdAt": "2016-07-12 05:04:56",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "16581",
          "pageId": "58f",
          "userId": "2",
          "edit": 6,
          "type": "newEdit",
          "createdAt": "2016-07-12 05:03:27",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "16580",
          "pageId": "58f",
          "userId": "2",
          "edit": 5,
          "type": "newEdit",
          "createdAt": "2016-07-12 00:10:04",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "16579",
          "pageId": "58f",
          "userId": "2",
          "edit": 4,
          "type": "newEdit",
          "createdAt": "2016-07-11 20:44:34",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "16578",
          "pageId": "58f",
          "userId": "2",
          "edit": 0,
          "type": "newSubject",
          "createdAt": "2016-07-11 20:25:55",
          "auxPageId": "58b",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "16576",
          "pageId": "58f",
          "userId": "2",
          "edit": 0,
          "type": "newTag",
          "createdAt": "2016-07-11 20:25:35",
          "auxPageId": "4v4",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "16575",
          "pageId": "58f",
          "userId": "2",
          "edit": 0,
          "type": "deleteTag",
          "createdAt": "2016-07-11 20:25:30",
          "auxPageId": "4bn",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "16573",
          "pageId": "58f",
          "userId": "2",
          "edit": 0,
          "type": "newTag",
          "createdAt": "2016-07-11 20:25:27",
          "auxPageId": "4bn",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "16572",
          "pageId": "58f",
          "userId": "2",
          "edit": 0,
          "type": "deleteTag",
          "createdAt": "2016-07-11 20:25:16",
          "auxPageId": "4v",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "16570",
          "pageId": "58f",
          "userId": "2",
          "edit": 3,
          "type": "newEdit",
          "createdAt": "2016-07-11 20:25:02",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "16565",
          "pageId": "58f",
          "userId": "2",
          "edit": 2,
          "type": "newEdit",
          "createdAt": "2016-07-11 18:42:35",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "16283",
          "pageId": "58f",
          "userId": "2",
          "edit": 0,
          "type": "newParent",
          "createdAt": "2016-07-09 00:33:14",
          "auxPageId": "58b",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "16284",
          "pageId": "58f",
          "userId": "2",
          "edit": 0,
          "type": "newTag",
          "createdAt": "2016-07-09 00:33:14",
          "auxPageId": "4v",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "16281",
          "pageId": "58f",
          "userId": "2",
          "edit": 1,
          "type": "newEdit",
          "createdAt": "2016-07-09 00:33:13",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        }
      ],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": true,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "5b2": {
      "likeableId": "0",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "5b2",
      "edit": 2,
      "editSummary": "",
      "prevEdit": 1,
      "currentEdit": 2,
      "wasPublished": true,
      "type": "wiki",
      "title": "Omega (alien philosopher-troll)",
      "clickbait": "The entity that sets up all those trolley problems.  An alien philosopher/troll imbued with unlimited powers, excellent predictive ability, and very odd motives.",
      "textLength": 3250,
      "alias": "omega_troll",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "2",
      "editCreatedAt": "2016-07-10 22:16:14",
      "pageCreatorId": "2",
      "pageCreatedAt": "2016-07-10 22:14:21",
      "seeDomainId": "0",
      "editDomainId": "123",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 0,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 117,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0
      ],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": -2,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "5n9": {
      "likeableId": "0",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "5n9",
      "edit": 4,
      "editSummary": "",
      "prevEdit": 3,
      "currentEdit": 4,
      "wasPublished": true,
      "type": "wiki",
      "title": "Causal decision theories",
      "clickbait": "On CDT, to choose rationally, you should imagine the world where your physical act changes, then imagine running that world forward in time.  (Therefore, it's irrational to vote in elections.)",
      "textLength": 17408,
      "alias": "causal_dt",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "2",
      "editCreatedAt": "2016-08-02 00:36:46",
      "pageCreatorId": "2",
      "pageCreatedAt": "2016-07-29 22:15:27",
      "seeDomainId": "0",
      "editDomainId": "123",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 2,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 186,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": 0,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "5px": {
      "likeableId": "0",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "5px",
      "edit": 1,
      "editSummary": "",
      "prevEdit": 0,
      "currentEdit": 1,
      "wasPublished": true,
      "type": "wiki",
      "title": "Evidential decision theories",
      "clickbait": "Theories which hold that the principle of rational choice is \"Choose the act that would be the best news, if somebody told you that you'd chosen that act.\"",
      "textLength": 1011,
      "alias": "evidential_dt",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "2",
      "editCreatedAt": "2016-08-01 00:22:14",
      "pageCreatorId": "2",
      "pageCreatedAt": "2016-08-01 00:22:14",
      "seeDomainId": "0",
      "editDomainId": "123",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 0,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 106,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": 0,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "5py": {
      "likeableId": "0",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "5py",
      "edit": 1,
      "editSummary": "",
      "prevEdit": 0,
      "currentEdit": 1,
      "wasPublished": true,
      "type": "wiki",
      "title": "Prisoner's Dilemma",
      "clickbait": "You and an accomplice have been arrested.  Both of you must decide, in isolation, whether to testify against the other prisoner--which subtracts one year from your sentence, and adds two to theirs.",
      "textLength": 7237,
      "alias": "prisoners_dilemma",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "2",
      "editCreatedAt": "2016-08-01 02:05:58",
      "pageCreatorId": "2",
      "pageCreatedAt": "2016-08-01 02:05:58",
      "seeDomainId": "0",
      "editDomainId": "123",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 0,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 65,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0
      ],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": -2,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "5pz": {
      "likeableId": "0",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "5pz",
      "edit": 2,
      "editSummary": "",
      "prevEdit": 1,
      "currentEdit": 2,
      "wasPublished": true,
      "type": "wiki",
      "title": "True Prisoner's Dilemma",
      "clickbait": "A scenario that would reproduce the ideal payoff matrix of the Prisoner's Dilemma about human beings who care about their public reputation and each other.",
      "textLength": 9530,
      "alias": "true_prisoners_dilemma",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "2",
      "editCreatedAt": "2016-10-16 04:19:11",
      "pageCreatorId": "2",
      "pageCreatedAt": "2016-08-01 03:07:29",
      "seeDomainId": "0",
      "editDomainId": "123",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 0,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 83,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0
      ],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": -2,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "5qn": {
      "likeableId": "3343",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "5qn",
      "edit": 9,
      "editSummary": "",
      "prevEdit": 8,
      "currentEdit": 9,
      "wasPublished": true,
      "type": "wiki",
      "title": "Death in Damascus",
      "clickbait": "Death tells you that It is coming for you tomorrow.  You can stay in Damascus or flee to Aleppo.  Whichever decision you actually make is the wrong one.  This gives some decision theories trouble.",
      "textLength": 12821,
      "alias": "death_in_damascus",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "5yw",
      "editCreatedAt": "2017-03-21 12:39:52",
      "pageCreatorId": "2",
      "pageCreatedAt": "2016-08-02 04:05:38",
      "seeDomainId": "0",
      "editDomainId": "123",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 0,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 502,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0
      ],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": -2,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "5tp": {
      "likeableId": "0",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "5tp",
      "edit": 6,
      "editSummary": "",
      "prevEdit": 5,
      "currentEdit": 6,
      "wasPublished": true,
      "type": "wiki",
      "title": "Ultimatum Game",
      "clickbait": "A Proposer decides how to split $10 between themselves and the Responder.  The Responder can take what is offered, or refuse, in which case both parties get nothing.",
      "textLength": 16790,
      "alias": "ultimatum_game",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "2",
      "editCreatedAt": "2016-08-10 06:53:20",
      "pageCreatorId": "2",
      "pageCreatedAt": "2016-08-10 03:28:27",
      "seeDomainId": "0",
      "editDomainId": "123",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 2,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 129,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0
      ],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": -2,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    }
  },
  "edits": {
    "58f": {
      "likeableId": "3054",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "58f",
      "edit": 2,
      "editSummary": "",
      "prevEdit": 1,
      "currentEdit": 14,
      "wasPublished": true,
      "type": "wiki",
      "title": "Intro for Economists",
      "clickbait": "An introduction to logical decision theory for economists",
      "textLength": 42482,
      "alias": "ldt_intro_econ",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "2",
      "editCreatedAt": "2016-07-11 18:42:35",
      "pageCreatorId": "2",
      "pageCreatedAt": "2016-07-09 00:33:13",
      "seeDomainId": "0",
      "editDomainId": "15",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": false,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 19,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 587,
      "text": "Currently, students of economics learn that:\n\n- The expected return on your time from voting in elections is very low, unless your vote *directly causes* many other people to vote.  Most elections are not settled by one vote, and your one vote is very unlikely to change that.\n- In the [ultimatum_game Ultimatum Game], the experimenter sets up a one-shot, two-player scenario for dividing \\$1.  One player, the Proposer, proposes how to split the \\$10 with the other player, the Responder.  If the Responder accepts the proposed split, it goes through.  Otherwise both players get nothing.  A *rational* Proposer should offer \\$1 to the other player and keep the remaining \\$9; a *rational* Responder should accept this bargain.  (Assuming that it's a one-time deal, maybe anonymous so that there's no reputation effects, etcetera.)\n- You and your enemy share a terrible secret.  Your enemy sets up an automatic mechanism so that unless your Bitcoin address pays 10 Bitcoins to a certain other address, a secret will be published, greatly harming you and your enemy.  In this case, paying your enemy the 10 Bitcoins has higher expected utility than not paying, and that is what a rational agent will do--even though your enemy predicted this response by you, and that's why your enemy set up the automatic mechanism.\n\nAll of the currently-standard replies above about *coordination problems,* *bargaining theory,* and *game theory* are derived from the decision theory that's dominant in modern analytic philosophy, namely *causal decision theory* (CDT).  Roughly, causal decision theory says, \"Decide based on the varying *causal* consequences you expect from different *physical acts.*\"  Most economics textbooks may not go into this, but it's where the standard analysis comes from.\n\nOn this standard analysis, if we want a theory of how an agent can resist having their money pumped out of them by blackmail, then we must look beyond theories of 'rationality' and talk about agents that are 'usefully irrational' in certain ways, or maybe 'socially rational' or some other not-very-well-formalized concept.\n\n*Logical* decision theorists argue that causal decision theory suffers from extremely severe problems.  Not just problems along the lines of \"We don't like the answer this gives for the Ultimatum Game\", but problems like \"We can make this decision theory [death_in_damascus go into infinite loops]\" or \"This decision theory can [ calculate] a negative [information_value value of information]\".\n\nArguendo, when we fix these problems, we end up with a new, foundational, formal principle of rational choice:  \"Decide based on the *logical* consequences of different potential outputs for your *decision algorithm*.\"  Although people are still working on variant ways to implement this principle, the general family is known as Logical Decision Theory or LDT.\n\nLogical decision theorists sometimes use the metaphor of computational agents that have definite or probabilistic knowledge about other agents' source code.  If you simulated another agent and saw for certain that it would reject any offer less than \\$5 in the Ultimatum Game, would you still offer it only \\$1?%note:  (What about if your simulation showed that the other agent rejected any offer less than \\$9?)%\n\nIn the real world, taking into account iterated interactions and reputational effects and subjective uncertainties, dilemmas are no longer simple as two agents playing the Ultimatum Game with knowledge of each other's source code.  We can't take the answers of logical decision theory for simple cases, as directly indicating what rational humans should do in much more complex cases.\n\nStill, the simplified games are often held up as an archetypal example or a base case.  Arguendo according to LDT, these base cases are being wrongly analyzed by standard causal decision theory.\n\n%%%knows-requisite([dt_prisonersdilemma]):\nLogical decision theory can be formalized at least as much as competing decision theories.  For example, there is now [running code](https://arxiv.org/abs/1401.5577) for simulating in a general way how multiple agents with knowledge of each other's code, reasoning about each other's actions, can arrive at an equilibrium.  Two similar (but not identical!) maximizing agents of this kind will end up cooperating in the [prisoners_dilemma Prisoner's Dilemma], assuming they have [common_knowledge common knowledge] of each other's code.  Again, you can actually try this out in simulation!\n%%%\n\nAnd yes, LDT seems to strongly suggest that, even in the real world, if you're part of a sufficiently large cohort of people all voting the same way for similar reasons, you should (all) vote in the election.  %%knows-requisite([dt_prisonersdilemma]):(For much the same reason LDT says you ought to Cooperate, if you're playing the Prisoner's Dilemma against a recent cloned copy of yourself.)%%\n\n# Different principles of expected utility\n\nAlmost everyone in the debate agrees that 'rational' agents maximize expected utility *conditional on* their decisions.  The central question of decision theory turns out to be, \"How exactly do we condition on a decision?\"\n\n## Evidential versus counterfactual conditioning\n\n[todo: condition this text on math2 and write a math1 alternate version.]\n\nMost economics papers show the expected utility formula as:\n\n$$\\mathbb E[\\mathcal U|a_x] = \\sum_{o_i \\in \\mathcal O} \\mathcal U(o_i) \\cdot \\mathbb P(o_i|a_x)$$\n\nwhere\n\n- $\\mathbb E[\\mathcal U|a_x]$ is our average expectation of utility, if action $a_x$ is chosen;\n- $\\mathcal O$ is the set of possible outcomes;\n- $\\mathcal U$ is our utility function, mapping outcomes onto real numbers;\n- $\\mathbb P(o_i|a_x)$ is the [1rj conditional probability] of outcome $o_i$ if $a_x$ is chosen.\n\nTechnically speaking, this formula is almost universally agreed to be wrong.\n\nThe problem is the use of standard evidential conditioning in $\\mathbb P(o_i|a_x).$  On this formula we are behaving as if we're asking, \"What would be my [1y6 revised] probability for $\\mathbb P(o_i),$ if I was *told the news* or *observed the evidence* that my action had been $a_x$?\"\n\nCausal decision theory says we should instead use the *counterfactual conditional* $\\ \\mathbb P(a_x \\ \\square \\! \\! \\rightarrow o_i).$\n\nThe difference between evidential and counterfactual conditioning is often explained by contrasting these two sentences:\n\n- If Lee Harvey Oswald didn't shoot John F. Kennedy, somebody else did.\n- If Lee Harvey Oswald hadn't shot John F. Kennedy, somebody else would have.\n\nIn the first sentence, we're being told as news that Oswald didn't shoot Kennedy, and [1ly updating our beliefs] accordingly to match the world we already saw.  In the second world, we're imagining how a counterfactual world would have played out if Oswald had acted differently.\n\nIf $K$ denotes the proposition that somebody else shot Kennedy and $O$ denotes the proposition that Oswald shot him, then the first sentence and second sentence are respectively talking about:\n\n- $\\mathbb P(K| \\neg O)$\n- $\\mathbb P(\\neg O \\ \\square \\!\\! \\rightarrow K)$\n\nCalculating expected utility using evidential conditioning is widely agreed to lead to an irrational policy of 'managing the news'.  For example, suppose that toxoplasmosis, a parasitic infection carried by cats, can cause toxoplasmosis-infected humans to become fonder of cats.%note:  (This was formerly thought to actually be true.  More recently, this result may have failed to replicate.)%\n\nYou are now faced with a cute cat that has been checked by a veterinarian who says this cat definitely does *not* have toxoplasmosis.  If you decide to pet the cat, an impartial observer watching you will conclude that you are 10% more likely to have toxoplasmosis, which can be a fairly detrimental infection.  If you don't pet the cat, you'll miss out on the hedonic enjoyment of petting it.  Do you pet the cat?\n\nMost decision theorist agree that in this case you should pet the cat.  Either you already have toxoplasmosis or you don't.  Petting the cat can't *cause* you to acquire toxoplasmosis.  You'd just be missing out on the pleasant sensation of cat-petting.  Afterwards you may update your beliefs based on observing your own decision, and realize that you had toxoplasmosis all along.  But when you're considering the consequences of actions, you should reason that *if counterfactually* you had not pet the cat, you *still* would have had toxoplasmosis *and* missed out on petting the cat.  (Just like, if Oswald *hadn't* shot Kennedy, nobody else would have.)\n\nThe decision theory which claims that we should condition on our actions via the standard [1rj conditional probability formula], as if we were being told our choices as news or [1ly Bayesian-updating] on our actions as observations, is termed [evidential_decision_theory evidential decision theory].  Evidential decision theory answers the central question \"How do I condition on my choices?\" by replying \"Condition on your choices as if observing them as evidence\" or \"Take the action that you would consider best if you heard it as news.\"\n\n(For a more severe criticism of evidential decision theory showing how more clever agents can pump money out of evidential decision agents, see the [termite_dilemma Termite Dilemma].)\n\n## Causal decision theory\n\nCausal decision theory, the current academic standard, says that the expected utility formula should be written:\n\n$$\\mathbb E[\\mathcal U|a_x] = \\sum_{o_i \\in \\mathcal O} \\mathcal U(o_i) \\cdot \\mathbb P(a_x \\ \\square \\!\\! \\rightarrow o_i)$$\n\nThis leads into the question of how we compute $\\mathbb P(a_x \\ \\square \\!\\! \\rightarrow o_i),$ since it's not a standard [1rj conditional probability].\n\nIn the philosophical literature, it's often assumed that we intuitively know what the counterfactual results must be.  (E.g., we're just taking for granted that you somehow know that if Oswald hadn't shot Kennedy, nobody else would have; this is intuitively obvious.)  This is formalized by having a conditional distribution $\\mathbb P(\\bullet \\ || \\ \\bullet)$ which is treated as heaven-sent and includes the results of all counterfactual conditionals.\n\nPeople working in Artificial Intelligence will probably find this unsatisfactory, and will want to refer to the theory of [causal_model causal models] developed by Judea Pearl et. al.  The theory of causal models formally states how to perform counterfactual surgery on graphical models of causal processes.\n\n[todo: condition the following text on math2, write weaker-math version]\n\nFormally, we have a directed acyclic graph such as:\n\n[todo: put real diagram here]\n\n- $X_1$ -> {$X_2$, $X_3$} -> $X_4$ -> $X_5$\n\n*Click for example:* %note:\n\nOne of Judea Pearl's examples of a causal graph is:\n\n[todo: real diagram here]\n\n- SEASON -> {RAINING, SPRINKLER} -> {SIDEWALK} -> {SLIPPERY}\n\nThis says, e.g.:\n\n- That the current SEASON affects the probability that it's RAINING, and separately affects the probability of the SPRINKLER turning on.  (But RAINING and SPRINKLER don't affect each other; if we know the current SEASON, we don't need to know whether it's RAINING to figure out the probability the SPRINKLER is on.)\n- RAINING and SPRINKLER can both cause the SIDEWALK to become wet.  (So if we did observe that the sidewalk was wet, then even already knowing the SEASON, we would estimate a different probability that it was RAINING depending on whether the SPRINKLER was on.  The SPRINKLER being on would 'explain away' the SIDEWALK's observed wetness without any need to postulate RAIN.)\n- Whether the SIDEWALK is wet is the sole determining factor for whether the SIDEWALK is SLIPPERY.  (So that if we *know* whether the SIDEWALK is wet, we learn nothing more about the probability that the path is SLIPPERY by being told that the SEASON is summer.  But if we didn't already know whether the SIDEWALK was wet, whether the SEASON was summer or fall might be very relevant for guessing whether the path was SLIPPERY!)\n%\n\nA causal model goes beyond the graph by including specific probability functions $\\mathbb P(X_i | \\mathbf{pa}_i)$ for how to calculate the probability of each node $X_i$ taking on the value $x_i$ given the values \\mathbf {pa}_i$ of $x_i$'s immediate ancestors.  It is implicitly assumed that the causal model [ factorizes], so that the probability of any value assignment $\\mathbf x$ to the whole graph can be calculated using the product:\n\n$$\\mathbb P(\\mathbf x) = \\prod_i \\mathbb P(x_i | \\mathbf{pa}_i)$$\n\nThen, rather straightforwardly, the counterfactual conditional $\\mathbb P(\\mathbf x | \\operatorname{do}(X_j=x_j))$ is calculated via:\n\n$$\\mathbb P(\\mathbf x | \\operatorname{do}(X_j=x_j)) = \\prod_{i \\neq j} \\mathbb P(x_i | \\mathbf{pa}_i)$$\n\n(We assume that $\\mathbf x$ has $x_j$ equaling the $\\operatorname{do}$-specified value of $X_j$; otherwise its conditioned probability is defined to be $0$.)\n\nThis formula implies - as one might intuitively expect - that conditioning on $\\operatorname{do}(X_j=x_j)$ can only affect the probabilities of variables $X_i$ that are \"downstream\" of $X_j$ in the directed acyclic graph that is the backbone of the causal model.  In much the same way that (ordinarily) we think our choices today affect how much money we have tomorrow, but not how much money we had yesterday.\n\nThen expected utility should be calculated as:\n\n$$\\mathbb E[\\mathcal U| \\operatorname{do}(a_x)] = \\sum_{o_i \\in \\mathcal O} \\mathcal U(o_i) \\cdot \\mathbb P(o_i | \\operatorname{do}(a_x))\n\nUnder this rule, we won't calculate that we can affect the probability of having toxoplasmosis by petting the cat, since our choice to pet the cat is causally downstream of whether we have toxoplasmosis.\n\n[todo: put diagram here]\n\n%%comment:\nOne class of problems is hinted-at by the point that the above expected utility formula, in the course of making its calculation, gives the wrong expected utility for the action actually implemented!\n\nSuppose the [1rm prior probability] of having toxoplasmosis is 10%, and the posterior probability after being seen to pet the cat is 20%.  Suppose that *not* having toxoplasmosis has \\$100 utility; that having toxoplasmosis has \\$0 utility; and that, given the amount you enjoy petting cats, petting the cat adds \\$1 of utility to your outcome.\n\nThen the above formula for deciding whether to pet the cat suggests that petting leads to an expected utility of \\$91, and not petting leads to an expected utility of \\$90.  This tells us to pet the cat, which is the correct decision, but it also tells us to expect \\$91 of expected utility after petting the cat, where we actually receive \\$81 in expectation.  It seems like the intuitively \"correct\" answer is that we should calculate \\$81 of utility for petting the cat and \\$80 utility for not petting it.\n\nYou might initially be tempted to solve this problem by doing the calculation in phases:\n\n- Phase 1:  Calculate the decision based on prior beliefs.\n- Phase 2:  Update our beliefs based on having observed our first-order decision.\n- Phase 3:  Recalculate the expected utilities based on the posterior beliefs, possibly picking a new action.\n\n...and then wait for this algorithm to settle into a consistent state.\n\nBut besides lacking the computational efficiency of computing our decision in one swoop, it's entirely possible for an agent like this to [death_in_damascus go into an infinite loop].\n%%\n\n## Newcomblike problems\n\nAlthough causal decision theory became widely accepted, there were also widespread suspicions that causal decision theory might not be optimal, or might be missing some key element of rationality.\n\nThe academic debate on this subject revolved mainly around *Newcomblike problems,* a broad class of dilemmas which turned out to include the Prisoner's Dilemma; commons problems and coordination problems (like voting in elections); blackmail and other dilemmas of negotiation; plus other problems of interest.\n\nRoughly, we could describe Newcomblike problems as those where somebody similar to you, or trying to predict you, exists in the environment.  In this case your decision can *correlate* with events outside you, without your action *physically causing* those events.\n\nThe original Newcomb's Problem was somewhat artificial, but it's worth going into for historical reasons:\n\nAn alien named [5b2 Omega] presents you with two boxes, a transparent box A containing \\$1,000, and an opaque Box B.  Omega then flies away, leaving you with the choice of whether to take only Box B ('one-box') or to take Box A plus Box B ('two-box').  Omega has put $1,000,000 in Box B if and only if Omega predicted that you would take only one box; otherwise Box B is empty.\n\nOmega has already departed, so Box B is already empty or already full.\n\nOmega is an excellent predictor and has been observed to be right in the 73 out of 73 cases it has previously run this experiment.\n\nDo you take both boxes, or only Box B?\n\n- Argument 1:  People who take only Box B tend to walk away rich.  People who two-box tend to walk away poor.  It is better to be rich than poor.\n- Argument 2:  Omega has already made its prediction.  Box B is already empty or already full.  It would be irrational to leave behind Box A for no reason.  It's true that Omega has chosen to reward irrationality in this setup, but Box B is now *already empty* for your rational self, and leaving it behind would just counterfactually result in your getting \\$0 instead of \\$1,000.\n\nThis setup went on to generate an incredible amount of debate.  Newcomb's Problem is conventionally seen as an example that splits the verdict of evidential decision theory (\"Taking Box B is good news!  Do that.\") versus causal decision theory (\"Taking both boxes does not *cause* Box B to be empty, it just adds \\$1,000 to the reward\") in a way that initially seems more favorable to evidential decision agents, who walk away rich.\n\n(Naturally, logical decision agents one-box, not because one-boxing is good news, but because the LDT algorithm chooses as if controlling the output of the algorithm; and if the output of the LDT algorithm is 'take one box', this leads to Box B containing a million dollars.)\n\nThe setup in Newcomb's Problem may seem contrived, but consider the following variant, Parfit's Hitchhiker:\n\nYou are lost in the desert, your water bottle almost exhausted, when somebody drives up in a lorry.  This person is (a) entirely selfish, and (b) very good at detecting lies--maybe the driver went through Paul Ekman's training for reading facial microexpressions, or the driver got a lot of experience interrogating potential liars in a context where they actually received feedback on how well they did.\n\nThe driver says that they will drive you into town, but only if you promise to give them \\$1,000 on arrival.\n\nYou are also entirely selfish.\n\nIf you value your life at \\$1,000,000 (pay \\$1,000 to avoid 0.1% risks of death) then this problem is nearly isomorphic to Gary Drescher's *transparent Newcomb's Problem,* in which Box B is transparent, and Omega has put \\$1,000,000 into Box B iff Omega predicts that you one-box when seeing a full Box B.  This makes Parfit's Hitchhiker a *Newcomblike problem,* but one in which, one observes, the driver's behavior seems quite economically sensible, and not at all contrived as in the case of Newcomb's Omega.\n\nParfit's Hitchhiker also bears a strong resemblance to some real-life dilemmas of central banks, e.g., threatening not to bail out too-big-to-fail institutions, or promising not to raise interest rates later.  In both cases, the central bank would benefit today from people believing that \"the central bank will not bail out irresponsible institutions\" or \"the central bank will wait before raising the targeted interest rate\".  But the markets are extremely good at predicting future events, and market actors know that later, the central bank will think that it is much more convenient to bail out the big bank, and that whatever irresponsibility has occurred is now already in the past and unalterable.\n\nSimilarly, on reaching the city in Parfit's Hitchhiker, you might be tempted to reason that the car has already driven you there, and so, when you *now* make the decision in your selfishness, you will reason that you are better off by \\$1,000 *now* if you refuse to pay, since your decision can't alter the past (right?)  Similarly if in Newcomb's Problem you see that Box B is already full; the money is right there and it can't vanish if you take both boxes, right?  But if you are a sort of agent that reasons like this, Parfit's driver asks you a few hard questions and then drives off to let you die in the desert.  Or the markets, which are sufficiently efficient to behave like extremely good predictors in many ways, may go ahead and call your bluff about moral hazard.\n\nBoth causal decision agents and evidential decision agents will two-box on the transparent version of Newcomb's Problem, or be left to die in the desert on Parfit's Hitchhiker.  A causal agent who sees a full Box B reasons \"I cannot cause Box B to become empty by leaving behind Box A\", and even an evidential agent reasons, \"It wouldn't be good news to leave behind Box A; I already *know* Box B is full, so Omega must have made a mistake.\" \n\nA logical decision theorist, on the other hand, cheerfully promises to pay the \\$1,000 to Parfit's driver, and then actually does so; they also cheerfully leave behind Box A in the transparent Newcomb's Problem.  An LDT agent is choosing the best output for their algorithm, and reasoning, \"If my algorithm had output 'don't pay' / 'take both boxes', then this would have implied my dying in the desert / Box B being empty.\"  (This calculation will be made more formal later.)\n\nOf course these simple scenarios are not exactly representative of what happens in the real world with the central bank and interest rates.  In the real world there are reputational effects and iterated games; if you bail out a bank today, this has a penalty in the form of people expecting you to bail out more banks later (albeit by that time you may have retired and left somebody else to run the central bank).  But this doesn't rule out Newcomblike channels as a *component* of the problem; people may have some idea of what kind of algorithm you're running and try to mentally simulate what your algorithm does.  There's also no rule saying that efficient markets *can't* read your expression well enough for what you secretly expect you'll do later %note: 'What you expect you'll do later' is produced by your current self's mental simulation of your future self, which is why the output of your future self's algorithm is appearing at two separate points in the overall process.% to have some effect on what people think of you now. %note: From the perspective of a causal decision agent, we should just regard our facial expressions as causal actions now that affect what other people believe, and control our facial expressions, darn it!  Of course, a logical decision theorist replies that if you could in fact control your facial expressions, and that market actors could mentally simulate you well enough to predict that you would try to control your facial expressions, the market actors would not already regard those facial expressions as evidence.%\n\nIn this sense, it may matter a non-zero amount whether people think that the *rational* course of action in the simplified Parfit's Hitchhiker dilemma is to pay \\$1,000 even after already reaching town, or if it is *rational* to die in the desert.\n\n## Logical decision theory\n\nIn general, a logical agent ought to calculate expected utility as follows:\n\n$$\\mathsf Q(s) = \\big ( \\underset{\\pi_x \\in \\Pi}{argmax} \\ \\sum_{o_i \\in \\mathcal O} \\mathcal U(o_i) \\cdot \\mathbb P(\\ulcorner \\mathsf Q = \\pi_x \\urcorner \\righttriangle o_i | \\mathsf I_0) \\big ) (s)$$\n\nWhere:\n\n- $\\mathsf Q$ is the agent's current decision algorithm - that is, the whole calculation presently running.\n- $s$ is the agent's sense data.\n- $\\pi_x \\in \\Pi$ is the output of $\\mathcal Q$, a *policy* that maps sense data to actions.\n- $\\ulcorner \\mathsf Q = \\pi_x \\urcorner$ is the proposition that, as a logical fact, the output of algorithm $\\mathsf Q$ is $\\pi_x.$\n- $\\mathbb P(X \\righttriangle o_i | \\mathsf I_0)$ is the probability of $o_i$ given the prior information $\\mathsf I_0$ and further *conditioning* on the logical fact $X.$\n\nTechnically, with regards to the key logical conditioning operator $X \\righttriangle Y$, logical decision theorists know two special-case ways to try to calculate it:\n\n- [ As a premise introduced into a standard proof algorithm], working out further logical implications.\n- As a Pearl-style $\\operatorname do()$ on a standard causal model that [ includes some nodes intended to denote unknown logical propositions.]\n\nLogical decision theorists are still trying to figure out a good candidate for a *general* formalization of $X -righttriangle Y,$ but meanwhile:\n\n- \"Treating $X$ as a new premise in a proof system and deriving its implications\" works to formalize [modal_agents], and let us formally simulate agents with common knowledge of each other's code negotiating on the Prisoner's Dilemma and other game-theoretic setups.\n- \"$\\operatorname{do}()$ on causal models that include some logical propositions\" suffices to formalize all the dilemmas, thought experiments, and scenarios (or rather, formalize the scenarios at least as far as causal decision theory formalized them).\n\nWe know these two formalizations aren't complete because:\n\n- The proof formalism we use for modal agents has [ weird edge cases] indicating that it only correctly formalizes the intuitive notion of logical conditioning some of the time.\n- We don't have a general algorithm for *building* causal models that include logical facts, and it's not clear that representation can model any setup more complicated than \"run this exact algorithm in two different places\".\n\n# Some LDT behaviors in economically suggestive scenarios\n\n(For the question of whether all of these are *rational* behaviors, or merely *usefully irrational* behaviors that happen to make agents richer, see the final section \"LDT as the principle of rational choice\" below.)\n\n## Voting\n\nLDT agents vote in elections if they believe they are part of a sufficiently large cohort of people voting for similar reasons that their cohort has a non-negligible probability of swinging the election (such that the expected value of possibly swinging this election outweighs the cost of everyone in their cohort voting).\n\nVoting is one of the most plausible candidates for something like a pure LDT analysis going through.  With so many voters participating in elections, there may plausibly be a large number of agents out there whom you should consider as being logically correlated with you.\n\n## Oneshot Prisoner's Dilemma\n\n%%!knows-requisite([dt_prisonersdilemma]):\n[todo: introduce PD here]\n%%\n\nMuch of the early work in LDT revolved around computational agents playing the Prisoner's Dilemma with [common_knowledge common knowledge] of each other's source code.\n\nSuppose, for instance, that you are a computational agent $\\mathsf {A}$ playing the Prisoner's Dilemma and you are told that the code of the other agent $\\mathsf {Fairbot}$ is as follows:\n\n    def Fairbot(A):\n      if is-provable(\"A(Fairbot) == Cooperate\"):\n        return Cooperate\n      else:\n        return Defect\n\nIn other words, $\\mathsf {Fairbot}$ tries to prove that $\\mathsf {A}$, in this case $\\mathsf {A}$ (you) cooperates with it.  If $\\mathsf {Fairbot}$ proves that $\\mathsf A,$ playing with $\\mathsf {Fairbot},$ cooperates, then $\\mathsf {Fairbot}$ cooperates; otherwise $\\mathsf {Fairbot}$ defects.\n\nWould you, in this case, defect on the Prisoner's Dilemma?  We can suppose even that $\\mathsf {Fairbot}$ has already run and has already made its move one way or another, so that your move cannot possibly have any causal effect on $\\mathsf {Fairbot}.$  The prescription of CDT is thus to Defect, which an LDT agent reasoning informally would regard as unwise.\n\nFairbot does not play optimally on the PD.  $\\mathsf{Fairbot}$ does cooperate with another $\\mathsf {Fairbot}$, even if the code of the other agent is not exactly similar (e.g. they can use different proof systems). %note: (If you want to know why Agent A trying to prove things about Agent B who is simultaneously trying to prove things Agent A (trying to prove things about agent B...) doesn't just collapse into an infinite recursion, the answer is \"[modal_agents Because] of [55w].\"  Sorry, this one takes a detailed explanation.%  If $\\mathsf {Fairbot}$ is reasoning in a [logic_soundness sound] system such as [3ft first-order arithmetic], then $\\mathsf {Fairbot}$ is also inexploitable; it never Cooperates when the opponent Defects.  However, $\\mathsf {Fairbot}$ cooperates with $\\mathsf {CooperateBot},$ the agent which simply always returns 'Cooperate'.  By the [true_prisoners_dilemma premises] of the Prisoner's Dilemma, we ought to at least bother to Defect against a rock with the word \"Cooperate\" written on it.  $\\mathsf {Fairbot}$ fails to exploit $\\mathsf {CooperateBot},$ so $\\mathsf {Fairbot}$'s play is not optimal.\n\nThe milestone paper \"[Robust Cooperation in the Prisoner's Dilemma](https://arxiv.org/abs/1401.5577)\" exhibited a proof (and running code!) that there existed a simple agent $\\mathsf{PrudentBot}$ which:\n\n- Mutually cooperated with $\\mathsf{Fairbot}$\n- Mutually cooperated with another $\\mathsf{PrudentBot}$\n- Defected against $\\mathsf{DefectBot}$\n- Defected against (hence exploited) $\\mathsf{CooperateBot}$\n- Was itself provably unexploitable.\n\nThis idiom of \"modal agents\" was later generalized to more natural and generic agents.\n\nReal-life versions of the Prisoner's Dilemma do not nearly match the setup of two computational agents with common knowledge of each other's source code.  In real life, there are reputation effects, people who care about other people, and a far more tenuous grasp on the other person's reasoning processes.  The Prisoner's Dilemma with probabilistic beliefs has yet to be formally analyzed in LDT.\n\nNonetheless. it seems fair to say that if you are an LDT agent:\n\n- You should not Defect against somebody who you are pretty sure will reason very similarly to you.\n- You should not Defect against a fair-minded agent that you think is pretty good at predicting you.\n- You should not Defect against an agent that is pretty good at predicting you, that you are pretty good at predicting, who you predict has decided to Cooperate iff it predicts you Cooperate (in order to incentivize you to do likewise, given your own predictive abilities).\n- As the relative benefits of Cooperation in a PD-like scenario increase, it seems increasingly plausible that some LDT-like reason for cooperation will end up going through.  E.g. if the payoff matrix is (1,1) vs (0, 101) vs (101, 0) vs (100, 100), even a small probability that the two of you are reasoning similarly might be enough to carry the handshake.\n\nIf you further accept the argument below that LDT is a better candidate than CDT for the principle of rational choice, then it is also fair to say that economists should stop going around proclaiming that rational agents defect in the Prisoner's Dilemma.  LDT is not a polyanna solution and it seems quite possible that two LDT agents might end up Defecting against each other (e.g. because they don't know each other to be LDT agents, or don't know the other knows, etcetera); but \"rational agents in general just can't do anything about the mutual-defection equilibrium in the Prisoner's Dilemma\" is much too harsh.\n\n%%knows-requisite([dt_gametheory]):\n## Game theory\n\nThe first-ever derivation of Nash-equilibrium play *in two agents doing pure expected utility maximization* and modeling each other, *without* each agent starting from the assumption that the other agent is already looking for Nash equilibria, was done by logical decision theorists [using a common-knowledge-of-code setup and reflective oracles](http://www.auai.org/uai2016/proceedings/papers/87.pdf).\n%%\n\n## Ultimatum Game\n\n\n\n## Iterated Prisoner's Dilemma\n\n## Blackmail\n\n# LDT as the principle of rational choice\n\nLogical decision theory claims to embody *the principle of rational decision*--or at least, embody it a lot better than causal decision theory and better than any currently known alternative.  This is primarily the domain of the more analytic-philosophical viewpoint on LDT, but is worth summarizing.\n\nThe first and foremost motivation behind LDT is that LDT agents systematically end up rich.  The current literature contains rich veins of discourse about \"one-boxers\" on Newcomb's Problem asking two-boxers \"Why aincha rich, if you're so rational?\" and various retorts along the lines of \"It's not my fault Omega decided to punish people who'd act rationally before this experiment started; it doesn't change what the rational choice is.\"\n\n(That retort may sound less persuasive if we're thinking about Parfit's Hitchhiker.  The driver is not making a weird arbitrary choice to punish 'rational' agents, the driver is just acting with undisputed economic rationality on their own part.  It makes no (selfish) sense to rescue someone you don't predict will pay up afterwards.)\n\nContrarily, the LDT agent thinks that Omega (and the driver in Parfit's Hitchhiker) are not being particularly 'unfair' to one particular kind of algorithm.  If Omega read the agent's source code and decided to reward only agents with an *algorithm* that output 'one box' *by picking the first choice in alphabetical order,* punishing all agents that behaved in exactly the same way due a different internal computation, then this would indeed be a rigged contest.  But in Newcomb's Problem, Omega only cares about the behavior, and not the kind of algorithm that produced it; and an agent can indeed take on whatever kind of behavior it likes; so, according to LDT, there's no point in saying that Omega is being unfair.  You can make the logical output of your algorithm be whatever you want (though one cannot perhaps want whatever one wants) so there's no point in picking an output that leaves you to die in the desert.\n\nSimilarly, LDT agents never need to resort to *precommitments* (since LDT agents never wish their future selves would act differently from the LDT algorithm) and LDT agents always calculate *a positive value of information* (where an evidential decision agent might beg you to *not* render Box B transparent, since then it will be empty).  From the standpoint of an LDT agent, CDT agents are undergoing [ preference reversals] and being subject to [ money pumps] in a way that the economic literature usually treats as prima facie indicators of economic irrationality.\n\nE.g., a CDT agent will, given the chance, pay a \\$10 fee to have a precommitment assistant stand around with a gun threatening to shoot them if they take both boxes in Newcomb's Problem.  Naturally, given the chance by surprise, the same agent would *later* (after Omega's departure) pay \\$10 to make the gun-toter go away. %note: Even believing that Omega has accurately predicted this, and that Box B is therefore empty!%  From the standpoint of an LDT agent, this is no more excusable than an agent exhibiting non-Bayesian behavior on the [allais_paradox Allais Paradox], paying \\$1 to throw a switch and then paying another \\$1 to throw it back.  A rational agent would not exhibit such incoherence and drive itself around in circles, any more than a rational agent would deign to have [ circular preferences].\n\nOf course, in real life, other people may not be certain of what algorithm we are running; so there will remain an in-practice use for publicly visible precommitments.  But it is worth asking questions like \"To the extent people *can* figure out what algorithm we're using, what should that algorithm be?\"  Or \"If we could publicly pay someone to force us to use a particular algorithm, what should that algorithm be?\"  (If [Distributed Autonomous Organizations](https://en.wikipedia.org/wiki/Decentralized_autonomous_organization) ever happen, there will be economic actors which can have publicly visible source code.)  Similarly, in real life there are reputational effects that serve as local incentives to behave the way we want people to expect us to behave later.  But it remains worth asking, \"What general algorithm do I want to have a reputation for using?\"\n\nThe modern economics literature takes for granted 'precommitments' and 'usefully irrational' behavior at the bargaining table that it may sound odd to claim that [ dynamic consistency] and [ reflective consistency] are desirable properties for the principle of rational choice to have - why, who would think in the first place that rational agents would want their future selves to behave rationally?  There are just two entirely different subjects of study, the study of 'What is rational', and the study of 'What rationalists wish they chose' or rationality as modified by precommitments, useful irrationality, etcetera.\n\nBut it would nonetheless be a remarkable fact if the second field of study happened to point strongly in the direction of a simple, general principle with various philosophically appealing properties which happened to be much more strongly consistent in various ways, whose corresponding agents stand around saying things like \"Why on Earth would a rational agent wistfully wish that their future selves would be irrational?\" or \"What do you mean *useful irrationality?*  If a choice pattern is useful, it's not irrational!  That's like [Spock the Straw Vulcan complaining about losing a chess game](http://intelligenceexplosion.com/en/2011/why-spock-is-not-rational/#fn5x6-bk) to an opponent who played 'illogically'!\"\n\nIf you are a pure causal decision agent, you will wave off all that irrationality with a sigh.  But if we let ourselves blank our minds of our previous thoughts and try to return to an intuitive, pretheoretic standpoint, we might suspect from looking over this situation that we have made a mistake about what to adopt as our explicit theory of rationality.\n\nIn analytic philosophy, the case for causal decision theory rests primarily on the intuition that one-boxing on Newcomb's problem cannot *cause* Box B to be full.  Or on the transparent Newcomb's Problem, with Box B transparently full (or empty), it cannot be reasonable to imagine that by leaving behind Box A and its \\$1,000 you can cause things to be different?  Is there not only one reasonable way to construe the counterfactual conditioning in the expected utility formula?  Is it not in some sense *true,* after Parfit's driver has conveyed the LDT agent to the city in the desert, that in the counterfactual world where the LDT agent does not at that time choose to pay, they remain in the city?  In this sense, must not an LDT agent be deluded about some question of fact, or act as if it is so deluded?\n\nThe LDT agent responds:\n\n- There aren't any actual worlds \"where I didn't pay\" floating out there.  The only *real* world is the one where my decision algorithm had the output it actually had.  To imagine other worlds is an act of imagination, computing a description of a certain world that doesn't exist; there's no corresponding actual world, so my description of the impossible can't be true or false under a correspondence theory of truth.  That being the case, \"Counterfactuals were made for humanity, not humanity for counterfactuals\"; I can decide to condition on the actions my algorithm doesn't take in whatever way produces the greatest wealth.  I am free to say \"In the nonexistent world where I don't pay now, I already died in the desert\".\n- I don't one-box in Newcomb's Problem *because I think it physically causes Box B to be full.*  I one-box in Newcomb's Problem because I have computed this as the optimal output in an entirely different way.  It begs the question to assume that a rational agent must make its decision by carrying out this particular ritual of cognition about which things physically cause which other things, therefore stating that I am \"acting as if\" I irrationally believe that my choice physically causes Box B to be full.\n\nTo this the LDT agent also adds that it *is* desirable for the action-conditionals we compute to draw accurate pictures of the world where we take the action we actually end up taking.  That is, if it's a fact that your decision algorithm outputs action $a_x,$ then your previous imagination of \"The world where I do $a_x$\" should match reality.  [ This is a condition that CDT violates!]  (In a way that can be used to pump money out of CDT agents.)\n\nFor this reason, logical decision theorists think that when we talk about what LDT agents would do, this is a much stronger claim to be speaking about *what rational agents would do* than if we talk about what CDT agents would do, with respect to our pretheoretic intuitions about what sort of thing 'rationality' ought to be.\n\n[todo:  Greenlink to a page where all this gets more extensively discussed.]\n\nRegardless of what one eventually decides about principles of normative choice, it ought to be uncontroversial that a reputation for acting like an LDT agent is a valuable reputation for an agent to have.  Then any analysis of what it is economically rational to do *in public* ought to concern itself with LDT agents rather than CDT agents.  If other people can see you playing the Ultimatum Game, or you are iterating it an unknown number of times, then it ought to be entirely uncontroversial that the rational agent acts according to LDT rather than CDT.  In the case of the Ultimatum Game, the LDT prescription also happens to be a better descriptive theory of how actual humans behave.  So if for some reason we *must* descriptively model humans as causal decision agents, we ought to model them as causal decision agents who've realized that they ought to acquire a reputation for acting like logical decision agents, thereby saving the phenomena.\n\nBut it is our own opinion that putting the outputs of the CDT algorithm into economics textbooks as the 'rational' choice was just an honest mistake, and that academic economics ought to chew over this situation for a year or two and then go back and announce that voting can be rational after all.\n\n# Further exploration\n\n[todo: put stuff here]\n\n",
      "metaText": "",
      "isTextLoaded": true,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 1,
      "maintainerCount": 1,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": 0,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 14,
      "redLinkCount": 0,
      "lockedBy": "2",
      "lockedUntil": "2018-10-30 20:52:33",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": {
        "edit": {
          "has": false,
          "reason": "You don't have domain permission to edit this page"
        },
        "proposeEdit": {
          "has": true,
          "reason": ""
        },
        "delete": {
          "has": false,
          "reason": "You don't have domain permission to delete this page"
        },
        "comment": {
          "has": false,
          "reason": "You can't comment in this domain because you are not a member"
        },
        "proposeComment": {
          "has": true,
          "reason": ""
        }
      },
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [
        "58b"
      ],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [
        "3rk"
      ],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [
        {
          "id": "5937",
          "parentId": "1lx",
          "childId": "58f",
          "type": "requirement",
          "creatorId": "2",
          "createdAt": "2016-08-03 22:43:12",
          "level": 2,
          "isStrong": true,
          "everPublished": true
        }
      ],
      "subjects": [
        {
          "id": "5189",
          "parentId": "58b",
          "childId": "58f",
          "type": "subject",
          "creatorId": "2",
          "createdAt": "2016-07-11 20:25:54",
          "level": 2,
          "isStrong": true,
          "everPublished": true
        },
        {
          "id": "5935",
          "parentId": "5n9",
          "childId": "58f",
          "type": "subject",
          "creatorId": "2",
          "createdAt": "2016-08-03 22:42:33",
          "level": 2,
          "isStrong": true,
          "everPublished": true
        },
        {
          "id": "5936",
          "parentId": "5px",
          "childId": "58f",
          "type": "subject",
          "creatorId": "2",
          "createdAt": "2016-08-03 22:42:57",
          "level": 1,
          "isStrong": true,
          "everPublished": true
        }
      ],
      "lenses": [],
      "lensParentId": "58b",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "23107",
          "pageId": "58f",
          "userId": "2",
          "edit": 14,
          "type": "newEdit",
          "createdAt": "2018-10-30 20:52:33",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "22240",
          "pageId": "58f",
          "userId": "2",
          "edit": 13,
          "type": "newEdit",
          "createdAt": "2017-03-03 18:51:52",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "18308",
          "pageId": "58f",
          "userId": "2",
          "edit": 0,
          "type": "newRequirement",
          "createdAt": "2016-08-03 22:43:13",
          "auxPageId": "1lx",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "18307",
          "pageId": "58f",
          "userId": "2",
          "edit": 0,
          "type": "newSubject",
          "createdAt": "2016-08-03 22:42:58",
          "auxPageId": "5px",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "18305",
          "pageId": "58f",
          "userId": "2",
          "edit": 0,
          "type": "newSubject",
          "createdAt": "2016-08-03 22:42:33",
          "auxPageId": "5n9",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "18303",
          "pageId": "58f",
          "userId": "2",
          "edit": 0,
          "type": "newTag",
          "createdAt": "2016-08-03 22:41:39",
          "auxPageId": "3rk",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "17875",
          "pageId": "58f",
          "userId": "2",
          "edit": 12,
          "type": "newEdit",
          "createdAt": "2016-08-01 00:50:50",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "17469",
          "pageId": "58f",
          "userId": "2",
          "edit": 11,
          "type": "newEdit",
          "createdAt": "2016-07-25 00:45:00",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "16956",
          "pageId": "58f",
          "userId": "2",
          "edit": 10,
          "type": "newEdit",
          "createdAt": "2016-07-16 21:17:16",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "16767",
          "pageId": "58f",
          "userId": "2",
          "edit": 9,
          "type": "newEdit",
          "createdAt": "2016-07-15 02:14:48",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "16710",
          "pageId": "58f",
          "userId": "2",
          "edit": 8,
          "type": "newEdit",
          "createdAt": "2016-07-14 20:13:31",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "16582",
          "pageId": "58f",
          "userId": "2",
          "edit": 7,
          "type": "newEdit",
          "createdAt": "2016-07-12 05:04:56",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "16581",
          "pageId": "58f",
          "userId": "2",
          "edit": 6,
          "type": "newEdit",
          "createdAt": "2016-07-12 05:03:27",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "16580",
          "pageId": "58f",
          "userId": "2",
          "edit": 5,
          "type": "newEdit",
          "createdAt": "2016-07-12 00:10:04",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "16579",
          "pageId": "58f",
          "userId": "2",
          "edit": 4,
          "type": "newEdit",
          "createdAt": "2016-07-11 20:44:34",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "16578",
          "pageId": "58f",
          "userId": "2",
          "edit": 0,
          "type": "newSubject",
          "createdAt": "2016-07-11 20:25:55",
          "auxPageId": "58b",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "16576",
          "pageId": "58f",
          "userId": "2",
          "edit": 0,
          "type": "newTag",
          "createdAt": "2016-07-11 20:25:35",
          "auxPageId": "4v4",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "16575",
          "pageId": "58f",
          "userId": "2",
          "edit": 0,
          "type": "deleteTag",
          "createdAt": "2016-07-11 20:25:30",
          "auxPageId": "4bn",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "16573",
          "pageId": "58f",
          "userId": "2",
          "edit": 0,
          "type": "newTag",
          "createdAt": "2016-07-11 20:25:27",
          "auxPageId": "4bn",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "16572",
          "pageId": "58f",
          "userId": "2",
          "edit": 0,
          "type": "deleteTag",
          "createdAt": "2016-07-11 20:25:16",
          "auxPageId": "4v",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "16570",
          "pageId": "58f",
          "userId": "2",
          "edit": 3,
          "type": "newEdit",
          "createdAt": "2016-07-11 20:25:02",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "16565",
          "pageId": "58f",
          "userId": "2",
          "edit": 2,
          "type": "newEdit",
          "createdAt": "2016-07-11 18:42:35",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "16283",
          "pageId": "58f",
          "userId": "2",
          "edit": 0,
          "type": "newParent",
          "createdAt": "2016-07-09 00:33:14",
          "auxPageId": "58b",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "16284",
          "pageId": "58f",
          "userId": "2",
          "edit": 0,
          "type": "newTag",
          "createdAt": "2016-07-09 00:33:14",
          "auxPageId": "4v",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "16281",
          "pageId": "58f",
          "userId": "2",
          "edit": 1,
          "type": "newEdit",
          "createdAt": "2016-07-09 00:33:13",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        }
      ],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": true,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    }
  },
  "users": {
    "1": {
      "id": "1",
      "firstName": "Alexei",
      "lastName": "Andreev",
      "lastWebsiteVisit": "2018-02-18 09:35:21",
      "isSubscribed": false,
      "domainMembershipMap": {}
    },
    "2": {
      "id": "2",
      "firstName": "Eliezer",
      "lastName": "Yudkowsky",
      "lastWebsiteVisit": "2019-12-21 03:34:41",
      "isSubscribed": false,
      "domainMembershipMap": {}
    },
    "5": {
      "id": "5",
      "firstName": "Eric",
      "lastName": "Rogstad",
      "lastWebsiteVisit": "2019-08-23 01:44:10",
      "isSubscribed": false,
      "domainMembershipMap": {}
    },
    "12y": {
      "id": "12y",
      "firstName": "Patrick",
      "lastName": "LaVictoire",
      "lastWebsiteVisit": "2019-04-18 00:41:25",
      "isSubscribed": false,
      "domainMembershipMap": {}
    },
    "1yq": {
      "id": "1yq",
      "firstName": "Eric",
      "lastName": "Bruylant",
      "lastWebsiteVisit": "2017-04-14 18:00:22",
      "isSubscribed": false,
      "domainMembershipMap": {}
    },
    "2vh": {
      "id": "2vh",
      "firstName": "Jaime",
      "lastName": "Sevilla Molina",
      "lastWebsiteVisit": "2018-12-06 12:14:41",
      "isSubscribed": false,
      "domainMembershipMap": {}
    },
    "5yw": {
      "id": "5yw",
      "firstName": "Duncan",
      "lastName": "Wilson",
      "lastWebsiteVisit": "2017-06-25 01:22:13",
      "isSubscribed": false,
      "domainMembershipMap": {}
    },
    "8pb": {
      "id": "8pb",
      "firstName": "Jacob",
      "lastName": "van Eeden",
      "lastWebsiteVisit": "2017-10-02 19:17:29",
      "isSubscribed": false,
      "domainMembershipMap": {}
    }
  },
  "domains": {
    "1": {
      "id": "1",
      "pageId": "1lw",
      "createdAt": "2016-01-15 03:02:51",
      "alias": "math",
      "canUsersComment": false,
      "canUsersProposeComment": true,
      "canUsersProposeEdits": true,
      "friendDomainIds": []
    },
    "3": {
      "id": "3",
      "pageId": "3d",
      "createdAt": "2015-03-30 22:19:47",
      "alias": "Arbital",
      "canUsersComment": false,
      "canUsersProposeComment": true,
      "canUsersProposeEdits": true,
      "friendDomainIds": []
    },
    "8": {
      "id": "8",
      "pageId": "198",
      "createdAt": "2015-12-13 23:14:48",
      "alias": "TeamArbital",
      "canUsersComment": false,
      "canUsersProposeComment": true,
      "canUsersProposeEdits": true,
      "friendDomainIds": []
    },
    "15": {
      "id": "15",
      "pageId": "58c",
      "createdAt": "2016-07-08 18:23:14",
      "alias": "DecisionTheory",
      "canUsersComment": false,
      "canUsersProposeComment": true,
      "canUsersProposeEdits": true,
      "friendDomainIds": []
    },
    "21": {
      "id": "21",
      "pageId": "1",
      "createdAt": "2015-02-10 17:12:19",
      "alias": "AlexeiAndreev",
      "canUsersComment": false,
      "canUsersProposeComment": true,
      "canUsersProposeEdits": true,
      "friendDomainIds": [
        "2069"
      ]
    },
    "32": {
      "id": "32",
      "pageId": "12y",
      "createdAt": "2015-09-02 21:53:02",
      "alias": "PatrickLaVictoir",
      "canUsersComment": false,
      "canUsersProposeComment": true,
      "canUsersProposeEdits": true,
      "friendDomainIds": []
    },
    "116": {
      "id": "116",
      "pageId": "1yq",
      "createdAt": "2016-02-12 16:14:31",
      "alias": "EricBruylant",
      "canUsersComment": false,
      "canUsersProposeComment": true,
      "canUsersProposeEdits": true,
      "friendDomainIds": []
    },
    "123": {
      "id": "123",
      "pageId": "2",
      "createdAt": "2015-03-05 18:45:34",
      "alias": "EliezerYudkowsky",
      "canUsersComment": false,
      "canUsersProposeComment": true,
      "canUsersProposeEdits": true,
      "friendDomainIds": []
    }
  },
  "masteries": {
    "1lx": {
      "pageId": "1lx",
      "has": false,
      "wants": false,
      "level": 0,
      "updatedAt": ""
    },
    "58b": {
      "pageId": "58b",
      "has": false,
      "wants": false,
      "level": 0,
      "updatedAt": ""
    },
    "5n9": {
      "pageId": "5n9",
      "has": false,
      "wants": false,
      "level": 0,
      "updatedAt": ""
    },
    "5px": {
      "pageId": "5px",
      "has": false,
      "wants": false,
      "level": 0,
      "updatedAt": ""
    }
  },
  "marks": {},
  "pageObjects": {},
  "result": {},
  "globalData": null
}
