{
  "resetEverything": false,
  "user": {
    "id": "",
    "firstName": "",
    "lastName": "",
    "lastWebsiteVisit": "",
    "isSubscribed": false,
    "domainMembershipMap": {},
    "fbUserId": "",
    "email": "",
    "isAdmin": false,
    "emailFrequency": "",
    "emailThreshold": 0,
    "ignoreMathjax": false,
    "showAdvancedEditorMode": false,
    "isSlackMember": false,
    "analyticsId": "aid:flFZ9Hqv7Kb4t7R2qpCfXQ5hJsGR2yvSO24+yc3lOh8",
    "hasReceivedMaintenanceUpdates": false,
    "hasReceivedNotifications": false,
    "newNotificationCount": 0,
    "newAchievementCount": 0,
    "maintenanceUpdateCount": 0,
    "invitesClaimed": [],
    "mailchimpInterests": {},
    "continueBayesPath": null,
    "continueLogPath": null
  },
  "pages": {
    "1": {
      "likeableId": "1",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "1",
      "edit": 5,
      "editSummary": "",
      "prevEdit": 4,
      "currentEdit": 5,
      "wasPublished": true,
      "type": "group",
      "title": "Alexei Andreev",
      "clickbait": "There is no spoon",
      "textLength": 304,
      "alias": "AlexeiAndreev",
      "externalUrl": "",
      "sortChildrenBy": "alphabetical",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "1",
      "editCreatedAt": "2016-12-13 02:34:00",
      "pageCreatorId": "1",
      "pageCreatedAt": "2015-09-04 16:14:58",
      "seeDomainId": "0",
      "editDomainId": "21",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 0,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 741,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": 0,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "178": {
      "likeableId": "202",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "178",
      "edit": 6,
      "editSummary": "",
      "prevEdit": 5,
      "currentEdit": 6,
      "wasPublished": true,
      "type": "wiki",
      "title": "Arbital \"tag\" relationship",
      "clickbait": "Tags are a way to connect pages that share a common topic.",
      "textLength": 2689,
      "alias": "Arbital_tag",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "1",
      "editCreatedAt": "2016-05-11 15:44:58",
      "pageCreatorId": "1",
      "pageCreatedAt": "2015-11-15 15:31:40",
      "seeDomainId": "0",
      "editDomainId": "3",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 0,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 96,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": 0,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "185": {
      "likeableId": "0",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "185",
      "edit": 0,
      "editSummary": "",
      "prevEdit": 0,
      "currentEdit": 0,
      "wasPublished": false,
      "type": "",
      "title": "",
      "clickbait": "",
      "textLength": 0,
      "alias": "",
      "externalUrl": "",
      "sortChildrenBy": "",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "",
      "editCreatedAt": "",
      "pageCreatorId": "",
      "pageCreatedAt": "",
      "seeDomainId": "",
      "editDomainId": "",
      "submitToDomainId": "",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": false,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 0,
      "isEditorComment": false,
      "isApprovedComment": false,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 0,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": 0,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "187": {
      "likeableId": "0",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "187",
      "edit": 0,
      "editSummary": "",
      "prevEdit": 0,
      "currentEdit": 0,
      "wasPublished": false,
      "type": "",
      "title": "",
      "clickbait": "",
      "textLength": 0,
      "alias": "",
      "externalUrl": "",
      "sortChildrenBy": "",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "",
      "editCreatedAt": "",
      "pageCreatorId": "",
      "pageCreatedAt": "",
      "seeDomainId": "",
      "editDomainId": "",
      "submitToDomainId": "",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": false,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 0,
      "isEditorComment": false,
      "isApprovedComment": false,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 0,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": 0,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "198": {
      "likeableId": "266",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "198",
      "edit": 4,
      "editSummary": "",
      "prevEdit": 3,
      "currentEdit": 4,
      "wasPublished": true,
      "type": "wiki",
      "title": "Team Arbital",
      "clickbait": "The people behind Arbital",
      "textLength": 184,
      "alias": "TeamArbital",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "1",
      "editCreatedAt": "2016-06-17 16:55:46",
      "pageCreatorId": "1",
      "pageCreatedAt": "2015-12-13 23:14:48",
      "seeDomainId": "0",
      "editDomainId": "8",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 0,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 1185,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": 0,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "370": {
      "likeableId": "2144",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "370",
      "edit": 4,
      "editSummary": "reflecting the fact that we only have one type of mark now.",
      "prevEdit": 3,
      "currentEdit": 4,
      "wasPublished": true,
      "type": "wiki",
      "title": "Arbital mark",
      "clickbait": "What is a mark on Arbital? When is it created? Why is it important?",
      "textLength": 1724,
      "alias": "arbital_mark",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "1yq",
      "editCreatedAt": "2016-07-22 00:08:32",
      "pageCreatorId": "1",
      "pageCreatedAt": "2016-04-14 23:12:16",
      "seeDomainId": "0",
      "editDomainId": "3",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 0,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 58,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": 0,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "569": {
      "likeableId": "2989",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 3,
      "dislikeCount": 0,
      "likeScore": 3,
      "individualLikes": [],
      "pageId": "569",
      "edit": 1,
      "editSummary": "",
      "prevEdit": 0,
      "currentEdit": 1,
      "wasPublished": true,
      "type": "wiki",
      "title": "Probability distribution: Motivated definition",
      "clickbait": "People keep writing things like P(sick)=0.3. What does this mean, on a technical level?",
      "textLength": 8762,
      "alias": "probability_distribution_motivated_definition",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "32",
      "editCreatedAt": "2016-07-07 00:49:26",
      "pageCreatorId": "32",
      "pageCreatedAt": "2016-07-07 00:49:26",
      "seeDomainId": "0",
      "editDomainId": "1",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 4,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 198,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": 0,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "595": {
      "likeableId": "0",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "595",
      "edit": 3,
      "editSummary": "",
      "prevEdit": 2,
      "currentEdit": 3,
      "wasPublished": true,
      "type": "wiki",
      "title": "Arbital page alias",
      "clickbait": "",
      "textLength": 1215,
      "alias": "arbital_alias",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "1yq",
      "editCreatedAt": "2016-07-21 23:06:57",
      "pageCreatorId": "5",
      "pageCreatedAt": "2016-07-10 00:52:28",
      "seeDomainId": "0",
      "editDomainId": "3",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 0,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 46,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": 0,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "596": {
      "likeableId": "0",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "596",
      "edit": 1,
      "editSummary": "",
      "prevEdit": 0,
      "currentEdit": 1,
      "wasPublished": true,
      "type": "wiki",
      "title": "Arbital page title",
      "clickbait": "",
      "textLength": 738,
      "alias": "Arbital_title",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "5",
      "editCreatedAt": "2016-07-10 01:18:37",
      "pageCreatorId": "5",
      "pageCreatedAt": "2016-07-10 01:18:37",
      "seeDomainId": "0",
      "editDomainId": "3",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 0,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 31,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": 0,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "597": {
      "likeableId": "3067",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "597",
      "edit": 2,
      "editSummary": "added clickbait",
      "prevEdit": 1,
      "currentEdit": 2,
      "wasPublished": true,
      "type": "wiki",
      "title": "Arbital page clickbait",
      "clickbait": "The text you are reading right now is clickbait.",
      "textLength": 1128,
      "alias": "Arbital_clickbait",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "1yq",
      "editCreatedAt": "2016-08-05 17:48:01",
      "pageCreatorId": "5",
      "pageCreatedAt": "2016-07-10 01:24:23",
      "seeDomainId": "0",
      "editDomainId": "3",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 0,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 43,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": 0,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "17b": {
      "likeableId": "204",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "17b",
      "edit": 16,
      "editSummary": "",
      "prevEdit": 15,
      "currentEdit": 16,
      "wasPublished": true,
      "type": "wiki",
      "title": "Arbital lens",
      "clickbait": "A lens is a page that presents another page's content from a different angle.",
      "textLength": 7216,
      "alias": "Arbital_lens",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "1yq",
      "editCreatedAt": "2016-12-05 13:10:54",
      "pageCreatorId": "1",
      "pageCreatedAt": "2015-11-15 18:01:48",
      "seeDomainId": "0",
      "editDomainId": "3",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 0,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 687,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": 0,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "1bv": {
      "likeableId": "312",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "1bv",
      "edit": 1,
      "editSummary": "",
      "prevEdit": 0,
      "currentEdit": 1,
      "wasPublished": true,
      "type": "wiki",
      "title": "Probability theory",
      "clickbait": "The logic of science; coherence relations on quantitative degrees of belief.",
      "textLength": 79,
      "alias": "probability_theory",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "2",
      "editCreatedAt": "2015-12-18 22:16:55",
      "pageCreatorId": "2",
      "pageCreatedAt": "2015-12-18 22:16:55",
      "seeDomainId": "0",
      "editDomainId": "1",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 0,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 716,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0
      ],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": -2,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "1ln": {
      "likeableId": "553",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "1ln",
      "edit": 6,
      "editSummary": "alias. note to self: come back and explain new requisites system.",
      "prevEdit": 5,
      "currentEdit": 6,
      "wasPublished": true,
      "type": "wiki",
      "title": "Arbital requisites",
      "clickbait": "To understand a thing you often need to understand some other things.",
      "textLength": 1210,
      "alias": "arbital_requisite",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "1yq",
      "editCreatedAt": "2016-07-19 23:24:15",
      "pageCreatorId": "1",
      "pageCreatedAt": "2016-01-11 17:09:53",
      "seeDomainId": "0",
      "editDomainId": "3",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 0,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 316,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": 0,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "1lw": {
      "likeableId": "559",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "1lw",
      "edit": 5,
      "editSummary": "added links",
      "prevEdit": 4,
      "currentEdit": 5,
      "wasPublished": true,
      "type": "wiki",
      "title": "Mathematics",
      "clickbait": "Mathematics is the study of numbers and other ideal objects that can be described by axioms.",
      "textLength": 745,
      "alias": "math",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "1yq",
      "editCreatedAt": "2016-06-22 17:49:03",
      "pageCreatorId": "2",
      "pageCreatedAt": "2016-01-15 03:02:51",
      "seeDomainId": "0",
      "editDomainId": "1",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 0,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 2288,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": 0,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "1ly": {
      "likeableId": "561",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "1ly",
      "edit": 26,
      "editSummary": "",
      "prevEdit": 25,
      "currentEdit": 26,
      "wasPublished": true,
      "type": "wiki",
      "title": "Bayesian update",
      "clickbait": "Bayesian updating: the ideal way to change probabilistic beliefs based on evidence.",
      "textLength": 1520,
      "alias": "bayes_update",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "2",
      "editCreatedAt": "2017-02-08 18:36:41",
      "pageCreatorId": "2",
      "pageCreatedAt": "2016-01-15 03:45:14",
      "seeDomainId": "0",
      "editDomainId": "1",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 0,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 994,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0
      ],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": -2,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "1r8": {
      "likeableId": "694",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 3,
      "dislikeCount": 0,
      "likeScore": 3,
      "individualLikes": [],
      "pageId": "1r8",
      "edit": 21,
      "editSummary": "",
      "prevEdit": 20,
      "currentEdit": 21,
      "wasPublished": true,
      "type": "wiki",
      "title": "Bayesian reasoning",
      "clickbait": "A probability-theory-based view of the world; a coherent way of changing probabilistic beliefs based on evidence.",
      "textLength": 752,
      "alias": "bayes_reasoning",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "1",
      "editCreatedAt": "2016-07-26 16:28:17",
      "pageCreatorId": "2",
      "pageCreatedAt": "2016-01-26 03:14:44",
      "seeDomainId": "0",
      "editDomainId": "1",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 0,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 1123,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": 0,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "1rf": {
      "likeableId": "699",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 4,
      "dislikeCount": 0,
      "likeScore": 4,
      "individualLikes": [],
      "pageId": "1rf",
      "edit": 11,
      "editSummary": "",
      "prevEdit": 10,
      "currentEdit": 11,
      "wasPublished": true,
      "type": "wiki",
      "title": "Probability",
      "clickbait": "The degree to which someone believes something, measured on a scale from 0 to 1, allowing us to do math to it.",
      "textLength": 4788,
      "alias": "probability",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "1yq",
      "editCreatedAt": "2016-08-26 11:14:18",
      "pageCreatorId": "2",
      "pageCreatedAt": "2016-01-26 22:05:40",
      "seeDomainId": "0",
      "editDomainId": "1",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 1,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 772,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": 0,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "1rt": {
      "likeableId": "711",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "1rt",
      "edit": 3,
      "editSummary": "",
      "prevEdit": 2,
      "currentEdit": 3,
      "wasPublished": true,
      "type": "wiki",
      "title": "Arbital path",
      "clickbait": "Arbital path is a linear sequence of pages tailored specifically to teach a given concept to a user.",
      "textLength": 2327,
      "alias": "Arbital_path",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "1",
      "editCreatedAt": "2016-05-11 20:53:18",
      "pageCreatorId": "1",
      "pageCreatedAt": "2016-01-27 16:33:23",
      "seeDomainId": "0",
      "editDomainId": "3",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 0,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 214,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": 0,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "1yq": {
      "likeableId": "897",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "1yq",
      "edit": 2,
      "editSummary": "",
      "prevEdit": 1,
      "currentEdit": 2,
      "wasPublished": true,
      "type": "group",
      "title": "Eric Bruylant",
      "clickbait": "Automatically generated group for Eric Bruylant",
      "textLength": 216,
      "alias": "EricBruylant",
      "externalUrl": "",
      "sortChildrenBy": "alphabetical",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "1yq",
      "editCreatedAt": "2016-02-17 17:38:40",
      "pageCreatorId": "1yq",
      "pageCreatedAt": "2016-02-12 16:14:31",
      "seeDomainId": "0",
      "editDomainId": "116",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 0,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 256,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": 0,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "1zq": {
      "likeableId": "929",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "1zq",
      "edit": 119,
      "editSummary": "",
      "prevEdit": 118,
      "currentEdit": 119,
      "wasPublished": true,
      "type": "wiki",
      "title": "Bayes' rule: Guide",
      "clickbait": "The Arbital guide to Bayes' rule",
      "textLength": 3820,
      "alias": "bayes_rule_guide",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "1",
      "editCreatedAt": "2016-10-25 23:43:14",
      "pageCreatorId": "2",
      "pageCreatedAt": "2016-02-14 00:00:33",
      "seeDomainId": "0",
      "editDomainId": "1",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": true,
      "todoCount": 0,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 126197,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0
      ],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": -2,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "35z": {
      "likeableId": "0",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "35z",
      "edit": 0,
      "editSummary": "",
      "prevEdit": 0,
      "currentEdit": 0,
      "wasPublished": false,
      "type": "",
      "title": "",
      "clickbait": "",
      "textLength": 0,
      "alias": "",
      "externalUrl": "",
      "sortChildrenBy": "",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "",
      "editCreatedAt": "",
      "pageCreatorId": "",
      "pageCreatedAt": "",
      "seeDomainId": "",
      "editDomainId": "",
      "submitToDomainId": "",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": false,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 0,
      "isEditorComment": false,
      "isApprovedComment": false,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 0,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": 0,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "3d": {
      "likeableId": "2273",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "3d",
      "edit": 33,
      "editSummary": "",
      "prevEdit": 32,
      "currentEdit": 33,
      "wasPublished": true,
      "type": "wiki",
      "title": "Arbital",
      "clickbait": "Arbital is the place for crowdsourced, intuitive math explanations.",
      "textLength": 5201,
      "alias": "Arbital",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "1",
      "editCreatedAt": "2016-08-08 16:07:52",
      "pageCreatorId": "1",
      "pageCreatedAt": "2015-03-30 22:19:47",
      "seeDomainId": "0",
      "editDomainId": "8",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 0,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 2635,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": 0,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "3hs": {
      "likeableId": "2499",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "3hs",
      "edit": 19,
      "editSummary": "added link to exemplar pages",
      "prevEdit": 18,
      "currentEdit": 19,
      "wasPublished": true,
      "type": "wiki",
      "title": "Author's guide to Arbital",
      "clickbait": "How to write intuitive, flexible content on Arbital.",
      "textLength": 4420,
      "alias": "author_guide_to_arbital",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "1yq",
      "editCreatedAt": "2016-08-08 14:32:40",
      "pageCreatorId": "1",
      "pageCreatedAt": "2016-05-10 17:55:35",
      "seeDomainId": "0",
      "editDomainId": "3",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 0,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 433,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": 0,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "3n": {
      "likeableId": "2281",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "3n",
      "edit": 7,
      "editSummary": "",
      "prevEdit": 6,
      "currentEdit": 7,
      "wasPublished": true,
      "type": "wiki",
      "title": "Arbital \"parent\" relationship",
      "clickbait": "Parent-child relationship between pages implies a strong, inseparable connection.",
      "textLength": 2510,
      "alias": "Arbital_parent_child",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "8pb",
      "editCreatedAt": "2017-09-20 13:30:49",
      "pageCreatorId": "1",
      "pageCreatedAt": "2015-04-01 19:51:44",
      "seeDomainId": "0",
      "editDomainId": "3",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 0,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 199,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": 0,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "4v": {
      "likeableId": "2318",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 2,
      "dislikeCount": 0,
      "likeScore": 2,
      "individualLikes": [],
      "pageId": "4v",
      "edit": 6,
      "editSummary": "",
      "prevEdit": 5,
      "currentEdit": 6,
      "wasPublished": true,
      "type": "wiki",
      "title": "Work in progress",
      "clickbait": "This page is being actively worked on by an editor. Check with them before making major changes.",
      "textLength": 131,
      "alias": "work_in_progress_meta_tag",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "1yq",
      "editCreatedAt": "2016-07-05 22:48:12",
      "pageCreatorId": "2",
      "pageCreatedAt": "2015-04-17 01:27:41",
      "seeDomainId": "0",
      "editDomainId": "8",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 1,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 98,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": 0,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "4v4": {
      "likeableId": "2858",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 1,
      "dislikeCount": 0,
      "likeScore": 1,
      "individualLikes": [],
      "pageId": "4v4",
      "edit": 1,
      "editSummary": "",
      "prevEdit": 0,
      "currentEdit": 1,
      "wasPublished": true,
      "type": "wiki",
      "title": "Still needs work",
      "clickbait": "The next step up from \"Work in Progress\".  The page can be read as complete, but is a draft that needs further review and fine-tuning.",
      "textLength": 155,
      "alias": "still_needs_work",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "2",
      "editCreatedAt": "2016-06-27 01:42:54",
      "pageCreatorId": "2",
      "pageCreatedAt": "2016-06-27 01:42:54",
      "seeDomainId": "0",
      "editDomainId": "3",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": false,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 0,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": true,
      "viewCount": 19,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": 0,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "4vr": {
      "likeableId": "2877",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 3,
      "dislikeCount": 0,
      "likeScore": 3,
      "individualLikes": [],
      "pageId": "4vr",
      "edit": 8,
      "editSummary": "",
      "prevEdit": 7,
      "currentEdit": 15,
      "wasPublished": true,
      "type": "wiki",
      "title": "Subjective (vs. frequentist) probability",
      "clickbait": "Probability is in the mind, not in the environment.  If you don't know whether a coin came up heads or tails, that's a fact about you, not a fact about the coin.",
      "textLength": 46860,
      "alias": "subjective_probability",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "32",
      "editCreatedAt": "2016-06-29 19:29:28",
      "pageCreatorId": "2",
      "pageCreatedAt": "2016-06-28 02:49:01",
      "seeDomainId": "0",
      "editDomainId": "1",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": false,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 0,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 383,
      "text": "What does it *mean* to say that a flipped coin has a 50% probability of landing heads?\n\nHistorically, there have been two different answers to this question, the \"frequentist\" versus the \"subjective\" or \"Bayesian\" answer, which have given rise to pragmatically different statistical approaches.  The debate has been very long and very complicated, and different parties within an approach have taken different positions.  Proceeding on regardless, one possible introduction might begin by describing the two approaches as follows:\n\n- Approach 1:  There's a *class* of events similar to this coinflip.  This class could be \"all the times we flip this coin\", or \"all the times we flip coins similar to this one\".  Across this class, the coin comes up heads half the time; that is, the *frequency* of the coin coming up heads is 50% inside this class.  When we state this, we are asserting a definite fact about the coin.\n- Approach 2:   Uncertainty is in the mind, not in the environment.  If I flip a coin and slap it onto my wrist, it's already landed either heads or tails.  The fact that I don't know whether the coin has landed heads or tails is a fact about me, not a fact about the coin.  The way I *express my own ignorance* is by saying that I'd bet at 1 : 1 odds that the coin came up heads.\n\nThe first approach is called *frequentism*.  The second approach is *subjectivism* or *subjective probability* or *subjective [1r8 Bayesianism].* \n\nOne example of a divide between the two approaches might be highlighted by the question, \"What is the probability of Hillary Clinton winning the 2016 US presidential election?\"\n\nA stereotypical (possibly straw) frequentist might say, \"The 2016 presidential election only happens once.  We can't *observe* a frequency with which Clinton wins presidential elections.  So we can't do any statistics or assign any probabilities here.\"\n\nA stereotypical Bayesian might say:  \"Well, prediction markets tend to be pretty *well-calibrated* about this sort of thing, in the sense that what prediction markets assign 20% probability happens around 1 time out of 5.  I'm willing to call that a good map-territory correspondence and useful cognitive work.  To put it another way, suppose somebody offered you odds of 20 : 1 *against* Clinton winning the election - they get \\$1 if she loses, you get \\$20 if she wins.  I suppose you could refuse to bet with them because you think you can't talk about probabilities of one-time events; but you'd be pointlessly passing up a really good bet.\"\n\nA non-straw frequentist might reply:  \"I'd take that bet too.  But my taking that bet *is not based on rigorous epistemology,* and we shouldn't allow analogous kinds of thinking in experimental publications.  We do indeed want to exclude *subjective* reasoning about probabilities, and allow only a particular kind of *objective* reasoning about probabilities, into our journal articles; that's what frequentist statistics are designed to do.  Your paper's conclusion section should not be allowed to say only, 'And therefore, having observed this sequence of carbon dioxide levels, I'd personally bet at 9 : 1 odds that anthropogenic global warming is real.'\"\n\n...and then it starts getting complicated.\n\n(This node is currently written from a Bayesian standpoint.  It may fail to pass the [ITT](https://en.wikipedia.org/wiki/Ideological_Turing_Test) in its representation of frequentism, and fail to do justice to frequentist replies.)\n\n# Map-territory correspondence links\n\nOne way of visualizing the difference between the two approaches, would be to see it as a difference in how to set up a correspondence between a *probabilistic* map, and a territory.\n\nIn non-probabilistic maps and territories, the notion of 'truth' is comparatively straightforward: [to say of what is, that it is](http://classicalwisdom.com/greek_books/metaphysics-by-aristotle-book-iv/7/), and of what is not, that it is not, is true.\n\n![ordinary truth](http://i.imgur.com/1Zl9l3R.jpg?4)\n\nBut how do you draw a correspondence link between a probabilistic map and a territory?  If you think a biased coin has a 70% probability of coming up heads, how do you draw a correspondence link between *that* and the territory?  If the coin is actually heads, is the map true?  Or 70% true?  Or what?\n\n![probability truth?](http://i.imgur.com/aiDr7Tu.jpg?2)\n\nA Bayesian imagines multiple probable worlds in the map, and draws a correspondence line between that probability distribution and a single territory.\n\nThe more probability the map assigns to the correct answer, the better the map.  If everywhere the map says \"30% probability it's like X\", the territory is that way around 30% of the time, the Bayesian calls that a well-calibrated map.\n\nBut there's no uncertainties *actually* out there in the territory; ignorance is a property of minds, not environments; blank sections of a map don't correspond to places where the world itself goes blank.\n\n![bayesian correspondence](http://i.imgur.com/qQIyVsb.jpg?2)\n\nA frequentist draws a line between a single probability-statement in the map, and multiple events in the territory.  If the map says \"30%\" about an outcome in a class, and the actual territory contains 30% outcomes like that in the corresponding class, the frequentist says the map is *factually true.*  The correspondence link is direct: the map makes a statement, and the world *is* like that.\n\n![frequentist correspondence](http://i.imgur.com/eKH3ReW.jpg?4)\n\nFrequentists see only this kind of actually-true statement as possibly being objective - under the subjectivist kind of correspondence, who can tell whether the map ought to say 70% or 75%, since it's not clearly 'true' or 'false' either way?  Therefore, scientific reports ought to be restricted to frequentist statements that can be definitely true or false, in order to increase the objectivity of science.\n\nThe Bayesian reply is that probability theory puts firm underpinnings under subjective probabilities.  Furthermore, restricting science to only frequentist statistics introduces process distortions that are pragmatically responsible for a significant part of the replication crisis and \"the trouble with p-values\".\n\n# Example:  P-values and confidence intervals\n\nSuppose we flip a possibly-biased coin six times, and observe the sequence HHHHHT.  Is this a suspiciously large number of heads out of six flips?  Should we suspect that the coin is biased towards heads, rather than being fair?\n\n## Frequentist standpoint\n\nFrom a frequentist standpoint, most of the statements we may be tempted to make here are too subjective for a scientific process.  \"Suspicion\" isn't objective; even if you quantify \"suspicion\", you can't agree on exactly how to quantify it.  Suppose Alice looks at the sequence HHHHHT and says \"I think it's 40% credible the coin is biased\", and Bob looks at the same result and says, \"I think it's 20% credible the coin is biased\".  A Bayesian might say that Bob's map was righter if the coin turns out to be in fact biased.  But we have no rigorous, third-person-settleable way of saying in advance that Bob is being more rational than Alice; because the Bayesian kind of probabilistic map-territory correspondence link can't be objectively true or false.\n\nWhat *can* we say that's objectively true or false, on the frequentist standpoint?  The frequentist replies:  \"If you flip a coin six times, you have a 7/64 chance of getting *five or more* heads, or a **p-value** of 0.11.  If we quantify 'suspicious' in that way, this result is '11% suspicious' because this class of result has an 11% chance of occurring if the 'null hypothesis' of a fair coin is true.  It is a definite, objective, and true statement that if we repeat this experiment 10,000 times with a *fair* coin, we'll get five or more heads in around 1,100 cases.  Only this is the kind of clear, factual statement that is suitable for inclusion in science papers.\"\n\nThis opinion was persuasive through almost all of the experimental world up until the 1980s.\n\nIt was a hard constraint for many students of science to understand.  People instinctively flipped around the [1rj conditional probabilities] for \"This result is 11% likely to be seen if the null hypothesis is true\" and \"It's 89% probable the coin is biased.\"\n\nThe problem with this flipover is clearer if we consider another common frequentist statistic, the **confidence interval**.\n\nSuppose you're trying to measure the mass of a proton, using a measuring instrument with some noise in it.  After taking a dozen measurements, you run your standard toolbox of frequentist statistics over the experimental results and get a 95% confidence interval of \\[-0.03, 2.07\\] yoctograms.  This doesn't mean we assign any *credibility* that the proton has negative mass.  What the frequentist statistic asserts is \"If we repeat this experiment 1000 times, on average the true value will lie inside the confidence interval around 950 times\" rather than \"We assign 95% credibility that the proton's mass is inside this interval.\"\n\nWith a noisier measurement and some bad luck, we might have ended up with a confidence interval of \\[-0.11, -0.02\\] for our 95% confidence interval.  In this case we assign ~0% *credibility* that the proton has negative mass.  From a frequentist standpoint, it remains as a true statement about the frequency of outcomes within a certain class of events that \"The interval \\[-0.11, -0.02] is the output of a procedure which will produce intervals containing the true value 95% of the time.\"\n\nThe student is then extremely tempted to flip around the [1rj conditional probabilities] and interpret this as the more natural-sounding statement \"This interval has a 95% chance of containing the true value.\"  But the latter statement, on the frequentist standpoint, is impossible to arrive at by any kind of objective procedure, and maybe even incoherent since there's no way to know if it's definitely true or definitely false.  We can know that a parameter is definitely and truly inside an interval - this is a sensible map-territory correspondence.  We may know that a certain estimation procedure produces intervals containing the true value 95% of the time.  There may not even be anything coherent meant by saying that \"There *is* a 95% probability that parameter x is inside interval y\" is true, unless we generated the parameter by rolling dice or some similar events about which true frequentist statements can be made.\n\nTraining young experimentalists to understand this may be hard work, but it's necessary (on a frequentist standpoint) because there's no other way to base our science papers around clear, agreeable, factually and definitely *true* statements.\n\n## Bayesian standpoint\n\nBayesian:  \"So, about that possibly-biased coin that came up HHHHHT.  We Bayesians have a bit of a different perspective on that.\"\n\nFrequentist:  \"Everyone is welcome to their own opinion.  However, not all opinions are suitable for inclusion into scientific publications.\"\n\nBayesian:  \"Well, let me start by agreeing with you that experimental reports should indeed be built around clear, rules-based, definitely-true statistical statements.\"\n\nFrequentist:  \"Then have you not conceded my point?  If we're not allowed to include a Conclusions section that says, 'And therefore, I personally think this coin has a 90% probability of being biased towards heads', then what's left to include in our reports except for objective frequentist statistics like 'A fair coin only produces a result this skewed towards \"heads\" 11% of the time'?\"\n\nBayesian:  \"You and I have a very different standpoint on what constitutes *clear, rules-based* statistics.  Again, let's start with that possibly-biased coin that came up HHHHHT.  You say that this result implies a 'p-value' of 11%.\"\n\nFrequentist:  \"Yes.  It's a clear, objective fact that if you flip a fair coin six times, you get 5 or more heads slightly less than 11% of the time.\"\n\nBayesian:  \"Perhaps, rather than intending to flip the coin six times and count the heads, the experimenter instead decided to keep flipping the coin until it came up tails, then observe how many times they had to flip the coin.  In *that* case, there's only a 1/32 probability that a fair coin would need to be flipped *six or more times* in order to see the first tails.  I call this a p-value of p<0.05 and publish my paper.\"\n\nFrequentist:  \"Yes, that's called p-hacking.  It's very bad and we have to trust to the honesty of scientists to avoid it, just like we have to trust them not to report entirely faked results.  Part of the replication crisis may be happening because p-hacking doesn't seem like Obvious Fraud the way that falsifying an experimental result seems like Obvious Fraud.  But indeed, for frequentist statistics to be valid statements, the scientist has to decide in advance exactly what statistical methods they use.\"\n\nBayesian:  \"I think that you, and indeed, all of the experimental community, need to stop and ponder that for a bit.  Why should I care what the experimenter *had in mind?*  How is any of *that* a fact about the coin itself?  We have the experimental apparatus, a coin-flipper, which asked Nature the question; we have the reply from Nature, which is HHHHHT.  Whatever we say we've learned about the coin, it should depend only on that result.  How can it possibly matter what the experimenter was *thinking,* if they're not a psychokinetic magically influencing the coin itself?\"\n\nFrequentist:  \"Because the exact nature of the *question* we're asking the coin, depends on whether we're performing the procedure 'flip the coin six times and count the heads' or 'flip the coin until it comes up tails and count the flips'.  Even if HHHHHT is a kind of answer that both questions can produce, they're basically different questions - for example, HTHHHH is a possible answer to the first question, but not the second.\"\n\nBayesian:  \"So... your current way of thinking is indeed consistent with a way of looking at probabilistic map-territory correspondences, that puts a single frequentist assertion on one side, and a class of possible events on the other side.  I can see how you'd think we were making different statements depending on whether the class of events we're talking about includes 'HTHHHH' or alternatively 'HHHHHHHHT'.  But I think this is, fundamentally, a very problematic way of looking at the world.  Why not say that if I observe HHHHHT, the null hypothesis is rejected with p<.05 because a fair coin flipped six times only has a 2/64 chance of producing results lying inside the class \\[HHHHHT, TTTTHT\\]?  What makes that class more special than the one containing \\[HHHHHH, THHHHH, HTHHHH, ... HHHHHT\\]?\"\n\nFrequentist:  \"I don't think any science journal would accept that argument.\"\n\nBayesian:  \"But you have no *principled* way of objecting to it, besides appealing to the intution 'well obviously that's cheating'.  And it just gets worse.  The notion of 'confidence intervals' puts the underlying problem into an even starker light: suppose I start with an experimental method for measuring the average height of a Harvard undergraduate that produces, in frequentist terms, a 99% confidence interval.  That is, the overall procedure will 99% of the time produce an interval containing the true average height of a Harvard undergraduate.  Now I take this procedure, and tack onto the end a step where, with probability $\\frac{5}{99},$ I return the confidence interval \\['strawberry', 'cheesecake'\\].  It won't take very long before one of my experiments produces what is, according to frequentist statistics, an entirely valid 95% confidence interval that the average height of a Harvard undergraduate is somewhere between the words 'strawberry' and 'cheesecake'.\"\n\nFrequentist:  \"This is called the problem of the reference class, and it's not a new observation.  It was in an [XKCD](https://xkcd.com/1132/), even.\"\n\n![xkcd: frequentists vs. bayesians](https://imgs.xkcd.com/comics/frequentists_vs_bayesians.png)\n\nBayesian:  \"I'm not saying the reference class problem is *new*.  I'm saying it's *problematic.*  Suppose I could show you a statistical method where it *didn't* matter what the experimenter had in mind - where the lesson we learn from Nature only depends on the fact that the experimental apparatus flipped the coin six times, and the results were HHHHHT.  A method where we *only* have to think about the result HHHHHT, and not possibilities we could have observed but didn't actually observe, like HHT or HTHHHH.  A method where there's no degrees of freedom that lets somebody make the p-value be 0.03 or 0.11 or turn the confidence interval into a strawberry cheesecake.  Would this method not be more objective, on your own terms?  Would it not eliminate one potential source of 'the trouble with p-values'?\"\n\nFrequentist:  \"Obviously, I'd have to see your proposed statistical method before I could say anything about it.  There are times when it's good to have a larger toolbox of possible statistical methods we can use; but whatever you're proposing, it may be useful on only some occasions, if any.\"\n\nBayesian:  \"Well, my proposal is this.  Let's say as a background that the two main hypotheses under consideration are 'the coin is fair' and 'the coin is biased to produce 75% heads'.  Then the experimenter should report that they got the result 'HHHHHT', which is... drum rolls, trumpets... [3.8](http://www.wolframalpha.com/input/?i=%280.50%5E6%29+:+%280.75%5E5+*+0.25%5E1%29) times [1rq more likely] to be observed from a 75%-heads coin vs. a 50%-heads coin.  That is, a [1rq likelihood ratio] of 1 : 3.8 for 50%-heads vs. 75%-heads.  This likelihood ratio is what a Bayesian identifies as the precise [22x strength of evidence] according to [1lz Bayes's Rule].  We can compute this statistic only by considering the one actual experimental result 'HHHHHT', without needing to ask what the experimenter *had in mind,* without needing to fix an arbitrary reference class, etcetera etcetera.\"\n\nFrequentist:  \"I can see how someone might consider those properties a benefit of that particular statistical method, but one size doesn't fit all.  I see a number of obvious drawbacks, here -\"\n\nBayesian:  \"This isn't just another statistical method.  It goes... deeper, than that.  But let's hear about those supposed drawbacks.\"\n\nFrequentist:  \"For one thing, your method requires that I specify an effect size to be tested.  That's kinda cheating, on my view.\"\n\nBayesian:  \"That's not a bug of Bayesianism, it's a feature.  'This coin is biased to give 90% heads' and 'This coin is biased to give 75% heads' are *different hypotheses* - we can tell, because they assign *different likelihoods* to observations such as 'HHTH'.  On my view of the world, it's meaningless to speak of 'an effect' without saying what size the effect might be.  Saying 'an effect' doesn't tell me what I should *expect to see in the world,* so it's not a [220 falsifiable and testable] hypothesis.\"\n\nFrequentist:  \"Meh.  In standard frequentism, I can perform a test to determine if the coin is biased, without saying in advance how much it's biased.  It seems to me that *in practice,* this is a useful thing to do during many investigations where we don't know the exact effect size in advance.\"\n\nBayesian:  \"Part of the replication crisis is the *amazing shrinking effect sizes* that result from p-hacking.  This suggests we should consider different effect sizes as different hypotheses, rather than lumping them all together as 'positive findings'.  It matter whether the effect size is 20% or 3%, because 20% effects are much more useful or important than 3% effect sizes.  If the original investigators found a 10% effect size, 95% confidence interval \\[0.7, 0.15\\] and the followup investigators found a 3% effect size \\[0.01, 0.04\\], maybe we *should* take more notice of that than just writing it down as '2 studies found positive effect sizes.'\"\n\nFrequentist:  \"It seems to me that this is a problem to be solved with better meta-analyses -\"\n\nBayesian:  \"*Furthermore,* another aspect of the replication crisis is the file-drawer effect where only 'positive' findings actually get published.  If the previous study supposedly found an effect size of 20%, I can do my new study and report, 'Our experiment shows a likelihood ratio of 27.3 : 1 for \"no effect\" over \"20% effect\".'  Your p-values way of thinking, combined with lumping together all the different hypotheses for different effect sizes into 'there is an effect', has in practice led journals to preferentially accept papers that show an 'effect', and to reject papers that *fail* to get p-values under 0.05 for the null hypothesis.  A Bayesian sees no fundamental difference between 'this is evidence with a likelihood ratio of 1 : 9.2 favoring 20% effect over no effect', and 'a likelihood ratio of 9.2 : 1 favoring no effect over 20% effect'.  We can report the latter experiment as what it is, *positive evidence* leading us to perform a quantifiable [1ly Bayesian update] in *favor* of no effect or small effects.  A Bayesian sees a successful experiment that produced evidence strongly discriminating two hypotheses under consideration, instead of *failing* to attain 'statistical significance'.\"\n\nFrequentist:  \"Perhaps this would be better addressed by journals striving for better acceptance policies, or study preregistrations, or even more radical solutions like pre-accepting papers in advance of their experiment reports being reported?  Or better meta-analysis practices -\"\n\nBayesian:  \"You wanna know how a Bayesian does meta-analyses?  We bleeping [1zj multiply the likelihood functions together].  If experiment 1 returns evidence that's 15 times more likely on hypothesis A than hypothesis B, and then experiment 2 returns evidence with a likelihood ratio of (20 : 1) for A vs. B, the combined evidence of experiments 1 and 2 is (300 : 1) for A vs. B.  The end.  Try *that* with p-values.  Or rather, don't try that with p-values, because a p-value of <0.05 and a p-value of <0.01 don't combine into anything remotely like p<0.0005.\"\n\nFrequentist:  \"So, I see several problems with this kind of meta-analysis.  Problem one, what if I want to compare hypothesis C to A and B, and the experimenters didn't think of C and didn't provide any likelihood ratios for it?\"\n\nBayesian:  \"Well, *that's* why everybody should be providing raw datasets in the age of the Web.  Everyone has that problem!  You can't draw blood from a stone!  On frequentism you have 'sufficient statistics' but without the raw data they're only going to be sufficient for a particular class of frequentist methods, and anyone thinking outside that box needs access to the raw data so they can compute other statistics instead.  Bayesian statistics is no different in that regard: the experimenters can try to report summary statistics like \"number of heads and number of tails\" that will let us compute the likelihood functions to a class of hypotheses that don't care what order the coin produced heads and tails.  If you think of an outside-the-box hypothesis like 'Maybe this sequence alternates heads and tails', your only recourse is the raw data.  Bayesianism makes this a little more obvious, maybe, because it's *clear* which hypotheses you can and can't compute likelihood functions for, and people would loudly demand the raw data.  But nothing can substitute for the raw data if you need to consider a weird hypothesis.  That's true on both our philosophies.\"\n\nFrequentist:  \"Okay, fine.  Problem two -\"\n\nBayesian:  \"Can I pre-emptively guess that your problem is going to be, 'What happens if there's different background conditions in two experiments, such that, unknown to us, a real effect is present in experiment 1 but not experiment 2?  What happens if I multiply likelihoods then?'\"\n\nFrequentist:  \"I was actually going to ask what a Bayesian does if hypothesis A and hypothesis B are both wrong.  It seems to *me* that in this case, it would be very useful indeed to be able to reject the null hypothesis without knowing what else is true instead.\"\n\nBayesian:  \"Ah.  I'd see that scenario you said as a more general case of the scenario I said.\"\n\nFrequentist:  \"Really.\"\n\nBayesian:  \"Yes.  In both cases the answer is that multiplying the likelihood functions will get you a *big, obvious* problem in the result, which is that the likelihood function ends up being tiny everywhere.  There'll be *no* hypothesis under consideration, no setting of the parameters, that predicts the outcomes as well as that hypothesis thinks it should.  Like, suppose your only hypotheses are that a coin is biased 90% heads or 70% heads, and you flip the coin and get ten tails in a row.  On average, the 90%-heads hypothesis expects to assign around [4%](http://www.wolframalpha.com/input/?i=0.9%5E9+*+0.1%5E1) probability to the exact result observed after flipping a coin ten times.  The 70%-heads hypothesis expects to assign around [0.2%](http://www.wolframalpha.com/input/?i=0.7%5E7+*+0.3%5E3) probability.  They actually end up assigning probabilities of [1e-10](http://www.wolframalpha.com/input/?i=0.1%5E10) and [6e-6](http://www.wolframalpha.com/input/?i=0.3%5E10) respectively, [227 which is way more surprised than either hypothesis expected to be].  If that happens, the Bayesian calls shenanigans.\"\n\nFrequentist:  \"You... 'call shenanigans'.\"\n\nBayesian:  \"Right.  That result tells us something is rotten in the state of Denmark.  It raises a big, loud, clear warning flag that the truth is outside our current hypothesis space.  This includes, as a special case, the scenario where there's some hidden background variable that turns the effect on and off.  Bayesians *notice* when things are going wrong that way.  It's clear just from [1zj multiplying the likelihood functions] and [227 checking for confusion].\"\n\nFrequentist:  \"And then what?\"\n\nBayesian:  \"If the only way to make sense of the data is to suppose that the effect switches on and off, the process of adding up the evidence from all experiments might force you to consider that hypothesis class.  Or if just one lab committed fraud, or did the experiment under unique circumstances, you'll see one likelihood function whose peak is way off all the other peaks.  But ultimately, *all* observations take place within a single real world, because it's not like some of your observations are coming from a separate reality.  The only way to reconcile your evidence is when you have a single picture of the world where all the evidence makes sense simultaneously.  That might require postulating fraud, or that the subjects are cheating, or that our instruments don't do what we think they do.  But if that's what *actually happened,* your likelihood functions will end up zero everywhere until you *do* consider a hypothesis class which includes the truth.  That's the Bayesian equivalent of meta-analysis - multiplying all the likelihood functions together, and if there's no hypothesis class that can currently cause everything to make sense at the same time, we have to find a larger hypothesis class.  That happens *automatically.*  Of *course* you'd multiply the likelihood functions.  So you notice when the result doesn't make sense.  And then you have to integrate everything together into a single picture, because the combined likelihood function will go on being tiny everywhere until you do.  That's not some ad-hoc statistical rule, or even a noble ideal, it's just what falls out of doing probability theory the obvious way.\"\n\nFrequentist:  \"Has this actually been tried at all -\"\n\nBayesian:  \"Not really, so far as I know.  But look at the current state of affairs where we *know* things are broken and there's a huge replication crisis.  Experiments just pile up, and some of them point in different directions and people think 'meh, that happens'.  Eventually a dozen authors do a dozen different meta-analyses with 200 degrees of freedom in the methods, and most authors end up finding whatever they want to find.  Researchers have given up on *expecting* that all honest experimenters should be interrogating the same world in some sufficiently general sense.  It's not considered *odd* when different labs find different effect sizes.  Meta-analyses don't try to provide a consistent picture of reality that could resolve it all, they just do weird damned statistic tricks and then announce whether the effect still looks 'significant' or not.\"\n\nFrequentist:  \"I get a sense that you're claiming too much progress from one innovation.  There doesn't have to be One Amazing Trick Traditional Statisticians Hate that solves *all* the problems of modern science.\"\n\nBayesian:  \"Oh, there's *plenty* of problems that can't be solved by Bayesianism, I agree.  But it's not as if I'm making up an ad-hoc method here.  There's a whole probability-theoretic viewpoint from over here where [1rq likelihood ratios] and [1zj likelihood functions] are *the* statistic for describing the [22x strength of evidence] and [1ly how to update our beliefs based on observations].\"\n\nFrequentist:  \"I tend to be suspicious of that kind of fundamentalism.\"\n\nBayesian:  \"Being suspicious is good!  But you shouldn't be *invincibly* suspicious.  On my version of the story, it makes *sense* that reporting likelihood ratios would fix a whole lot of problems simultaneously.  Using anything that's *not* a likelihood ratio will lead to inconsistencies and paradoxes.  We have *theorems* about that sort of thing - coherence theorems, they're called.  The whole Bayesian viewpoint is that we should be deploying theorems that are provable from the axioms of probability theory, which will literally never return internally inconsistent results because they are theorems.  Like, that thing where one way of looking at HHHHHT yields p < 0.05 and another way of looking yields p > 0.10 is just *never* supposed to happen to Bayesians, any more than you're supposed to be able to derive $2 + 2 = 5$ and $\\neg(2 + 2 = 5)$  in arithmetic.  If instead you can toss an extra '3' or '4' into your arithmetic formulas, so that you're no longer trying to produce *the* answer, it's no surprise that would cause lots of simultaneous problems.\"\n\nFrequentist:  \"And again, I tend to be suspicious of sweeping one-size-fits-all projects that promise *the* method or *the* answer.  And we can also prove from the probability axioms that a fair coin flipped six coins only returns five or more heads 11% of the time, so it's not as if we're just making up our own theorems.\"\n\nBayesian:  \"I don't expect anyone to offer their total assent to this sweeping weird new project right away.  For now, I'm just trying to defend why it's not ludicrous to think that simple changes *could* fix a lot of problems simultaneously.  There's this whole parallel Bayesian edifice that was built up over decades, a separate line of thinking from frequentist statistics, and part of the *point* is that it's allegedly less arbitrary.  We could be wrong about that, of course.  But if that viewpoint is right, it's not a coincidence that the *one and only* mathematical object a Bayesian uses to describe the impact of evidence, does not depend on the investigator's state of mind.  Nor encourage journal editors to divide the world into 'null' results and 'significant' p-values.  That's the sort of sweeping improvement you might reasonably expect to see, if somebody had, in fact, rebuilt the whole edifice of statistics on firmer mathematical grounds.\"\n\nFrequentist:  \"Alternatively:  People in the current scientific ecology have adapted to the current rules.  If the law of the land and the journals were Bayesian, people would try much harder to find ways to make their papers look good according to Bayesian methods.  Soon everyone would be complaining about how easy it is to fool your version of statistics.  In fact, your simple utopian system would probably be much easier to fool.  You wouldn't have built up the experience that has fine-tuned the battle-hardened statistical methods we actually use in practice.\"\n\nBayesian:  \"That's... not impossible, I guess, but we'd have to be doing something *wrong* for the theorems of probability to lead us astray -\"\n\nFrequentist:  \"For example.  You say that you don't care what the experimenter is *thinking,* you just care that the result was HHHHHT.  How about if I take a fair coin, and keep flipping it until it randomly has an excess of heads or tails, and then stop and report that result?  What if I keep flipping the coin and keep recalculating the likelihood ratio, until I see a likelihood ratio I like, and then I stop and report the result?\"\n\nBayesian:  \"By all means, go ahead.\"\n\nFrequentist:  \"What do you mean, go ahead?\"\n\nBayesian:  \"I mean that's a problem for *your* system, not mine.  So long as you honestly report everything you saw - you don't *omit* or alter any coinflips - then your motives for each individual coinflip are your own concern, not mine.\"\n\nFrequentist:  \"Have you checked what happens if somebody tries to exploit that ideal purity?  A lot of frequentist statistics are built around the basic idiom of 'So long as people obey the rules, we make it hard for them to produce an exciting paper about how they discovered smoke, unless there's actually fire.'  If the coin *is* fair, you only get p<0.05 one in a twenty times -\"\n\nBayesian:  \"Yeah, how's that working out for you?  Cause, I kinda get the impression that's not what actually happened.\"\n\nFrequentist:  \"Then our methods may need repairing.  Or maybe we just need to ask for lower p-values than 0.05.  The *goal* is that you shouldn't be able to *manufacture* discoveries by being clever with when you stop flipping coins - which is why you have to write down the number to be flipped in advance.  We're aware that academic incentives to exist, and we try to create statistical systems that force people to make actual discoveries in order to be able to honestly report apparent discoveries.  If I can choose when to stop flipping the coin, maybe I can manufacture an apparent discovery under your system, and pick up fame and fortune.  If your system is exploitable that way, without ever doing anything you classify as scientific dishonesty, people are going to exploit the hell out of it.  That's exactly the scenario that we prevent, in full generality, via the rules for calculating p-values - even if they do take into account things like the rule the experimenter used to stop flipping the coin.\"\n\nBayesian:  \"My belief updates are *theorems,* not arbitrary 'statistical methods' that I need to patch when they run into problems.  If you're not actually falsifying your results, the theorems are valid.  If I think hypothesis A is three times as probable as hypothesis B, and I see evidence with a [1rq likelihood ratio] for A vs. B of (4 : 1), then by [1x5 Bayes's Rule], my [1rp posterior odds] for A vs. B must go to (12 : 1).  It's not optional.  The end.  There's no degrees of freedom I *could* use to adjust my answer.  So long as you're honest in reporting everything you actually observed, I *must* go on multiplying my prior odds by the likelihood ratio to get my posterior odds.\"\n\nFrequentist:  \"Then I can make you believe anything I like, just by choosing when to stop flipping the coin?\"\n\nBayesian:  \"I don't care.  The theorems of probability theory are *theorems.*  If I do anything except blindly and helplessly apply Bayes's Rule as you dangle me from your puppet strings, I'll get the *wrong answer.*\"\n\nFrequentist:  \"...Are you trolling me?\"\n\nBayesian:  \"Well, yes.  Like I said, this kind of manipulation is a problem in your statistical edifice, not mine.  The fact that it's possible to break p-values that way is another sign of how broken they are.  You cannot make me believe anything you want by reporting all your experimental results honestly.  How would that even work?  The more honest data I get, the more accurate my world-view gets.\"\n\nFrequentist:  \"And I'm saying you need to ask what happens when your idealism runs into the cold fact of academic incentives.  Or did somebody check the likelihood-ratios method and prove that it can't be used to manufacture artificial discoveries by choosing when to stop?\"\n\nBayesian:  \"And *this* is the beauty of working within a single coherent probability theory, or as you would have it, my naive fundamentalist adherence to a lovely-sounding ideal.  Trying to manipulate my beliefs that way is *obviously* impossible because it's a theorem that $\\mathbb P(H) = \\mathbb P(H|E)\\mathbb P(E) + \\mathbb P(H|\\neg E)\\mathbb P(\\neg E),$ aka [Conservation of Expected Evidence](https://wiki.lesswrong.com/wiki/Conservation_of_expected_evidence).  It's impossible for me to expect in advance that any future observation I make will shift my beliefs in a known direction.  When you describe to me what you plan to do with your coin, my current expectation of my posterior belief in any hypothesis about that coin, after you perform your procedure, equals my current belief in that hypothesis.\"\n\nFrequentist:  \"I don't speak Bayesian yet.  Can you translate?\"\n\nBayesian:  \"There's a ridiculously simple theorem proving once and for all that no possible shenanigan in the entire class of shenanigans you're talking about can be used to manipulate my beliefs in a predictable net direction.  Providing you don't falsify or omit any data etcetera, or use knowledge you didn't share with me to inform your choice of what to look at, etcetera.\"\n\nFrequentist:  \"Okay... I'm going to set that aside temporarily until I can learn more about the math.  Taking things closer to the ground floor, let me circle back to your rule that every possible effect size has to be explicitly considered as a different hypothesis.  People in *actual laboratories* need to go looking for effects of unknown effect size all the time.  If you refuse to let them do that on high-minded mathematical grounds, if you refuse to have a *language* for *talking about* an effect of unknown size, you're just getting in their way.\"\n\nBayesian:  \"Actually, Bayesians do have a language for talking about that.  We can say things like, 'We're looking for a bias that's equally probable to be anywhere between 0 and 1.  We can then deploy the [21c Rule of Succession] to figure out how that whole probability distribution over biases performed, relative to e.g. a fair coin.  For example, if we flip a coin 100 times and see 53 heads and 47 tails, that's a likelihood ratio of [6.7 : 1](http://www.wolframalpha.com/input/?i=%280.5%5E%2853%2B47%29%29+:+%28%2853!+*+47!%29+%2F+%2853%2B47%2B1%29!%29) favoring the hypothesis 'fair coin' over the hypothesis 'coin with unknown bias between 0 and 1'.\"\n\nFrequentist:  \"And what if it happens that the bias *isn't* equally probable to be anywhere between 0 and 1?  My statistical method for rejecting the null hypothesis requires no such unlikely assumption.\"\n\nBayesian:  \"Uh... that thing you just said makes no sense from a subjectivist perspective.  I'm saying, if I'm looking for a bias between 0 and 1, but I don't *know* where I'm looking, such that *in my state of ignorance* I don't *know* that the bias is any more likely to lie between 0.03-0.04 or 0.55-0.56, then I deploy the Rule of Succession.\"\n\nFrequentist:  \"Why not say that the logarithm is equally likely to be anywhere between negative infinity and 0?  That is, suppose the interval 0.01-0.02 is equally likely to contain the bias as 0.1-0.2?\"\n\nBayesian:  \"Because that doesn't normalize; that is, [improper_prior the total probability mass adds up to infinity].  By that rule, the bias is infinitely likely to be less than any epsilon you care to name -\"\n\nFrequentist:  \"Yes, I get the point.  Why not assume the natural logarithm is equally likely to be anywhere between -20 and 0?\"\n\nBayesian:  \"I can work with that.  It puts probabilities on what I should actually see as a result, so it's a legitimate hypothesis-class-I-can-actually-test.\"\n\nFrequentist:  \"I'm asking *how you choose* which assumption to make, if that choice isn't to become a degree of freedom letting you obtain any answer you like.  I do confess, I was waiting this whole time to pounce on your [are_priors_arbitrary arbitrary choice] of [27p prior], which has always been the great weakness of Bayesianism.\"\n\nBayesian:  \"Well, that's why I originally answered that the obvious thing a Bayesian should do with experimental results is report *likelihood ratios,* not priors or posteriors.  Indeed the conclusion section of your paper should never say, 'And therefore, having observed this sequence of carbon dioxide levels, I obtain a posterior 90% probability that anthropogenic global warming is real.'  Why, who am *I* to calculate the whole posterior probability?  On a Bayesian view, the rational posterior probability ought to take into account *all* the evidence we have.  Maybe I haven't read all the papers that have ever been published, and there's another likelihood function in there I ought to be multiplying by.  It's not strictly part of my job as an experimentalist to assign priors or posteriors.  I just add more likelihood functions as fuel for the fire.\"\n\nFrequentist:  \"And how do you reconcile that philosophy with being able to say, 'I think this parameter is equally likely to be anywhere between 0 and 1?'\"\n\nBayesian:  \"I'd call it a mere convenience of calculation, and then I'd be sure to publish the raw data in case anyone else wanted to look at the result through the eyes of a different probability distribution.  Say, if somebody wanted to go back after more experiments had been done, and see if all the data would make sense for a parameter of 0.703 specifically.\"\n\nFrequentist:  \"But as I understand the Bayesian philosophy, at *some* point you have to pull a prior out of nowhere and combine it with all those likelihood functions in order to ever end up with an actual posterior probability.  Right?\"\n\nBayesian:  \"Again, that's not strictly the experimenter's job.  Maybe you want to get somebody from the Good Judgment Project who makes money on prediction markets and has lots of practice assigning well-calibrated probabilities to subtle or complicated issues, and let them look over all the likelihood functions.\"\n\nFrequentist:  \"Are you being serious?  I can't tell if you're being serious.\"\n\nBayesian:  \"Oh, not really, I guess.  Look, in a lot of cases, you're going to take one look at the accumulated likelihood function and no realistic choice of prior is going to make much of a difference to the result - the likelihood function will be peaked sharply enough that the point of maximum likelihood just *is* the posterior.  In other places, there's a whole clever art to picking exactly the right mathematical form for the ignorance prior, and Bayes-as-math textbooks will tell you about that.  In other places, there isn't an elegant mathematical symmetry *and* you don't have a ton of evidence, and then maybe you really do want somebody who makes money on prediction markets.  In those places, you can take your pick between formal theories of Occam priors that nobody can actually compute, like [11w Solomonoff Induction]; versus informal rules that humans can actually think about, like outside views or the informal version of Occam's Razor.  And yes, in real life experimentalists have to consider the question of 'How likely is it that there's a discovery here worth looking for?' and they have to figure that out before they've even done the experiment!  But should *you* really be critiquing *me* on those grounds?  As a Bayesian, I have a language for talking about 'the prior probability, before I even do my experiment, that there's a link between petting cats and cancer' and I can *try* to do formal or practical theories about how to assign those probabilities.  A frequentist can't even ask the question!\"\n\nFrequentist:  \"I can too ask questions about informal chances of things being true.  I just don't pretend that my answers are objective facts that deserve to be incorporated into science papers.  If I'm just making something up, I call it a guess, I don't dress it up as a 'Bayesian prior probability'.\"\n\nBayesian:  \"But your brain isn't magic.  Like, if you can say '80%' and have the thing happen 80% of the time, *it's not magic.*  There has to be a reason why your brain is able to successfully produce judgments like that.  We can ask about the algorithms that make it possible, and try to interpret those algorithms in terms of probability theory, which provides something like a laws-of-thermodynamics-of-cognition.  If you say 80% and it happens 80% of the time, you must have had some combination of a good prior and sufficiently strong evidence, or you couldn't possibly arrive at a posterior judgment that accurate.  The Bayesian has a language for continuing to ask 'Why?' after we see somebody on the Good Judgment Project producing well-calibrated judgments of political events.  We don't sweep it under the rug as 'informal'.  There's a reason why you use Bayesian particle filters to integrate the sensory information in a robotic car, instead of saying, 'Oh, driving a car is an informal problem'.\"\n\nFrequentist:  \"I'm not familiar with Bayesian methods for driving robotic cars, so I can't speak to that.  I continue to maintain that for journal articles, we want to apply stricter standards than 'hey let's slap together a car-driving algorithm'.  We want to, you know, constrain ourselves to statements that are objectively true instead of probability assignments anyone is allowed to just make up.\"\n\nBayesian:  \"Sure.  And that's why I say that journal papers should report likelihood functions on well-specified hypotheses, plus the raw data in case anyone needs to investigate a different hypothesis.  Separately, there's a part where the experimentalist tries to guess in advance the probability that there's a discoverable phenomenon to investigate, and afterwards, grantwriters try to infer fire from smoke.  That part still has to happen, but it doesn't have to be part of the experimentalist's report.  And hey, the thing with the grantwriters has to happen in frequentism too.  On your worldview also, somebody needs to actually do something with the p-values and decide whether to ban the allegedly cancer-causing pill.  I don't think that Bayesians having *a language to describe* what grantwriters might do with our likelihood functions, should be held against us - even if you consider that process to be an opaque and informal one that is never allowed to rest on any firm epistemological grounds.\"\n\n*(There's probably a further rejoinder that frequentists make past this point, but the author of this article does not know what it is.)*\n",
      "metaText": "",
      "isTextLoaded": true,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 1,
      "maintainerCount": 1,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": 0,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 15,
      "redLinkCount": 0,
      "lockedBy": "2",
      "lockedUntil": "2017-02-08 18:36:49",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": {
        "edit": {
          "has": false,
          "reason": "You don't have domain permission to edit this page"
        },
        "proposeEdit": {
          "has": true,
          "reason": ""
        },
        "delete": {
          "has": false,
          "reason": "You don't have domain permission to delete this page"
        },
        "comment": {
          "has": false,
          "reason": "You can't comment in this domain because you are not a member"
        },
        "proposeComment": {
          "has": true,
          "reason": ""
        }
      },
      "summaries": {},
      "creatorIds": [],
      "childIds": [
        "569"
      ],
      "parentIds": [
        "1r8",
        "4y9"
      ],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [
        "4y7"
      ],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [
        {
          "id": "5848",
          "parentId": "1rf",
          "childId": "4vr",
          "type": "requirement",
          "creatorId": "1",
          "createdAt": "2016-08-02 17:26:32",
          "level": 2,
          "isStrong": true,
          "everPublished": true
        }
      ],
      "subjects": [
        {
          "id": "5847",
          "parentId": "4vr",
          "childId": "4vr",
          "type": "subject",
          "creatorId": "1",
          "createdAt": "2016-08-02 17:26:23",
          "level": 1,
          "isStrong": true,
          "everPublished": true
        },
        {
          "id": "5852",
          "parentId": "1rf",
          "childId": "4vr",
          "type": "subject",
          "creatorId": "1",
          "createdAt": "2016-08-02 17:28:10",
          "level": 2,
          "isStrong": false,
          "everPublished": true
        }
      ],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "21976",
          "pageId": "4vr",
          "userId": "2",
          "edit": 15,
          "type": "newEdit",
          "createdAt": "2017-02-08 18:36:49",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "18344",
          "pageId": "4vr",
          "userId": "1yq",
          "edit": 0,
          "type": "newTag",
          "createdAt": "2016-08-04 14:08:30",
          "auxPageId": "4y7",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "18145",
          "pageId": "4vr",
          "userId": "1",
          "edit": 0,
          "type": "newSubject",
          "createdAt": "2016-08-02 17:28:11",
          "auxPageId": "1rf",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "18138",
          "pageId": "4vr",
          "userId": "1",
          "edit": 0,
          "type": "newRequirement",
          "createdAt": "2016-08-02 17:26:32",
          "auxPageId": "1rf",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "18136",
          "pageId": "4vr",
          "userId": "1",
          "edit": 0,
          "type": "newTeacher",
          "createdAt": "2016-08-02 17:26:24",
          "auxPageId": "4vr",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "18137",
          "pageId": "4vr",
          "userId": "1",
          "edit": 0,
          "type": "newSubject",
          "createdAt": "2016-08-02 17:26:24",
          "auxPageId": "4vr",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "15873",
          "pageId": "4vr",
          "userId": "32",
          "edit": 0,
          "type": "newChild",
          "createdAt": "2016-07-07 00:49:29",
          "auxPageId": "569",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "15283",
          "pageId": "4vr",
          "userId": "1yq",
          "edit": 0,
          "type": "deleteChild",
          "createdAt": "2016-07-04 19:06:23",
          "auxPageId": "51q",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "15280",
          "pageId": "4vr",
          "userId": "1yq",
          "edit": 0,
          "type": "newChild",
          "createdAt": "2016-07-04 18:39:38",
          "auxPageId": "51q",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "15214",
          "pageId": "4vr",
          "userId": "32",
          "edit": 13,
          "type": "newEdit",
          "createdAt": "2016-07-04 06:19:10",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "15213",
          "pageId": "4vr",
          "userId": "32",
          "edit": 0,
          "type": "newParent",
          "createdAt": "2016-07-04 05:49:36",
          "auxPageId": "4y9",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "14897",
          "pageId": "4vr",
          "userId": "2",
          "edit": 11,
          "type": "newEdit",
          "createdAt": "2016-06-30 03:05:02",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "2876",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 1,
          "dislikeCount": 0,
          "likeScore": 1,
          "individualLikes": [],
          "id": "14896",
          "pageId": "4vr",
          "userId": "2",
          "edit": 10,
          "type": "newEdit",
          "createdAt": "2016-06-30 03:03:44",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "14887",
          "pageId": "4vr",
          "userId": "2",
          "edit": 9,
          "type": "newEdit",
          "createdAt": "2016-06-30 02:56:07",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "14820",
          "pageId": "4vr",
          "userId": "32",
          "edit": 8,
          "type": "newEdit",
          "createdAt": "2016-06-29 19:29:28",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "14818",
          "pageId": "4vr",
          "userId": "32",
          "edit": 7,
          "type": "newEdit",
          "createdAt": "2016-06-29 19:21:03",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "14800",
          "pageId": "4vr",
          "userId": "2",
          "edit": 6,
          "type": "newEdit",
          "createdAt": "2016-06-29 05:38:50",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "14790",
          "pageId": "4vr",
          "userId": "2",
          "edit": 0,
          "type": "newTag",
          "createdAt": "2016-06-29 04:19:56",
          "auxPageId": "4v4",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "14789",
          "pageId": "4vr",
          "userId": "2",
          "edit": 5,
          "type": "newEdit",
          "createdAt": "2016-06-29 04:19:37",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "14786",
          "pageId": "4vr",
          "userId": "2",
          "edit": 0,
          "type": "deleteTag",
          "createdAt": "2016-06-29 04:15:23",
          "auxPageId": "4v",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "14788",
          "pageId": "4vr",
          "userId": "2",
          "edit": 0,
          "type": "deleteTag",
          "createdAt": "2016-06-29 04:15:23",
          "auxPageId": "4v",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "14764",
          "pageId": "4vr",
          "userId": "2",
          "edit": 0,
          "type": "newTag",
          "createdAt": "2016-06-29 00:54:11",
          "auxPageId": "4v",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "14755",
          "pageId": "4vr",
          "userId": "2",
          "edit": 4,
          "type": "newEdit",
          "createdAt": "2016-06-28 23:50:53",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "14661",
          "pageId": "4vr",
          "userId": "2",
          "edit": 2,
          "type": "newEdit",
          "createdAt": "2016-06-28 06:02:34",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "14656",
          "pageId": "4vr",
          "userId": "2",
          "edit": 0,
          "type": "newParent",
          "createdAt": "2016-06-28 02:49:03",
          "auxPageId": "1r8",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "14654",
          "pageId": "4vr",
          "userId": "2",
          "edit": 1,
          "type": "newEdit",
          "createdAt": "2016-06-28 02:49:01",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        }
      ],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": true,
      "hasParents": true,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "4y7": {
      "likeableId": "3096",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 1,
      "dislikeCount": 0,
      "likeScore": 1,
      "individualLikes": [],
      "pageId": "4y7",
      "edit": 8,
      "editSummary": "applied ER's change to the clickbait",
      "prevEdit": 7,
      "currentEdit": 8,
      "wasPublished": true,
      "type": "wiki",
      "title": "C-Class",
      "clickbait": "This page has substantial content, but may not thoroughly cover the topic, may not meet style and prose standards, or may not explain the concept in a way the target audience will reliably understand.",
      "textLength": 709,
      "alias": "c_class_meta_tag",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "1yq",
      "editCreatedAt": "2016-08-27 17:22:59",
      "pageCreatorId": "1yq",
      "pageCreatedAt": "2016-06-30 02:11:33",
      "seeDomainId": "0",
      "editDomainId": "3",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 0,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 136,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": 0,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "4y9": {
      "likeableId": "2889",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 4,
      "dislikeCount": 0,
      "likeScore": 4,
      "individualLikes": [],
      "pageId": "4y9",
      "edit": 6,
      "editSummary": "",
      "prevEdit": 5,
      "currentEdit": 6,
      "wasPublished": true,
      "type": "wiki",
      "title": "Interpretations of \"probability\"",
      "clickbait": "What does it *mean* to say that a fair coin has a 50% probability of coming up heads?",
      "textLength": 12182,
      "alias": "probability_interpretations",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "32",
      "editCreatedAt": "2016-07-01 07:22:14",
      "pageCreatorId": "32",
      "pageCreatedAt": "2016-06-30 07:36:22",
      "seeDomainId": "0",
      "editDomainId": "1",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 0,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 1137,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": 0,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "4yj": {
      "likeableId": "2890",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "4yj",
      "edit": 6,
      "editSummary": "",
      "prevEdit": 5,
      "currentEdit": 6,
      "wasPublished": true,
      "type": "wiki",
      "title": "Correspondence visualizations for different interpretations of \"probability\"",
      "clickbait": "",
      "textLength": 9212,
      "alias": "probability_interpretations_correspondence",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "32",
      "editCreatedAt": "2016-07-10 12:58:40",
      "pageCreatorId": "32",
      "pageCreatedAt": "2016-06-30 07:37:15",
      "seeDomainId": "0",
      "editDomainId": "1",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 1,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 348,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0
      ],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": -2,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "51q": {
      "likeableId": "2930",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 1,
      "dislikeCount": 0,
      "likeScore": 1,
      "individualLikes": [],
      "pageId": "51q",
      "edit": 1,
      "editSummary": "Extracted EY's page from edit history into a lens.",
      "prevEdit": 0,
      "currentEdit": 1,
      "wasPublished": true,
      "type": "wiki",
      "title": "Subjectivism vs Frequentism ",
      "clickbait": "Probability is in the mind, not in the environment.  If you don't know whether a coin came up heads or tails, that's a fact about you, not a fact about the coin.",
      "textLength": 17319,
      "alias": "subjectivism_vs_frequentism",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "1yq",
      "editCreatedAt": "2016-07-04 18:39:37",
      "pageCreatorId": "1yq",
      "pageCreatedAt": "2016-07-04 18:39:37",
      "seeDomainId": "0",
      "editDomainId": "116",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": false,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 0,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": true,
      "viewCount": 14,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": 0,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "58c": {
      "likeableId": "0",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "58c",
      "edit": 1,
      "editSummary": "",
      "prevEdit": 0,
      "currentEdit": 1,
      "wasPublished": true,
      "type": "wiki",
      "title": "Decision Theory",
      "clickbait": "",
      "textLength": 161,
      "alias": "DecisionTheory",
      "externalUrl": "",
      "sortChildrenBy": "alphabetical",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "1",
      "editCreatedAt": "2016-07-08 18:23:14",
      "pageCreatorId": "1",
      "pageCreatedAt": "2016-07-08 18:23:14",
      "seeDomainId": "0",
      "editDomainId": "15",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 0,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 178,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": 0,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "7ry": {
      "likeableId": "0",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "7ry",
      "edit": 1,
      "editSummary": "",
      "prevEdit": 0,
      "currentEdit": 1,
      "wasPublished": true,
      "type": "wiki",
      "title": "Coherence theorems",
      "clickbait": "A 'coherence theorem' shows that something bad happens to an agent if its decisions can't be viewed as 'coherent' in some sense. E.g., an inconsistent preference ordering leads to going in circles.",
      "textLength": 6853,
      "alias": "coherence_theorems",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "2",
      "editCreatedAt": "2017-02-07 21:07:09",
      "pageCreatorId": "2",
      "pageCreatedAt": "2017-02-07 21:07:09",
      "seeDomainId": "0",
      "editDomainId": "15",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 0,
      "isEditorComment": false,
      "isApprovedComment": false,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 540,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0
      ],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": -2,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    }
  },
  "edits": {
    "4vr": {
      "likeableId": "2877",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 3,
      "dislikeCount": 0,
      "likeScore": 3,
      "individualLikes": [],
      "pageId": "4vr",
      "edit": 8,
      "editSummary": "",
      "prevEdit": 7,
      "currentEdit": 15,
      "wasPublished": true,
      "type": "wiki",
      "title": "Subjective (vs. frequentist) probability",
      "clickbait": "Probability is in the mind, not in the environment.  If you don't know whether a coin came up heads or tails, that's a fact about you, not a fact about the coin.",
      "textLength": 46860,
      "alias": "subjective_probability",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "32",
      "editCreatedAt": "2016-06-29 19:29:28",
      "pageCreatorId": "2",
      "pageCreatedAt": "2016-06-28 02:49:01",
      "seeDomainId": "0",
      "editDomainId": "1",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": false,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 0,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 383,
      "text": "What does it *mean* to say that a flipped coin has a 50% probability of landing heads?\n\nHistorically, there have been two different answers to this question, the \"frequentist\" versus the \"subjective\" or \"Bayesian\" answer, which have given rise to pragmatically different statistical approaches.  The debate has been very long and very complicated, and different parties within an approach have taken different positions.  Proceeding on regardless, one possible introduction might begin by describing the two approaches as follows:\n\n- Approach 1:  There's a *class* of events similar to this coinflip.  This class could be \"all the times we flip this coin\", or \"all the times we flip coins similar to this one\".  Across this class, the coin comes up heads half the time; that is, the *frequency* of the coin coming up heads is 50% inside this class.  When we state this, we are asserting a definite fact about the coin.\n- Approach 2:   Uncertainty is in the mind, not in the environment.  If I flip a coin and slap it onto my wrist, it's already landed either heads or tails.  The fact that I don't know whether the coin has landed heads or tails is a fact about me, not a fact about the coin.  The way I *express my own ignorance* is by saying that I'd bet at 1 : 1 odds that the coin came up heads.\n\nThe first approach is called *frequentism*.  The second approach is *subjectivism* or *subjective probability* or *subjective [1r8 Bayesianism].* \n\nOne example of a divide between the two approaches might be highlighted by the question, \"What is the probability of Hillary Clinton winning the 2016 US presidential election?\"\n\nA stereotypical (possibly straw) frequentist might say, \"The 2016 presidential election only happens once.  We can't *observe* a frequency with which Clinton wins presidential elections.  So we can't do any statistics or assign any probabilities here.\"\n\nA stereotypical Bayesian might say:  \"Well, prediction markets tend to be pretty *well-calibrated* about this sort of thing, in the sense that what prediction markets assign 20% probability happens around 1 time out of 5.  I'm willing to call that a good map-territory correspondence and useful cognitive work.  To put it another way, suppose somebody offered you odds of 20 : 1 *against* Clinton winning the election - they get \\$1 if she loses, you get \\$20 if she wins.  I suppose you could refuse to bet with them because you think you can't talk about probabilities of one-time events; but you'd be pointlessly passing up a really good bet.\"\n\nA non-straw frequentist might reply:  \"I'd take that bet too.  But my taking that bet *is not based on rigorous epistemology,* and we shouldn't allow analogous kinds of thinking in experimental publications.  We do indeed want to exclude *subjective* reasoning about probabilities, and allow only a particular kind of *objective* reasoning about probabilities, into our journal articles; that's what frequentist statistics are designed to do.  Your paper's conclusion section should not be allowed to say only, 'And therefore, having observed this sequence of carbon dioxide levels, I'd personally bet at 9 : 1 odds that anthropogenic global warming is real.'\"\n\n...and then it starts getting complicated.\n\n(This node is currently written from a Bayesian standpoint.  It may fail to pass the [ITT](https://en.wikipedia.org/wiki/Ideological_Turing_Test) in its representation of frequentism, and fail to do justice to frequentist replies.)\n\n# Map-territory correspondence links\n\nOne way of visualizing the difference between the two approaches, would be to see it as a difference in how to set up a correspondence between a *probabilistic* map, and a territory.\n\nIn non-probabilistic maps and territories, the notion of 'truth' is comparatively straightforward: [to say of what is, that it is](http://classicalwisdom.com/greek_books/metaphysics-by-aristotle-book-iv/7/), and of what is not, that it is not, is true.\n\n![ordinary truth](http://i.imgur.com/1Zl9l3R.jpg?4)\n\nBut how do you draw a correspondence link between a probabilistic map and a territory?  If you think a biased coin has a 70% probability of coming up heads, how do you draw a correspondence link between *that* and the territory?  If the coin is actually heads, is the map true?  Or 70% true?  Or what?\n\n![probability truth?](http://i.imgur.com/aiDr7Tu.jpg?2)\n\nA Bayesian imagines multiple probable worlds in the map, and draws a correspondence line between that probability distribution and a single territory.\n\nThe more probability the map assigns to the correct answer, the better the map.  If everywhere the map says \"30% probability it's like X\", the territory is that way around 30% of the time, the Bayesian calls that a well-calibrated map.\n\nBut there's no uncertainties *actually* out there in the territory; ignorance is a property of minds, not environments; blank sections of a map don't correspond to places where the world itself goes blank.\n\n![bayesian correspondence](http://i.imgur.com/qQIyVsb.jpg?2)\n\nA frequentist draws a line between a single probability-statement in the map, and multiple events in the territory.  If the map says \"30%\" about an outcome in a class, and the actual territory contains 30% outcomes like that in the corresponding class, the frequentist says the map is *factually true.*  The correspondence link is direct: the map makes a statement, and the world *is* like that.\n\n![frequentist correspondence](http://i.imgur.com/eKH3ReW.jpg?4)\n\nFrequentists see only this kind of actually-true statement as possibly being objective - under the subjectivist kind of correspondence, who can tell whether the map ought to say 70% or 75%, since it's not clearly 'true' or 'false' either way?  Therefore, scientific reports ought to be restricted to frequentist statements that can be definitely true or false, in order to increase the objectivity of science.\n\nThe Bayesian reply is that probability theory puts firm underpinnings under subjective probabilities.  Furthermore, restricting science to only frequentist statistics introduces process distortions that are pragmatically responsible for a significant part of the replication crisis and \"the trouble with p-values\".\n\n# Example:  P-values and confidence intervals\n\nSuppose we flip a possibly-biased coin six times, and observe the sequence HHHHHT.  Is this a suspiciously large number of heads out of six flips?  Should we suspect that the coin is biased towards heads, rather than being fair?\n\n## Frequentist standpoint\n\nFrom a frequentist standpoint, most of the statements we may be tempted to make here are too subjective for a scientific process.  \"Suspicion\" isn't objective; even if you quantify \"suspicion\", you can't agree on exactly how to quantify it.  Suppose Alice looks at the sequence HHHHHT and says \"I think it's 40% credible the coin is biased\", and Bob looks at the same result and says, \"I think it's 20% credible the coin is biased\".  A Bayesian might say that Bob's map was righter if the coin turns out to be in fact biased.  But we have no rigorous, third-person-settleable way of saying in advance that Bob is being more rational than Alice; because the Bayesian kind of probabilistic map-territory correspondence link can't be objectively true or false.\n\nWhat *can* we say that's objectively true or false, on the frequentist standpoint?  The frequentist replies:  \"If you flip a coin six times, you have a 7/64 chance of getting *five or more* heads, or a **p-value** of 0.11.  If we quantify 'suspicious' in that way, this result is '11% suspicious' because this class of result has an 11% chance of occurring if the 'null hypothesis' of a fair coin is true.  It is a definite, objective, and true statement that if we repeat this experiment 10,000 times with a *fair* coin, we'll get five or more heads in around 1,100 cases.  Only this is the kind of clear, factual statement that is suitable for inclusion in science papers.\"\n\nThis opinion was persuasive through almost all of the experimental world up until the 1980s.\n\nIt was a hard constraint for many students of science to understand.  People instinctively flipped around the [1rj conditional probabilities] for \"This result is 11% likely to be seen if the null hypothesis is true\" and \"It's 89% probable the coin is biased.\"\n\nThe problem with this flipover is clearer if we consider another common frequentist statistic, the **confidence interval**.\n\nSuppose you're trying to measure the mass of a proton, using a measuring instrument with some noise in it.  After taking a dozen measurements, you run your standard toolbox of frequentist statistics over the experimental results and get a 95% confidence interval of \\[-0.03, 2.07\\] yoctograms.  This doesn't mean we assign any *credibility* that the proton has negative mass.  What the frequentist statistic asserts is \"If we repeat this experiment 1000 times, on average the true value will lie inside the confidence interval around 950 times\" rather than \"We assign 95% credibility that the proton's mass is inside this interval.\"\n\nWith a noisier measurement and some bad luck, we might have ended up with a confidence interval of \\[-0.11, -0.02\\] for our 95% confidence interval.  In this case we assign ~0% *credibility* that the proton has negative mass.  From a frequentist standpoint, it remains as a true statement about the frequency of outcomes within a certain class of events that \"The interval \\[-0.11, -0.02] is the output of a procedure which will produce intervals containing the true value 95% of the time.\"\n\nThe student is then extremely tempted to flip around the [1rj conditional probabilities] and interpret this as the more natural-sounding statement \"This interval has a 95% chance of containing the true value.\"  But the latter statement, on the frequentist standpoint, is impossible to arrive at by any kind of objective procedure, and maybe even incoherent since there's no way to know if it's definitely true or definitely false.  We can know that a parameter is definitely and truly inside an interval - this is a sensible map-territory correspondence.  We may know that a certain estimation procedure produces intervals containing the true value 95% of the time.  There may not even be anything coherent meant by saying that \"There *is* a 95% probability that parameter x is inside interval y\" is true, unless we generated the parameter by rolling dice or some similar events about which true frequentist statements can be made.\n\nTraining young experimentalists to understand this may be hard work, but it's necessary (on a frequentist standpoint) because there's no other way to base our science papers around clear, agreeable, factually and definitely *true* statements.\n\n## Bayesian standpoint\n\nBayesian:  \"So, about that possibly-biased coin that came up HHHHHT.  We Bayesians have a bit of a different perspective on that.\"\n\nFrequentist:  \"Everyone is welcome to their own opinion.  However, not all opinions are suitable for inclusion into scientific publications.\"\n\nBayesian:  \"Well, let me start by agreeing with you that experimental reports should indeed be built around clear, rules-based, definitely-true statistical statements.\"\n\nFrequentist:  \"Then have you not conceded my point?  If we're not allowed to include a Conclusions section that says, 'And therefore, I personally think this coin has a 90% probability of being biased towards heads', then what's left to include in our reports except for objective frequentist statistics like 'A fair coin only produces a result this skewed towards \"heads\" 11% of the time'?\"\n\nBayesian:  \"You and I have a very different standpoint on what constitutes *clear, rules-based* statistics.  Again, let's start with that possibly-biased coin that came up HHHHHT.  You say that this result implies a 'p-value' of 11%.\"\n\nFrequentist:  \"Yes.  It's a clear, objective fact that if you flip a fair coin six times, you get 5 or more heads slightly less than 11% of the time.\"\n\nBayesian:  \"Perhaps, rather than intending to flip the coin six times and count the heads, the experimenter instead decided to keep flipping the coin until it came up tails, then observe how many times they had to flip the coin.  In *that* case, there's only a 1/32 probability that a fair coin would need to be flipped *six or more times* in order to see the first tails.  I call this a p-value of p<0.05 and publish my paper.\"\n\nFrequentist:  \"Yes, that's called p-hacking.  It's very bad and we have to trust to the honesty of scientists to avoid it, just like we have to trust them not to report entirely faked results.  Part of the replication crisis may be happening because p-hacking doesn't seem like Obvious Fraud the way that falsifying an experimental result seems like Obvious Fraud.  But indeed, for frequentist statistics to be valid statements, the scientist has to decide in advance exactly what statistical methods they use.\"\n\nBayesian:  \"I think that you, and indeed, all of the experimental community, need to stop and ponder that for a bit.  Why should I care what the experimenter *had in mind?*  How is any of *that* a fact about the coin itself?  We have the experimental apparatus, a coin-flipper, which asked Nature the question; we have the reply from Nature, which is HHHHHT.  Whatever we say we've learned about the coin, it should depend only on that result.  How can it possibly matter what the experimenter was *thinking,* if they're not a psychokinetic magically influencing the coin itself?\"\n\nFrequentist:  \"Because the exact nature of the *question* we're asking the coin, depends on whether we're performing the procedure 'flip the coin six times and count the heads' or 'flip the coin until it comes up tails and count the flips'.  Even if HHHHHT is a kind of answer that both questions can produce, they're basically different questions - for example, HTHHHH is a possible answer to the first question, but not the second.\"\n\nBayesian:  \"So... your current way of thinking is indeed consistent with a way of looking at probabilistic map-territory correspondences, that puts a single frequentist assertion on one side, and a class of possible events on the other side.  I can see how you'd think we were making different statements depending on whether the class of events we're talking about includes 'HTHHHH' or alternatively 'HHHHHHHHT'.  But I think this is, fundamentally, a very problematic way of looking at the world.  Why not say that if I observe HHHHHT, the null hypothesis is rejected with p<.05 because a fair coin flipped six times only has a 2/64 chance of producing results lying inside the class \\[HHHHHT, TTTTHT\\]?  What makes that class more special than the one containing \\[HHHHHH, THHHHH, HTHHHH, ... HHHHHT\\]?\"\n\nFrequentist:  \"I don't think any science journal would accept that argument.\"\n\nBayesian:  \"But you have no *principled* way of objecting to it, besides appealing to the intution 'well obviously that's cheating'.  And it just gets worse.  The notion of 'confidence intervals' puts the underlying problem into an even starker light: suppose I start with an experimental method for measuring the average height of a Harvard undergraduate that produces, in frequentist terms, a 99% confidence interval.  That is, the overall procedure will 99% of the time produce an interval containing the true average height of a Harvard undergraduate.  Now I take this procedure, and tack onto the end a step where, with probability $\\frac{5}{99},$ I return the confidence interval \\['strawberry', 'cheesecake'\\].  It won't take very long before one of my experiments produces what is, according to frequentist statistics, an entirely valid 95% confidence interval that the average height of a Harvard undergraduate is somewhere between the words 'strawberry' and 'cheesecake'.\"\n\nFrequentist:  \"This is called the problem of the reference class, and it's not a new observation.  It was in an [XKCD](https://xkcd.com/1132/), even.\"\n\n![xkcd: frequentists vs. bayesians](https://imgs.xkcd.com/comics/frequentists_vs_bayesians.png)\n\nBayesian:  \"I'm not saying the reference class problem is *new*.  I'm saying it's *problematic.*  Suppose I could show you a statistical method where it *didn't* matter what the experimenter had in mind - where the lesson we learn from Nature only depends on the fact that the experimental apparatus flipped the coin six times, and the results were HHHHHT.  A method where we *only* have to think about the result HHHHHT, and not possibilities we could have observed but didn't actually observe, like HHT or HTHHHH.  A method where there's no degrees of freedom that lets somebody make the p-value be 0.03 or 0.11 or turn the confidence interval into a strawberry cheesecake.  Would this method not be more objective, on your own terms?  Would it not eliminate one potential source of 'the trouble with p-values'?\"\n\nFrequentist:  \"Obviously, I'd have to see your proposed statistical method before I could say anything about it.  There are times when it's good to have a larger toolbox of possible statistical methods we can use; but whatever you're proposing, it may be useful on only some occasions, if any.\"\n\nBayesian:  \"Well, my proposal is this.  Let's say as a background that the two main hypotheses under consideration are 'the coin is fair' and 'the coin is biased to produce 75% heads'.  Then the experimenter should report that they got the result 'HHHHHT', which is... drum rolls, trumpets... [3.8](http://www.wolframalpha.com/input/?i=%280.50%5E6%29+:+%280.75%5E5+*+0.25%5E1%29) times [1rq more likely] to be observed from a 75%-heads coin vs. a 50%-heads coin.  That is, a [1rq likelihood ratio] of 1 : 3.8 for 50%-heads vs. 75%-heads.  This likelihood ratio is what a Bayesian identifies as the precise [22x strength of evidence] according to [1lz Bayes's Rule].  We can compute this statistic only by considering the one actual experimental result 'HHHHHT', without needing to ask what the experimenter *had in mind,* without needing to fix an arbitrary reference class, etcetera etcetera.\"\n\nFrequentist:  \"I can see how someone might consider those properties a benefit of that particular statistical method, but one size doesn't fit all.  I see a number of obvious drawbacks, here -\"\n\nBayesian:  \"This isn't just another statistical method.  It goes... deeper, than that.  But let's hear about those supposed drawbacks.\"\n\nFrequentist:  \"For one thing, your method requires that I specify an effect size to be tested.  That's kinda cheating, on my view.\"\n\nBayesian:  \"That's not a bug of Bayesianism, it's a feature.  'This coin is biased to give 90% heads' and 'This coin is biased to give 75% heads' are *different hypotheses* - we can tell, because they assign *different likelihoods* to observations such as 'HHTH'.  On my view of the world, it's meaningless to speak of 'an effect' without saying what size the effect might be.  Saying 'an effect' doesn't tell me what I should *expect to see in the world,* so it's not a [220 falsifiable and testable] hypothesis.\"\n\nFrequentist:  \"Meh.  In standard frequentism, I can perform a test to determine if the coin is biased, without saying in advance how much it's biased.  It seems to me that *in practice,* this is a useful thing to do during many investigations where we don't know the exact effect size in advance.\"\n\nBayesian:  \"Part of the replication crisis is the *amazing shrinking effect sizes* that result from p-hacking.  This suggests we should consider different effect sizes as different hypotheses, rather than lumping them all together as 'positive findings'.  It matter whether the effect size is 20% or 3%, because 20% effects are much more useful or important than 3% effect sizes.  If the original investigators found a 10% effect size, 95% confidence interval \\[0.7, 0.15\\] and the followup investigators found a 3% effect size \\[0.01, 0.04\\], maybe we *should* take more notice of that than just writing it down as '2 studies found positive effect sizes.'\"\n\nFrequentist:  \"It seems to me that this is a problem to be solved with better meta-analyses -\"\n\nBayesian:  \"*Furthermore,* another aspect of the replication crisis is the file-drawer effect where only 'positive' findings actually get published.  If the previous study supposedly found an effect size of 20%, I can do my new study and report, 'Our experiment shows a likelihood ratio of 27.3 : 1 for \"no effect\" over \"20% effect\".'  Your p-values way of thinking, combined with lumping together all the different hypotheses for different effect sizes into 'there is an effect', has in practice led journals to preferentially accept papers that show an 'effect', and to reject papers that *fail* to get p-values under 0.05 for the null hypothesis.  A Bayesian sees no fundamental difference between 'this is evidence with a likelihood ratio of 1 : 9.2 favoring 20% effect over no effect', and 'a likelihood ratio of 9.2 : 1 favoring no effect over 20% effect'.  We can report the latter experiment as what it is, *positive evidence* leading us to perform a quantifiable [1ly Bayesian update] in *favor* of no effect or small effects.  A Bayesian sees a successful experiment that produced evidence strongly discriminating two hypotheses under consideration, instead of *failing* to attain 'statistical significance'.\"\n\nFrequentist:  \"Perhaps this would be better addressed by journals striving for better acceptance policies, or study preregistrations, or even more radical solutions like pre-accepting papers in advance of their experiment reports being reported?  Or better meta-analysis practices -\"\n\nBayesian:  \"You wanna know how a Bayesian does meta-analyses?  We bleeping [1zj multiply the likelihood functions together].  If experiment 1 returns evidence that's 15 times more likely on hypothesis A than hypothesis B, and then experiment 2 returns evidence with a likelihood ratio of (20 : 1) for A vs. B, the combined evidence of experiments 1 and 2 is (300 : 1) for A vs. B.  The end.  Try *that* with p-values.  Or rather, don't try that with p-values, because a p-value of <0.05 and a p-value of <0.01 don't combine into anything remotely like p<0.0005.\"\n\nFrequentist:  \"So, I see several problems with this kind of meta-analysis.  Problem one, what if I want to compare hypothesis C to A and B, and the experimenters didn't think of C and didn't provide any likelihood ratios for it?\"\n\nBayesian:  \"Well, *that's* why everybody should be providing raw datasets in the age of the Web.  Everyone has that problem!  You can't draw blood from a stone!  On frequentism you have 'sufficient statistics' but without the raw data they're only going to be sufficient for a particular class of frequentist methods, and anyone thinking outside that box needs access to the raw data so they can compute other statistics instead.  Bayesian statistics is no different in that regard: the experimenters can try to report summary statistics like \"number of heads and number of tails\" that will let us compute the likelihood functions to a class of hypotheses that don't care what order the coin produced heads and tails.  If you think of an outside-the-box hypothesis like 'Maybe this sequence alternates heads and tails', your only recourse is the raw data.  Bayesianism makes this a little more obvious, maybe, because it's *clear* which hypotheses you can and can't compute likelihood functions for, and people would loudly demand the raw data.  But nothing can substitute for the raw data if you need to consider a weird hypothesis.  That's true on both our philosophies.\"\n\nFrequentist:  \"Okay, fine.  Problem two -\"\n\nBayesian:  \"Can I pre-emptively guess that your problem is going to be, 'What happens if there's different background conditions in two experiments, such that, unknown to us, a real effect is present in experiment 1 but not experiment 2?  What happens if I multiply likelihoods then?'\"\n\nFrequentist:  \"I was actually going to ask what a Bayesian does if hypothesis A and hypothesis B are both wrong.  It seems to *me* that in this case, it would be very useful indeed to be able to reject the null hypothesis without knowing what else is true instead.\"\n\nBayesian:  \"Ah.  I'd see that scenario you said as a more general case of the scenario I said.\"\n\nFrequentist:  \"Really.\"\n\nBayesian:  \"Yes.  In both cases the answer is that multiplying the likelihood functions will get you a *big, obvious* problem in the result, which is that the likelihood function ends up being tiny everywhere.  There'll be *no* hypothesis under consideration, no setting of the parameters, that predicts the outcomes as well as that hypothesis thinks it should.  Like, suppose your only hypotheses are that a coin is biased 90% heads or 70% heads, and you flip the coin and get ten tails in a row.  On average, the 90%-heads hypothesis expects to assign around [4%](http://www.wolframalpha.com/input/?i=0.9%5E9+*+0.1%5E1) probability to the exact result observed after flipping a coin ten times.  The 70%-heads hypothesis expects to assign around [0.2%](http://www.wolframalpha.com/input/?i=0.7%5E7+*+0.3%5E3) probability.  They actually end up assigning probabilities of [1e-10](http://www.wolframalpha.com/input/?i=0.1%5E10) and [6e-6](http://www.wolframalpha.com/input/?i=0.3%5E10) respectively, [227 which is way more surprised than either hypothesis expected to be].  If that happens, the Bayesian calls shenanigans.\"\n\nFrequentist:  \"You... 'call shenanigans'.\"\n\nBayesian:  \"Right.  That result tells us something is rotten in the state of Denmark.  It raises a big, loud, clear warning flag that the truth is outside our current hypothesis space.  This includes, as a special case, the scenario where there's some hidden background variable that turns the effect on and off.  Bayesians *notice* when things are going wrong that way.  It's clear just from [1zj multiplying the likelihood functions] and [227 checking for confusion].\"\n\nFrequentist:  \"And then what?\"\n\nBayesian:  \"If the only way to make sense of the data is to suppose that the effect switches on and off, the process of adding up the evidence from all experiments might force you to consider that hypothesis class.  Or if just one lab committed fraud, or did the experiment under unique circumstances, you'll see one likelihood function whose peak is way off all the other peaks.  But ultimately, *all* observations take place within a single real world, because it's not like some of your observations are coming from a separate reality.  The only way to reconcile your evidence is when you have a single picture of the world where all the evidence makes sense simultaneously.  That might require postulating fraud, or that the subjects are cheating, or that our instruments don't do what we think they do.  But if that's what *actually happened,* your likelihood functions will end up zero everywhere until you *do* consider a hypothesis class which includes the truth.  That's the Bayesian equivalent of meta-analysis - multiplying all the likelihood functions together, and if there's no hypothesis class that can currently cause everything to make sense at the same time, we have to find a larger hypothesis class.  That happens *automatically.*  Of *course* you'd multiply the likelihood functions.  So you notice when the result doesn't make sense.  And then you have to integrate everything together into a single picture, because the combined likelihood function will go on being tiny everywhere until you do.  That's not some ad-hoc statistical rule, or even a noble ideal, it's just what falls out of doing probability theory the obvious way.\"\n\nFrequentist:  \"Has this actually been tried at all -\"\n\nBayesian:  \"Not really, so far as I know.  But look at the current state of affairs where we *know* things are broken and there's a huge replication crisis.  Experiments just pile up, and some of them point in different directions and people think 'meh, that happens'.  Eventually a dozen authors do a dozen different meta-analyses with 200 degrees of freedom in the methods, and most authors end up finding whatever they want to find.  Researchers have given up on *expecting* that all honest experimenters should be interrogating the same world in some sufficiently general sense.  It's not considered *odd* when different labs find different effect sizes.  Meta-analyses don't try to provide a consistent picture of reality that could resolve it all, they just do weird damned statistic tricks and then announce whether the effect still looks 'significant' or not.\"\n\nFrequentist:  \"I get a sense that you're claiming too much progress from one innovation.  There doesn't have to be One Amazing Trick Traditional Statisticians Hate that solves *all* the problems of modern science.\"\n\nBayesian:  \"Oh, there's *plenty* of problems that can't be solved by Bayesianism, I agree.  But it's not as if I'm making up an ad-hoc method here.  There's a whole probability-theoretic viewpoint from over here where [1rq likelihood ratios] and [1zj likelihood functions] are *the* statistic for describing the [22x strength of evidence] and [1ly how to update our beliefs based on observations].\"\n\nFrequentist:  \"I tend to be suspicious of that kind of fundamentalism.\"\n\nBayesian:  \"Being suspicious is good!  But you shouldn't be *invincibly* suspicious.  On my version of the story, it makes *sense* that reporting likelihood ratios would fix a whole lot of problems simultaneously.  Using anything that's *not* a likelihood ratio will lead to inconsistencies and paradoxes.  We have *theorems* about that sort of thing - coherence theorems, they're called.  The whole Bayesian viewpoint is that we should be deploying theorems that are provable from the axioms of probability theory, which will literally never return internally inconsistent results because they are theorems.  Like, that thing where one way of looking at HHHHHT yields p < 0.05 and another way of looking yields p > 0.10 is just *never* supposed to happen to Bayesians, any more than you're supposed to be able to derive $2 + 2 = 5$ and $\\neg(2 + 2 = 5)$  in arithmetic.  If instead you can toss an extra '3' or '4' into your arithmetic formulas, so that you're no longer trying to produce *the* answer, it's no surprise that would cause lots of simultaneous problems.\"\n\nFrequentist:  \"And again, I tend to be suspicious of sweeping one-size-fits-all projects that promise *the* method or *the* answer.  And we can also prove from the probability axioms that a fair coin flipped six coins only returns five or more heads 11% of the time, so it's not as if we're just making up our own theorems.\"\n\nBayesian:  \"I don't expect anyone to offer their total assent to this sweeping weird new project right away.  For now, I'm just trying to defend why it's not ludicrous to think that simple changes *could* fix a lot of problems simultaneously.  There's this whole parallel Bayesian edifice that was built up over decades, a separate line of thinking from frequentist statistics, and part of the *point* is that it's allegedly less arbitrary.  We could be wrong about that, of course.  But if that viewpoint is right, it's not a coincidence that the *one and only* mathematical object a Bayesian uses to describe the impact of evidence, does not depend on the investigator's state of mind.  Nor encourage journal editors to divide the world into 'null' results and 'significant' p-values.  That's the sort of sweeping improvement you might reasonably expect to see, if somebody had, in fact, rebuilt the whole edifice of statistics on firmer mathematical grounds.\"\n\nFrequentist:  \"Alternatively:  People in the current scientific ecology have adapted to the current rules.  If the law of the land and the journals were Bayesian, people would try much harder to find ways to make their papers look good according to Bayesian methods.  Soon everyone would be complaining about how easy it is to fool your version of statistics.  In fact, your simple utopian system would probably be much easier to fool.  You wouldn't have built up the experience that has fine-tuned the battle-hardened statistical methods we actually use in practice.\"\n\nBayesian:  \"That's... not impossible, I guess, but we'd have to be doing something *wrong* for the theorems of probability to lead us astray -\"\n\nFrequentist:  \"For example.  You say that you don't care what the experimenter is *thinking,* you just care that the result was HHHHHT.  How about if I take a fair coin, and keep flipping it until it randomly has an excess of heads or tails, and then stop and report that result?  What if I keep flipping the coin and keep recalculating the likelihood ratio, until I see a likelihood ratio I like, and then I stop and report the result?\"\n\nBayesian:  \"By all means, go ahead.\"\n\nFrequentist:  \"What do you mean, go ahead?\"\n\nBayesian:  \"I mean that's a problem for *your* system, not mine.  So long as you honestly report everything you saw - you don't *omit* or alter any coinflips - then your motives for each individual coinflip are your own concern, not mine.\"\n\nFrequentist:  \"Have you checked what happens if somebody tries to exploit that ideal purity?  A lot of frequentist statistics are built around the basic idiom of 'So long as people obey the rules, we make it hard for them to produce an exciting paper about how they discovered smoke, unless there's actually fire.'  If the coin *is* fair, you only get p<0.05 one in a twenty times -\"\n\nBayesian:  \"Yeah, how's that working out for you?  Cause, I kinda get the impression that's not what actually happened.\"\n\nFrequentist:  \"Then our methods may need repairing.  Or maybe we just need to ask for lower p-values than 0.05.  The *goal* is that you shouldn't be able to *manufacture* discoveries by being clever with when you stop flipping coins - which is why you have to write down the number to be flipped in advance.  We're aware that academic incentives to exist, and we try to create statistical systems that force people to make actual discoveries in order to be able to honestly report apparent discoveries.  If I can choose when to stop flipping the coin, maybe I can manufacture an apparent discovery under your system, and pick up fame and fortune.  If your system is exploitable that way, without ever doing anything you classify as scientific dishonesty, people are going to exploit the hell out of it.  That's exactly the scenario that we prevent, in full generality, via the rules for calculating p-values - even if they do take into account things like the rule the experimenter used to stop flipping the coin.\"\n\nBayesian:  \"My belief updates are *theorems,* not arbitrary 'statistical methods' that I need to patch when they run into problems.  If you're not actually falsifying your results, the theorems are valid.  If I think hypothesis A is three times as probable as hypothesis B, and I see evidence with a [1rq likelihood ratio] for A vs. B of (4 : 1), then by [1x5 Bayes's Rule], my [1rp posterior odds] for A vs. B must go to (12 : 1).  It's not optional.  The end.  There's no degrees of freedom I *could* use to adjust my answer.  So long as you're honest in reporting everything you actually observed, I *must* go on multiplying my prior odds by the likelihood ratio to get my posterior odds.\"\n\nFrequentist:  \"Then I can make you believe anything I like, just by choosing when to stop flipping the coin?\"\n\nBayesian:  \"I don't care.  The theorems of probability theory are *theorems.*  If I do anything except blindly and helplessly apply Bayes's Rule as you dangle me from your puppet strings, I'll get the *wrong answer.*\"\n\nFrequentist:  \"...Are you trolling me?\"\n\nBayesian:  \"Well, yes.  Like I said, this kind of manipulation is a problem in your statistical edifice, not mine.  The fact that it's possible to break p-values that way is another sign of how broken they are.  You cannot make me believe anything you want by reporting all your experimental results honestly.  How would that even work?  The more honest data I get, the more accurate my world-view gets.\"\n\nFrequentist:  \"And I'm saying you need to ask what happens when your idealism runs into the cold fact of academic incentives.  Or did somebody check the likelihood-ratios method and prove that it can't be used to manufacture artificial discoveries by choosing when to stop?\"\n\nBayesian:  \"And *this* is the beauty of working within a single coherent probability theory, or as you would have it, my naive fundamentalist adherence to a lovely-sounding ideal.  Trying to manipulate my beliefs that way is *obviously* impossible because it's a theorem that $\\mathbb P(H) = \\mathbb P(H|E)\\mathbb P(E) + \\mathbb P(H|\\neg E)\\mathbb P(\\neg E),$ aka [Conservation of Expected Evidence](https://wiki.lesswrong.com/wiki/Conservation_of_expected_evidence).  It's impossible for me to expect in advance that any future observation I make will shift my beliefs in a known direction.  When you describe to me what you plan to do with your coin, my current expectation of my posterior belief in any hypothesis about that coin, after you perform your procedure, equals my current belief in that hypothesis.\"\n\nFrequentist:  \"I don't speak Bayesian yet.  Can you translate?\"\n\nBayesian:  \"There's a ridiculously simple theorem proving once and for all that no possible shenanigan in the entire class of shenanigans you're talking about can be used to manipulate my beliefs in a predictable net direction.  Providing you don't falsify or omit any data etcetera, or use knowledge you didn't share with me to inform your choice of what to look at, etcetera.\"\n\nFrequentist:  \"Okay... I'm going to set that aside temporarily until I can learn more about the math.  Taking things closer to the ground floor, let me circle back to your rule that every possible effect size has to be explicitly considered as a different hypothesis.  People in *actual laboratories* need to go looking for effects of unknown effect size all the time.  If you refuse to let them do that on high-minded mathematical grounds, if you refuse to have a *language* for *talking about* an effect of unknown size, you're just getting in their way.\"\n\nBayesian:  \"Actually, Bayesians do have a language for talking about that.  We can say things like, 'We're looking for a bias that's equally probable to be anywhere between 0 and 1.  We can then deploy the [21c Rule of Succession] to figure out how that whole probability distribution over biases performed, relative to e.g. a fair coin.  For example, if we flip a coin 100 times and see 53 heads and 47 tails, that's a likelihood ratio of [6.7 : 1](http://www.wolframalpha.com/input/?i=%280.5%5E%2853%2B47%29%29+:+%28%2853!+*+47!%29+%2F+%2853%2B47%2B1%29!%29) favoring the hypothesis 'fair coin' over the hypothesis 'coin with unknown bias between 0 and 1'.\"\n\nFrequentist:  \"And what if it happens that the bias *isn't* equally probable to be anywhere between 0 and 1?  My statistical method for rejecting the null hypothesis requires no such unlikely assumption.\"\n\nBayesian:  \"Uh... that thing you just said makes no sense from a subjectivist perspective.  I'm saying, if I'm looking for a bias between 0 and 1, but I don't *know* where I'm looking, such that *in my state of ignorance* I don't *know* that the bias is any more likely to lie between 0.03-0.04 or 0.55-0.56, then I deploy the Rule of Succession.\"\n\nFrequentist:  \"Why not say that the logarithm is equally likely to be anywhere between negative infinity and 0?  That is, suppose the interval 0.01-0.02 is equally likely to contain the bias as 0.1-0.2?\"\n\nBayesian:  \"Because that doesn't normalize; that is, [improper_prior the total probability mass adds up to infinity].  By that rule, the bias is infinitely likely to be less than any epsilon you care to name -\"\n\nFrequentist:  \"Yes, I get the point.  Why not assume the natural logarithm is equally likely to be anywhere between -20 and 0?\"\n\nBayesian:  \"I can work with that.  It puts probabilities on what I should actually see as a result, so it's a legitimate hypothesis-class-I-can-actually-test.\"\n\nFrequentist:  \"I'm asking *how you choose* which assumption to make, if that choice isn't to become a degree of freedom letting you obtain any answer you like.  I do confess, I was waiting this whole time to pounce on your [are_priors_arbitrary arbitrary choice] of [27p prior], which has always been the great weakness of Bayesianism.\"\n\nBayesian:  \"Well, that's why I originally answered that the obvious thing a Bayesian should do with experimental results is report *likelihood ratios,* not priors or posteriors.  Indeed the conclusion section of your paper should never say, 'And therefore, having observed this sequence of carbon dioxide levels, I obtain a posterior 90% probability that anthropogenic global warming is real.'  Why, who am *I* to calculate the whole posterior probability?  On a Bayesian view, the rational posterior probability ought to take into account *all* the evidence we have.  Maybe I haven't read all the papers that have ever been published, and there's another likelihood function in there I ought to be multiplying by.  It's not strictly part of my job as an experimentalist to assign priors or posteriors.  I just add more likelihood functions as fuel for the fire.\"\n\nFrequentist:  \"And how do you reconcile that philosophy with being able to say, 'I think this parameter is equally likely to be anywhere between 0 and 1?'\"\n\nBayesian:  \"I'd call it a mere convenience of calculation, and then I'd be sure to publish the raw data in case anyone else wanted to look at the result through the eyes of a different probability distribution.  Say, if somebody wanted to go back after more experiments had been done, and see if all the data would make sense for a parameter of 0.703 specifically.\"\n\nFrequentist:  \"But as I understand the Bayesian philosophy, at *some* point you have to pull a prior out of nowhere and combine it with all those likelihood functions in order to ever end up with an actual posterior probability.  Right?\"\n\nBayesian:  \"Again, that's not strictly the experimenter's job.  Maybe you want to get somebody from the Good Judgment Project who makes money on prediction markets and has lots of practice assigning well-calibrated probabilities to subtle or complicated issues, and let them look over all the likelihood functions.\"\n\nFrequentist:  \"Are you being serious?  I can't tell if you're being serious.\"\n\nBayesian:  \"Oh, not really, I guess.  Look, in a lot of cases, you're going to take one look at the accumulated likelihood function and no realistic choice of prior is going to make much of a difference to the result - the likelihood function will be peaked sharply enough that the point of maximum likelihood just *is* the posterior.  In other places, there's a whole clever art to picking exactly the right mathematical form for the ignorance prior, and Bayes-as-math textbooks will tell you about that.  In other places, there isn't an elegant mathematical symmetry *and* you don't have a ton of evidence, and then maybe you really do want somebody who makes money on prediction markets.  In those places, you can take your pick between formal theories of Occam priors that nobody can actually compute, like [11w Solomonoff Induction]; versus informal rules that humans can actually think about, like outside views or the informal version of Occam's Razor.  And yes, in real life experimentalists have to consider the question of 'How likely is it that there's a discovery here worth looking for?' and they have to figure that out before they've even done the experiment!  But should *you* really be critiquing *me* on those grounds?  As a Bayesian, I have a language for talking about 'the prior probability, before I even do my experiment, that there's a link between petting cats and cancer' and I can *try* to do formal or practical theories about how to assign those probabilities.  A frequentist can't even ask the question!\"\n\nFrequentist:  \"I can too ask questions about informal chances of things being true.  I just don't pretend that my answers are objective facts that deserve to be incorporated into science papers.  If I'm just making something up, I call it a guess, I don't dress it up as a 'Bayesian prior probability'.\"\n\nBayesian:  \"But your brain isn't magic.  Like, if you can say '80%' and have the thing happen 80% of the time, *it's not magic.*  There has to be a reason why your brain is able to successfully produce judgments like that.  We can ask about the algorithms that make it possible, and try to interpret those algorithms in terms of probability theory, which provides something like a laws-of-thermodynamics-of-cognition.  If you say 80% and it happens 80% of the time, you must have had some combination of a good prior and sufficiently strong evidence, or you couldn't possibly arrive at a posterior judgment that accurate.  The Bayesian has a language for continuing to ask 'Why?' after we see somebody on the Good Judgment Project producing well-calibrated judgments of political events.  We don't sweep it under the rug as 'informal'.  There's a reason why you use Bayesian particle filters to integrate the sensory information in a robotic car, instead of saying, 'Oh, driving a car is an informal problem'.\"\n\nFrequentist:  \"I'm not familiar with Bayesian methods for driving robotic cars, so I can't speak to that.  I continue to maintain that for journal articles, we want to apply stricter standards than 'hey let's slap together a car-driving algorithm'.  We want to, you know, constrain ourselves to statements that are objectively true instead of probability assignments anyone is allowed to just make up.\"\n\nBayesian:  \"Sure.  And that's why I say that journal papers should report likelihood functions on well-specified hypotheses, plus the raw data in case anyone needs to investigate a different hypothesis.  Separately, there's a part where the experimentalist tries to guess in advance the probability that there's a discoverable phenomenon to investigate, and afterwards, grantwriters try to infer fire from smoke.  That part still has to happen, but it doesn't have to be part of the experimentalist's report.  And hey, the thing with the grantwriters has to happen in frequentism too.  On your worldview also, somebody needs to actually do something with the p-values and decide whether to ban the allegedly cancer-causing pill.  I don't think that Bayesians having *a language to describe* what grantwriters might do with our likelihood functions, should be held against us - even if you consider that process to be an opaque and informal one that is never allowed to rest on any firm epistemological grounds.\"\n\n*(There's probably a further rejoinder that frequentists make past this point, but the author of this article does not know what it is.)*\n",
      "metaText": "",
      "isTextLoaded": true,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 1,
      "maintainerCount": 1,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": 0,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 15,
      "redLinkCount": 0,
      "lockedBy": "2",
      "lockedUntil": "2017-02-08 18:36:49",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": {
        "edit": {
          "has": false,
          "reason": "You don't have domain permission to edit this page"
        },
        "proposeEdit": {
          "has": true,
          "reason": ""
        },
        "delete": {
          "has": false,
          "reason": "You don't have domain permission to delete this page"
        },
        "comment": {
          "has": false,
          "reason": "You can't comment in this domain because you are not a member"
        },
        "proposeComment": {
          "has": true,
          "reason": ""
        }
      },
      "summaries": {},
      "creatorIds": [],
      "childIds": [
        "569"
      ],
      "parentIds": [
        "1r8",
        "4y9"
      ],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [
        "4y7"
      ],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [
        {
          "id": "5848",
          "parentId": "1rf",
          "childId": "4vr",
          "type": "requirement",
          "creatorId": "1",
          "createdAt": "2016-08-02 17:26:32",
          "level": 2,
          "isStrong": true,
          "everPublished": true
        }
      ],
      "subjects": [
        {
          "id": "5847",
          "parentId": "4vr",
          "childId": "4vr",
          "type": "subject",
          "creatorId": "1",
          "createdAt": "2016-08-02 17:26:23",
          "level": 1,
          "isStrong": true,
          "everPublished": true
        },
        {
          "id": "5852",
          "parentId": "1rf",
          "childId": "4vr",
          "type": "subject",
          "creatorId": "1",
          "createdAt": "2016-08-02 17:28:10",
          "level": 2,
          "isStrong": false,
          "everPublished": true
        }
      ],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "21976",
          "pageId": "4vr",
          "userId": "2",
          "edit": 15,
          "type": "newEdit",
          "createdAt": "2017-02-08 18:36:49",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "18344",
          "pageId": "4vr",
          "userId": "1yq",
          "edit": 0,
          "type": "newTag",
          "createdAt": "2016-08-04 14:08:30",
          "auxPageId": "4y7",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "18145",
          "pageId": "4vr",
          "userId": "1",
          "edit": 0,
          "type": "newSubject",
          "createdAt": "2016-08-02 17:28:11",
          "auxPageId": "1rf",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "18138",
          "pageId": "4vr",
          "userId": "1",
          "edit": 0,
          "type": "newRequirement",
          "createdAt": "2016-08-02 17:26:32",
          "auxPageId": "1rf",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "18136",
          "pageId": "4vr",
          "userId": "1",
          "edit": 0,
          "type": "newTeacher",
          "createdAt": "2016-08-02 17:26:24",
          "auxPageId": "4vr",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "18137",
          "pageId": "4vr",
          "userId": "1",
          "edit": 0,
          "type": "newSubject",
          "createdAt": "2016-08-02 17:26:24",
          "auxPageId": "4vr",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "15873",
          "pageId": "4vr",
          "userId": "32",
          "edit": 0,
          "type": "newChild",
          "createdAt": "2016-07-07 00:49:29",
          "auxPageId": "569",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "15283",
          "pageId": "4vr",
          "userId": "1yq",
          "edit": 0,
          "type": "deleteChild",
          "createdAt": "2016-07-04 19:06:23",
          "auxPageId": "51q",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "15280",
          "pageId": "4vr",
          "userId": "1yq",
          "edit": 0,
          "type": "newChild",
          "createdAt": "2016-07-04 18:39:38",
          "auxPageId": "51q",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "15214",
          "pageId": "4vr",
          "userId": "32",
          "edit": 13,
          "type": "newEdit",
          "createdAt": "2016-07-04 06:19:10",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "15213",
          "pageId": "4vr",
          "userId": "32",
          "edit": 0,
          "type": "newParent",
          "createdAt": "2016-07-04 05:49:36",
          "auxPageId": "4y9",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "14897",
          "pageId": "4vr",
          "userId": "2",
          "edit": 11,
          "type": "newEdit",
          "createdAt": "2016-06-30 03:05:02",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "2876",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 1,
          "dislikeCount": 0,
          "likeScore": 1,
          "individualLikes": [],
          "id": "14896",
          "pageId": "4vr",
          "userId": "2",
          "edit": 10,
          "type": "newEdit",
          "createdAt": "2016-06-30 03:03:44",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "14887",
          "pageId": "4vr",
          "userId": "2",
          "edit": 9,
          "type": "newEdit",
          "createdAt": "2016-06-30 02:56:07",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "14820",
          "pageId": "4vr",
          "userId": "32",
          "edit": 8,
          "type": "newEdit",
          "createdAt": "2016-06-29 19:29:28",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "14818",
          "pageId": "4vr",
          "userId": "32",
          "edit": 7,
          "type": "newEdit",
          "createdAt": "2016-06-29 19:21:03",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "14800",
          "pageId": "4vr",
          "userId": "2",
          "edit": 6,
          "type": "newEdit",
          "createdAt": "2016-06-29 05:38:50",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "14790",
          "pageId": "4vr",
          "userId": "2",
          "edit": 0,
          "type": "newTag",
          "createdAt": "2016-06-29 04:19:56",
          "auxPageId": "4v4",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "14789",
          "pageId": "4vr",
          "userId": "2",
          "edit": 5,
          "type": "newEdit",
          "createdAt": "2016-06-29 04:19:37",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "14786",
          "pageId": "4vr",
          "userId": "2",
          "edit": 0,
          "type": "deleteTag",
          "createdAt": "2016-06-29 04:15:23",
          "auxPageId": "4v",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "14788",
          "pageId": "4vr",
          "userId": "2",
          "edit": 0,
          "type": "deleteTag",
          "createdAt": "2016-06-29 04:15:23",
          "auxPageId": "4v",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "14764",
          "pageId": "4vr",
          "userId": "2",
          "edit": 0,
          "type": "newTag",
          "createdAt": "2016-06-29 00:54:11",
          "auxPageId": "4v",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "14755",
          "pageId": "4vr",
          "userId": "2",
          "edit": 4,
          "type": "newEdit",
          "createdAt": "2016-06-28 23:50:53",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "14661",
          "pageId": "4vr",
          "userId": "2",
          "edit": 2,
          "type": "newEdit",
          "createdAt": "2016-06-28 06:02:34",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "14656",
          "pageId": "4vr",
          "userId": "2",
          "edit": 0,
          "type": "newParent",
          "createdAt": "2016-06-28 02:49:03",
          "auxPageId": "1r8",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "14654",
          "pageId": "4vr",
          "userId": "2",
          "edit": 1,
          "type": "newEdit",
          "createdAt": "2016-06-28 02:49:01",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        }
      ],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": true,
      "hasParents": true,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    }
  },
  "users": {
    "1": {
      "id": "1",
      "firstName": "Alexei",
      "lastName": "Andreev",
      "lastWebsiteVisit": "2018-02-18 09:35:21",
      "isSubscribed": false,
      "domainMembershipMap": {}
    },
    "2": {
      "id": "2",
      "firstName": "Eliezer",
      "lastName": "Yudkowsky",
      "lastWebsiteVisit": "2019-12-21 03:34:41",
      "isSubscribed": false,
      "domainMembershipMap": {}
    },
    "5": {
      "id": "5",
      "firstName": "Eric",
      "lastName": "Rogstad",
      "lastWebsiteVisit": "2019-08-23 01:44:10",
      "isSubscribed": false,
      "domainMembershipMap": {}
    },
    "32": {
      "id": "32",
      "firstName": "Nate",
      "lastName": "Soares",
      "lastWebsiteVisit": "2017-05-10 13:41:18",
      "isSubscribed": false,
      "domainMembershipMap": {}
    },
    "1yq": {
      "id": "1yq",
      "firstName": "Eric",
      "lastName": "Bruylant",
      "lastWebsiteVisit": "2017-04-14 18:00:22",
      "isSubscribed": false,
      "domainMembershipMap": {}
    },
    "8pb": {
      "id": "8pb",
      "firstName": "Jacob",
      "lastName": "van Eeden",
      "lastWebsiteVisit": "2017-10-02 19:17:29",
      "isSubscribed": false,
      "domainMembershipMap": {}
    }
  },
  "domains": {
    "1": {
      "id": "1",
      "pageId": "1lw",
      "createdAt": "2016-01-15 03:02:51",
      "alias": "math",
      "canUsersComment": false,
      "canUsersProposeComment": true,
      "canUsersProposeEdits": true,
      "friendDomainIds": []
    },
    "3": {
      "id": "3",
      "pageId": "3d",
      "createdAt": "2015-03-30 22:19:47",
      "alias": "Arbital",
      "canUsersComment": false,
      "canUsersProposeComment": true,
      "canUsersProposeEdits": true,
      "friendDomainIds": []
    },
    "8": {
      "id": "8",
      "pageId": "198",
      "createdAt": "2015-12-13 23:14:48",
      "alias": "TeamArbital",
      "canUsersComment": false,
      "canUsersProposeComment": true,
      "canUsersProposeEdits": true,
      "friendDomainIds": []
    },
    "15": {
      "id": "15",
      "pageId": "58c",
      "createdAt": "2016-07-08 18:23:14",
      "alias": "DecisionTheory",
      "canUsersComment": false,
      "canUsersProposeComment": true,
      "canUsersProposeEdits": true,
      "friendDomainIds": []
    },
    "21": {
      "id": "21",
      "pageId": "1",
      "createdAt": "2015-02-10 17:12:19",
      "alias": "AlexeiAndreev",
      "canUsersComment": false,
      "canUsersProposeComment": true,
      "canUsersProposeEdits": true,
      "friendDomainIds": [
        "2069"
      ]
    },
    "116": {
      "id": "116",
      "pageId": "1yq",
      "createdAt": "2016-02-12 16:14:31",
      "alias": "EricBruylant",
      "canUsersComment": false,
      "canUsersProposeComment": true,
      "canUsersProposeEdits": true,
      "friendDomainIds": []
    }
  },
  "masteries": {
    "1rf": {
      "pageId": "1rf",
      "has": false,
      "wants": false,
      "level": 0,
      "updatedAt": ""
    },
    "4vr": {
      "pageId": "4vr",
      "has": false,
      "wants": false,
      "level": 0,
      "updatedAt": ""
    }
  },
  "marks": {},
  "pageObjects": {},
  "result": {},
  "globalData": null
}
