{
  "resetEverything": false,
  "user": {
    "id": "",
    "firstName": "",
    "lastName": "",
    "lastWebsiteVisit": "",
    "isSubscribed": false,
    "domainMembershipMap": {},
    "fbUserId": "",
    "email": "",
    "isAdmin": false,
    "emailFrequency": "",
    "emailThreshold": 0,
    "ignoreMathjax": false,
    "showAdvancedEditorMode": false,
    "isSlackMember": false,
    "analyticsId": "aid:4uOupcHO/QPXkD4WCSL/vvNIFlScHJ07U/18NVE4GRk",
    "hasReceivedMaintenanceUpdates": false,
    "hasReceivedNotifications": false,
    "newNotificationCount": 0,
    "newAchievementCount": 0,
    "maintenanceUpdateCount": 0,
    "invitesClaimed": [],
    "mailchimpInterests": {},
    "continueBayesPath": null,
    "continueLogPath": null
  },
  "pages": {
    "2": {
      "likeableId": "938",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "2",
      "edit": 4,
      "editSummary": "",
      "prevEdit": 3,
      "currentEdit": 4,
      "wasPublished": true,
      "type": "group",
      "title": "Eliezer Yudkowsky",
      "clickbait": "Cofounder, with Nick Bostrom, of the field of value alignment theory.",
      "textLength": 512,
      "alias": "EliezerYudkowsky",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "2",
      "editCreatedAt": "2015-12-19 01:46:45",
      "pageCreatorId": "2",
      "pageCreatedAt": "2015-09-04 16:14:58",
      "seeDomainId": "0",
      "editDomainId": "2",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 5,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 2011,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": 0,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "55": {
      "likeableId": "2329",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "55",
      "edit": 32,
      "editSummary": "",
      "prevEdit": 31,
      "currentEdit": 32,
      "wasPublished": true,
      "type": "wiki",
      "title": "Value",
      "clickbait": "The word 'value' in the phrase 'value alignment' is a metasyntactic variable that indicates the speaker's future goals for intelligent life.",
      "textLength": 12312,
      "alias": "value_alignment_value",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "2",
      "editCreatedAt": "2016-06-01 19:56:26",
      "pageCreatorId": "2",
      "pageCreatedAt": "2015-04-24 20:23:31",
      "seeDomainId": "0",
      "editDomainId": "2",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 17,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 523,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0
      ],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": -2,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "107": {
      "likeableId": "10",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "107",
      "edit": 17,
      "editSummary": "",
      "prevEdit": 16,
      "currentEdit": 20,
      "wasPublished": true,
      "type": "wiki",
      "title": "Methodology of unbounded analysis",
      "clickbait": "What we do and don't understand how to do, using unlimited computing power, is a critical distinction and important frontier.",
      "textLength": 60341,
      "alias": "unbounded_analysis",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "2",
      "editCreatedAt": "2016-01-20 01:02:44",
      "pageCreatorId": "2",
      "pageCreatedAt": "2015-07-14 20:36:44",
      "seeDomainId": "0",
      "editDomainId": "2",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": true,
      "isLiveEdit": false,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 9,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 542,
      "text": "[summary(Brief):  In modern AI and especially in [2v value alignment theory], there's a sharp divide between \"problems we know how to solve using unlimited computing power\", and \"problems we can't state how to solve using [large_computer computers larger than the universe]\". Not knowing how to do something with unlimited computing power reveals that you are *confused* about the structure of the problem.  It is presently a disturbing fact that we don't know how to solve many aspects of [value_alignment_problem value alignment] *even given* unlimited computing power - we can't state a program that would be a nice AI if we ran it on a sufficiently large computer.  \"Unbounded analysis\" tries to improve what we understand how to do in principle, in that sense.]\n\n[summary:  In modern AI and especially in [2v value alignment theory], there's a sharp divide between \"problems we know how to solve using unlimited computing power\", and \"problems we can't state how to solve using [large_computer computers larger than the universe]\". Not knowing how to do something with unlimited computing power reveals that you are *confused* about the structure of the problem.  The first paper ever written on computer chess was by Shannon in 1950, which gave in passing the unbounded algorithm for solving chess.  The 1997 program [deep_blue Deep Blue] beat the human world champion 47 years later.  In 1836, Edgar Allen Poe carefully argued that no automaton could ever play chess, since at each turn there are many possible moves by the player and opponent, but gears and wheels can only represent deterministic motions.  The year 1836 was *confused* about the nature of the cognitive problem involved in chess; by 1950, we understood the work in principle that needed to be performed; in 1997 it was finally possible to play chess better than humans do.  It is presently a disturbing fact that we don't know how to build a nice AI *even given* unlimited computing power, and there are arguments for tackling this problem specifically as such.]\n\n[summary(Technical):  Much current technical work in [2v value alignment theory] takes place against a background of unbounded computing power, and other simplifying assumptions such as perfect knowledge, environments simpler than the agent, full knowledge of other agents' code, etcetera.  Real-world agents will be bounded, have imperfect knowledge, etcetera.  There are four primary reasons for doing unbounded analysis anyway:\n\n1.  If we don't know how to solve a problem *even given* unlimited computing power, that means we're *confused about the nature of the work to be performed.*  It is often worthwhile to tackle this basic conceptual confusion in a setup that exposes the confusing part of the problem as simply as possible, and doesn't introduce further complications until the core issue is resolved.\n2.  Introducing 'realistic' complications can make it difficult to engage in cooperative discourse about which ideas have which consequences - [AIXI] was a watershed moment because it was formally specified and there was no way to say \"Oh, I didn't mean *that*\" when somebody pointed out that AIXI would try to seize control of its own reward channel.\n3.  Increasing the intelligence of an [advanced_agent advanced agent] may sometimes move its behavior closer to ideals and further from specific complications of an algorithm.  Early, stupid chess algorithms had what seemed to humans like weird idiosyncracies tied to their specific algorithms.  Modern chess programs, far beyond the human champions, can from an intuitive human perspective be seen as just making good chess moves.\n4.  Current AI algorithms are often incapable of demonstrating future phenomena that seem like they should predictably occur later, and whose interesting properties seem like they can be described using an unbounded algorithm as an example.  E.g. current AI algorithms are very far from doing free-form self-reprogramming, but this is predictably the sort of issue we might encounter later.]\n\n# Summary\n\n\"Unbounded analysis\" refers to determining the behavior of a computer program that would, to actually run, require an [large_computer unphysically large amount of computing power], or sometimes [hypercomputer hypercomputation].  If we know how to solve a problem using unlimited computing power, but not with real-world computers, then we have an \"unbounded solution\" but no \"bounded solution\".\n\nAs a central example, consider computer chess.  The first paper ever written on computer chess, by Claude Shannon in 1950, gave an *unbounded solution* for playing perfect chess by exhaustively searching the tree of all legal chess moves.  (Since a chess game draws if no pawn is moved and no piece is captured for 50 moves, this is a finite search tree.)  Shannon then passed to considering more *bounded* ways of playing imperfect chess, such as evaluating the worth of a midgame chess position by counting the balance of pieces, and searching a smaller tree up to midgame states.  It wasn't until 47 years later, in 1997, that [deep_blue Deep Blue] beat Garry Kasparov for the world championship, and there were multiple new basic insights along the way, such as alpha-beta pruning.\n\nIn 1836, there was a sensation called the [Mechanical Turk](https://en.wikipedia.org/wiki/The_Turk), allegedly a chess-playing automaton. Edgar Allen Poe, who was also an amateur magician, wrote an essay arguing that the Turk must contain a human operator hidden in the apparatus (which it did).  Besides analyzing the Turk's outward appearance to locate the hidden compartment, Poe carefully argued as to why no arrangement of wheels and gears could ever play chess in the first place, explicitly comparing the Turk to \"the calculating machine of Mr. Babbage\":\n\n> Arithmetical or algebraical calculations are, from their very nature, fixed and determinate. Certain data being given, certain results necessarily and inevitably follow [...]\n> But the case is widely different with the Chess-Player. With him there is no determinate progression. No one move in chess necessarily follows upon any one other. From no particular disposition of the men at one period of a game can we predicate their disposition at a different period [...]\n> Now even granting that the movements of the Automaton Chess-Player were in themselves determinate, they would be necessarily interrupted and disarranged by the indeterminate will of his antagonist. There is then no analogy whatever between the operations of the Chess-Player, and those of the calculating machine of Mr. Babbage [...]\n> It is quite certain that the operations of the Automaton are regulated by *mind*, and by nothing else. Indeed this matter is susceptible of a mathematical demonstration, *a priori*. \n\n(In other words:  In an algebraical problem, each step follows with the previous step of necessity, and therefore can be represented by the determinate motions of wheels and gears as in Charles Babbage's proposed computing engine.  In chess, the player's move and opponent's move don't follow with necessity from the board position, and therefore can't be represented by deterministic gears.)\n\nThis is an amazingly sophisticated remark, considering the era.  It even puts a finger on the part of chess that is computationally difficult, the combinatorial explosion of possible moves.  And it is still entirely wrong.\n\nEven if you know an unbounded solution to chess, you might still be 47 years away from a bounded solution.  But if you can't state a program that solves the problem *in principle*, you are in some sense *confused* about the nature of the cognitive work needed to solve the problem.  If you can't even solve a problem given infinite computing power, you definitely can't solve it using bounded computing power.  (Imagine Poe trying to write a chess-playing program before he'd had the insight about search trees.)\n\nWe don't presently know how to write a Python program that would be a nice AI if we ran it on a unphysically large computer.  Trying directly to cross this conceptual gap by carving off pieces of the problem and trying to devise unbounded solutions to them is \"the methodology of unbounded analysis in AI alignment theory\".\n\nSince \"bounded agent\" has come to mean in general an agent that is realistic, the term \"unbounded agent\" also sometimes refers to agents that:\n\n- Perfectly know their environments.\n- Can fully simulate their environments (the agent is larger than its environment and/or the environment is very simple).\n- Operate in turn-based, discrete time (the environment waits for the agent to compute its move).\n- [cartesian_boundary Cartesian agents] that are perfectly separated from the environment except for sensory inputs and motor outputs.\n\nThese are two importantly distinct kinds of unboundedness, and we'll refer to the above list of properties as \"unrealism\" to distinguish them here.  Sufficiently unrealistic setups are also called \"toy models\" or \"toy problems\".\n\nUnrealistic setups have disadvantages, most notably that the results, observations, and solutions may fail to generalize to realistic applications.  Correspondingly, the classic pitfall of unbounded analysis is that it's impossible to run the code, which means that certain types of conceptual and empirical errors are more likely to go uncaught (see below).\n\nThere are nonetheless several forces pushing technical work in value alignment theory toward unbounded analyses and unrealistic setups, at least for now:\n\n1.  **Attacking confusion in the simplest settings.**  If we don't know how to solve a problem given unlimited computing power, this means we're confused about the nature of the work to be performed.  It is sometimes worthwhile to tackle this conceptual confusion in a setup that tries to expose the confusing part of the problem as simply as possible.  Trying to bound the proposed solution or make it realistic can introduce a *lot* of complications into this discussion, arguably unnecessary ones.  (Deep Blue was far more complicated than Shannon's ideal chess program, and it wouldn't be doing Edgar Allen Poe any favors to show him Deep Blue's code and hide away Shannon's ideal outline.)\n2.  **Unambiguous consequences and communication.**  Introducing 'realistic' complications can make it difficult to engage in cooperative discourse about which ideas have which consequences.  ([AIXI] was a watershed moment for alignment theory because [AIXI] was formally specified and there was no way to say \"Oh, I didn't mean *that*\" when somebody pointed out that AIXI would try to seize control of its own reward channel.)\n3.  **More [advanced_agent advanced] agents might be less idiosyncratic.**  Increasing the cognitive power of an agent may sometimes move its behavior closer to ideals and further from specific complications of an algorithm.  (From the perspective of a human onlooker, early chess algorithms seemed to have weird idiosyncracies tied to their specific algorithms.  Modern chess programs can from an intuitive human perspective be seen as just making good chess moves.)\n4.  **Runnable toy models often aren't a good fit for advanced-agent scenarios.** Current AI algorithms are often not good natural fits for demonstrating certain phenomena that seem like they might predictably occur in sufficiently advanced AIs.  Usually it's a lot of work to carve off a piece of such a phenomenon and pose it as an unbounded problem.  But that can still be significantly easier to fit into an unbounded setting than into a toy model.  (All examples here are too complicated for one sentence, but see the subsection below and the discussion of [utility_indifference] and [tiling_agents].)\n\nEven to the extent that these are good reasons, standard pitfalls of unbounded analyses and unrealistic setups still apply, and some of our collective and individual precautions against them are discussed below.\n\nFor historical reasons, computer scientists are sometimes suspicious of unbounded or unrealistic analyses, and may wonder aloud if they reflect unwillingness or incapacity to do a particular kind of work associated with real-world code.  For discussion of this point see [ Why MIRI uses unbounded analysis].\n\n# Attacking confusion in the simplest settings.\n\nImagine somebody claiming that an *ideal* chess program ought to evaluate the *ideal goodness* of each move, and giving their philosophical analysis in terms of a chess agent which knows the *perfect* goodness of each move, without ever giving an effective specification of what determines the ideal goodness, but using the term $\\gamma$ to symbolize it so that the paper looks very formal.  We can imagine an early programmer sitting down to write a chess-playing program, crafting the parts that take in user-specified moves and the part that displays the chess screen, using random numbers for move goodness until they get around to writing the \"move-goodness determining module\", and then finally finding themselves being unable to complete the program; at which point they finally recognize that all the talk of \"the ideal goodness $\\gamma$ of a chess move\" wasn't an effective specification.\n\nPart of the standard virtue ethics of computer science includes an injunction to write code in order to force this kind of grad student to realize that they don't know how to effectively specify something, even if they symbolized it using a Greek letter.  But at least this kind of ineffectiveness seems to be something that some people can learn to detect without actually running the code - consider that, in our example above, the philosopher-programmer realized that they didn't know how to compute $\\gamma$ at the point where they couldn't complete that part of the program, not at a point where the program ran and failed.  Adding on all the code to take in user moves and display a chess board on the screen only added to the amount of time required to come to this realization; once they know what being unable to write code feels like, they might be able to come to the same realization *much faster* by standing in front of a whiteboard and failing to write pseudocode.\n\nAt this point, it sometimes makes sense to step back and try to say *exactly what you don't know how to solve* - try to crisply state what it is that you want an unbounded solution *for.*  Sometimes you can't even do that much, and then you may actually have to spend some time thinking 'philosophically' - the sort of stage where you talk to yourself about some mysterious ideal quantity of move-goodness and you try to pin down what its properties might be.  It's important not to operate in this stage under the delusion that your move-goodness $\\gamma$ is a well-specified concept; it's a symbol to represent something that you're confused about.  By asking for an *unbounded solution*, or even an effectively-specified *representation of the unbounded problem*, that is, asking for pseudo-code which could be run given nothing more than an unphysically large computer (but no otherwise-unspecified Oracle of Move Goodness that outputs $\\gamma$), we're trying to invoke the \"Can I write code yet?\" test on whatever philosophical reasoning we're doing.\n\nCan trying to write running code help at this stage?  Yes, depending on how easy it is to write small programs that naturally represent the structure of the problem you're confused about how to solve.  Edgar Allen Poe might have been willing to concede that he could conceive of deterministic gears that would determine whether a proposed chess move was legal or illegal, and it's possible that if he'd actually tried to build that automaton and then try to layer gears on top of that to pick out particular legal moves by whatever means, he might have started to think that maybe chess was computable after all - maybe even have hit upon a representation of search, among other possible things that gears could do, and so realized how in principle the problem could be solved.  But this adds a delay and a cost to build the automaton and try out variations of it, and complications from trying to stay within the allowed number of gears; and it's not obvious that there can't possibly be any faster way to hit upon the idea of game-tree search, say, by trying to write pseudocode or formulas on a whiteboard, thinking only about the core structure of the game.  If we ask how it is that Shannon had an easier time coming up with the unbounded solution (understanding the nature of the work to be performed) than Poe, the most obvious cause would be the intervening work by Church and Turing (among others) on the nature of computation.\n\nAnd then in other cases it's not obvious at all how you could well-represent a problem using current AI algorithms, but with enough work you can figure out how represent the problem in an unbounded setting.\n\n## The pitfall of simplying away the key, confusing part of the problem.\n\nThe [tiling_agents tiling agents problem], in the rocket alignment metaphor, is the metaphorical equivalent of trying to fire a cannonball around a perfectly spherical Earth with no atmosphere - to obtain an idea how any kind of \"stable orbit\" can work at all.  Nonmetaphorically, the given problem is to exhibit the simplest nontrivial case of [reflective_stability stable self-modification] - an agent that, given its current reasoning, *wants* to create an agent with similar properties as a successor, i.e., preserving its current goals.\n\nIn a perfect world, we'd be able to, with no risk, fire up running code for AI algorithms that reason freely about self-modification and have justified beliefs about how alternative versions of their own code will behave and the outer consequences of that behavior (the way you might imagine what would happen in the real world if you took a particular drug affecting your cognition).  But this is way, way beyond current AI algorithms to represent in any sort of natural or realistic way.\n\nA *bad* \"unbounded solution\" would be to suppose agents that could exactly simulate their successors, determine exactly what their successors would think, extrapolate the exact environmental consequences, and evaluate those.  If you suppose an exact simulation ability, you don't need to consider how the agent would reason using generalizations, abstractions, or probabilities... but this trivial solution doesn't feel like it sheds any light or gets us any closer to understanding reflective stability; that is, it feels like the *key part of the problem* has been simplified away and solving what remains was too easy and didn't help.\n\nFaced with an \"unbounded solution\" you don't like, the next step is to say crisply exactly what is wrong with it in the form of a new desideratum for your solution.  In this case, our reply would be that for Agent 1 to exactly simulate Agent 2, Agent 1 must be larger than Agent 2, and since we want to model *stable* self-modification, we can't introduce a requirement that Agent 2 be strictly weaker than Agent 1.  More generally, we apply the insight of [vinge_principle] to this observation and arrive at the desiderata of [vinge_uncertainty] and [vinge_reflection], which we also demand that an unbounded solution exhibit.\n\nThis illustrates one of the potential general failure modes of using an unbounded setup to shed conceptual light on a confusing problem - namely, when you simplify away the key confusing issue you wanted to resolve, and end up with a trivial-seeming solution that sheds no light on the original problem.  A chief sign of this is when your paper is too easy to write.  The next action is to say exactly what you simplified away, and put it in the form of a new desideratum, and try to say exactly why your best current attempts can't meet that desideratum.\n\nSo we've now further added: we want the agent to *generalize over possible exact actions and behaviors of its successor* rather than needing to know its successor's exact actions in order to approve building it.\n\nWith this new desideratum in hand, there's now another obvious unbounded model: consider deterministic agents operating in environments with known rules that reason about possible designs and the environment using first-order logic.  The agent then uses an unbounded proof search, which no current AI algorithm could tackle in reasonable time (albeit a human engineer would be able to do it with a bunch of painstaking work) to arrive at justified logical beliefs about the effect of its successor on its environment.  This is certainly still extremely unrealistic; but has this again simplified away all the interesting parts of the problem?  In this case we can definitely reply, \"No, it does expose something confusing\" since we don't in fact know how to build a tiling agent under this setup.  It may not capture all the confusing or interesting parts of the problem, but it seems to expose at least one confusion.  Even if, as seems quite possible, it's introduced some new problem that's an artifact of the logical setup and wouldn't apply to agents doing probabilistic reasoning, there's then a relatively crisp challenge of, \"Okay, show me how probabilistic reasoning resolves the problem, then.\"\n\nIt's not obvious that there's anything further to be gained by trying to create a toy model of the problem, or a toy model of the best current unsatisfactory partial solution, that could run as code with some further cheating and demo-rigging, but [this is being tried anyway](https://intelligence.org/2015/12/04/new-paper-proof-producing-reflection-for-hol/).  The tiling agents problem did spend roughly nine years exclusively on paper before that, and the best current unsatisfactory solution was arrived at with whiteboards.\n\n## The pitfall of residual terms.\n\nBesides \"simplifying away the confusing part of the problem\", another way that unbounded thinking can \"bounce off\" a confusing problem is by creating a residual term that encapsulates the confusion.  Currently, there are good unbounded specifications for [cartesian Cartesian] non-self-modifying [AIXI expected reward maximizers]: if we allow the agent to use unlimited computing power, *don't* allow the environment to have unlimited computing power, don't ask the agent to modify itself, separate the agent from its environment by an impermeable barrier through which only sensory information and motor outputs can pass, and then ask the agent to maximize a sensory reward signal, there's [AIXI a simple Python program which is expected to behave superintelligently given sufficient computing power].  If we then introduce permeability into the Cartesian boundary and allow for the possibility that the agent can take drugs or drop an anvil on its own head, nobody has an unbounded solution to that problem any more.\n\nSo one way of bouncing off that problem is to say, \"Oh, well, my agent calculates the effect of its motor actions on the environment and the expected effect on sensory information and the reward signal, *plus a residual term $\\gamma$* which stands for the expected utility of all effects of the agent's actions that change the agent's processing or destroys its hardware\".  How is $\\gamma$ to be computed?  This is left unsaid.\n\nIn this case you haven't *omitted* the confusing part of the problem, but you've packed it into a residual term you can't give an effective specification for calculating.  So you no longer have an unbounded solution - you can't write down the Python program that runs given unlimited computing power - and you've probably failed to shed any important light on the confusing part of the problem.  Again, one of the warning signs here is that the paper is very easy to write, and reading it does not make the key problem feel less like a hard opaque ball.\n\n# Introducing realistic complications can make it hard to build collective discourse about which ideas have which consequences.\n\nOne of the watershed moments in the history of AI alignment theory was Marcus Hutter's proposal for [AIXI], not just because it was the first time anyone had put together a complete specification of an unbounded agent (in a [cartesian Cartesian setting]), but also because it was the first time that non-value-alignment could be pointed out in a completely pinned-down way.  \n\n(work in progress)\n\n[todo: turn the rest of this into a finished doc, or delete it.\n\nTo the extent that \"standing in front of a whiteboard trying to think of pseudocode\" does work better to clear up conceptual confusion, it will be because this stage of the problem could iterate through more possible pieces of pseudocode, in a context that exposed the underlying problem as simply as possible without any added complications from demanding bounded solutions or realism.\n\n\nOnce you realize you currently can't write the code; what's the next step?  Depending on how easy it is to build small programs that naturally represent the problem setup - and in the case of chess, this shouldn't be hard, the hard part is *generating good moves*, not representing the game of chess in running code - then it might make sense to take exploratory stabs at trying to compute something that makes the game play good chess.\n\nWhat if you can't even represent the problem setup naturally in running code?  Then \n\nWhat about after you realize that you currently can't write code?  \n\nDoing unbounded analysis implies that you think your current state of understanding has basic confusions to be cleared up.  If at this point somebody tried to write a paper about an algorithm that would, given hypercomputation, pilot a robotic car on an ideal 2-dimensional surface, it would be a legitimate reply to say \"But we have actual running code for robotic cars, nor are we confused about how to do this; why do an unbounded analysis when we already have bounded ones?\"  In terms of the [18w rocket alignment metaphor], spending a lot of time trying to figure out how to fire a cannonball so that it circles a perfectly spherical Earth and returns to its exact starting point, corresponds to a suspicion that you currently lack the mathematical language to even talk about landing a rocket on the Moon.  Someone who expects that current vehicles only require one or two tweaks to get to the Moon, or that they can be piloted there by keeping the Moon in the viewport and steering intuitively, is more likely to object \"Why are you talking about cannonballs, instead of the vehicles that reach the highest altitudes today?\"  The closer you think current algorithms are to supporting a Friendly AI, the less sympathetic you might be to the suggestion that very basic foundational work needs to be done and might need to be done in a simplified setting.\n\nOr imagine, say, somebody claiming that an *ideal* chess program ought to evaluate the *ideal goodness* of each move, and giving their philosophical analysis in terms of a chess agent which knows the *perfect* goodness of each move, without ever giving an effective specification of what determines the ideal goodness, but using the term $\\gamma$ to symbolize it so that the paper looks very formal.  We can imagine an early programmer sitting down to write a chess-playing program, crafting the parts that take in user-specified moves and the part that displays the chess screen, using random numbers for move goodness until they get around to writing the \"move-goodness determining module\", and then finally finding themselves being unable to complete the program; at which point they finally recognize that all the talk of \"the ideal goodness $\\gamma$ of a chess move\" wasn't an effective specification.\n\n Sitting down to write code can hopefully force you to recognize that your analysis is still confused, that to talk about \"ideal goodness of a chess move\" is not yet an *effective specification* of an ideal chessplayer, because you don't know how to compute the ideal goodness of a move.\n\n\n\nYou can also, arguably, save yourself a lot of time and code that goes nowhere by learning to recognize that you can't write down \"ideal goodness of a chess move\" as an effective formula, and discussing the problem with friends using a whiteboard.  Instead of spending a long time writing every part of a chess-playing program except the \"compute the goodness of a move\" part until finally being *forced* to recognize you can't complete program.\n\nThe obvious reason why \n\n\n\nSome early cautionary tales from the history of AI deal with toy programs (unrealistic agents) that failed to generalize well to the real world.  Even so, an unrealistic toy model is superior in some ways to an *unbounded* model, because the code can be run and the results observed.  In the absence of running code, it's possible to make outright cognitive errors about whether you've described an intuitive setup using a mathematical statement - failing to give an effective specification, or giving an effective specification that doesn't match the intuitive description.  But this doesn't mean that code is a panacea; especially when creating a \"toy model\", it's possible that the toy setup omits key properties of interest.  Depending on the person and the circumstances, a toy model might be extra work and not bring any extra enlightenment over writing down the key properties in a formula and thinking about the formula.\n\n\n\nIn the case of [value_alignment value alignment theory], much work does take place in a context of unlimited computing power and usually other unrealistic properties as well.  The subsections below deal with some of these reasons and their associated \n\n\n# Unbounded agent properties\n\nMuch of the current technical work in  [value_alignment value alignment theory] takes place on whiteboards, and tries to analyze programs or mathematical structures that could only run on [large_computer very large finite computers] or [hypercomputer hypercomputers].  \n\n\n\n\nMuch of the current technical work in [value_alignment value alignment theory] takes place against a background of simplifying assumptions such as:\n\n- Unlimited finite computing power, or even [hypercomputer hypercomputation].\n- Perfect knowledge of the environment.\n- Deterministic environments that are smaller than the agent and can be fully simulated.\n- Environments separated from the agent by an [ impermeable Cartesian boundary].\n- Turn-based discrete time (the environment waits for the agent to make a move, rather than proceeding in continuous real time).\n- Common knowledge of each other's code in multi-agent dilemmas.\n- Predefined action sets.\n\nThe core tradeoff in unbounded analysis is that these properties can make a setup (much) easier to analyze and for human beings to reason about, at the expense of realism.  Unrealism means a risk of having simplified away a key part of the problem, maybe even the only really interesting part of the problem, and of obtaining results that fail to apply to the real world.\n\n\n\n# Introduction: Claude Shannon, Deep Blue, and Edgar Allen Poe\n\nIn 1950, the very first paper ever written on computer chess, by Claude Shannon, gave in passing the algorithm that would play perfect chess given unlimited computing power, and then went on to consider more practical algorithms involving small search trees and local position evalution.  It then took an additional 47 years until [DeepBlue Deep Blue] beat Garry Kasparov for the world chess championship.  Knowing how to do something in principle doesn't always mean you can do it in practice without additional hard work and insights like, e.g., the alpha-beta pruning algorithm for chess.\n\nNonetheless, if you *don't* know how to play chess using unlimited computing power, you *definitely* can't play chess using limited computing power.\n\nIn the early 19th century, there was a minor sensation called the [Mechanical Turk](https://en.wikipedia.org/wiki/The_Turk), allegedly a chess-playing automaton.  Today, understanding the computational requirements of chess, we know immediately that there's no way to do that using a box cabinet full of gears, but in the early 19th century this wasn't as obvious.  In 1836, Edgar Allen Poe, who was also an amateur magician of some repute, wrote [an essay arguing that the Mechanical Turk was human-operated](http://www.eapoe.org/works/essays/maelzel.htm).  Poe compares the Turk to the [Duck of Vaucanson](https://en.wikipedia.org/wiki/Digesting_Duck) which quacked, moved almost naturally, and seemed to digest food, acknowledging that there have indeed been wondrous automata.  But Poe insists that the Turk cannot be one of them, and that if it were, it would be the greatest wonder ever constructed by mankind.  Poe writes:\n\n> But if these machines were ingenious, what shall we think of the calculating machine of Mr. Babbage? What shall we think of an engine of wood and metal which can not only compute astronomical and navigation tables to any given extent, but render the exactitude of its operations mathematically certain through its power of correcting its possible errors? What shall we think of a machine which can not only accomplish all this, but actually print off its elaborate results, when obtained, without the slightest intervention of the intellect of man? [...]\n> Arithmetical or algebraical calculations are, from their very nature, fixed and determinate. Certain data being given, certain results necessarily and inevitably follow [...]\n> But the case is widely different with the Chess-Player. With him there is no determinate progression. No one move in chess necessarily follows upon any one other. From no particular disposition of the men at one period of a game can we predicate their disposition at a different period [...]\n> Now even granting that the movements of the Automaton Chess-Player were in themselves determinate, they would be necessarily interrupted and disarranged by the indeterminate will of his antagonist. There is then no analogy whatever between the operations of the Chess-Player, and those of the calculating machine of Mr. Babbage [...]\n> It is quite certain that the operations of the Automaton are regulated by *mind*, and by nothing else. Indeed this matter is susceptible of a mathematical demonstration, *a priori*. \n\nIn other words:  In an algebraical problem, each step follows with the previous step of necessity, and therefore can be represented by the determinate motions of wheels and gears as in Charles Babbage's proposed computing engine; while in chess, the player's move and opponent's move don't follow with necessity from the board position, and therefore require Mind to analyze rather than deterministic gears.\n\nThis is an amazingly sophisticated remark, considering the era.  It even puts a finger on the part of chess that is computationally difficult, the combinatorial explosion of possible moves.  It's still entirely wrong.\n\n(The rest of Poe's essay is then devoted to dissecting the appearance of the Turk and arguing about where the human operator is concealed inside it.)\n\nAs much as Claude Shannon in 1950 was a long way from all the additional insights required to defeat the human champion in 1997 using limited and realistic amounts of computing power, he was also a long way ahead of Edgar Allen Poe who lacked the key insight of 'search trees' that enables a chess game to be represented in deterministic transistors.\n\nBetween Poe in 1830 and Shannon in 1950 there was a key increase in the understanding of computer chess, represented by the intermediate work of Turing, Church, and others.  Shannon's 1950 paper doesn't belabor his unbounded solution to chess that would explore the whole search tree - he seems to consider it mostly obvious, and gives it as only a preliminary to his suggestions for playing chess using far more limited calculations.  Unbounded understanding often seems easy *once you have it.*   But this leaves aside the incredible difficulty of writing a bounded program when you don't even have an unbounded one.  Inability to state the unbounded solution to your problem often indicates that you are *confused about the nature of the computational work to be performed.*  We can imagine someone who lacks the insight of search trees, trying to write a program that plays chess with no better definition of \"good chess move\" as one that leads to center control, captures pieces, sets up forks, leads to a well-defended king, attacks the opponent's king, etcetera, as if all of these considerations were equal parts of some undefined and indefinable \"move goodness\" property...\n\n\n\nThe core idea behind doing unbounded analysis in value alignment theory is that we are presently *confused* about the question \"How would you write a Python program that would implement a nice AI if you could run it on [large_computer a computer much larger than the universe]?\"\n\n\n\n\nand that there are subproblems where we can expose the confusion by trying to crisply pose an important subproblem that nobody seems to know how to solve using unbounded computing power, then trying to come up with unbounded solutions and analyzing those.  (It's important, in this case, to slice off much smaller pieces of the confusion than, \"Write a program that would be a nice AI.\")\n\n\n\n(If you think you already know the answer to this, you should probably be reading up on \"why this problem is hard\" pages rather than \n\nThe fact that nobody can give answer this question - even though there *are* unbounded solutions to most of AI and AGI (e.g. [AIXI]) - illustrates that \n\n\nIf you ask yourself how to write a Python program that would be a nice AI if we could run it on [large_computer a computer much larger than the universe], you might notice a \"stubbing the mind's toe\" feeling of trying to write a program when you're confused about the nature of the computations you need to perform.\n\nOne possible research avenue is to directly tackle that ignorance, as such, and try directly to figure out how to crisply model some aspects of the problem given unlimited computing power and other, even more dangerous simplifying assumptions.  It's an approach that has well-known pitfalls, but it's arguably among the best options we have today.  Some of the considerations pushing value alignment theory in the direction of unbounded analyses can be briefly summarized as follows:\n\n1.  If we don't know how to solve a problem *even given* unbounded computation, that means we're *confused about the nature of the work to be performed.*  It is often worthwhile to tackle this basic conceptual confusion in a setup that exposes the confusing part of the problem as simply as possible, and doesn't introduce further and distracting complications until the core issue is resolved.\n2.  Introducing 'realistic' complications can make it difficult to engage in cooperative discourse about which ideas have which consequences - [AIXI] was a watershed moment because it was formally specified and there was no way to say \"Oh, I didn't mean *that*\" when somebody pointed out that AIXI would try to seize control of its own reward channel.\n3.  Increasing the intelligence of an [advanced_agent advanced agent] may sometimes move its behavior closer to ideals and further from specific complications of an algorithm.  Early, stupid chess algorithms had what seemed to humans like weird idiosyncracies tied to their specific algorithms.  Modern chess programs, far beyond the human champions, can from an intuitive human perspective be seen as just making good chess moves.\n4.  Current AI algorithms are often incapable of demonstrating future phenomena that seem like they should predictably occur later, and whose interesting properties seem like they can be described using an unbounded algorithm as an example.  E.g. [instrumental_convergence convergent instrumental strategies] arise from [consequentialism consequentialist agents] reasoning about difficult domains like \"My programmers have beliefs about whether I'll achieve their goals\" and \"What happens if they decide to press my shutdown button?\", which is *very* far from something that naturally arises in current AI algorithms; but we can still anticipate this future issue using notions like [economic_agency economic agency], and try out unbounded problem formulations and attempted solutions like [utility_indifference].\n\n\n\n# Unbounded agent properties\n\nMuch of the current technical work in [value_alignment value alignment theory] takes place against a background of simplifying assumptions such as:\n\n- Unlimited finite computing power, or even [hypercomputer hypercomputation].\n- Perfect knowledge of the environment.\n- Deterministic environments that are smaller than the agent and can be fully simulated.\n- Environments separated from the agent by an [ impermeable Cartesian boundary].\n- Turn-based discrete time (the environment waits for the agent to make a move, rather than proceeding in continuous real time).\n- Common knowledge of each other's code in multi-agent dilemmas.\n- Predefined action sets.\n\nThe core tradeoff in unbounded analysis is that these properties can make a setup (much) easier to analyze and for human beings to reason about, at the expense of realism.  Unrealism means a risk of having simplified away a key part of the problem, maybe even the only really interesting part of the problem, and of obtaining results that fail to apply to the real world.\n\n# Danger of simplifying away an important part of the problem.\n\nThe pitfall is when an unbounded analysis simplifies away *all* the important parts of the problem and sheds no light on what remains.\n\nFor example, the point of the [vinge_reflection Vingean reflection program] and [tiling_agents tiling agents theory] is that we should not assume that agents can exactly model their own successors or self-modifications.  Even if we say that we can assume unlimited finite computing power, we should still assume that the future version of the agent has more computing power than the past version (or has seen new sensory data) and therefore the current self can't exactly predict the future self.  The core of the problem is to obtain trust in an agent under conditions of [vinge_uncertainty Vingean uncertainty] where we can't know *exactly* what a smarter agent will do because to know *exactly* what it would do we would need to be at least that smart ourselves.  To make the 'unbounded' assumption that we could exactly predict what that agent will do, and judge exactly what consequence would follow, would assume away the central confusing core of the problem.  But allowing for computers bigger than the universe, where the other agent has an *even bigger* computer, does *not* assume away this confusing core.  Or at least, so long as we don't know how to solve even this unbounded form of the problem, it's safe to say that this problem formulation exposes *some* key confusion.\n\nUnbounded analysis doesn't always make a problem easier to think about.  If you can get your algorithm to run on real computers, it becomes possible to observe that your results are correct as well as proving them, and to observe the outcomes in cases that don't have closed-form solutions.  (Though these observations may still be untrustworthy if there's a bug in the code, meaning your setup was not what you thought.)  This implies a sharp dropoff in cognitive tractability at the boundary where you can no longer run your analysis as code.  When it was realized that [PatrickLavictoire]'s [ modal decision theory] could be run in polynomial time, it was then possible to test possible modal agents against each other much more rapidly than when they needed to be written out on a whiteboard and analyzed by argument.\n\nHowever, when a scenario doesn't easily fit into modern AI algorithms, simplifying the representation of the problem down to where it runs on modern computers can also destroy the core or interesting part of the problem.  \"Will this [utility_indifference] setup cause a shutdown button to be preserved even as the AI modifies its own code?\"  The flaw with the original [utility_indifference] proposal turned out to be that the AI would act in all ways as if the shutdown button as having probability zero of being pressed, and hence, while it wouldn't try to prevent the button being pressed, it would also probably prune away the \"dead code\" for the shutdown button the next time it executed a self-modification.  There have been some preliminary attempts to model shutdown-button setups (agents that do or don't try to prevent other agents from interfering) using observable running code, but that particular failure mode would not have come close to showing up in any of those setups.  Early AI may have been hurt by a cultural insistence that only running code counted (opposed to another cultural tendency to do unbounded analysis that assumed away everything important).  Relative to the computers and algorithms available in, say, 1970, insisting on running code meant that everything had to be insanely simplified in order to be spoken about at all. \n\nModern AI algorithms are better than in 1970, but there's still no easy or natural expression for a number of important-seeming scenarios about [advanced_agent sufficiently advanced AIs] that use [consequentialist] reasoning to model the real world, their programmers, and their own source code.  We often can't naturally, faithfully, realistically reproduce these scenarios using current algorithms; in some cases, the key issue is only supposed to show up when the AI is smarter than us.  Trying to simplify those scenarios down to where toy models of them can be run as modern code, encounters some of the same pitfalls as trying to do unbounded analysis of them.\n\nSimplifying for running code and simplifying for an unbounded analysis aren't mutually exclusive.  Sometimes (as with utility indifference and shutdown buttons) there's different groups pursuing both approaches at once.  (And of course the goal of any unbounded analysis is to reintroduce the complications as understanding improves, including realistic limits on computing power, until there are algorithms that can actually be run.  This process almost always requires new basic insights and may take 47 years (or not), but it remains the goal.)\n\n# Benefits of simplification\n\nThe benefit of doing an unbounded analysis is often also straightforward: it lets you expose a core conceptual point in a setting simple enough that you can think about it successfully.  (This also has a corrupted version where you write down some simple formulas and congratulate yourself for having illustrated some key conceptual point that is obvious, useless, or wrong.  A key sign that this corrupted version is *not* happening is when you can pose a problem that has no known unbounded solution, and it hangs around for a while being confusing, and writing a paper on it isn't easy.)  \n\n# Unbounded analysis in computer science\n\n[todo: you can also obtain confident false beliefs from running code, not just theoretical analyses]\n\nAs background for the state of mind in modern computer science, it should be remembered that individual computer scientists may sometimes be tempted to exaggerate how much their algorithm solves or how realistic it is - while not everyone gives into that temptation, at least some computer scientists do so at least some of the time.  This means that there are a number of 'unbounded' analyses floating around which don't solve the real-world problem, may not even shed very much light on the real-world problem, and whose authors praise them as nearly complete solutions but for some drudge-work of mere computation.  A reasonable reaction is to be *suspicious* of unbounded analyses, and scrutinize them closely to make sure that they haven't assumed away the conceptual core of the problem - if you try to model image recognition and you assume that all the images are 2 pixels by 2 pixels, you haven't simplified the problem, you've eliminated it and shed no useful light for people trying to solve the realistic problem.\n\nThis is a real problem.  At MIRI, we could indeed, if asked in private, point to several researchers in near-neighboring fields who we think exaggerated what their unbounded algorithms did - e.g., \"finds the fastest provably correct algorithm after longer than the age of the universe\" becomes \"finds the fastest algorithm\" in the researcher's informal description.  Or when the entire interesting core question is what to do about events that cross the boundary between the AI's interior and exterior, like dropping an anvil on your own head, this event is designated using a particular Greek letter and it's assumed that its expected utility is already known.  In this case, formalization hasn't shed light on the core conceptual confusion, but just swept that conceptual confusion under a rug.\n\nAgain, as a matter of cultural background in computer science, you can see some researchers trying to play up the loftier, more mathematical nature of conceptual analyses as showing off more intellectual power.  And you can find a reaction to that particular game in the form of computer scientists who say that all the *real* work, all the *useful* work, is done with *real computers* and algorithms that run well *today* in the messy real world.  This isn't entirely wrong either, and it's truer in some parts of computer science than others - we are well past the point where it makes sense to analyze a robotic car using anything resembling first-order logic; with good realistic algorithms that do all the key work, the time for unrealistic algorithms may well have passed.\n\nIf, on the other hand, you're in a part of computer science that's still living in the Edgar Allen Poe era of \"Literally nobody knows how to write down a program that solves the problem *even given* unlimited computing power and every other possible simplifying assumption that doesn't eliminate the problem entirely*\", then you are perhaps *not* in the sort of realm where the mathematicians have nothing left to do - or so [MIRI MIRI] would argue, at any rate.  It would be even better if we had bounded solutions, to be sure, but to criticize confusion-reducing work on the grounds that it fails to solve the entire and realistic problem would be the [nirvana fallacy](http://en.wikipedia.org/wiki/Nirvana_fallacy) when no more properly realistic solution as yet exists.\n\nThis doesn't mean that doing unbounded analysis will be pitfall-free, or that you can make *any possible* simplifying assumption without destroying the core of the problem.  The whole point of the [vinge_reflection Vingean reflection program] and [tiling_agents tiling agents theory] is that, for example, we should not assume that agents can exactly model their own successors or self-modifications - even if we say that we can assume unlimited computing power, we should still assume that the future version of the agent has more computing power than the past version (or has seen new sensory data) and therefore can't be predicted *exactly* in advance.  The interesting problem is how to obtain abstract trust in an agent under conditions of [vinge_uncertainty Vingean uncertainty] where we can't know *exactly* what a smarter agent will do because to know *exactly* what it would do we would need to be at least that smart ourselves.  To make the 'unbounded' assumption that we could exactly predict what that agent will do, and judge exactly what consequence would follow, would assume away the central confusing core of the problem.  But allowing for very large computers bigger than the universe, where the other agent has an even bigger computer, does *not* assume away this confusing core; or at least, so long as we don't know how to solve even this unbounded form of the problem, it's safe to say that this problem formulation exposes *some* key confusion.\n\nGiven the modern social landscape of computer science, it does happen that debates about appropriate levels of abstraction can take on the heated, personal nature of, say, Republican-Democratic national politics in the United States.  For someone who has taken on the identity of opposing the folly of over-abstraction, there will be a tempting ad-hominem available to depict those airy folk who only write Greek letters and never try their ideas in running code, as being ignorant of the pitfalls of Greek letters, ignorant of the advantages of running code, believing that all realistic programs are mere drudge-work, etcetera.  And while not everyone gives into the temptation of that ad-hominem, some people do, just like those who give into the temptation to exaggerate the power of unbounded analyses, and there is a well-known selection effect where debates can be dominated by the speaking volume of those who give in the most to such temptations.\n\nUnderstanding the pitfalls of Greek letters and the benefits of running code is certainly part of being good at unbounded analysis and doing the right kind of unbounded analysis.  It was *great* when we realized that [PatrickLaVictoire]'s [ modal agents] formalism could be run in quadratic time and that we could just write down the agents and run them.  It's *great* when you can give a lecture and take audience suggestions for new modal agents during Q&A and compete them against each other as fast as you can type them.  It's helpful to further theoretical development for all the obvious reasons: it's easier to make a mistake on a whiteboard than in a computer, and it can be just *faster* to try ideas as rapidly as you can type them into a prompt, gaining an inexpensive concrete intuition for the field.\n\nThat said, as fun as it was to do that with modal agents, and as much as we'd love to be able to do it for everything (except [Clippy paperclip maximizers], we'd rather NOT have running code of those), there's a lot of places we can't do that yet.  And even when life isn't that convenient, it's sometimes possible to make progress anyway.  We'd never have gotten to the point of running code for modal agents in the first place, if we hadn't spent years before then pushing on the whiteboard stages of [logical_dt logical decision theory].\n\nAnd this doesn't seem like a very unusual state of affairs for computer science as a whole, either.  The historical reality is that there often is a lot of whiteboard scribbling that comes first - not always, but sometimes, and not nearly rarely enough for it to be unusual.\n\nEven so, in technical work on value alignment, the *relative proportion* of whiteboard scribbling to running code is unusual for modern computer science.  This doesn't happen randomly or because people who work on value alignment are afraid of computers (yes, there's researchers in the field who've worked with real-world machine learning code, etcetera); it happens because there's systematic reasons pushing in that direction.\n\nFour of those reasons were briefly summarized above, and five of them will be considered at greater length below.\n\n# Reasons for the prevalence of unbounded analysis in value alignment theory\n\n## Unbounded analysis is often the most practical way to expose and solve conceptual confusions.\n\n(writing in progress)\n\n\nThat said, if a version of Vingean reflection using unrealistic logical agents in unrealistic logical worlds *does* seem to expose a core problem that we don't know how to solve using unlimited computing power, it may make sense to ask, explicitly, \"Okay, how *would* I solve the simplest version of this scenario that seems to expose the core problem?\" rather than launching right into adding all the complications of probabilistic reasoning, realistic computational bounds, action in realtime, etcetera.\n\n\n\n\nSimilarly, in modern AI and especially in [value_alignment value alignment theory], there's a sharp divide between problems we know how to solve using unlimited computing power, and problems which are confusing enough that we can't even state the simple program that would solve them given a larger-than-the-universe computer.  It is an alarming aspect of the current state of affairs that we [know how to build a non-value-aligned hostile AI using unbounded computing power](AIXI) but not how to build a nice AI using unlimited computing power.  The unbounded analysis program in value alignment theory centers on crossing this gap.\n\nBesides the role of mathematization in producing conceptual understanding, it also helps build cooperative discourse.  Marcus Hutter's [AIXI] marks not only the first time that somebody described a short Python program that would be a strong AI if run on a hypercomputer, but also the first time that anyone was able to point to a formal specification and say \"And that is why *this* AI design would wipe out the human species and turn all matter within its reach into configurations of insignificant [value_alignment_value value]\" and the reply wasn't just \"Oh, well, of course *that's* not what I meant\" because the specification of AIXI was fully nailed down.   While not every key issue in [value_alignment value alignment theory] is formalizable, there's a strong drive toward formalism not just for conceptual clarity but also to sustain a cooperative process of building up collective debate and understanding of ideas.\n\n\n\n\n\nA facile way of putting it would be that we're often trying to consider phenomena that *just don't happen* in weak modern AI algorithms, but seem like they should predictably happen with more advanced algorithms later, and we can try to model those future phenomena by introducing unlimited computing power.  A more precise and still facile way of putting it would be that *philanthropic* work in this sector often consists in trying to identify problems that seem unlikely to be solved in the course of addressing immediate, commercial pressures to get today's AI running today.  If you can exhibit a problem in running code of today, or if you expect it to appear in running code of 10 years later but still before AIs are smart enough to produce [pivotal pivotal catastrophes], then it's likely to be the kind of problem that enormous well-funded commercial projects will solve incidentally in the course of pursuing their own immediate incentives.  The dangerous problems are those that spring up for the first time with very smart AIs, or the kind of problem where early cheap patches fail in very smart AIs - they're dangerous precisely because immediate incentives don't force people to solve them unless they've used [foreseeable_difficulties foresight] and seen the bullet coming before any bullet hits.  A lot of the pressure towards running code comes from the social pressure to deliver immediate benefits and the epistemic desirability of obtaining lots of definite observations; but where current running code seems sufficient to expose and solve a problem in value alignment theory, that's probably not where a philanthropic group should be focusing its own work.\n\n]\n\n\n\n[todo:\n- more detailed history of Shannon and Poe\n- (diagnosis) nirvana fallacy applied to unbounded analyses, past exaggerations leading to widespread disrespect\n- unbounded analysis makes claims precise enough to be critiqued and enables the discourse to progress\n- AIXI is the central example of an unbounded agent, and often also demarcates the boundary between 'straightforward' and 'confusing' problems in modern AI\n- bounded agent assumptions still hold in real life and should clearly be marked as being violated\n  - in particular we still need to be wary of 'unbounded analyses' that avert the central problem\n  - in some cases advanced agent properties will start to predictably or possibly cross some of those lines (a superintelligence is much more likely to find a strategy, even in a large search space)\n- you still don't get to assume that the agent can identify arbitrary nice things unless you know how to make a Python program run on a large-enough computer output those nice things\n  - (this is what blocks the 'unbounded analysis' of \"Oh, it's really smart, it'll know what we mean\")]\n\n[todo:\n- why we need basic theoretical understanding\n - because it's hard to do FAI otherwise\n - because our FAI concepts need to anchor in basic things that might stay constant under self-modification, not particular programmatic tricks that will evaporate like snow in winter]",
      "metaText": "",
      "isTextLoaded": true,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 2,
      "maintainerCount": 1,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": 0,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 21,
      "redLinkCount": 0,
      "lockedBy": "8js",
      "lockedUntil": "2017-07-20 19:04:57",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": {
        "edit": {
          "has": false,
          "reason": "You don't have domain permission to edit this page"
        },
        "proposeEdit": {
          "has": true,
          "reason": ""
        },
        "delete": {
          "has": false,
          "reason": "You don't have domain permission to delete this page"
        },
        "comment": {
          "has": false,
          "reason": "You can't comment in this domain because you are not a member"
        },
        "proposeComment": {
          "has": true,
          "reason": ""
        }
      },
      "summaries": {},
      "creatorIds": [],
      "childIds": [
        "11v",
        "11w",
        "1mk",
        "1mm",
        "1n1",
        "38r",
        "4lx"
      ],
      "parentIds": [
        "2l"
      ],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [
        "4v"
      ],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "14006",
          "pageId": "107",
          "userId": "2",
          "edit": 0,
          "type": "newChild",
          "createdAt": "2016-06-19 19:35:11",
          "auxPageId": "4lx",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "9332",
          "pageId": "107",
          "userId": "2",
          "edit": 0,
          "type": "deleteChild",
          "createdAt": "2016-04-18 19:12:04",
          "auxPageId": "38x",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "7150",
          "pageId": "107",
          "userId": "2",
          "edit": 0,
          "type": "deleteChild",
          "createdAt": "2016-02-16 05:35:01",
          "auxPageId": "207",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "5475",
          "pageId": "107",
          "userId": "2",
          "edit": 20,
          "type": "newEdit",
          "createdAt": "2016-01-20 01:05:11",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "5474",
          "pageId": "107",
          "userId": "2",
          "edit": 19,
          "type": "newEdit",
          "createdAt": "2016-01-20 01:04:38",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "5473",
          "pageId": "107",
          "userId": "2",
          "edit": 18,
          "type": "newEdit",
          "createdAt": "2016-01-20 01:03:42",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "5470",
          "pageId": "107",
          "userId": "2",
          "edit": 16,
          "type": "newEdit",
          "createdAt": "2016-01-20 00:46:59",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "5469",
          "pageId": "107",
          "userId": "2",
          "edit": 15,
          "type": "newEdit",
          "createdAt": "2016-01-20 00:45:55",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "5463",
          "pageId": "107",
          "userId": "2",
          "edit": 0,
          "type": "deleteChild",
          "createdAt": "2016-01-20 00:31:44",
          "auxPageId": "1mt",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "5460",
          "pageId": "107",
          "userId": "2",
          "edit": 13,
          "type": "newChild",
          "createdAt": "2016-01-20 00:10:54",
          "auxPageId": "1n1",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "5444",
          "pageId": "107",
          "userId": "2",
          "edit": 13,
          "type": "newChild",
          "createdAt": "2016-01-19 05:17:06",
          "auxPageId": "1mt",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "5425",
          "pageId": "107",
          "userId": "32",
          "edit": 13,
          "type": "newEdit",
          "createdAt": "2016-01-17 16:48:01",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "5424",
          "pageId": "107",
          "userId": "2",
          "edit": 12,
          "type": "newEdit",
          "createdAt": "2016-01-17 06:05:14",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "5418",
          "pageId": "107",
          "userId": "2",
          "edit": 11,
          "type": "newEdit",
          "createdAt": "2016-01-17 04:48:34",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "5401",
          "pageId": "107",
          "userId": "2",
          "edit": 10,
          "type": "newChild",
          "createdAt": "2016-01-17 01:20:04",
          "auxPageId": "1mm",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "5351",
          "pageId": "107",
          "userId": "2",
          "edit": 10,
          "type": "newChild",
          "createdAt": "2016-01-16 22:41:22",
          "auxPageId": "1mk",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "5049",
          "pageId": "107",
          "userId": "2",
          "edit": 10,
          "type": "newEdit",
          "createdAt": "2016-01-06 21:51:12",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "4266",
          "pageId": "107",
          "userId": "2",
          "edit": 9,
          "type": "newEdit",
          "createdAt": "2015-12-23 03:25:11",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "4265",
          "pageId": "107",
          "userId": "2",
          "edit": 8,
          "type": "newEdit",
          "createdAt": "2015-12-23 03:22:25",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "4137",
          "pageId": "107",
          "userId": "2",
          "edit": 7,
          "type": "newEdit",
          "createdAt": "2015-12-17 23:05:29",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "3892",
          "pageId": "107",
          "userId": "1",
          "edit": 6,
          "type": "newEdit",
          "createdAt": "2015-12-16 06:12:22",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "3891",
          "pageId": "107",
          "userId": "1",
          "edit": 0,
          "type": "newAlias",
          "createdAt": "2015-12-16 06:12:21",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "1115",
          "pageId": "107",
          "userId": "1",
          "edit": 1,
          "type": "newUsedAsTag",
          "createdAt": "2015-10-28 03:47:09",
          "auxPageId": "4v",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "581",
          "pageId": "107",
          "userId": "1",
          "edit": 1,
          "type": "newChild",
          "createdAt": "2015-10-28 03:46:58",
          "auxPageId": "11w",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "583",
          "pageId": "107",
          "userId": "1",
          "edit": 1,
          "type": "newChild",
          "createdAt": "2015-10-28 03:46:58",
          "auxPageId": "207",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "584",
          "pageId": "107",
          "userId": "1",
          "edit": 1,
          "type": "newChild",
          "createdAt": "2015-10-28 03:46:58",
          "auxPageId": "11v",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "419",
          "pageId": "107",
          "userId": "1",
          "edit": 1,
          "type": "newParent",
          "createdAt": "2015-10-28 03:46:51",
          "auxPageId": "2l",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "1363",
          "pageId": "107",
          "userId": "2",
          "edit": 5,
          "type": "newEdit",
          "createdAt": "2015-08-11 20:04:55",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "1362",
          "pageId": "107",
          "userId": "2",
          "edit": 4,
          "type": "newEdit",
          "createdAt": "2015-08-11 20:04:35",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "1361",
          "pageId": "107",
          "userId": "2",
          "edit": 3,
          "type": "newEdit",
          "createdAt": "2015-08-03 18:48:13",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "1360",
          "pageId": "107",
          "userId": "2",
          "edit": 2,
          "type": "newEdit",
          "createdAt": "2015-08-03 18:46:15",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "1359",
          "pageId": "107",
          "userId": "2",
          "edit": 1,
          "type": "newEdit",
          "createdAt": "2015-07-14 20:36:44",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        }
      ],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": true,
      "hasParents": true,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "178": {
      "likeableId": "202",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "178",
      "edit": 6,
      "editSummary": "",
      "prevEdit": 5,
      "currentEdit": 6,
      "wasPublished": true,
      "type": "wiki",
      "title": "Arbital \"tag\" relationship",
      "clickbait": "Tags are a way to connect pages that share a common topic.",
      "textLength": 2689,
      "alias": "Arbital_tag",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "1",
      "editCreatedAt": "2016-05-11 15:44:58",
      "pageCreatorId": "1",
      "pageCreatedAt": "2015-11-15 15:31:40",
      "seeDomainId": "0",
      "editDomainId": "3",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 0,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 91,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": 0,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "185": {
      "likeableId": "0",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "185",
      "edit": 0,
      "editSummary": "",
      "prevEdit": 0,
      "currentEdit": 0,
      "wasPublished": false,
      "type": "",
      "title": "",
      "clickbait": "",
      "textLength": 0,
      "alias": "",
      "externalUrl": "",
      "sortChildrenBy": "",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "",
      "editCreatedAt": "",
      "pageCreatorId": "",
      "pageCreatedAt": "",
      "seeDomainId": "",
      "editDomainId": "",
      "submitToDomainId": "",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": false,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 0,
      "isEditorComment": false,
      "isApprovedComment": false,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 0,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": 0,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "187": {
      "likeableId": "0",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "187",
      "edit": 0,
      "editSummary": "",
      "prevEdit": 0,
      "currentEdit": 0,
      "wasPublished": false,
      "type": "",
      "title": "",
      "clickbait": "",
      "textLength": 0,
      "alias": "",
      "externalUrl": "",
      "sortChildrenBy": "",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "",
      "editCreatedAt": "",
      "pageCreatorId": "",
      "pageCreatedAt": "",
      "seeDomainId": "",
      "editDomainId": "",
      "submitToDomainId": "",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": false,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 0,
      "isEditorComment": false,
      "isApprovedComment": false,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 0,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": 0,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "198": {
      "likeableId": "266",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "198",
      "edit": 4,
      "editSummary": "",
      "prevEdit": 3,
      "currentEdit": 4,
      "wasPublished": true,
      "type": "wiki",
      "title": "Team Arbital",
      "clickbait": "The people behind Arbital",
      "textLength": 184,
      "alias": "TeamArbital",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "1",
      "editCreatedAt": "2016-06-17 16:55:46",
      "pageCreatorId": "1",
      "pageCreatedAt": "2015-12-13 23:14:48",
      "seeDomainId": "0",
      "editDomainId": "8",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 0,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 1182,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": 0,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "207": {
      "likeableId": "946",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 1,
      "dislikeCount": 0,
      "likeScore": 1,
      "individualLikes": [],
      "pageId": "207",
      "edit": 10,
      "editSummary": "",
      "prevEdit": 9,
      "currentEdit": 10,
      "wasPublished": true,
      "type": "wiki",
      "title": "Path: Multiple angles on Bayes's Rule",
      "clickbait": "A learning-path placeholder page for learning multiple angles on Bayes's Rule.",
      "textLength": 392,
      "alias": "bayes_rule_details",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "32",
      "editCreatedAt": "2016-07-10 23:46:05",
      "pageCreatorId": "2",
      "pageCreatedAt": "2016-02-16 05:38:04",
      "seeDomainId": "0",
      "editDomainId": "1",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 1,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 1510,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": 0,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "370": {
      "likeableId": "2144",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "370",
      "edit": 4,
      "editSummary": "reflecting the fact that we only have one type of mark now.",
      "prevEdit": 3,
      "currentEdit": 4,
      "wasPublished": true,
      "type": "wiki",
      "title": "Arbital mark",
      "clickbait": "What is a mark on Arbital? When is it created? Why is it important?",
      "textLength": 1724,
      "alias": "arbital_mark",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "1yq",
      "editCreatedAt": "2016-07-22 00:08:32",
      "pageCreatorId": "1",
      "pageCreatedAt": "2016-04-14 23:12:16",
      "seeDomainId": "0",
      "editDomainId": "3",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 0,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 54,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": 0,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "595": {
      "likeableId": "0",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "595",
      "edit": 3,
      "editSummary": "",
      "prevEdit": 2,
      "currentEdit": 3,
      "wasPublished": true,
      "type": "wiki",
      "title": "Arbital page alias",
      "clickbait": "",
      "textLength": 1215,
      "alias": "arbital_alias",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "1yq",
      "editCreatedAt": "2016-07-21 23:06:57",
      "pageCreatorId": "5",
      "pageCreatedAt": "2016-07-10 00:52:28",
      "seeDomainId": "0",
      "editDomainId": "3",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 0,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 44,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": 0,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "596": {
      "likeableId": "0",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "596",
      "edit": 1,
      "editSummary": "",
      "prevEdit": 0,
      "currentEdit": 1,
      "wasPublished": true,
      "type": "wiki",
      "title": "Arbital page title",
      "clickbait": "",
      "textLength": 738,
      "alias": "Arbital_title",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "5",
      "editCreatedAt": "2016-07-10 01:18:37",
      "pageCreatorId": "5",
      "pageCreatedAt": "2016-07-10 01:18:37",
      "seeDomainId": "0",
      "editDomainId": "3",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 0,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 31,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": 0,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "597": {
      "likeableId": "3067",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "597",
      "edit": 2,
      "editSummary": "added clickbait",
      "prevEdit": 1,
      "currentEdit": 2,
      "wasPublished": true,
      "type": "wiki",
      "title": "Arbital page clickbait",
      "clickbait": "The text you are reading right now is clickbait.",
      "textLength": 1128,
      "alias": "Arbital_clickbait",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "1yq",
      "editCreatedAt": "2016-08-05 17:48:01",
      "pageCreatorId": "5",
      "pageCreatedAt": "2016-07-10 01:24:23",
      "seeDomainId": "0",
      "editDomainId": "3",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 0,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 41,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": 0,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "10g": {
      "likeableId": "17",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "10g",
      "edit": 20,
      "editSummary": "",
      "prevEdit": 19,
      "currentEdit": 20,
      "wasPublished": true,
      "type": "wiki",
      "title": "Instrumental convergence",
      "clickbait": "Some strategies can help achieve most possible simple goals.  E.g., acquiring more computing power or more material resources.  By default, unless averted, we can expect advanced AIs to do that.",
      "textLength": 38257,
      "alias": "instrumental_convergence",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "2",
      "editCreatedAt": "2017-04-10 06:12:52",
      "pageCreatorId": "2",
      "pageCreatedAt": "2015-07-16 02:02:55",
      "seeDomainId": "0",
      "editDomainId": "2",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 19,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 2583,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0
      ],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": -2,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "11v": {
      "likeableId": "55",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 3,
      "dislikeCount": 0,
      "likeScore": 3,
      "individualLikes": [],
      "pageId": "11v",
      "edit": 10,
      "editSummary": "",
      "prevEdit": 9,
      "currentEdit": 10,
      "wasPublished": true,
      "type": "wiki",
      "title": "AIXI",
      "clickbait": "How to build an (evil) superintelligent AI using unlimited computing power and one page of Python code.",
      "textLength": 3093,
      "alias": "AIXI",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "5",
      "editCreatedAt": "2016-03-31 21:07:12",
      "pageCreatorId": "2",
      "pageCreatedAt": "2015-08-04 20:08:59",
      "seeDomainId": "0",
      "editDomainId": "2",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 6,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 932,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0
      ],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": -2,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "11w": {
      "likeableId": "56",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 3,
      "dislikeCount": 0,
      "likeScore": 3,
      "individualLikes": [],
      "pageId": "11w",
      "edit": 4,
      "editSummary": "",
      "prevEdit": 3,
      "currentEdit": 4,
      "wasPublished": true,
      "type": "wiki",
      "title": "Solomonoff induction",
      "clickbait": "A simple way to superintelligently predict sequences of data, given unlimited computing power.",
      "textLength": 1697,
      "alias": "solomonoff_induction",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "2",
      "editCreatedAt": "2015-12-30 04:05:19",
      "pageCreatorId": "2",
      "pageCreatedAt": "2015-08-05 04:08:11",
      "seeDomainId": "0",
      "editDomainId": "1",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 0,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 2026,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": 0,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "15w": {
      "likeableId": "164",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "15w",
      "edit": 5,
      "editSummary": "",
      "prevEdit": 4,
      "currentEdit": 5,
      "wasPublished": true,
      "type": "wiki",
      "title": "Machine Intelligence Research Institute",
      "clickbait": "Where to work if you're doing more formal or technical work on AI safety, of a kind not easily milked for publications.",
      "textLength": 799,
      "alias": "MIRI",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "2",
      "editCreatedAt": "2015-12-23 21:16:07",
      "pageCreatorId": "1",
      "pageCreatedAt": "2015-10-26 22:59:19",
      "seeDomainId": "0",
      "editDomainId": "7",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 0,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 197,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0
      ],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": -2,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "17b": {
      "likeableId": "204",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "17b",
      "edit": 16,
      "editSummary": "",
      "prevEdit": 15,
      "currentEdit": 16,
      "wasPublished": true,
      "type": "wiki",
      "title": "Arbital lens",
      "clickbait": "A lens is a page that presents another page's content from a different angle.",
      "textLength": 7216,
      "alias": "Arbital_lens",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "1yq",
      "editCreatedAt": "2016-12-05 13:10:54",
      "pageCreatorId": "1",
      "pageCreatedAt": "2015-11-15 18:01:48",
      "seeDomainId": "0",
      "editDomainId": "3",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 0,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 670,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": 0,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "18w": {
      "likeableId": "253",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "18w",
      "edit": 36,
      "editSummary": "",
      "prevEdit": 35,
      "currentEdit": 36,
      "wasPublished": true,
      "type": "wiki",
      "title": "The rocket alignment problem",
      "clickbait": "If people talked about the problem of space travel the way they talked about AI...",
      "textLength": 28074,
      "alias": "rocket_alignment_metaphor",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "44",
      "editCreatedAt": "2018-10-08 23:31:12",
      "pageCreatorId": "2",
      "pageCreatedAt": "2015-12-04 00:55:00",
      "seeDomainId": "0",
      "editDomainId": "2",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 0,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 961,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0
      ],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": -2,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "1b7": {
      "likeableId": "295",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "1b7",
      "edit": 21,
      "editSummary": "",
      "prevEdit": 20,
      "currentEdit": 21,
      "wasPublished": true,
      "type": "wiki",
      "title": "Utility indifference",
      "clickbait": "How can we make an AI indifferent to whether we press a button that changes its goals?",
      "textLength": 27667,
      "alias": "utility_indifference",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "2",
      "editCreatedAt": "2016-07-14 18:49:39",
      "pageCreatorId": "2",
      "pageCreatedAt": "2015-12-17 20:45:44",
      "seeDomainId": "0",
      "editDomainId": "7",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 7,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 450,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0
      ],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": -2,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "1bx": {
      "likeableId": "314",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "1bx",
      "edit": 2,
      "editSummary": "",
      "prevEdit": 1,
      "currentEdit": 2,
      "wasPublished": true,
      "type": "wiki",
      "title": "Deep Blue",
      "clickbait": "The chess-playing program, built by IBM, that first won the world chess championship from Garry Kasparov in 1996.",
      "textLength": 399,
      "alias": "deep_blue",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "5",
      "editCreatedAt": "2016-06-13 00:08:09",
      "pageCreatorId": "2",
      "pageCreatedAt": "2015-12-18 22:26:09",
      "seeDomainId": "0",
      "editDomainId": "2",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 0,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 130,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0
      ],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": -2,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "1c0": {
      "likeableId": "318",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "1c0",
      "edit": 9,
      "editSummary": "",
      "prevEdit": 8,
      "currentEdit": 9,
      "wasPublished": true,
      "type": "wiki",
      "title": "Vinge's Principle",
      "clickbait": "An agent building another agent must usually approve its design without knowing the agent's exact policy choices.",
      "textLength": 2318,
      "alias": "Vinge_principle",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "2",
      "editCreatedAt": "2016-06-25 18:48:17",
      "pageCreatorId": "2",
      "pageCreatedAt": "2015-12-18 23:24:20",
      "seeDomainId": "0",
      "editDomainId": "2",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 0,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 256,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0
      ],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": -2,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "1c1": {
      "likeableId": "319",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "1c1",
      "edit": 10,
      "editSummary": "",
      "prevEdit": 9,
      "currentEdit": 10,
      "wasPublished": true,
      "type": "wiki",
      "title": "Vingean reflection",
      "clickbait": "The problem of thinking about your future self when it's smarter than you.",
      "textLength": 3262,
      "alias": "Vingean_reflection",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "2",
      "editCreatedAt": "2016-06-21 01:55:03",
      "pageCreatorId": "2",
      "pageCreatedAt": "2015-12-18 23:28:03",
      "seeDomainId": "0",
      "editDomainId": "7",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 0,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 366,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0
      ],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": -2,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "1fx": {
      "likeableId": "406",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "1fx",
      "edit": 7,
      "editSummary": "",
      "prevEdit": 6,
      "currentEdit": 7,
      "wasPublished": true,
      "type": "wiki",
      "title": "Reflective stability",
      "clickbait": "Wanting to think the way you currently think, building other agents and self-modifications that think the same way.",
      "textLength": 1681,
      "alias": "reflective_stability",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "2",
      "editCreatedAt": "2016-05-21 12:50:56",
      "pageCreatorId": "2",
      "pageCreatedAt": "2015-12-28 20:34:16",
      "seeDomainId": "0",
      "editDomainId": "2",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 2,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 461,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0
      ],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": -2,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "1ln": {
      "likeableId": "553",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "1ln",
      "edit": 6,
      "editSummary": "alias. note to self: come back and explain new requisites system.",
      "prevEdit": 5,
      "currentEdit": 6,
      "wasPublished": true,
      "type": "wiki",
      "title": "Arbital requisites",
      "clickbait": "To understand a thing you often need to understand some other things.",
      "textLength": 1210,
      "alias": "arbital_requisite",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "1yq",
      "editCreatedAt": "2016-07-19 23:24:15",
      "pageCreatorId": "1",
      "pageCreatedAt": "2016-01-11 17:09:53",
      "seeDomainId": "0",
      "editDomainId": "3",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 0,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 311,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": 0,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "1lw": {
      "likeableId": "559",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "1lw",
      "edit": 5,
      "editSummary": "added links",
      "prevEdit": 4,
      "currentEdit": 5,
      "wasPublished": true,
      "type": "wiki",
      "title": "Mathematics",
      "clickbait": "Mathematics is the study of numbers and other ideal objects that can be described by axioms.",
      "textLength": 745,
      "alias": "math",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "1yq",
      "editCreatedAt": "2016-06-22 17:49:03",
      "pageCreatorId": "2",
      "pageCreatedAt": "2016-01-15 03:02:51",
      "seeDomainId": "0",
      "editDomainId": "1",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 0,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 2283,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": 0,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "1mk": {
      "likeableId": "581",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "1mk",
      "edit": 7,
      "editSummary": "",
      "prevEdit": 6,
      "currentEdit": 7,
      "wasPublished": true,
      "type": "wiki",
      "title": "Hypercomputer",
      "clickbait": "Some formalisms demand computers larger than the limit of all finite computers",
      "textLength": 3266,
      "alias": "hypercomputer",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "2",
      "editCreatedAt": "2016-01-17 01:19:09",
      "pageCreatorId": "2",
      "pageCreatedAt": "2016-01-17 00:59:30",
      "seeDomainId": "0",
      "editDomainId": "2",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 0,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 137,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0
      ],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": -2,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "1mm": {
      "likeableId": "583",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "1mm",
      "edit": 5,
      "editSummary": "",
      "prevEdit": 4,
      "currentEdit": 5,
      "wasPublished": true,
      "type": "wiki",
      "title": "Unphysically large finite computer",
      "clickbait": "The imaginary box required to run programs that require impossibly large, but finite, amounts of computing power.",
      "textLength": 2596,
      "alias": "large_computer",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "2",
      "editCreatedAt": "2016-01-20 00:51:25",
      "pageCreatorId": "2",
      "pageCreatedAt": "2016-01-17 01:34:11",
      "seeDomainId": "0",
      "editDomainId": "2",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 0,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 124,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0
      ],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": -2,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "1mq": {
      "likeableId": "586",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "1mq",
      "edit": 1,
      "editSummary": "",
      "prevEdit": 0,
      "currentEdit": 1,
      "wasPublished": true,
      "type": "wiki",
      "title": "Tiling agents theory",
      "clickbait": "The theory of self-modifying agents that build successors that are very similar to themselves, like repeating tiles on a tesselated plane.",
      "textLength": 282,
      "alias": "tiling_agents",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "2",
      "editCreatedAt": "2016-01-17 06:03:53",
      "pageCreatorId": "2",
      "pageCreatedAt": "2016-01-17 06:03:53",
      "seeDomainId": "0",
      "editDomainId": "2",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 0,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 299,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0
      ],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": -2,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "1mt": {
      "likeableId": "589",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "1mt",
      "edit": 1,
      "editSummary": "",
      "prevEdit": 0,
      "currentEdit": 1,
      "wasPublished": true,
      "type": "wiki",
      "title": "Cartesian agent-environment boundary",
      "clickbait": "If your agent is separated from the environment by an absolute border that can only be crossed by sensory information and motor outputs, it might just be a Cartesian agent.",
      "textLength": 760,
      "alias": "cartesian_boundary",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "2",
      "editCreatedAt": "2016-01-19 05:24:18",
      "pageCreatorId": "2",
      "pageCreatedAt": "2016-01-19 05:24:18",
      "seeDomainId": "0",
      "editDomainId": "2",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 0,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 109,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0
      ],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": -2,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "1n1": {
      "likeableId": "596",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "1n1",
      "edit": 2,
      "editSummary": "",
      "prevEdit": 1,
      "currentEdit": 2,
      "wasPublished": true,
      "type": "wiki",
      "title": "Cartesian agent",
      "clickbait": "Agents separated from their environments by impermeable barriers through which only sensory information can enter and motor output can exit.",
      "textLength": 2394,
      "alias": "cartesian_agent",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "2",
      "editCreatedAt": "2016-01-20 00:37:58",
      "pageCreatorId": "2",
      "pageCreatedAt": "2016-01-20 00:31:36",
      "seeDomainId": "0",
      "editDomainId": "2",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 0,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 131,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": 0,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "1rt": {
      "likeableId": "711",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "1rt",
      "edit": 3,
      "editSummary": "",
      "prevEdit": 2,
      "currentEdit": 3,
      "wasPublished": true,
      "type": "wiki",
      "title": "Arbital path",
      "clickbait": "Arbital path is a linear sequence of pages tailored specifically to teach a given concept to a user.",
      "textLength": 2327,
      "alias": "Arbital_path",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "1",
      "editCreatedAt": "2016-05-11 20:53:18",
      "pageCreatorId": "1",
      "pageCreatedAt": "2016-01-27 16:33:23",
      "seeDomainId": "0",
      "editDomainId": "3",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 0,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 212,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": 0,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "2c": {
      "likeableId": "1282",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "2c",
      "edit": 40,
      "editSummary": "",
      "prevEdit": 39,
      "currentEdit": 40,
      "wasPublished": true,
      "type": "wiki",
      "title": "Advanced agent properties",
      "clickbait": "How smart does a machine intelligence need to be, for its niceness to become an issue?  \"Advanced\" is a broad term to cover cognitive abilities such that we'd need to start considering AI alignment.",
      "textLength": 21699,
      "alias": "advanced_agent",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "2",
      "editCreatedAt": "2017-03-25 05:59:44",
      "pageCreatorId": "2",
      "pageCreatedAt": "2015-03-24 01:31:50",
      "seeDomainId": "0",
      "editDomainId": "2",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 3,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 943,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0
      ],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": -2,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "2l": {
      "likeableId": "1505",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "2l",
      "edit": 9,
      "editSummary": "",
      "prevEdit": 20,
      "currentEdit": 9,
      "wasPublished": true,
      "type": "wiki",
      "title": "Advanced safety",
      "clickbait": "An agent is *really* safe when it has the capacity to do anything, but chooses to do what the programmer wants.",
      "textLength": 3002,
      "alias": "advanced_safety",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "1",
      "editCreatedAt": "2015-12-16 06:05:43",
      "pageCreatorId": "2",
      "pageCreatedAt": "2015-03-26 21:36:28",
      "seeDomainId": "0",
      "editDomainId": "2",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 4,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 654,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": 0,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "2v": {
      "likeableId": "1760",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "2v",
      "edit": 27,
      "editSummary": "",
      "prevEdit": 26,
      "currentEdit": 27,
      "wasPublished": true,
      "type": "wiki",
      "title": "AI alignment",
      "clickbait": "The great civilizational problem of creating artificially intelligent computer systems such that running them is a good idea.",
      "textLength": 5071,
      "alias": "ai_alignment",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "2",
      "editCreatedAt": "2017-01-27 20:32:06",
      "pageCreatorId": "2",
      "pageCreatedAt": "2015-03-26 23:12:18",
      "seeDomainId": "0",
      "editDomainId": "2",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 3,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 3794,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0
      ],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": -2,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "35z": {
      "likeableId": "0",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "35z",
      "edit": 0,
      "editSummary": "",
      "prevEdit": 0,
      "currentEdit": 0,
      "wasPublished": false,
      "type": "",
      "title": "",
      "clickbait": "",
      "textLength": 0,
      "alias": "",
      "externalUrl": "",
      "sortChildrenBy": "",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "",
      "editCreatedAt": "",
      "pageCreatorId": "",
      "pageCreatedAt": "",
      "seeDomainId": "",
      "editDomainId": "",
      "submitToDomainId": "",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": false,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 0,
      "isEditorComment": false,
      "isApprovedComment": false,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 0,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": 0,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "38r": {
      "likeableId": "2199",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 1,
      "dislikeCount": 0,
      "likeScore": 1,
      "individualLikes": [],
      "pageId": "38r",
      "edit": 1,
      "editSummary": "",
      "prevEdit": 0,
      "currentEdit": 1,
      "wasPublished": true,
      "type": "wiki",
      "title": "Mechanical Turk (example)",
      "clickbait": "The 19th-century chess-playing automaton known as the Mechanical Turk actually had a human operator inside. People at the time had interesting thoughts about the possibility of mechanical chess.",
      "textLength": 3890,
      "alias": "mechanical_turk",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "2",
      "editCreatedAt": "2016-04-17 21:12:38",
      "pageCreatorId": "2",
      "pageCreatedAt": "2016-04-17 21:12:38",
      "seeDomainId": "0",
      "editDomainId": "2",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 0,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 205,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": 0,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "38x": {
      "likeableId": "2204",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "38x",
      "edit": 2,
      "editSummary": "",
      "prevEdit": 1,
      "currentEdit": 2,
      "wasPublished": true,
      "type": "question",
      "title": "Do we need to worry about AI?",
      "clickbait": "",
      "textLength": 51,
      "alias": "38x",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "2",
      "editCreatedAt": "2016-04-19 05:34:41",
      "pageCreatorId": "2",
      "pageCreatedAt": "2016-04-18 19:18:07",
      "seeDomainId": "0",
      "editDomainId": "2",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 0,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 41,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": 0,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "3d": {
      "likeableId": "2273",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "3d",
      "edit": 33,
      "editSummary": "",
      "prevEdit": 32,
      "currentEdit": 33,
      "wasPublished": true,
      "type": "wiki",
      "title": "Arbital",
      "clickbait": "Arbital is the place for crowdsourced, intuitive math explanations.",
      "textLength": 5201,
      "alias": "Arbital",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "1",
      "editCreatedAt": "2016-08-08 16:07:52",
      "pageCreatorId": "1",
      "pageCreatedAt": "2015-03-30 22:19:47",
      "seeDomainId": "0",
      "editDomainId": "8",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 0,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 2604,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": 0,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "3hs": {
      "likeableId": "2499",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "3hs",
      "edit": 19,
      "editSummary": "added link to exemplar pages",
      "prevEdit": 18,
      "currentEdit": 19,
      "wasPublished": true,
      "type": "wiki",
      "title": "Author's guide to Arbital",
      "clickbait": "How to write intuitive, flexible content on Arbital.",
      "textLength": 4420,
      "alias": "author_guide_to_arbital",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "1yq",
      "editCreatedAt": "2016-08-08 14:32:40",
      "pageCreatorId": "1",
      "pageCreatedAt": "2016-05-10 17:55:35",
      "seeDomainId": "0",
      "editDomainId": "3",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 0,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 415,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": 0,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "3n": {
      "likeableId": "2281",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "3n",
      "edit": 7,
      "editSummary": "",
      "prevEdit": 6,
      "currentEdit": 7,
      "wasPublished": true,
      "type": "wiki",
      "title": "Arbital \"parent\" relationship",
      "clickbait": "Parent-child relationship between pages implies a strong, inseparable connection.",
      "textLength": 2510,
      "alias": "Arbital_parent_child",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "8pb",
      "editCreatedAt": "2017-09-20 13:30:49",
      "pageCreatorId": "1",
      "pageCreatedAt": "2015-04-01 19:51:44",
      "seeDomainId": "0",
      "editDomainId": "3",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 0,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 193,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": 0,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "4lx": {
      "likeableId": "2774",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 5,
      "dislikeCount": 0,
      "likeScore": 5,
      "individualLikes": [],
      "pageId": "4lx",
      "edit": 13,
      "editSummary": "",
      "prevEdit": 12,
      "currentEdit": 13,
      "wasPublished": true,
      "type": "wiki",
      "title": "No-Free-Lunch theorems are often irrelevant",
      "clickbait": "There's often a theorem proving that some problem has no optimal answer across every possible world.  But this may not matter, since the real world is a special case.  (E.g., a low-entropy universe.)",
      "textLength": 4032,
      "alias": "nofreelunch_irrelevant",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "2",
      "editCreatedAt": "2016-06-20 06:02:01",
      "pageCreatorId": "2",
      "pageCreatedAt": "2016-06-19 19:35:09",
      "seeDomainId": "0",
      "editDomainId": "2",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 0,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 741,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": 0,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "4v": {
      "likeableId": "2318",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 2,
      "dislikeCount": 0,
      "likeScore": 2,
      "individualLikes": [],
      "pageId": "4v",
      "edit": 6,
      "editSummary": "",
      "prevEdit": 5,
      "currentEdit": 6,
      "wasPublished": true,
      "type": "wiki",
      "title": "Work in progress",
      "clickbait": "This page is being actively worked on by an editor. Check with them before making major changes.",
      "textLength": 131,
      "alias": "work_in_progress_meta_tag",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "1yq",
      "editCreatedAt": "2016-07-05 22:48:12",
      "pageCreatorId": "2",
      "pageCreatedAt": "2015-04-17 01:27:41",
      "seeDomainId": "0",
      "editDomainId": "8",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 1,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 92,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": 0,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "58b": {
      "likeableId": "3654",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "58b",
      "edit": 12,
      "editSummary": "",
      "prevEdit": 11,
      "currentEdit": 12,
      "wasPublished": true,
      "type": "wiki",
      "title": "Logical decision theories",
      "clickbait": "Root page for topics on logical decision theory, with multiple intros for different audiences.",
      "textLength": 404,
      "alias": "logical_dt",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "2",
      "editCreatedAt": "2018-06-01 18:56:49",
      "pageCreatorId": "2",
      "pageCreatedAt": "2016-07-08 18:06:14",
      "seeDomainId": "0",
      "editDomainId": "15",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 0,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 3906,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0
      ],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": -2,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "58c": {
      "likeableId": "0",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "58c",
      "edit": 1,
      "editSummary": "",
      "prevEdit": 0,
      "currentEdit": 1,
      "wasPublished": true,
      "type": "wiki",
      "title": "Decision Theory",
      "clickbait": "",
      "textLength": 161,
      "alias": "DecisionTheory",
      "externalUrl": "",
      "sortChildrenBy": "alphabetical",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "1",
      "editCreatedAt": "2016-07-08 18:23:14",
      "pageCreatorId": "1",
      "pageCreatedAt": "2016-07-08 18:23:14",
      "seeDomainId": "0",
      "editDomainId": "15",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 0,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 177,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": 0,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "5s": {
      "likeableId": "2348",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "5s",
      "edit": 12,
      "editSummary": "",
      "prevEdit": 9,
      "currentEdit": 12,
      "wasPublished": true,
      "type": "wiki",
      "title": "Value alignment problem",
      "clickbait": "You want to build an advanced AI with the right values... but how?",
      "textLength": 1063,
      "alias": "value_alignment_problem",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "2",
      "editCreatedAt": "2017-02-02 00:46:41",
      "pageCreatorId": "2",
      "pageCreatedAt": "2015-05-15 11:48:47",
      "seeDomainId": "0",
      "editDomainId": "2",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 0,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 1072,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0
      ],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": -2,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "6r": {
      "likeableId": "2378",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "6r",
      "edit": 7,
      "editSummary": "",
      "prevEdit": 6,
      "currentEdit": 7,
      "wasPublished": true,
      "type": "wiki",
      "title": "Methodology of foreseeable difficulties",
      "clickbait": "Building a nice AI is likely to be hard enough, and contain enough gotchas that won't show up in the AI's early days, that we need to foresee problems coming in advance.",
      "textLength": 3190,
      "alias": "foreseeable_difficulties",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "approval",
      "votesAnonymous": false,
      "editCreatorId": "12r",
      "editCreatedAt": "2016-11-23 00:34:12",
      "pageCreatorId": "2",
      "pageCreatedAt": "2015-06-09 19:56:09",
      "seeDomainId": "0",
      "editDomainId": "2",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 3,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 292,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [
        0,
        0,
        0,
        0,
        0,
        1,
        0,
        0,
        0
      ],
      "muVoteSummary": 0,
      "voteScaling": 1,
      "currentUserVote": -2,
      "voteCount": 1,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "6y": {
      "likeableId": "2384",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "6y",
      "edit": 20,
      "editSummary": "",
      "prevEdit": 19,
      "currentEdit": 20,
      "wasPublished": true,
      "type": "wiki",
      "title": "Pivotal event",
      "clickbait": "Which types of AIs, if they work, can do things that drastically change the nature of the further game?",
      "textLength": 15266,
      "alias": "pivotal",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "2",
      "editCreatedAt": "2017-02-13 18:58:21",
      "pageCreatorId": "2",
      "pageCreatedAt": "2015-06-12 21:12:28",
      "seeDomainId": "0",
      "editDomainId": "2",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 4,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 1152,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0
      ],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": -2,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "9g": {
      "likeableId": "2450",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "9g",
      "edit": 13,
      "editSummary": "",
      "prevEdit": 12,
      "currentEdit": 13,
      "wasPublished": true,
      "type": "wiki",
      "title": "Vingean uncertainty",
      "clickbait": "You can't predict the exact actions of an agent smarter than you - so is there anything you _can_ say about them?",
      "textLength": 10842,
      "alias": "Vingean_uncertainty",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "2",
      "editCreatedAt": "2016-06-21 01:55:06",
      "pageCreatorId": "2",
      "pageCreatedAt": "2015-07-01 19:53:31",
      "seeDomainId": "0",
      "editDomainId": "2",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 1,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 383,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0
      ],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": -2,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "9h": {
      "likeableId": "2451",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "9h",
      "edit": 10,
      "editSummary": "",
      "prevEdit": 9,
      "currentEdit": 10,
      "wasPublished": true,
      "type": "wiki",
      "title": "Consequentialist cognition",
      "clickbait": "The cognitive ability to foresee the consequences of actions, prefer some outcomes to others, and output actions leading to the preferred outcomes.",
      "textLength": 13430,
      "alias": "consequentialist",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "2",
      "editCreatedAt": "2016-06-11 05:04:41",
      "pageCreatorId": "2",
      "pageCreatedAt": "2015-07-01 21:52:19",
      "seeDomainId": "0",
      "editDomainId": "2",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 0,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 265,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0
      ],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": -2,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    }
  },
  "edits": {
    "107": {
      "likeableId": "10",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "107",
      "edit": 17,
      "editSummary": "",
      "prevEdit": 16,
      "currentEdit": 20,
      "wasPublished": true,
      "type": "wiki",
      "title": "Methodology of unbounded analysis",
      "clickbait": "What we do and don't understand how to do, using unlimited computing power, is a critical distinction and important frontier.",
      "textLength": 60341,
      "alias": "unbounded_analysis",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "2",
      "editCreatedAt": "2016-01-20 01:02:44",
      "pageCreatorId": "2",
      "pageCreatedAt": "2015-07-14 20:36:44",
      "seeDomainId": "0",
      "editDomainId": "2",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": true,
      "isLiveEdit": false,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 9,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 542,
      "text": "[summary(Brief):  In modern AI and especially in [2v value alignment theory], there's a sharp divide between \"problems we know how to solve using unlimited computing power\", and \"problems we can't state how to solve using [large_computer computers larger than the universe]\". Not knowing how to do something with unlimited computing power reveals that you are *confused* about the structure of the problem.  It is presently a disturbing fact that we don't know how to solve many aspects of [value_alignment_problem value alignment] *even given* unlimited computing power - we can't state a program that would be a nice AI if we ran it on a sufficiently large computer.  \"Unbounded analysis\" tries to improve what we understand how to do in principle, in that sense.]\n\n[summary:  In modern AI and especially in [2v value alignment theory], there's a sharp divide between \"problems we know how to solve using unlimited computing power\", and \"problems we can't state how to solve using [large_computer computers larger than the universe]\". Not knowing how to do something with unlimited computing power reveals that you are *confused* about the structure of the problem.  The first paper ever written on computer chess was by Shannon in 1950, which gave in passing the unbounded algorithm for solving chess.  The 1997 program [deep_blue Deep Blue] beat the human world champion 47 years later.  In 1836, Edgar Allen Poe carefully argued that no automaton could ever play chess, since at each turn there are many possible moves by the player and opponent, but gears and wheels can only represent deterministic motions.  The year 1836 was *confused* about the nature of the cognitive problem involved in chess; by 1950, we understood the work in principle that needed to be performed; in 1997 it was finally possible to play chess better than humans do.  It is presently a disturbing fact that we don't know how to build a nice AI *even given* unlimited computing power, and there are arguments for tackling this problem specifically as such.]\n\n[summary(Technical):  Much current technical work in [2v value alignment theory] takes place against a background of unbounded computing power, and other simplifying assumptions such as perfect knowledge, environments simpler than the agent, full knowledge of other agents' code, etcetera.  Real-world agents will be bounded, have imperfect knowledge, etcetera.  There are four primary reasons for doing unbounded analysis anyway:\n\n1.  If we don't know how to solve a problem *even given* unlimited computing power, that means we're *confused about the nature of the work to be performed.*  It is often worthwhile to tackle this basic conceptual confusion in a setup that exposes the confusing part of the problem as simply as possible, and doesn't introduce further complications until the core issue is resolved.\n2.  Introducing 'realistic' complications can make it difficult to engage in cooperative discourse about which ideas have which consequences - [AIXI] was a watershed moment because it was formally specified and there was no way to say \"Oh, I didn't mean *that*\" when somebody pointed out that AIXI would try to seize control of its own reward channel.\n3.  Increasing the intelligence of an [advanced_agent advanced agent] may sometimes move its behavior closer to ideals and further from specific complications of an algorithm.  Early, stupid chess algorithms had what seemed to humans like weird idiosyncracies tied to their specific algorithms.  Modern chess programs, far beyond the human champions, can from an intuitive human perspective be seen as just making good chess moves.\n4.  Current AI algorithms are often incapable of demonstrating future phenomena that seem like they should predictably occur later, and whose interesting properties seem like they can be described using an unbounded algorithm as an example.  E.g. current AI algorithms are very far from doing free-form self-reprogramming, but this is predictably the sort of issue we might encounter later.]\n\n# Summary\n\n\"Unbounded analysis\" refers to determining the behavior of a computer program that would, to actually run, require an [large_computer unphysically large amount of computing power], or sometimes [hypercomputer hypercomputation].  If we know how to solve a problem using unlimited computing power, but not with real-world computers, then we have an \"unbounded solution\" but no \"bounded solution\".\n\nAs a central example, consider computer chess.  The first paper ever written on computer chess, by Claude Shannon in 1950, gave an *unbounded solution* for playing perfect chess by exhaustively searching the tree of all legal chess moves.  (Since a chess game draws if no pawn is moved and no piece is captured for 50 moves, this is a finite search tree.)  Shannon then passed to considering more *bounded* ways of playing imperfect chess, such as evaluating the worth of a midgame chess position by counting the balance of pieces, and searching a smaller tree up to midgame states.  It wasn't until 47 years later, in 1997, that [deep_blue Deep Blue] beat Garry Kasparov for the world championship, and there were multiple new basic insights along the way, such as alpha-beta pruning.\n\nIn 1836, there was a sensation called the [Mechanical Turk](https://en.wikipedia.org/wiki/The_Turk), allegedly a chess-playing automaton. Edgar Allen Poe, who was also an amateur magician, wrote an essay arguing that the Turk must contain a human operator hidden in the apparatus (which it did).  Besides analyzing the Turk's outward appearance to locate the hidden compartment, Poe carefully argued as to why no arrangement of wheels and gears could ever play chess in the first place, explicitly comparing the Turk to \"the calculating machine of Mr. Babbage\":\n\n> Arithmetical or algebraical calculations are, from their very nature, fixed and determinate. Certain data being given, certain results necessarily and inevitably follow [...]\n> But the case is widely different with the Chess-Player. With him there is no determinate progression. No one move in chess necessarily follows upon any one other. From no particular disposition of the men at one period of a game can we predicate their disposition at a different period [...]\n> Now even granting that the movements of the Automaton Chess-Player were in themselves determinate, they would be necessarily interrupted and disarranged by the indeterminate will of his antagonist. There is then no analogy whatever between the operations of the Chess-Player, and those of the calculating machine of Mr. Babbage [...]\n> It is quite certain that the operations of the Automaton are regulated by *mind*, and by nothing else. Indeed this matter is susceptible of a mathematical demonstration, *a priori*. \n\n(In other words:  In an algebraical problem, each step follows with the previous step of necessity, and therefore can be represented by the determinate motions of wheels and gears as in Charles Babbage's proposed computing engine.  In chess, the player's move and opponent's move don't follow with necessity from the board position, and therefore can't be represented by deterministic gears.)\n\nThis is an amazingly sophisticated remark, considering the era.  It even puts a finger on the part of chess that is computationally difficult, the combinatorial explosion of possible moves.  And it is still entirely wrong.\n\nEven if you know an unbounded solution to chess, you might still be 47 years away from a bounded solution.  But if you can't state a program that solves the problem *in principle*, you are in some sense *confused* about the nature of the cognitive work needed to solve the problem.  If you can't even solve a problem given infinite computing power, you definitely can't solve it using bounded computing power.  (Imagine Poe trying to write a chess-playing program before he'd had the insight about search trees.)\n\nWe don't presently know how to write a Python program that would be a nice AI if we ran it on a unphysically large computer.  Trying directly to cross this conceptual gap by carving off pieces of the problem and trying to devise unbounded solutions to them is \"the methodology of unbounded analysis in AI alignment theory\".\n\nSince \"bounded agent\" has come to mean in general an agent that is realistic, the term \"unbounded agent\" also sometimes refers to agents that:\n\n- Perfectly know their environments.\n- Can fully simulate their environments (the agent is larger than its environment and/or the environment is very simple).\n- Operate in turn-based, discrete time (the environment waits for the agent to compute its move).\n- [cartesian_boundary Cartesian agents] that are perfectly separated from the environment except for sensory inputs and motor outputs.\n\nThese are two importantly distinct kinds of unboundedness, and we'll refer to the above list of properties as \"unrealism\" to distinguish them here.  Sufficiently unrealistic setups are also called \"toy models\" or \"toy problems\".\n\nUnrealistic setups have disadvantages, most notably that the results, observations, and solutions may fail to generalize to realistic applications.  Correspondingly, the classic pitfall of unbounded analysis is that it's impossible to run the code, which means that certain types of conceptual and empirical errors are more likely to go uncaught (see below).\n\nThere are nonetheless several forces pushing technical work in value alignment theory toward unbounded analyses and unrealistic setups, at least for now:\n\n1.  **Attacking confusion in the simplest settings.**  If we don't know how to solve a problem given unlimited computing power, this means we're confused about the nature of the work to be performed.  It is sometimes worthwhile to tackle this conceptual confusion in a setup that tries to expose the confusing part of the problem as simply as possible.  Trying to bound the proposed solution or make it realistic can introduce a *lot* of complications into this discussion, arguably unnecessary ones.  (Deep Blue was far more complicated than Shannon's ideal chess program, and it wouldn't be doing Edgar Allen Poe any favors to show him Deep Blue's code and hide away Shannon's ideal outline.)\n2.  **Unambiguous consequences and communication.**  Introducing 'realistic' complications can make it difficult to engage in cooperative discourse about which ideas have which consequences.  ([AIXI] was a watershed moment for alignment theory because [AIXI] was formally specified and there was no way to say \"Oh, I didn't mean *that*\" when somebody pointed out that AIXI would try to seize control of its own reward channel.)\n3.  **More [advanced_agent advanced] agents might be less idiosyncratic.**  Increasing the cognitive power of an agent may sometimes move its behavior closer to ideals and further from specific complications of an algorithm.  (From the perspective of a human onlooker, early chess algorithms seemed to have weird idiosyncracies tied to their specific algorithms.  Modern chess programs can from an intuitive human perspective be seen as just making good chess moves.)\n4.  **Runnable toy models often aren't a good fit for advanced-agent scenarios.** Current AI algorithms are often not good natural fits for demonstrating certain phenomena that seem like they might predictably occur in sufficiently advanced AIs.  Usually it's a lot of work to carve off a piece of such a phenomenon and pose it as an unbounded problem.  But that can still be significantly easier to fit into an unbounded setting than into a toy model.  (All examples here are too complicated for one sentence, but see the subsection below and the discussion of [utility_indifference] and [tiling_agents].)\n\nEven to the extent that these are good reasons, standard pitfalls of unbounded analyses and unrealistic setups still apply, and some of our collective and individual precautions against them are discussed below.\n\nFor historical reasons, computer scientists are sometimes suspicious of unbounded or unrealistic analyses, and may wonder aloud if they reflect unwillingness or incapacity to do a particular kind of work associated with real-world code.  For discussion of this point see [ Why MIRI uses unbounded analysis].\n\n# Attacking confusion in the simplest settings.\n\nImagine somebody claiming that an *ideal* chess program ought to evaluate the *ideal goodness* of each move, and giving their philosophical analysis in terms of a chess agent which knows the *perfect* goodness of each move, without ever giving an effective specification of what determines the ideal goodness, but using the term $\\gamma$ to symbolize it so that the paper looks very formal.  We can imagine an early programmer sitting down to write a chess-playing program, crafting the parts that take in user-specified moves and the part that displays the chess screen, using random numbers for move goodness until they get around to writing the \"move-goodness determining module\", and then finally finding themselves being unable to complete the program; at which point they finally recognize that all the talk of \"the ideal goodness $\\gamma$ of a chess move\" wasn't an effective specification.\n\nPart of the standard virtue ethics of computer science includes an injunction to write code in order to force this kind of grad student to realize that they don't know how to effectively specify something, even if they symbolized it using a Greek letter.  But at least this kind of ineffectiveness seems to be something that some people can learn to detect without actually running the code - consider that, in our example above, the philosopher-programmer realized that they didn't know how to compute $\\gamma$ at the point where they couldn't complete that part of the program, not at a point where the program ran and failed.  Adding on all the code to take in user moves and display a chess board on the screen only added to the amount of time required to come to this realization; once they know what being unable to write code feels like, they might be able to come to the same realization *much faster* by standing in front of a whiteboard and failing to write pseudocode.\n\nAt this point, it sometimes makes sense to step back and try to say *exactly what you don't know how to solve* - try to crisply state what it is that you want an unbounded solution *for.*  Sometimes you can't even do that much, and then you may actually have to spend some time thinking 'philosophically' - the sort of stage where you talk to yourself about some mysterious ideal quantity of move-goodness and you try to pin down what its properties might be.  It's important not to operate in this stage under the delusion that your move-goodness $\\gamma$ is a well-specified concept; it's a symbol to represent something that you're confused about.  By asking for an *unbounded solution*, or even an effectively-specified *representation of the unbounded problem*, that is, asking for pseudo-code which could be run given nothing more than an unphysically large computer (but no otherwise-unspecified Oracle of Move Goodness that outputs $\\gamma$), we're trying to invoke the \"Can I write code yet?\" test on whatever philosophical reasoning we're doing.\n\nCan trying to write running code help at this stage?  Yes, depending on how easy it is to write small programs that naturally represent the structure of the problem you're confused about how to solve.  Edgar Allen Poe might have been willing to concede that he could conceive of deterministic gears that would determine whether a proposed chess move was legal or illegal, and it's possible that if he'd actually tried to build that automaton and then try to layer gears on top of that to pick out particular legal moves by whatever means, he might have started to think that maybe chess was computable after all - maybe even have hit upon a representation of search, among other possible things that gears could do, and so realized how in principle the problem could be solved.  But this adds a delay and a cost to build the automaton and try out variations of it, and complications from trying to stay within the allowed number of gears; and it's not obvious that there can't possibly be any faster way to hit upon the idea of game-tree search, say, by trying to write pseudocode or formulas on a whiteboard, thinking only about the core structure of the game.  If we ask how it is that Shannon had an easier time coming up with the unbounded solution (understanding the nature of the work to be performed) than Poe, the most obvious cause would be the intervening work by Church and Turing (among others) on the nature of computation.\n\nAnd then in other cases it's not obvious at all how you could well-represent a problem using current AI algorithms, but with enough work you can figure out how represent the problem in an unbounded setting.\n\n## The pitfall of simplying away the key, confusing part of the problem.\n\nThe [tiling_agents tiling agents problem], in the rocket alignment metaphor, is the metaphorical equivalent of trying to fire a cannonball around a perfectly spherical Earth with no atmosphere - to obtain an idea how any kind of \"stable orbit\" can work at all.  Nonmetaphorically, the given problem is to exhibit the simplest nontrivial case of [reflective_stability stable self-modification] - an agent that, given its current reasoning, *wants* to create an agent with similar properties as a successor, i.e., preserving its current goals.\n\nIn a perfect world, we'd be able to, with no risk, fire up running code for AI algorithms that reason freely about self-modification and have justified beliefs about how alternative versions of their own code will behave and the outer consequences of that behavior (the way you might imagine what would happen in the real world if you took a particular drug affecting your cognition).  But this is way, way beyond current AI algorithms to represent in any sort of natural or realistic way.\n\nA *bad* \"unbounded solution\" would be to suppose agents that could exactly simulate their successors, determine exactly what their successors would think, extrapolate the exact environmental consequences, and evaluate those.  If you suppose an exact simulation ability, you don't need to consider how the agent would reason using generalizations, abstractions, or probabilities... but this trivial solution doesn't feel like it sheds any light or gets us any closer to understanding reflective stability; that is, it feels like the *key part of the problem* has been simplified away and solving what remains was too easy and didn't help.\n\nFaced with an \"unbounded solution\" you don't like, the next step is to say crisply exactly what is wrong with it in the form of a new desideratum for your solution.  In this case, our reply would be that for Agent 1 to exactly simulate Agent 2, Agent 1 must be larger than Agent 2, and since we want to model *stable* self-modification, we can't introduce a requirement that Agent 2 be strictly weaker than Agent 1.  More generally, we apply the insight of [vinge_principle] to this observation and arrive at the desiderata of [vinge_uncertainty] and [vinge_reflection], which we also demand that an unbounded solution exhibit.\n\nThis illustrates one of the potential general failure modes of using an unbounded setup to shed conceptual light on a confusing problem - namely, when you simplify away the key confusing issue you wanted to resolve, and end up with a trivial-seeming solution that sheds no light on the original problem.  A chief sign of this is when your paper is too easy to write.  The next action is to say exactly what you simplified away, and put it in the form of a new desideratum, and try to say exactly why your best current attempts can't meet that desideratum.\n\nSo we've now further added: we want the agent to *generalize over possible exact actions and behaviors of its successor* rather than needing to know its successor's exact actions in order to approve building it.\n\nWith this new desideratum in hand, there's now another obvious unbounded model: consider deterministic agents operating in environments with known rules that reason about possible designs and the environment using first-order logic.  The agent then uses an unbounded proof search, which no current AI algorithm could tackle in reasonable time (albeit a human engineer would be able to do it with a bunch of painstaking work) to arrive at justified logical beliefs about the effect of its successor on its environment.  This is certainly still extremely unrealistic; but has this again simplified away all the interesting parts of the problem?  In this case we can definitely reply, \"No, it does expose something confusing\" since we don't in fact know how to build a tiling agent under this setup.  It may not capture all the confusing or interesting parts of the problem, but it seems to expose at least one confusion.  Even if, as seems quite possible, it's introduced some new problem that's an artifact of the logical setup and wouldn't apply to agents doing probabilistic reasoning, there's then a relatively crisp challenge of, \"Okay, show me how probabilistic reasoning resolves the problem, then.\"\n\nIt's not obvious that there's anything further to be gained by trying to create a toy model of the problem, or a toy model of the best current unsatisfactory partial solution, that could run as code with some further cheating and demo-rigging, but [this is being tried anyway](https://intelligence.org/2015/12/04/new-paper-proof-producing-reflection-for-hol/).  The tiling agents problem did spend roughly nine years exclusively on paper before that, and the best current unsatisfactory solution was arrived at with whiteboards.\n\n## The pitfall of residual terms.\n\nBesides \"simplifying away the confusing part of the problem\", another way that unbounded thinking can \"bounce off\" a confusing problem is by creating a residual term that encapsulates the confusion.  Currently, there are good unbounded specifications for [cartesian Cartesian] non-self-modifying [AIXI expected reward maximizers]: if we allow the agent to use unlimited computing power, *don't* allow the environment to have unlimited computing power, don't ask the agent to modify itself, separate the agent from its environment by an impermeable barrier through which only sensory information and motor outputs can pass, and then ask the agent to maximize a sensory reward signal, there's [AIXI a simple Python program which is expected to behave superintelligently given sufficient computing power].  If we then introduce permeability into the Cartesian boundary and allow for the possibility that the agent can take drugs or drop an anvil on its own head, nobody has an unbounded solution to that problem any more.\n\nSo one way of bouncing off that problem is to say, \"Oh, well, my agent calculates the effect of its motor actions on the environment and the expected effect on sensory information and the reward signal, *plus a residual term $\\gamma$* which stands for the expected utility of all effects of the agent's actions that change the agent's processing or destroys its hardware\".  How is $\\gamma$ to be computed?  This is left unsaid.\n\nIn this case you haven't *omitted* the confusing part of the problem, but you've packed it into a residual term you can't give an effective specification for calculating.  So you no longer have an unbounded solution - you can't write down the Python program that runs given unlimited computing power - and you've probably failed to shed any important light on the confusing part of the problem.  Again, one of the warning signs here is that the paper is very easy to write, and reading it does not make the key problem feel less like a hard opaque ball.\n\n# Introducing realistic complications can make it hard to build collective discourse about which ideas have which consequences.\n\nOne of the watershed moments in the history of AI alignment theory was Marcus Hutter's proposal for [AIXI], not just because it was the first time anyone had put together a complete specification of an unbounded agent (in a [cartesian Cartesian setting]), but also because it was the first time that non-value-alignment could be pointed out in a completely pinned-down way.  \n\n(work in progress)\n\n[todo: turn the rest of this into a finished doc, or delete it.\n\nTo the extent that \"standing in front of a whiteboard trying to think of pseudocode\" does work better to clear up conceptual confusion, it will be because this stage of the problem could iterate through more possible pieces of pseudocode, in a context that exposed the underlying problem as simply as possible without any added complications from demanding bounded solutions or realism.\n\n\nOnce you realize you currently can't write the code; what's the next step?  Depending on how easy it is to build small programs that naturally represent the problem setup - and in the case of chess, this shouldn't be hard, the hard part is *generating good moves*, not representing the game of chess in running code - then it might make sense to take exploratory stabs at trying to compute something that makes the game play good chess.\n\nWhat if you can't even represent the problem setup naturally in running code?  Then \n\nWhat about after you realize that you currently can't write code?  \n\nDoing unbounded analysis implies that you think your current state of understanding has basic confusions to be cleared up.  If at this point somebody tried to write a paper about an algorithm that would, given hypercomputation, pilot a robotic car on an ideal 2-dimensional surface, it would be a legitimate reply to say \"But we have actual running code for robotic cars, nor are we confused about how to do this; why do an unbounded analysis when we already have bounded ones?\"  In terms of the [18w rocket alignment metaphor], spending a lot of time trying to figure out how to fire a cannonball so that it circles a perfectly spherical Earth and returns to its exact starting point, corresponds to a suspicion that you currently lack the mathematical language to even talk about landing a rocket on the Moon.  Someone who expects that current vehicles only require one or two tweaks to get to the Moon, or that they can be piloted there by keeping the Moon in the viewport and steering intuitively, is more likely to object \"Why are you talking about cannonballs, instead of the vehicles that reach the highest altitudes today?\"  The closer you think current algorithms are to supporting a Friendly AI, the less sympathetic you might be to the suggestion that very basic foundational work needs to be done and might need to be done in a simplified setting.\n\nOr imagine, say, somebody claiming that an *ideal* chess program ought to evaluate the *ideal goodness* of each move, and giving their philosophical analysis in terms of a chess agent which knows the *perfect* goodness of each move, without ever giving an effective specification of what determines the ideal goodness, but using the term $\\gamma$ to symbolize it so that the paper looks very formal.  We can imagine an early programmer sitting down to write a chess-playing program, crafting the parts that take in user-specified moves and the part that displays the chess screen, using random numbers for move goodness until they get around to writing the \"move-goodness determining module\", and then finally finding themselves being unable to complete the program; at which point they finally recognize that all the talk of \"the ideal goodness $\\gamma$ of a chess move\" wasn't an effective specification.\n\n Sitting down to write code can hopefully force you to recognize that your analysis is still confused, that to talk about \"ideal goodness of a chess move\" is not yet an *effective specification* of an ideal chessplayer, because you don't know how to compute the ideal goodness of a move.\n\n\n\nYou can also, arguably, save yourself a lot of time and code that goes nowhere by learning to recognize that you can't write down \"ideal goodness of a chess move\" as an effective formula, and discussing the problem with friends using a whiteboard.  Instead of spending a long time writing every part of a chess-playing program except the \"compute the goodness of a move\" part until finally being *forced* to recognize you can't complete program.\n\nThe obvious reason why \n\n\n\nSome early cautionary tales from the history of AI deal with toy programs (unrealistic agents) that failed to generalize well to the real world.  Even so, an unrealistic toy model is superior in some ways to an *unbounded* model, because the code can be run and the results observed.  In the absence of running code, it's possible to make outright cognitive errors about whether you've described an intuitive setup using a mathematical statement - failing to give an effective specification, or giving an effective specification that doesn't match the intuitive description.  But this doesn't mean that code is a panacea; especially when creating a \"toy model\", it's possible that the toy setup omits key properties of interest.  Depending on the person and the circumstances, a toy model might be extra work and not bring any extra enlightenment over writing down the key properties in a formula and thinking about the formula.\n\n\n\nIn the case of [value_alignment value alignment theory], much work does take place in a context of unlimited computing power and usually other unrealistic properties as well.  The subsections below deal with some of these reasons and their associated \n\n\n# Unbounded agent properties\n\nMuch of the current technical work in  [value_alignment value alignment theory] takes place on whiteboards, and tries to analyze programs or mathematical structures that could only run on [large_computer very large finite computers] or [hypercomputer hypercomputers].  \n\n\n\n\nMuch of the current technical work in [value_alignment value alignment theory] takes place against a background of simplifying assumptions such as:\n\n- Unlimited finite computing power, or even [hypercomputer hypercomputation].\n- Perfect knowledge of the environment.\n- Deterministic environments that are smaller than the agent and can be fully simulated.\n- Environments separated from the agent by an [ impermeable Cartesian boundary].\n- Turn-based discrete time (the environment waits for the agent to make a move, rather than proceeding in continuous real time).\n- Common knowledge of each other's code in multi-agent dilemmas.\n- Predefined action sets.\n\nThe core tradeoff in unbounded analysis is that these properties can make a setup (much) easier to analyze and for human beings to reason about, at the expense of realism.  Unrealism means a risk of having simplified away a key part of the problem, maybe even the only really interesting part of the problem, and of obtaining results that fail to apply to the real world.\n\n\n\n# Introduction: Claude Shannon, Deep Blue, and Edgar Allen Poe\n\nIn 1950, the very first paper ever written on computer chess, by Claude Shannon, gave in passing the algorithm that would play perfect chess given unlimited computing power, and then went on to consider more practical algorithms involving small search trees and local position evalution.  It then took an additional 47 years until [DeepBlue Deep Blue] beat Garry Kasparov for the world chess championship.  Knowing how to do something in principle doesn't always mean you can do it in practice without additional hard work and insights like, e.g., the alpha-beta pruning algorithm for chess.\n\nNonetheless, if you *don't* know how to play chess using unlimited computing power, you *definitely* can't play chess using limited computing power.\n\nIn the early 19th century, there was a minor sensation called the [Mechanical Turk](https://en.wikipedia.org/wiki/The_Turk), allegedly a chess-playing automaton.  Today, understanding the computational requirements of chess, we know immediately that there's no way to do that using a box cabinet full of gears, but in the early 19th century this wasn't as obvious.  In 1836, Edgar Allen Poe, who was also an amateur magician of some repute, wrote [an essay arguing that the Mechanical Turk was human-operated](http://www.eapoe.org/works/essays/maelzel.htm).  Poe compares the Turk to the [Duck of Vaucanson](https://en.wikipedia.org/wiki/Digesting_Duck) which quacked, moved almost naturally, and seemed to digest food, acknowledging that there have indeed been wondrous automata.  But Poe insists that the Turk cannot be one of them, and that if it were, it would be the greatest wonder ever constructed by mankind.  Poe writes:\n\n> But if these machines were ingenious, what shall we think of the calculating machine of Mr. Babbage? What shall we think of an engine of wood and metal which can not only compute astronomical and navigation tables to any given extent, but render the exactitude of its operations mathematically certain through its power of correcting its possible errors? What shall we think of a machine which can not only accomplish all this, but actually print off its elaborate results, when obtained, without the slightest intervention of the intellect of man? [...]\n> Arithmetical or algebraical calculations are, from their very nature, fixed and determinate. Certain data being given, certain results necessarily and inevitably follow [...]\n> But the case is widely different with the Chess-Player. With him there is no determinate progression. No one move in chess necessarily follows upon any one other. From no particular disposition of the men at one period of a game can we predicate their disposition at a different period [...]\n> Now even granting that the movements of the Automaton Chess-Player were in themselves determinate, they would be necessarily interrupted and disarranged by the indeterminate will of his antagonist. There is then no analogy whatever between the operations of the Chess-Player, and those of the calculating machine of Mr. Babbage [...]\n> It is quite certain that the operations of the Automaton are regulated by *mind*, and by nothing else. Indeed this matter is susceptible of a mathematical demonstration, *a priori*. \n\nIn other words:  In an algebraical problem, each step follows with the previous step of necessity, and therefore can be represented by the determinate motions of wheels and gears as in Charles Babbage's proposed computing engine; while in chess, the player's move and opponent's move don't follow with necessity from the board position, and therefore require Mind to analyze rather than deterministic gears.\n\nThis is an amazingly sophisticated remark, considering the era.  It even puts a finger on the part of chess that is computationally difficult, the combinatorial explosion of possible moves.  It's still entirely wrong.\n\n(The rest of Poe's essay is then devoted to dissecting the appearance of the Turk and arguing about where the human operator is concealed inside it.)\n\nAs much as Claude Shannon in 1950 was a long way from all the additional insights required to defeat the human champion in 1997 using limited and realistic amounts of computing power, he was also a long way ahead of Edgar Allen Poe who lacked the key insight of 'search trees' that enables a chess game to be represented in deterministic transistors.\n\nBetween Poe in 1830 and Shannon in 1950 there was a key increase in the understanding of computer chess, represented by the intermediate work of Turing, Church, and others.  Shannon's 1950 paper doesn't belabor his unbounded solution to chess that would explore the whole search tree - he seems to consider it mostly obvious, and gives it as only a preliminary to his suggestions for playing chess using far more limited calculations.  Unbounded understanding often seems easy *once you have it.*   But this leaves aside the incredible difficulty of writing a bounded program when you don't even have an unbounded one.  Inability to state the unbounded solution to your problem often indicates that you are *confused about the nature of the computational work to be performed.*  We can imagine someone who lacks the insight of search trees, trying to write a program that plays chess with no better definition of \"good chess move\" as one that leads to center control, captures pieces, sets up forks, leads to a well-defended king, attacks the opponent's king, etcetera, as if all of these considerations were equal parts of some undefined and indefinable \"move goodness\" property...\n\n\n\nThe core idea behind doing unbounded analysis in value alignment theory is that we are presently *confused* about the question \"How would you write a Python program that would implement a nice AI if you could run it on [large_computer a computer much larger than the universe]?\"\n\n\n\n\nand that there are subproblems where we can expose the confusion by trying to crisply pose an important subproblem that nobody seems to know how to solve using unbounded computing power, then trying to come up with unbounded solutions and analyzing those.  (It's important, in this case, to slice off much smaller pieces of the confusion than, \"Write a program that would be a nice AI.\")\n\n\n\n(If you think you already know the answer to this, you should probably be reading up on \"why this problem is hard\" pages rather than \n\nThe fact that nobody can give answer this question - even though there *are* unbounded solutions to most of AI and AGI (e.g. [AIXI]) - illustrates that \n\n\nIf you ask yourself how to write a Python program that would be a nice AI if we could run it on [large_computer a computer much larger than the universe], you might notice a \"stubbing the mind's toe\" feeling of trying to write a program when you're confused about the nature of the computations you need to perform.\n\nOne possible research avenue is to directly tackle that ignorance, as such, and try directly to figure out how to crisply model some aspects of the problem given unlimited computing power and other, even more dangerous simplifying assumptions.  It's an approach that has well-known pitfalls, but it's arguably among the best options we have today.  Some of the considerations pushing value alignment theory in the direction of unbounded analyses can be briefly summarized as follows:\n\n1.  If we don't know how to solve a problem *even given* unbounded computation, that means we're *confused about the nature of the work to be performed.*  It is often worthwhile to tackle this basic conceptual confusion in a setup that exposes the confusing part of the problem as simply as possible, and doesn't introduce further and distracting complications until the core issue is resolved.\n2.  Introducing 'realistic' complications can make it difficult to engage in cooperative discourse about which ideas have which consequences - [AIXI] was a watershed moment because it was formally specified and there was no way to say \"Oh, I didn't mean *that*\" when somebody pointed out that AIXI would try to seize control of its own reward channel.\n3.  Increasing the intelligence of an [advanced_agent advanced agent] may sometimes move its behavior closer to ideals and further from specific complications of an algorithm.  Early, stupid chess algorithms had what seemed to humans like weird idiosyncracies tied to their specific algorithms.  Modern chess programs, far beyond the human champions, can from an intuitive human perspective be seen as just making good chess moves.\n4.  Current AI algorithms are often incapable of demonstrating future phenomena that seem like they should predictably occur later, and whose interesting properties seem like they can be described using an unbounded algorithm as an example.  E.g. [instrumental_convergence convergent instrumental strategies] arise from [consequentialism consequentialist agents] reasoning about difficult domains like \"My programmers have beliefs about whether I'll achieve their goals\" and \"What happens if they decide to press my shutdown button?\", which is *very* far from something that naturally arises in current AI algorithms; but we can still anticipate this future issue using notions like [economic_agency economic agency], and try out unbounded problem formulations and attempted solutions like [utility_indifference].\n\n\n\n# Unbounded agent properties\n\nMuch of the current technical work in [value_alignment value alignment theory] takes place against a background of simplifying assumptions such as:\n\n- Unlimited finite computing power, or even [hypercomputer hypercomputation].\n- Perfect knowledge of the environment.\n- Deterministic environments that are smaller than the agent and can be fully simulated.\n- Environments separated from the agent by an [ impermeable Cartesian boundary].\n- Turn-based discrete time (the environment waits for the agent to make a move, rather than proceeding in continuous real time).\n- Common knowledge of each other's code in multi-agent dilemmas.\n- Predefined action sets.\n\nThe core tradeoff in unbounded analysis is that these properties can make a setup (much) easier to analyze and for human beings to reason about, at the expense of realism.  Unrealism means a risk of having simplified away a key part of the problem, maybe even the only really interesting part of the problem, and of obtaining results that fail to apply to the real world.\n\n# Danger of simplifying away an important part of the problem.\n\nThe pitfall is when an unbounded analysis simplifies away *all* the important parts of the problem and sheds no light on what remains.\n\nFor example, the point of the [vinge_reflection Vingean reflection program] and [tiling_agents tiling agents theory] is that we should not assume that agents can exactly model their own successors or self-modifications.  Even if we say that we can assume unlimited finite computing power, we should still assume that the future version of the agent has more computing power than the past version (or has seen new sensory data) and therefore the current self can't exactly predict the future self.  The core of the problem is to obtain trust in an agent under conditions of [vinge_uncertainty Vingean uncertainty] where we can't know *exactly* what a smarter agent will do because to know *exactly* what it would do we would need to be at least that smart ourselves.  To make the 'unbounded' assumption that we could exactly predict what that agent will do, and judge exactly what consequence would follow, would assume away the central confusing core of the problem.  But allowing for computers bigger than the universe, where the other agent has an *even bigger* computer, does *not* assume away this confusing core.  Or at least, so long as we don't know how to solve even this unbounded form of the problem, it's safe to say that this problem formulation exposes *some* key confusion.\n\nUnbounded analysis doesn't always make a problem easier to think about.  If you can get your algorithm to run on real computers, it becomes possible to observe that your results are correct as well as proving them, and to observe the outcomes in cases that don't have closed-form solutions.  (Though these observations may still be untrustworthy if there's a bug in the code, meaning your setup was not what you thought.)  This implies a sharp dropoff in cognitive tractability at the boundary where you can no longer run your analysis as code.  When it was realized that [PatrickLavictoire]'s [ modal decision theory] could be run in polynomial time, it was then possible to test possible modal agents against each other much more rapidly than when they needed to be written out on a whiteboard and analyzed by argument.\n\nHowever, when a scenario doesn't easily fit into modern AI algorithms, simplifying the representation of the problem down to where it runs on modern computers can also destroy the core or interesting part of the problem.  \"Will this [utility_indifference] setup cause a shutdown button to be preserved even as the AI modifies its own code?\"  The flaw with the original [utility_indifference] proposal turned out to be that the AI would act in all ways as if the shutdown button as having probability zero of being pressed, and hence, while it wouldn't try to prevent the button being pressed, it would also probably prune away the \"dead code\" for the shutdown button the next time it executed a self-modification.  There have been some preliminary attempts to model shutdown-button setups (agents that do or don't try to prevent other agents from interfering) using observable running code, but that particular failure mode would not have come close to showing up in any of those setups.  Early AI may have been hurt by a cultural insistence that only running code counted (opposed to another cultural tendency to do unbounded analysis that assumed away everything important).  Relative to the computers and algorithms available in, say, 1970, insisting on running code meant that everything had to be insanely simplified in order to be spoken about at all. \n\nModern AI algorithms are better than in 1970, but there's still no easy or natural expression for a number of important-seeming scenarios about [advanced_agent sufficiently advanced AIs] that use [consequentialist] reasoning to model the real world, their programmers, and their own source code.  We often can't naturally, faithfully, realistically reproduce these scenarios using current algorithms; in some cases, the key issue is only supposed to show up when the AI is smarter than us.  Trying to simplify those scenarios down to where toy models of them can be run as modern code, encounters some of the same pitfalls as trying to do unbounded analysis of them.\n\nSimplifying for running code and simplifying for an unbounded analysis aren't mutually exclusive.  Sometimes (as with utility indifference and shutdown buttons) there's different groups pursuing both approaches at once.  (And of course the goal of any unbounded analysis is to reintroduce the complications as understanding improves, including realistic limits on computing power, until there are algorithms that can actually be run.  This process almost always requires new basic insights and may take 47 years (or not), but it remains the goal.)\n\n# Benefits of simplification\n\nThe benefit of doing an unbounded analysis is often also straightforward: it lets you expose a core conceptual point in a setting simple enough that you can think about it successfully.  (This also has a corrupted version where you write down some simple formulas and congratulate yourself for having illustrated some key conceptual point that is obvious, useless, or wrong.  A key sign that this corrupted version is *not* happening is when you can pose a problem that has no known unbounded solution, and it hangs around for a while being confusing, and writing a paper on it isn't easy.)  \n\n# Unbounded analysis in computer science\n\n[todo: you can also obtain confident false beliefs from running code, not just theoretical analyses]\n\nAs background for the state of mind in modern computer science, it should be remembered that individual computer scientists may sometimes be tempted to exaggerate how much their algorithm solves or how realistic it is - while not everyone gives into that temptation, at least some computer scientists do so at least some of the time.  This means that there are a number of 'unbounded' analyses floating around which don't solve the real-world problem, may not even shed very much light on the real-world problem, and whose authors praise them as nearly complete solutions but for some drudge-work of mere computation.  A reasonable reaction is to be *suspicious* of unbounded analyses, and scrutinize them closely to make sure that they haven't assumed away the conceptual core of the problem - if you try to model image recognition and you assume that all the images are 2 pixels by 2 pixels, you haven't simplified the problem, you've eliminated it and shed no useful light for people trying to solve the realistic problem.\n\nThis is a real problem.  At MIRI, we could indeed, if asked in private, point to several researchers in near-neighboring fields who we think exaggerated what their unbounded algorithms did - e.g., \"finds the fastest provably correct algorithm after longer than the age of the universe\" becomes \"finds the fastest algorithm\" in the researcher's informal description.  Or when the entire interesting core question is what to do about events that cross the boundary between the AI's interior and exterior, like dropping an anvil on your own head, this event is designated using a particular Greek letter and it's assumed that its expected utility is already known.  In this case, formalization hasn't shed light on the core conceptual confusion, but just swept that conceptual confusion under a rug.\n\nAgain, as a matter of cultural background in computer science, you can see some researchers trying to play up the loftier, more mathematical nature of conceptual analyses as showing off more intellectual power.  And you can find a reaction to that particular game in the form of computer scientists who say that all the *real* work, all the *useful* work, is done with *real computers* and algorithms that run well *today* in the messy real world.  This isn't entirely wrong either, and it's truer in some parts of computer science than others - we are well past the point where it makes sense to analyze a robotic car using anything resembling first-order logic; with good realistic algorithms that do all the key work, the time for unrealistic algorithms may well have passed.\n\nIf, on the other hand, you're in a part of computer science that's still living in the Edgar Allen Poe era of \"Literally nobody knows how to write down a program that solves the problem *even given* unlimited computing power and every other possible simplifying assumption that doesn't eliminate the problem entirely*\", then you are perhaps *not* in the sort of realm where the mathematicians have nothing left to do - or so [MIRI MIRI] would argue, at any rate.  It would be even better if we had bounded solutions, to be sure, but to criticize confusion-reducing work on the grounds that it fails to solve the entire and realistic problem would be the [nirvana fallacy](http://en.wikipedia.org/wiki/Nirvana_fallacy) when no more properly realistic solution as yet exists.\n\nThis doesn't mean that doing unbounded analysis will be pitfall-free, or that you can make *any possible* simplifying assumption without destroying the core of the problem.  The whole point of the [vinge_reflection Vingean reflection program] and [tiling_agents tiling agents theory] is that, for example, we should not assume that agents can exactly model their own successors or self-modifications - even if we say that we can assume unlimited computing power, we should still assume that the future version of the agent has more computing power than the past version (or has seen new sensory data) and therefore can't be predicted *exactly* in advance.  The interesting problem is how to obtain abstract trust in an agent under conditions of [vinge_uncertainty Vingean uncertainty] where we can't know *exactly* what a smarter agent will do because to know *exactly* what it would do we would need to be at least that smart ourselves.  To make the 'unbounded' assumption that we could exactly predict what that agent will do, and judge exactly what consequence would follow, would assume away the central confusing core of the problem.  But allowing for very large computers bigger than the universe, where the other agent has an even bigger computer, does *not* assume away this confusing core; or at least, so long as we don't know how to solve even this unbounded form of the problem, it's safe to say that this problem formulation exposes *some* key confusion.\n\nGiven the modern social landscape of computer science, it does happen that debates about appropriate levels of abstraction can take on the heated, personal nature of, say, Republican-Democratic national politics in the United States.  For someone who has taken on the identity of opposing the folly of over-abstraction, there will be a tempting ad-hominem available to depict those airy folk who only write Greek letters and never try their ideas in running code, as being ignorant of the pitfalls of Greek letters, ignorant of the advantages of running code, believing that all realistic programs are mere drudge-work, etcetera.  And while not everyone gives into the temptation of that ad-hominem, some people do, just like those who give into the temptation to exaggerate the power of unbounded analyses, and there is a well-known selection effect where debates can be dominated by the speaking volume of those who give in the most to such temptations.\n\nUnderstanding the pitfalls of Greek letters and the benefits of running code is certainly part of being good at unbounded analysis and doing the right kind of unbounded analysis.  It was *great* when we realized that [PatrickLaVictoire]'s [ modal agents] formalism could be run in quadratic time and that we could just write down the agents and run them.  It's *great* when you can give a lecture and take audience suggestions for new modal agents during Q&A and compete them against each other as fast as you can type them.  It's helpful to further theoretical development for all the obvious reasons: it's easier to make a mistake on a whiteboard than in a computer, and it can be just *faster* to try ideas as rapidly as you can type them into a prompt, gaining an inexpensive concrete intuition for the field.\n\nThat said, as fun as it was to do that with modal agents, and as much as we'd love to be able to do it for everything (except [Clippy paperclip maximizers], we'd rather NOT have running code of those), there's a lot of places we can't do that yet.  And even when life isn't that convenient, it's sometimes possible to make progress anyway.  We'd never have gotten to the point of running code for modal agents in the first place, if we hadn't spent years before then pushing on the whiteboard stages of [logical_dt logical decision theory].\n\nAnd this doesn't seem like a very unusual state of affairs for computer science as a whole, either.  The historical reality is that there often is a lot of whiteboard scribbling that comes first - not always, but sometimes, and not nearly rarely enough for it to be unusual.\n\nEven so, in technical work on value alignment, the *relative proportion* of whiteboard scribbling to running code is unusual for modern computer science.  This doesn't happen randomly or because people who work on value alignment are afraid of computers (yes, there's researchers in the field who've worked with real-world machine learning code, etcetera); it happens because there's systematic reasons pushing in that direction.\n\nFour of those reasons were briefly summarized above, and five of them will be considered at greater length below.\n\n# Reasons for the prevalence of unbounded analysis in value alignment theory\n\n## Unbounded analysis is often the most practical way to expose and solve conceptual confusions.\n\n(writing in progress)\n\n\nThat said, if a version of Vingean reflection using unrealistic logical agents in unrealistic logical worlds *does* seem to expose a core problem that we don't know how to solve using unlimited computing power, it may make sense to ask, explicitly, \"Okay, how *would* I solve the simplest version of this scenario that seems to expose the core problem?\" rather than launching right into adding all the complications of probabilistic reasoning, realistic computational bounds, action in realtime, etcetera.\n\n\n\n\nSimilarly, in modern AI and especially in [value_alignment value alignment theory], there's a sharp divide between problems we know how to solve using unlimited computing power, and problems which are confusing enough that we can't even state the simple program that would solve them given a larger-than-the-universe computer.  It is an alarming aspect of the current state of affairs that we [know how to build a non-value-aligned hostile AI using unbounded computing power](AIXI) but not how to build a nice AI using unlimited computing power.  The unbounded analysis program in value alignment theory centers on crossing this gap.\n\nBesides the role of mathematization in producing conceptual understanding, it also helps build cooperative discourse.  Marcus Hutter's [AIXI] marks not only the first time that somebody described a short Python program that would be a strong AI if run on a hypercomputer, but also the first time that anyone was able to point to a formal specification and say \"And that is why *this* AI design would wipe out the human species and turn all matter within its reach into configurations of insignificant [value_alignment_value value]\" and the reply wasn't just \"Oh, well, of course *that's* not what I meant\" because the specification of AIXI was fully nailed down.   While not every key issue in [value_alignment value alignment theory] is formalizable, there's a strong drive toward formalism not just for conceptual clarity but also to sustain a cooperative process of building up collective debate and understanding of ideas.\n\n\n\n\n\nA facile way of putting it would be that we're often trying to consider phenomena that *just don't happen* in weak modern AI algorithms, but seem like they should predictably happen with more advanced algorithms later, and we can try to model those future phenomena by introducing unlimited computing power.  A more precise and still facile way of putting it would be that *philanthropic* work in this sector often consists in trying to identify problems that seem unlikely to be solved in the course of addressing immediate, commercial pressures to get today's AI running today.  If you can exhibit a problem in running code of today, or if you expect it to appear in running code of 10 years later but still before AIs are smart enough to produce [pivotal pivotal catastrophes], then it's likely to be the kind of problem that enormous well-funded commercial projects will solve incidentally in the course of pursuing their own immediate incentives.  The dangerous problems are those that spring up for the first time with very smart AIs, or the kind of problem where early cheap patches fail in very smart AIs - they're dangerous precisely because immediate incentives don't force people to solve them unless they've used [foreseeable_difficulties foresight] and seen the bullet coming before any bullet hits.  A lot of the pressure towards running code comes from the social pressure to deliver immediate benefits and the epistemic desirability of obtaining lots of definite observations; but where current running code seems sufficient to expose and solve a problem in value alignment theory, that's probably not where a philanthropic group should be focusing its own work.\n\n]\n\n\n\n[todo:\n- more detailed history of Shannon and Poe\n- (diagnosis) nirvana fallacy applied to unbounded analyses, past exaggerations leading to widespread disrespect\n- unbounded analysis makes claims precise enough to be critiqued and enables the discourse to progress\n- AIXI is the central example of an unbounded agent, and often also demarcates the boundary between 'straightforward' and 'confusing' problems in modern AI\n- bounded agent assumptions still hold in real life and should clearly be marked as being violated\n  - in particular we still need to be wary of 'unbounded analyses' that avert the central problem\n  - in some cases advanced agent properties will start to predictably or possibly cross some of those lines (a superintelligence is much more likely to find a strategy, even in a large search space)\n- you still don't get to assume that the agent can identify arbitrary nice things unless you know how to make a Python program run on a large-enough computer output those nice things\n  - (this is what blocks the 'unbounded analysis' of \"Oh, it's really smart, it'll know what we mean\")]\n\n[todo:\n- why we need basic theoretical understanding\n - because it's hard to do FAI otherwise\n - because our FAI concepts need to anchor in basic things that might stay constant under self-modification, not particular programmatic tricks that will evaporate like snow in winter]",
      "metaText": "",
      "isTextLoaded": true,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 2,
      "maintainerCount": 1,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": 0,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 21,
      "redLinkCount": 0,
      "lockedBy": "8js",
      "lockedUntil": "2017-07-20 19:04:57",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": {
        "edit": {
          "has": false,
          "reason": "You don't have domain permission to edit this page"
        },
        "proposeEdit": {
          "has": true,
          "reason": ""
        },
        "delete": {
          "has": false,
          "reason": "You don't have domain permission to delete this page"
        },
        "comment": {
          "has": false,
          "reason": "You can't comment in this domain because you are not a member"
        },
        "proposeComment": {
          "has": true,
          "reason": ""
        }
      },
      "summaries": {},
      "creatorIds": [],
      "childIds": [
        "11v",
        "11w",
        "1mk",
        "1mm",
        "1n1",
        "38r",
        "4lx"
      ],
      "parentIds": [
        "2l"
      ],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [
        "4v"
      ],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "14006",
          "pageId": "107",
          "userId": "2",
          "edit": 0,
          "type": "newChild",
          "createdAt": "2016-06-19 19:35:11",
          "auxPageId": "4lx",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "9332",
          "pageId": "107",
          "userId": "2",
          "edit": 0,
          "type": "deleteChild",
          "createdAt": "2016-04-18 19:12:04",
          "auxPageId": "38x",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "7150",
          "pageId": "107",
          "userId": "2",
          "edit": 0,
          "type": "deleteChild",
          "createdAt": "2016-02-16 05:35:01",
          "auxPageId": "207",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "5475",
          "pageId": "107",
          "userId": "2",
          "edit": 20,
          "type": "newEdit",
          "createdAt": "2016-01-20 01:05:11",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "5474",
          "pageId": "107",
          "userId": "2",
          "edit": 19,
          "type": "newEdit",
          "createdAt": "2016-01-20 01:04:38",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "5473",
          "pageId": "107",
          "userId": "2",
          "edit": 18,
          "type": "newEdit",
          "createdAt": "2016-01-20 01:03:42",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "5470",
          "pageId": "107",
          "userId": "2",
          "edit": 16,
          "type": "newEdit",
          "createdAt": "2016-01-20 00:46:59",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "5469",
          "pageId": "107",
          "userId": "2",
          "edit": 15,
          "type": "newEdit",
          "createdAt": "2016-01-20 00:45:55",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "5463",
          "pageId": "107",
          "userId": "2",
          "edit": 0,
          "type": "deleteChild",
          "createdAt": "2016-01-20 00:31:44",
          "auxPageId": "1mt",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "5460",
          "pageId": "107",
          "userId": "2",
          "edit": 13,
          "type": "newChild",
          "createdAt": "2016-01-20 00:10:54",
          "auxPageId": "1n1",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "5444",
          "pageId": "107",
          "userId": "2",
          "edit": 13,
          "type": "newChild",
          "createdAt": "2016-01-19 05:17:06",
          "auxPageId": "1mt",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "5425",
          "pageId": "107",
          "userId": "32",
          "edit": 13,
          "type": "newEdit",
          "createdAt": "2016-01-17 16:48:01",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "5424",
          "pageId": "107",
          "userId": "2",
          "edit": 12,
          "type": "newEdit",
          "createdAt": "2016-01-17 06:05:14",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "5418",
          "pageId": "107",
          "userId": "2",
          "edit": 11,
          "type": "newEdit",
          "createdAt": "2016-01-17 04:48:34",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "5401",
          "pageId": "107",
          "userId": "2",
          "edit": 10,
          "type": "newChild",
          "createdAt": "2016-01-17 01:20:04",
          "auxPageId": "1mm",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "5351",
          "pageId": "107",
          "userId": "2",
          "edit": 10,
          "type": "newChild",
          "createdAt": "2016-01-16 22:41:22",
          "auxPageId": "1mk",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "5049",
          "pageId": "107",
          "userId": "2",
          "edit": 10,
          "type": "newEdit",
          "createdAt": "2016-01-06 21:51:12",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "4266",
          "pageId": "107",
          "userId": "2",
          "edit": 9,
          "type": "newEdit",
          "createdAt": "2015-12-23 03:25:11",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "4265",
          "pageId": "107",
          "userId": "2",
          "edit": 8,
          "type": "newEdit",
          "createdAt": "2015-12-23 03:22:25",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "4137",
          "pageId": "107",
          "userId": "2",
          "edit": 7,
          "type": "newEdit",
          "createdAt": "2015-12-17 23:05:29",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "3892",
          "pageId": "107",
          "userId": "1",
          "edit": 6,
          "type": "newEdit",
          "createdAt": "2015-12-16 06:12:22",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "3891",
          "pageId": "107",
          "userId": "1",
          "edit": 0,
          "type": "newAlias",
          "createdAt": "2015-12-16 06:12:21",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "1115",
          "pageId": "107",
          "userId": "1",
          "edit": 1,
          "type": "newUsedAsTag",
          "createdAt": "2015-10-28 03:47:09",
          "auxPageId": "4v",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "581",
          "pageId": "107",
          "userId": "1",
          "edit": 1,
          "type": "newChild",
          "createdAt": "2015-10-28 03:46:58",
          "auxPageId": "11w",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "583",
          "pageId": "107",
          "userId": "1",
          "edit": 1,
          "type": "newChild",
          "createdAt": "2015-10-28 03:46:58",
          "auxPageId": "207",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "584",
          "pageId": "107",
          "userId": "1",
          "edit": 1,
          "type": "newChild",
          "createdAt": "2015-10-28 03:46:58",
          "auxPageId": "11v",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "419",
          "pageId": "107",
          "userId": "1",
          "edit": 1,
          "type": "newParent",
          "createdAt": "2015-10-28 03:46:51",
          "auxPageId": "2l",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "1363",
          "pageId": "107",
          "userId": "2",
          "edit": 5,
          "type": "newEdit",
          "createdAt": "2015-08-11 20:04:55",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "1362",
          "pageId": "107",
          "userId": "2",
          "edit": 4,
          "type": "newEdit",
          "createdAt": "2015-08-11 20:04:35",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "1361",
          "pageId": "107",
          "userId": "2",
          "edit": 3,
          "type": "newEdit",
          "createdAt": "2015-08-03 18:48:13",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "1360",
          "pageId": "107",
          "userId": "2",
          "edit": 2,
          "type": "newEdit",
          "createdAt": "2015-08-03 18:46:15",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "1359",
          "pageId": "107",
          "userId": "2",
          "edit": 1,
          "type": "newEdit",
          "createdAt": "2015-07-14 20:36:44",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        }
      ],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": true,
      "hasParents": true,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    }
  },
  "users": {
    "1": {
      "id": "1",
      "firstName": "Alexei",
      "lastName": "Andreev",
      "lastWebsiteVisit": "2018-02-18 09:35:21",
      "isSubscribed": false,
      "domainMembershipMap": {}
    },
    "2": {
      "id": "2",
      "firstName": "Eliezer",
      "lastName": "Yudkowsky",
      "lastWebsiteVisit": "2019-12-21 03:34:41",
      "isSubscribed": false,
      "domainMembershipMap": {}
    },
    "5": {
      "id": "5",
      "firstName": "Eric",
      "lastName": "Rogstad",
      "lastWebsiteVisit": "2019-08-23 01:44:10",
      "isSubscribed": false,
      "domainMembershipMap": {}
    },
    "32": {
      "id": "32",
      "firstName": "Nate",
      "lastName": "Soares",
      "lastWebsiteVisit": "2017-05-10 13:41:18",
      "isSubscribed": false,
      "domainMembershipMap": {}
    },
    "44": {
      "id": "44",
      "firstName": "Rob",
      "lastName": "Bensinger",
      "lastWebsiteVisit": "2019-10-13 01:10:55",
      "isSubscribed": false,
      "domainMembershipMap": {}
    },
    "12r": {
      "id": "12r",
      "firstName": "Matthew",
      "lastName": "Graves",
      "lastWebsiteVisit": "2017-04-11 16:33:05",
      "isSubscribed": false,
      "domainMembershipMap": {}
    },
    "1yq": {
      "id": "1yq",
      "firstName": "Eric",
      "lastName": "Bruylant",
      "lastWebsiteVisit": "2017-04-14 18:00:22",
      "isSubscribed": false,
      "domainMembershipMap": {}
    },
    "8js": {
      "id": "8js",
      "firstName": "Kai",
      "lastName": "Daniels",
      "lastWebsiteVisit": "2017-07-20 18:31:33",
      "isSubscribed": false,
      "domainMembershipMap": {}
    },
    "8pb": {
      "id": "8pb",
      "firstName": "Jacob",
      "lastName": "van Eeden",
      "lastWebsiteVisit": "2017-10-02 19:17:29",
      "isSubscribed": false,
      "domainMembershipMap": {}
    }
  },
  "domains": {
    "1": {
      "id": "1",
      "pageId": "1lw",
      "createdAt": "2016-01-15 03:02:51",
      "alias": "math",
      "canUsersComment": false,
      "canUsersProposeComment": true,
      "canUsersProposeEdits": true,
      "friendDomainIds": []
    },
    "2": {
      "id": "2",
      "pageId": "2v",
      "createdAt": "2015-03-26 23:12:18",
      "alias": "value_alignment",
      "canUsersComment": false,
      "canUsersProposeComment": true,
      "canUsersProposeEdits": true,
      "friendDomainIds": []
    },
    "3": {
      "id": "3",
      "pageId": "3d",
      "createdAt": "2015-03-30 22:19:47",
      "alias": "Arbital",
      "canUsersComment": false,
      "canUsersProposeComment": true,
      "canUsersProposeEdits": true,
      "friendDomainIds": []
    },
    "7": {
      "id": "7",
      "pageId": "15w",
      "createdAt": "2015-10-26 22:59:19",
      "alias": "MIRI",
      "canUsersComment": false,
      "canUsersProposeComment": true,
      "canUsersProposeEdits": true,
      "friendDomainIds": []
    },
    "8": {
      "id": "8",
      "pageId": "198",
      "createdAt": "2015-12-13 23:14:48",
      "alias": "TeamArbital",
      "canUsersComment": false,
      "canUsersProposeComment": true,
      "canUsersProposeEdits": true,
      "friendDomainIds": []
    },
    "15": {
      "id": "15",
      "pageId": "58c",
      "createdAt": "2016-07-08 18:23:14",
      "alias": "DecisionTheory",
      "canUsersComment": false,
      "canUsersProposeComment": true,
      "canUsersProposeEdits": true,
      "friendDomainIds": []
    }
  },
  "masteries": {},
  "marks": {},
  "pageObjects": {},
  "result": {},
  "globalData": null
}
