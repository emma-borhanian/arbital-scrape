{
  "resetEverything": false,
  "user": {
    "id": "",
    "firstName": "",
    "lastName": "",
    "lastWebsiteVisit": "",
    "isSubscribed": false,
    "domainMembershipMap": {},
    "fbUserId": "",
    "email": "",
    "isAdmin": false,
    "emailFrequency": "",
    "emailThreshold": 0,
    "ignoreMathjax": false,
    "showAdvancedEditorMode": false,
    "isSlackMember": false,
    "analyticsId": "aid:7SNzM+K36dkoTmmOM1x24zaniZtPbvI2BVlH07T3k1M",
    "hasReceivedMaintenanceUpdates": false,
    "hasReceivedNotifications": false,
    "newNotificationCount": 0,
    "newAchievementCount": 0,
    "maintenanceUpdateCount": 0,
    "invitesClaimed": [],
    "mailchimpInterests": {},
    "continueBayesPath": null,
    "continueLogPath": null
  },
  "pages": {
    "1": {
      "likeableId": "1",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "1",
      "edit": 5,
      "editSummary": "",
      "prevEdit": 4,
      "currentEdit": 5,
      "wasPublished": true,
      "type": "group",
      "title": "Alexei Andreev",
      "clickbait": "There is no spoon",
      "textLength": 304,
      "alias": "AlexeiAndreev",
      "externalUrl": "",
      "sortChildrenBy": "alphabetical",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "1",
      "editCreatedAt": "2016-12-13 02:34:00",
      "pageCreatorId": "1",
      "pageCreatedAt": "2015-09-04 16:14:58",
      "seeDomainId": "0",
      "editDomainId": "21",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 0,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 741,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": 0,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "178": {
      "likeableId": "202",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "178",
      "edit": 6,
      "editSummary": "",
      "prevEdit": 5,
      "currentEdit": 6,
      "wasPublished": true,
      "type": "wiki",
      "title": "Arbital \"tag\" relationship",
      "clickbait": "Tags are a way to connect pages that share a common topic.",
      "textLength": 2689,
      "alias": "Arbital_tag",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "1",
      "editCreatedAt": "2016-05-11 15:44:58",
      "pageCreatorId": "1",
      "pageCreatedAt": "2015-11-15 15:31:40",
      "seeDomainId": "0",
      "editDomainId": "3",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 0,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 96,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": 0,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "185": {
      "likeableId": "0",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "185",
      "edit": 0,
      "editSummary": "",
      "prevEdit": 0,
      "currentEdit": 0,
      "wasPublished": false,
      "type": "",
      "title": "",
      "clickbait": "",
      "textLength": 0,
      "alias": "",
      "externalUrl": "",
      "sortChildrenBy": "",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "",
      "editCreatedAt": "",
      "pageCreatorId": "",
      "pageCreatedAt": "",
      "seeDomainId": "",
      "editDomainId": "",
      "submitToDomainId": "",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": false,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 0,
      "isEditorComment": false,
      "isApprovedComment": false,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 0,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": 0,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "187": {
      "likeableId": "0",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "187",
      "edit": 0,
      "editSummary": "",
      "prevEdit": 0,
      "currentEdit": 0,
      "wasPublished": false,
      "type": "",
      "title": "",
      "clickbait": "",
      "textLength": 0,
      "alias": "",
      "externalUrl": "",
      "sortChildrenBy": "",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "",
      "editCreatedAt": "",
      "pageCreatorId": "",
      "pageCreatedAt": "",
      "seeDomainId": "",
      "editDomainId": "",
      "submitToDomainId": "",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": false,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 0,
      "isEditorComment": false,
      "isApprovedComment": false,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 0,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": 0,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "198": {
      "likeableId": "266",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "198",
      "edit": 4,
      "editSummary": "",
      "prevEdit": 3,
      "currentEdit": 4,
      "wasPublished": true,
      "type": "wiki",
      "title": "Team Arbital",
      "clickbait": "The people behind Arbital",
      "textLength": 184,
      "alias": "TeamArbital",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "1",
      "editCreatedAt": "2016-06-17 16:55:46",
      "pageCreatorId": "1",
      "pageCreatedAt": "2015-12-13 23:14:48",
      "seeDomainId": "0",
      "editDomainId": "8",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 0,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 1185,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": 0,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "370": {
      "likeableId": "2144",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "370",
      "edit": 4,
      "editSummary": "reflecting the fact that we only have one type of mark now.",
      "prevEdit": 3,
      "currentEdit": 4,
      "wasPublished": true,
      "type": "wiki",
      "title": "Arbital mark",
      "clickbait": "What is a mark on Arbital? When is it created? Why is it important?",
      "textLength": 1724,
      "alias": "arbital_mark",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "1yq",
      "editCreatedAt": "2016-07-22 00:08:32",
      "pageCreatorId": "1",
      "pageCreatedAt": "2016-04-14 23:12:16",
      "seeDomainId": "0",
      "editDomainId": "3",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 0,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 58,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": 0,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "595": {
      "likeableId": "0",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "595",
      "edit": 3,
      "editSummary": "",
      "prevEdit": 2,
      "currentEdit": 3,
      "wasPublished": true,
      "type": "wiki",
      "title": "Arbital page alias",
      "clickbait": "",
      "textLength": 1215,
      "alias": "arbital_alias",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "1yq",
      "editCreatedAt": "2016-07-21 23:06:57",
      "pageCreatorId": "5",
      "pageCreatedAt": "2016-07-10 00:52:28",
      "seeDomainId": "0",
      "editDomainId": "3",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 0,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 46,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": 0,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "596": {
      "likeableId": "0",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "596",
      "edit": 1,
      "editSummary": "",
      "prevEdit": 0,
      "currentEdit": 1,
      "wasPublished": true,
      "type": "wiki",
      "title": "Arbital page title",
      "clickbait": "",
      "textLength": 738,
      "alias": "Arbital_title",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "5",
      "editCreatedAt": "2016-07-10 01:18:37",
      "pageCreatorId": "5",
      "pageCreatedAt": "2016-07-10 01:18:37",
      "seeDomainId": "0",
      "editDomainId": "3",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 0,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 31,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": 0,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "597": {
      "likeableId": "3067",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "597",
      "edit": 2,
      "editSummary": "added clickbait",
      "prevEdit": 1,
      "currentEdit": 2,
      "wasPublished": true,
      "type": "wiki",
      "title": "Arbital page clickbait",
      "clickbait": "The text you are reading right now is clickbait.",
      "textLength": 1128,
      "alias": "Arbital_clickbait",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "1yq",
      "editCreatedAt": "2016-08-05 17:48:01",
      "pageCreatorId": "5",
      "pageCreatedAt": "2016-07-10 01:24:23",
      "seeDomainId": "0",
      "editDomainId": "3",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 0,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 43,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": 0,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "11v": {
      "likeableId": "55",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "11v",
      "edit": 10,
      "editSummary": "",
      "prevEdit": 9,
      "currentEdit": 10,
      "wasPublished": true,
      "type": "wiki",
      "title": "AIXI",
      "clickbait": "How to build an (evil) superintelligent AI using unlimited computing power and one page of Python code.",
      "textLength": 3093,
      "alias": "AIXI",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "5",
      "editCreatedAt": "2016-03-31 21:07:12",
      "pageCreatorId": "2",
      "pageCreatedAt": "2015-08-04 20:08:59",
      "seeDomainId": "0",
      "editDomainId": "2",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 6,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 933,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0
      ],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": -2,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "11w": {
      "likeableId": "56",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 3,
      "dislikeCount": 0,
      "likeScore": 3,
      "individualLikes": [],
      "pageId": "11w",
      "edit": 4,
      "editSummary": "",
      "prevEdit": 3,
      "currentEdit": 4,
      "wasPublished": true,
      "type": "wiki",
      "title": "Solomonoff induction",
      "clickbait": "A simple way to superintelligently predict sequences of data, given unlimited computing power.",
      "textLength": 1697,
      "alias": "solomonoff_induction",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "2",
      "editCreatedAt": "2015-12-30 04:05:19",
      "pageCreatorId": "2",
      "pageCreatedAt": "2015-08-05 04:08:11",
      "seeDomainId": "0",
      "editDomainId": "1",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 0,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 2029,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": 0,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "17b": {
      "likeableId": "204",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "17b",
      "edit": 16,
      "editSummary": "",
      "prevEdit": 15,
      "currentEdit": 16,
      "wasPublished": true,
      "type": "wiki",
      "title": "Arbital lens",
      "clickbait": "A lens is a page that presents another page's content from a different angle.",
      "textLength": 7216,
      "alias": "Arbital_lens",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "1yq",
      "editCreatedAt": "2016-12-05 13:10:54",
      "pageCreatorId": "1",
      "pageCreatedAt": "2015-11-15 18:01:48",
      "seeDomainId": "0",
      "editDomainId": "3",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 0,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 687,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": 0,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "1hh": {
      "likeableId": "457",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 10,
      "dislikeCount": 0,
      "likeScore": 10,
      "individualLikes": [],
      "pageId": "1hh",
      "edit": 10,
      "editSummary": "",
      "prevEdit": 9,
      "currentEdit": 54,
      "wasPublished": true,
      "type": "wiki",
      "title": "Semitechnical intro dialogue",
      "clickbait": "An introduction to Solomonoff induction for the unfamiliar reader who isn't bad at math",
      "textLength": 60990,
      "alias": "1hh",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "2",
      "editCreatedAt": "2016-01-07 04:22:09",
      "pageCreatorId": "2",
      "pageCreatedAt": "2015-12-30 03:55:19",
      "seeDomainId": "0",
      "editDomainId": "1",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": false,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 4,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 1032,
      "text": "Ashley:  So, you're going to tell me about... Solomon's induction?\n\nBlaine:  Solomonoff induction.  Goes back to the 1960s and the mathematician Ray Solomonoff.  The key idea in Solomonoff induction is to do sequence prediction by having a probability distribution over -\n\nAshley:  Wait.  Before you launch right into an explanation of what Solomonoff induction *is*, I'd like you to try to tell me what it *does*, or why people study it in the first place.  I find that helps me organize my listening.\n\nBlaine:  Um... okay.  Let me think for a second...\n\nAshley:  Also, while I can imagine things that \"sequence prediction\" might mean, I haven't actually encountered it in a technical context, so you'd better go a bit further back and start more at the beginning.  I do know what a probability distribution is.\n\nBlaine:  Okay.  So... one way of framing the usual reason why people study this sort of thing in the first place, is that sometimes, by studying what it would be possible to do with unlimited computing power, we can gain valuable intuitions about epistemology - that's, uh, the field that studies how to reason about factual questions, how to build a map of reality that reflects the territory...\n\nAshley:  I have some idea what 'epistemology' is, yes.  But I think you might need to start even further back, maybe with some sort of concrete example or something.\n\nBlaine:  Okay.  Um.  So one anecdote that I sometimes use to frame the value of computer science to the study of epistemology, is Edgar Allen Poe's argument in 1833 that chess was uncomputable.\n\nAshley:  That doesn't sound like a thing that actually happened.\n\nBlaine:  I know, but it totally *did* happen and not in a metaphorical sense either!  Edgar Allen Poe wrote an essay explaining why no automaton would ever be able to play chess, and he specifically mentioned \"Mr. Babbage's computing engine\" as an example.  You see, in the nineteenth century, there was for a time this sensation known as the Mechanical Turk - supposedly a machine, an automaton, that could play chess.  At the grandmaster level, no less.  Now today, when we're accustomed to the idea that a computer chip needs to be doing billions of operations per second to play chess, you or I know *immediately* that the Mechanical Turk must have been a fraud and that there must have been a concealed operator inside - a midget, as it turned out - especially when, if you reached out to upset one of the Turk's pieces, the Turk would reach back out and set it back up again, and if you kept on doing it, the Turk would refuse to play, and so on.  *Today* we know that this sort of thing is *hard* to build into a machine.  But in the 19th century, even that much wasn't known.  So when Edgar Allen Poe, who besides being an author was also an accomplished magician, set out to write an essay about the Mechanical Turk, he spent the *second* half of the essay dissecting what was known about the Turk's appearance to (correctly) figure out where the human operator was hiding.  But Poe spent the *first* half of the essay arguing that no automaton - nothing like Mr. Babbage's computing engine - could possibly play chess, which was how he knew *a priori* that the Turk had a concealed human operator.\n\nAshley:  And what was Poe's argument?\n\nBlaine:  Poe observed that in an algebraical problem, each step followed from the previous step of necessity, which was why the steps in solving an algebraical problem could be represented by the deterministic motions of gears in something like Mr. Babbage's computing engine.  But in a chess problem, Poe said, there are many possible chess moves, and no move follows with necessity from the position of the board; and even if you did select one move, the opponent's move would not follow with necessity, so you couldn't represent it with the determined motion of automatic gears.  Therefore, Poe said, whatever was operating the Mechanical Turk must have the nature of Cartesian mind, rather than the nature of deterministic matter, and this was knowable *a priori*.  And then he started figuring out where the required operator was hiding.\n\nAshley:  That's... actually really impressive reasoning, and completely wrong at the same time.\n\nBlaine:  I know!  Isn't it great?\n\nAshley:  I mean, that sounds like Poe correctly identified the *hard* part of playing computer chess - the branching factor of moves and countermoves - the reason why no *simple* machine could do it, and he just didn't realize that a deterministic machine could deterministically check many possible moves in order to figure out the game tree.  So close, and yet so far.\n\nBlaine:  More than a century later, in 1950, Claude Shannon published the first paper ever written on computer chess... and in passing, Shannon gave the formula for playing perfect chess if you had unlimited computing power, the algorithm you'd use to extrapolate the entire game tree.  We could say that Shannon gave a short program that would solve chess if you ran it on a hypercomputer, where a hypercomputer is an ideal computer that can run any finite computation immediately.  And *then* Shannon passed on to talking about the problem of locally guessing how good a board position was, so that you could play chess using only a *small* search.  I say all this to make a point about the value of knowing how to solve problems using hypercomputers, even though hypercomputers don't exist.  Yes, there's often a *huge* gap between the unbounded solution and the practical solution.  It wasn't until 1997, forty-seven years after Shannon's paper giving the unbounded solution, that Deep Blue actually won the world chess championship -\n\nAshley:  And that wasn't just a question of faster computing hardware running Shannon's ideal search algorithm.  There were a lot of new insights along the way, most notably the alpha-beta pruning algorithm and a lot of improvements in positional evaluation.\n\nBlaine:  Right!  But I think some people overreact to that forty-seven year gap, and act like it's *worthless* to have an unbounded understanding of a computer program, just because you might still be forty-seven years away from a practical solution.  But if you don't even have a solution that would run on a hyercomputer, you're Poe in 1833, not Shannon in 1950.  The reason I tell the anecdote about Poe is to illustrate that Poe was *confused* about computer chess in a way that Shannon was not.  When we don't know how to solve a problem even given infinite computing power, the very work we are trying to do is in some sense murky to us.  When we can state code that would solve the problem given a hypercomputer, we have become *less* confused.  Once we have the unbounded solution we understand, in some basic sense, *the kind of work we are trying to perform,* and then we can try to figure out how to do it *efficiently* -\n\nAshley:  Which may well require new insights into the structure of the problem, or even a conceptual revolution in how we imagine the work we're trying to do.\n\nBlaine:  Yes, but the point is that *you can't even get started on that* if you're arguing about how playing chess has the nature of Cartesian mind rather than matter, because deterministic gear motions can't represent non-determined chess moves.  At that point you're not 50 years away from winning the chess championship, you're 150 years away, because it took an extra 100 years to move humanity's understanding to the point where you could easily see how to play chess using a hypercomputer.  I'm not trying to exalt the unbounded solution by denigrating the work required to get a bounded solution, I'm not saying that when we have an unbounded solution we're practically there and the rest is a matter of mere lowly efficiency, I'm trying to compare having the unbounded solution to the horrific confusion of *not knowing what kind of work we're trying to do.*\n\nAshley:  Okay.  I think I understand why, on your view, it's important to know how to solve problems using infinitely fast computers, or hypercomputers as you call them.  When we can say how to answer a question using infinite computing power, that means we crisply understand the question itself, in some sense, while if we can't figure out how to solve a problem using unbounded computing power, that means we're confused about the problem in some sense.  I can see that, yes.  I mean, if you've ever tried to teach the more doomed sort of undergraduate to write computer programs, you know what it means to be confused about what it takes to compute something.  So what does this have to do with \"Solomonoff induction\"?\n\nBlaine:  Ah!  Well, suppose I asked you how to do epistemology using infinite computing power?\n\nAshley:  My good fellow, I would at once reply, \"Beep.  Whirr.  Problem 'do epistemology' not crisply specified.\"  At this stage of affairs, I do not think this reply indicates any fundamental confusion on my part; rather I think it is you who must be clearer.  And don't tell me that 'epistemology' means 'how we should reason in order to construct an accurate map of reality,' because that's still not crisply specified.\n\nBlaine:  Well, perhaps.  But even there I would suggest that it's a mark of intellectual progress to be able to take vague and underspecified ideas like 'do good epistemology' and turn them *into* crisply specified problems.  Imagine that I went up to my friend Cecil, and said, \"How would you do good epistemology given unlimited computing power and a short Python program?\" and Cecil at once came back with an answer, and it was actually a pretty reasonable and good answer once explained.  Cecil would probably know something quite interesting that you do not presently know - for one thing, the knowledge of *a good way to formalize* the problem of 'doing good epistemology' in the first place.\n\nAshley:  I confess to being rather skeptical of this hypothetical.  But if that actually happened - if I agreed, to my own satisfaction, that someone had stated a short Python program that would 'do good epistemology' if run on an unboundedly fast computer - then I agree that I'd probably have learned something *quite interesting* about epistemology.\n\nBlaine:  What Cecil knows about, in this hypothetical, is Solomonoff induction.  In the same way that Claude Shannon answered \"Given infinite computing power, how would you play perfect chess?\", Ray Solomonoff answered \"Given infinite computing power, how would you perfectly find the best hypothesis that fits the facts?\"\n\nAshley:  Suddenly, I find myself strongly suspicious of whatever you are about to say to me.\n\nBlaine:  That's understandable.\n\nAshley:  In particular, I'll ask at once whether \"Solomonoff induction\" assumes that our *hypotheses* are being given to us on a silver platter by the programmers along with the exact data we're supposed to explain, or whether the algorithm is organizing its own data from a big messy situation and *inventing* good hypotheses from scratch -\n\nBlaine:  Great question!  It's the second one.\n\nAshley:  Really?  Okay, now I have to ask whether Solomonoff induction is a recognized concept in good standing in the field of academic computer science, because that does not sound like something modern-day computer science knows how to do.\n\nBlaine:  I wouldn't say it's a widely known concept, but it's one that's in good academic standing.  The method isn't used in modern machine learning because it requires an infinitely fast computer and isn't easily approximated the way that chess is.\n\nAshley:  This really sounds *very* suspicious.  Last time I checked, we hadn't *begun* to formalize the creation of good new hypotheses from scratch.  I've heard about claims to have 'automated' the work that, say, Newton did in inventing classical mechanics, and I've found them all to be incredibly dubious.  Which is to say, they were rigged demos and lies.\n\nBlaine:  I know, but -\n\nAshley:  And then I'm even more suspicious of a claim that someone's algorithm would solve this problem if only they had infinite computing power.  Having some researcher claim that their Good-Old-Fashioned AI semantic network *would* be intelligent if run on a computer so large that, conveniently, nobody can ever test their theory, is not going to persuade me.\n\nBlaine:  ...Do I really strike you as that much of a charlatan?  What have I ever done to you, that you would expect me to try pulling a scam like that?\n\nAshley:  That's fair.  I shouldn't accuse you of planning that scam when I haven't seen you say it.  But I'm pretty sure the problem of \"coming up with good new hypotheses in a world full of messy data\" is basically AI-complete, like, solving this problem means you have strong AI.  And even Mentif-\n\nBlaine:  Do not say the name, or he will appear!\n\nAshley:  Sorry.  Even the legendary first and greatest of all AI crackpots, He-Who-Googles-His-Name, could assert that his algorithms would be all-powerful on a computer large enough to make his claim unfalsifiable.  So what?\n\nBlaine:  That's a very sensible reply given your current background knowledge and this, again, is *exactly* the kind of mental state that reflects a problem that is *confusing* rather than just *hard to implement.*  It's the sort of confusion Poe might feel in 1833, or close to it.  In other words, it's just the sort of conceptual issue we *would* have solved at the point where we could state a short program that could run on a hypercomputer.  Which Ray Solomonoff did in 1964.\n\nAshley:  Okay, let's hear about this supposed general solution to epistemology.\n\nBlaine:  First, try to solve the following puzzle.  1, 3, 4, 7, 11, 18, 29...?\n\nAshley:  Let me look at those for a moment... 47.\n\nBlaine:  Congratulations on engaging in, as we snooty types would call it, 'sequence prediction'.\n\nAshley:  I'm following you so far.\n\nBlaine:  The smarter you are, the more easily you can find the hidden patterns in sequences and predict them successfully. You had to notice the resemblance to the Fibonacci rule to guess the next number. Someone who didn't already know about Fibonacci, or who was worse at mathematical thinking, would have taken longer to understand the sequence or maybe never learned to predict it at all.\n\nAshley: Still with you.\n\nBlaine: It's not a sequence of *numbers* per se... but can you see how the question, \"The sun has risen on the last million days. What is the probability that it rises tomorrow?\" could be viewed as a kind of sequence prediction problem?\n\nAshley: Only if some programmer neatly parses up the world into a series of \"Did the Sun rise on day X starting in 4.5 billion BCE, 0 means no and 1 means yes?  1, 1, 1, 1, 1...\" and so on.  Which is exactly the sort of shenanigan that I see as cheating.\n\nBlaine:  I agree that's cheating.  But suppose I have a robot running around with a webcam showing it a 1920x1080 pixel field that refreshes 60 times a second with 32-bit colors.  I could view that as a giant sequence and ask the robot to predict what it will see happen when it rolls out to watch a sunrise the next day.\n\nAshley: I can't help but notice that the 'sequence' of webcam frames is absolutely enormous, like, the sequence is made up of 66-megabit 'numbers' appearing 3600 times per minute... oh, right, computers much bigger than the universe. And now you're smiling evilly, so I guess that's the point. I also notice that the sequence is no longer deterministically predictable, that it is no longer a purely mathematical object, and that the sequence of webcam frames observed will depend on the robot's choices. This makes me feel a bit shaky about the analogy to predicting the mathematical sequence 1, 1, 2, 3, 5.\n\nBlaine: I'll try to address those points in order. First, Solomonoff induction is about assigning *probabilities* to the next item in the sequence. I mean, if I showed you a box that said 1, 1, 2, 3, 5, 8 you would not be absolutely certain that the next item would be 13. There could be some more complicated rule that just looked Fibonacci-ish but then diverged. You might guess with 90% probability but not 100% probability, or something like that.\n\nAshley: This has stopped feeling to me like math.\n\nBlaine: There is a *large* branch of math, to say nothing of computer science, that deals in probabilities and statistical prediction.  We are going to be describing absolutely lawful and deterministic ways of assigning probabilities after seeing 1, 3, 4, 7, 11, 18.\n\nAshley:  Okay, but if you're later going to tell me that this lawful probabilistic prediction rule underlies a generally intelligent epistemological reasoner, I'm already skeptical.  No matter how large a computer it's run on, I find it hard to imagine that some simple set of rules for assigning probabilities is going to encompass truly and generally intelligent answers about sequence prediction, like Terence Tao would give after looking at the sequence for a while.  We just have no idea how Terence Tao works, so we can't duplicate his abilities in a formal rule, no matter how much computing power that rule gets... you're smiling evilly again.  I'll be *quite* interested if that evil smile turns out to be justified.\n\nBlaine:  Indeed.\n\nAshley: I also find it hard to imagine that this deterministic mathematical rule for assigning probabilities would notice if a box was outputting an encoded version of \"To be or not to be\" from Shakespeare by mapping A to Z onto 1 to 26, which I would notice eventually though not immediately on seeing 20, 15, 2, 5, 15, 18... and you're *still* smiling evilly.\n\nBlaine:  Indeed.  That is *exactly* what Solomonoff induction does.  Furthermore, we have theorems establishing that Solomonoff induction can do it better than you or Terence Tao.\n\nAshley:  A *theorem* proves this.  As in a necessary mathematical truth.  Even though we have no idea how Terence Tao works empirically... and there's evil smile number four.  Okay.  I am *very* skeptical, but willing to be convinced.\n\nBlaine:  So if you actually did have a hypercomputer, you could *cheat*, right?  And Solomonoff induction is the most ridiculously cheating cheat in the history of cheating.\n\nAshley:  Go on.\n\nBlaine:  We just run all possible computer programs to see which are the simplest computer programs that best predict the data seen so far, and use those programs to predict what comes next.  This mixture contains, among other things, an exact copy of Terence Tao, thereby allowing us to prove theorems about their relative performance.\n\nAshley:  What.\n\nBlaine:  That's what the equation for Solomonoff induction says.\n\nAshley:  Is this an actual reputable math thing?  I mean really?\n\nBlaine:  I'll get to formalizing the math shortly, but you did ask me to first state the point of it all.  The point of Solomonoff induction is that it gives us a gold-standard ideal for sequence prediction, and this gold-standard prediction only errs by a bounded amount, over infinite time, relative to the best computable sequence predictor.  We can also see it as formalizing the intuitive idea that was expressed by William Ockham a few centuries earlier that simpler theories are more likely to be correct, and as telling us that 'simplicity' should be measured in algorithmic complexity, which is the size of a computer program required to output a hypothesis's predictions.\n\nAshley:  I think I would have to read more on this subject to actually follow that.  What I'm hearing is that Solomonoff induction is a reputable idea that is important because it gives us a kind of ideal for sequence prediction.  This ideal also has something to do with Occam's Razor and the idea that the simplest theory is the one that can be represented by the shortest computer program.  You identify this with \"doing good epistemology\".\n\nBlaine:  Yes, those are legitimate takeaways.  Another way of looking at it is that Solomonoff induction is an ideal but uncomputable answer to the question \"What should our priors be?\", which is left open by understanding [ Bayesian updating].\n\nAshley:  Can you say how Solomonoff induction answers the question of, say, the prior probability that Canada is planning to invade the United States?  To name one example of a thing I've seen being estimated ludicrously high on a crackpot website that actually tried to invoke Bayesian probability about it.  Does Solomonoff induction let me tell him that he's making a math error, instead of just calling him silly in an admittedly informal fashion?\n\nBlaine:  If you're expecting to sit down with Leibniz and say, \"Gentlemen, let us calculate\" then you're setting your expectations too high.  Solomonoff gives us an idea of how we *should* compute that quantity given *unlimited* computing power.  It doesn't give us a firm procedural recipe for how we can best approximate that ideal in real life using bounded computing power or human brains.  Knowing the ideal, we *can* extract some intuitive advice that might help our online crackpot if only he'd actually listen to it.\n\nAshley:  But according to you, Solomonoff induction does say in principle what is the prior probability that Canada will invade the United States.\n\nBlaine:  Yes, up to a choice of universal Turing machine.\n\nAshley *(looking highly skeptical):*  So I plug a universal Turing machine into the formalism, and in principle, I get out a uniquely determined probability that Canada invades the US.\n\nBlaine:  Exactly!\n\nAshley:  Uh huh.  Well, go on.\n\nBlaine:  So, first, we have to transform this into a sequence prediction problem.\n\nAshley:  Like a sequence of years in which Canada has and hasn't invaded the US, mostly zero except around 1812 -\n\nBlaine:  *No!*  To get a good prediction about Canada we need much more data than that, and I don't mean a dump of Canadian GDP either.  Imagine a sequence that contains all the sensory data you have ever received over your lifetime - not just the hospital room that you saw when you opened your eyes right after your birth, but the darkness your brain received as input while you were still in your mother's womb.  Every word you've ever heard.  Every letter you've ever seen on a computer screen, not as ASCII letters but as the raw pattern of neural impulses that gets sent down from your retina.\n\nAshley:  That seems like a lot of data and some of it is redundant, there'll be lots of similar pixels for blue sky -\n\nBlaine:  That data is what *you* got as an agent.  If we want to translate the question of the prediction problem Ashley faces into theoretical terms, we should give the sequence predictor *all* the data that you had available, including all those repeating blue pixels of the sky.  Who knows?  Maybe there was a Canadian warplane in there that you didn't notice.\n\nAshley:  But it's impossible for my brain to remember all that data.  If we neglect for the moment how the retina actually works and suppose that I'm seeing the same 1920 x 1080 picture the robot would, that's far more data than I can *realistically* take into account.\n\nBlaine:  So then Solomonoff induction can do better than you can, using its unlimited computing power and memory.  That's fine.  When we're talking about the sequence prediction problem in principle, we want to start by talking about *all* the inputs and considering later which inputs might be forgotten or neglected; in the limit of unlimited computing power, there'd be no reason not to take into account all of it.\n\nAshley:  But what if you can do better by forgetting more?\n\nBlaine:  If you have limited computing power, that makes sense.  With unlimited computing power, that really shouldn't happen and that indeed is one of the lessons of Solomonoff induction.  An unbounded Bayesian never expects to do worse by updating on another item of evidence - for one thing, you can always just do the same thing you would have done if you hadn't seen the item.  That kind of lesson *is* one of the things that might not be intuitively obvious, but which you can start to feel deeply by walking through the math of probability theory that provides a kind of conceptual underpinning and clarity for epistemology.  With *unlimited* computing power, nothing goes wrong as a result of trying to process 4 gigabits per second; every extra bit just produces a better expected future prediction.\n\nAshley:  Okay, so we start with literally all the data I have available, 4 gigabits per second of it if we imagine 1920 by 1080 frames of 32-bit pixels at 60Hz - though I remember hearing 100 megabits would be a better estimate of what the retina sends out, and that it's pared down to 1 megabit per second very quickly.\n\nBlaine:  We start with all the data you *had* available since the day you were born.\n\nAshley:  In that case, your sequence of sensory inputs seems incomplete.  There are some things I know not just by observing the outside world.  Chimpanzees learn to be afraid of skulls and snakes much faster than they learn to be afraid of other arbitrary shapes.  I was probably better at learning to walk in Earth gravity than I would have been at navigating in zero G.  There are heuristics I'm born with, based on how my brain was wired, which ultimately stems from my DNA specifying the way that proteins should fold to form neurons -\n\nBlaine:  So, for purposes of following along with the argument, let's say that your DNA is analogous to the code of the computer program that makes predictions.  What you're observing here is that humans have 750 megabytes of DNA, and even if most of that is junk and not all of what's left is specifying brain behavior, it still leaves a pretty large computer program that could have a lot of prior information programmed into it.  Let's say that your brain, or rather, your infant pre-brain wiring algorithm, was effectively a 7.5 megabyte program - if it's actually 75 megabytes, that makes little difference.  By exposing that 7.5 megabyte program to all the information coming in from your eyes, ears, nose, proprioceptive sensors telling you where your limbs were, stomach pains, and so on, your brain updated itself into forming the modern Ashley, whose hundred trillion synapses might be encoded by, say, one petabyte of information.\n\nAshley:  A key part of that process was my brain learning to *control* the environment, not just passively observing it.  Like, I'm *certain* it mattered to my brain wiring that my brain saw the room shift in a certain way when it sent out signals telling my eyes to move, and so on.\n\nBlaine:  Yes, that's surely true.  But talking about the sequential *control* problem is more complicated math.  [11v] is the agent that uses Solomonoff induction as its epistemology and expected reward as its decision theory.  That introduces extra complexity, so it makes sense to talk about just Solomonoff induction first.  Imagine for the moment that we were *just* looking at your sensory data, and trying to predict what would come next in that.\n\nAshley:  Wouldn't it make more sense to look at the brain's inputs and outputs, if we wanted to predict the next input, and not just the previous input?\n\nBlaine:  It'd make the problem easier for a Solomonoff inductor to solve, sure; but it also makes the problem more complicated.  Let's talk instead about what would happen if you took the complete sensory record of your life, gave it to an ideally smart agent, and asked the agent to predict what you would see next.  Maybe the agent could do an even better job of prediction if we also told it about your brain's outputs, but I don't think it would be helpless to see patterns in the inputs.\n\nAshley:  It sounds like a pretty hard problem to me.\n\nBlaine:    In terms of what can be predicted *in principle* given this kind of data, what facts are *actually reflected in it* that Solomonoff induction might uncover, we shouldn't imagine a human trying to analyze the data, we should imagine [an entire advanced civilization pondering it for years](http://lesswrong.com/lw/qk/that_alien_message/).  Like, if the Ashley had already read Shakespeare's Hamlet - if the image of those pages had already crossed the sensory stream - and then the Ashley saw a mysterious box outputting 20, 15, 2, 5, 15, 18, I think somebody eavesdropping on that sensory data would be equally able to guess that this was encoding 'tobeor' and guess that the next thing the Ashley saw would be the box outputting 14.  You wouldn't even need an entire alien civilization of superintelligent cryptographers to guess that.\n\nAshley:  But the next item in the Ashley-sequence wouldn't actually be 14.  It would be this huge 1920 x 1080 visual field that showed the box flashing a little picture of '14'.\n\nBlaine:  Sure.  Otherwise it would be a rigged demo, as you say.\n\nAshley:  And what with all the dust specks in my visual field, and maybe my deciding to tilt my head or saccade my eyes using motor instructions that aren't in the sequence, there's no way any possible agent could exactly predict the 66-megabit integer representing the next visual frame.\n\nBlaine:  Indeed, there'd be some element of thermodynamic and quantum randomness preventing that exact prediction even in principle.  So instead of predicting an *exact* next frame, we put a probability distribution on it.\n\nAshley:  A probability distribution over possible 66-megabit frames?  Like, a table with 2 to the 2 to the 66 million entries, summing to 1?\n\nBlaine:  Sure.  2^(32^(1920 x 1080)) isn't a large number when you have unlimited computing power.  As Martin Gardner once observed, \"most finite numbers are very much larger\".  Like I said, Solomonoff induction is an epistemic ideal that requires an unreasonably large amount of computing power.\n\nAshley:  I don't deny that big computations can sometimes help us understand little ones.  But at the point when we're talking about probability distributions that large, I have some trouble holding onto what the probability distribution is supposed to *mean*.\n\nBlaine:  Really?  Just imagine a probability distribution over N possibilities, then let N go to 2 to the 2 to the 66 million.  If we were talking about a letter ranging from A to Z, then putting 100 times as much probability mass on (X, Y, Z) as on the rest of the alphabet, would say that, although you didn't know *exactly* what letter would happen, you expected in a general sense that it would be toward the end of the alphabet.  You would have used 26 probabilities, summing to 1, to very precisely state that vague prediction.  In Solomonoff induction, since we have unlimited computing power, we express our uncertainty about a 1920 x 1080 video frame the same way.  All the various pixel fields you could see if your eye jumped to a plausible place, saw a plausible number of dust specks, and saw the box flash something that visually encoded '14', would have high probability.  Pixel fields where the box vanished and was replaced with a glow-in-the-dark unicorn would have very low, though not zero, probability.\n\nAshley:  Is that really reasonable?\n\nBlaine:  If we could not make identifications like these *in principle*, there would be no principled way in which we could say that you had ever *expected to see something happen* - no way to say that one visual field your eyes saw, had higher probability than any other sensory experience.  We couldn't justify science; we couldn't say that, having performed Galileo's experiment by rolling an inclined cylinder down a plane, Galileo's theory was thereby to some degree supported by having assigned *a high relative probability* to the only actual observations our eyes ever report.\n\nAshley:  I feel a little unsure of that jump, but I suppose I can go along with that for now.  Then the question of \"What probability does Solomonoff induction assign to Canada invading?\" is to be identified, in principle, with the question \"Given my past life experiences and all the visual information that's entered my eyes, what is the relative probability of seeing visual information that encodes Google News with the headline 'CANADA INVADES USA' at some point during the next 300 million seconds?\"\n\nBlaine:  Right!\n\nAshley:  And Solomonoff induction has an in-principle way of assigning this a relatively low probability, which our online silly person could do well to learn from as a matter of principle, even if we couldn't carry out the exact calculations.\n\nBlaine:  Precisely!\n\nAshley:  Fairness requires that I congratulate you on having come further in formalizing 'do good epistemology' as a sequence prediction problem than I previously thought you might.  I mean, you haven't satisfied me yet,  but I wasn't expecting you to get even this far.  So what exactly *is* the ideal calculation?\n\nBlaine:  We're not quite ready to launch right into that.  Next we consider how to represent a *hypothesis* inside this formalism.\n\nAshley:  Hmm.  You said something earlier about updating on a probabilistic mixture of computer programs, which leads me to suspect that in this formalism, a hypothesis or *way the world can be* is a computer program that outputs a sequence of integers.\n\nBlaine:  There's indeed a version of Solomonoff induction that works like that.  But I prefer the version where a hypothesis assigns *probabilities* to sequences of integers.  Like, if the hypothesis is that the world is a fair coin, then we shouldn't try to make that hypothesis predict \"heads - tails - tails - tails - heads\" but should let it just assign a 1/32 prior probability to the sequence HTTTH.\n\nAshley:  I can see that for coins, but I feel a bit iffier on what this means for *the real world*.\n\nBlaine:  A single hypothesis inside the Solomonoff mixture would be a computer program that took in a series of video frames, and assigned a probability to each possible next video frame.  Or for greater simplicity and elegance, imagine a program that took in a sequence of bits, ones and zeroes, and output a rational number for the probability of the next bit being '1'.  We can readily go back and forth between a program like that, and a probability distribution over sequences.  Like, if you can answer all of the questions, \"What's the probability that the coin comes up heads on the first flip?\", \"What's the probability of the coin coming up heads on the second flip, if it came up heads on the first flip?\", and \"What's the probability that the coin comes up heads on the second flip, if it came up tails on the first flip?\" then we can turn that into a probability distribution over sequences of two coinflips.  Analogously, if we have a program that outputs the probability of the next bit, conditioned on a finite number of previous bits taken as input, that program corresponds to a probability distribution over all infinite sequences of bits.\n\nAshley:  I think I followed along with that in theory, though it's not a type of math I'm used to (yet).  So then in what sense is reality a program like that?  In what sense is a program like that a way the world could be - a hypothesis about the world?\n\nBlaine:  Well, I mean, for one thing, we can see the infant Ashley as a program with 7.5 megabytes of information about how to wire up its brain in response to sense data, that sees a bunch of sense data and then has some degree of relative surprise that the Ashley would express about possible additional sense data.  Like in the baby-looking-paradigm experiments where you show a baby an object disappearing behind a screen, and the baby looks longer at those cases, and so we suspect that babies have a concept of object permanence.\n\nAshley:  That sounds like a program that's a way Ashley could be, not a program that's a way the world could be.\n\nBlaine:  Those indeed are dual perspectives on the meaning of Solmonoff induction.  Maybe we can shed some light on this by considering a simpler induction rule, Laplace's Rule of Succession, so-called because it was the original problem and solution considered by Thomas Bayes in the 1750s, and Pierre-Simon Laplace is the one who invented Bayesian inference.\n\nAshley:  Wait, what?\n\nBlaine:  Suppose you have a biased coin with an unknown bias, and every possible bias between 0 and 1 is equally probable.\n\nAshley:  Sounds like a very unlikely situation to arise in the real world.  In the real world, it's quite likely that an unknown frequency is exactly 0, 1, or 1/2, while if you assign equal probability density to every part of the real number field between 0 and 1, the probability of 1 is 0.  Indeed, the probability of all rational numbers put together is zero.\n\nBlaine:  The original problem considered by Thomas Bayes was about an ideal billiard ball bouncing back and forth on an ideal billiard table many times and eventually slowing to a halt; and then bouncing other billiards to see if they halted to the left or the right of the first billiard.  You can see why, in first considering the simplest form of this problem without any complications, we might consider every position of the first billiard to be equally probable.\n\nAshley:  Oh, fine, but if the billiard was really an ideal rolling sphere and the walls were perfectly reflective, it'd never halt in the first place.\n\nBlaine:  Suppose we're told that, after rolling 5 more billiard balls, one billiard ball was to the right, or R, and the other four were to the left, or Ls.  What is the probability that the next billiard ball rolled will be on the left?\n\nAshley:  Five sevenths.\n\nBlaine:  Ah, you've heard this problem before?\n\nAshley:  No, but it's obvious.\n\nBlaine:  Uh... really?\n\nAshley:   Combinatorics.  Consider just the orderings of the balls, instead of their exact positions.  Designate the original ball with the symbol **|**, the next five balls as **LLLLR**, and the next ball to be rolled as **+**.  Given that the current ordering is **LLLL|R** and that all positions and spacings of the underlying balls are equally likely, after rolling the **+**, there will be seven equally likely orderings **+LLLL|R**, **L+LLL|R**, **LL+LL|R**, and so on up to **LLLL|L+R** and **LLLL|R+**.  In five of those seven orderings, the **+** is on the left of the **|**.  In general, if we see M of **L** and N of **R**, the probability of the next item being an **L** is (M + 1) / (M + N + 2).\n\nBlaise:  Gosh...  Well, there's also a more complicated proof devised by Thomas Bayes that starts by considering every position of the original ball to be equally likely a priori, the additional balls as providing evidence about that position, and then integrating over the posterior probabilities of the original ball's possible positions to arrive at the probability that the next ball lands on the left or right.\n\nAshley:  Heh.  And is all that extra work useful if you also happen to know a little combinatorics?\n\nBlaise:  Wel, it tells me exactly how my beliefs about the original ball change with each new piece of evidence.  Suppose I instead asked you something along the lines of, \"Given 4 **L** and 1 **R**, where do you think the original ball **+** is most likely to be on the number line?  How likely is it to be within 0.1 distance of there?\"  Thomas Bayes said how to calculate the exact posterior density function for the ball's probable position after seeing the evidence.\n\nAshley:  That's fair; I don't see a simple combinatoric answer for the later part.\n\nBlaise:  Anyway, let's just take at face value that Laplace's Rule of Succession says that, after observing M 1s and N 0s, the probability of getting a 1 next is (M + 1) / (M + N + 2).\n\nAshley:  But of course.\n\nBlaise:  We can consider Laplace's Rule as a short Python program that takes in a sequence of 1s and 0s, and spits out the probability that the next bit in the sequence will be 1.  We can also consider it as a probability distribution over infinite sequences, like this:\n\n- **1** :  1/2\n- **0** :  1/2\n- **10** : 1/2 * 1/3 = 1/6\n- **11** : 1/2 * 2/3 = 1/3\n- **01** : 1/6\n\nBlaise:  ...and so on.  Now, we can view this as a rule someone might espouse for *predicting* coinflips, but also view it as corresponding to a particular class of possible worlds containing randomness.  I mean, Laplace's Rule isn't the only rule you could use.  Suppose I had a barrel containing ten white balls and ten green balls.  If you already knew this about the barrel, then after seeing M white balls and N green balls, you'd predict the next ball being white with probability (10 - M) / (20 - M - N).  If you use Laplace's Rule, that's like believing the world was like a billiards table with an original ball rolling to a stop at a random point and new balls ending up on the left or right.  If you use (10 - M) / (20 - M - N), that's like the hypothesis that there's ten red balls and ten white balls in a barrel.  There isn't really a sharp border between rules we can use to predict the world, and rules for how the world behaves -\n\nAshley:  Well, that sounds just plain wrong.  The map is not the territory, don'cha know?  If Solomonoff induction can't tell the difference between maps and territories, maybe it doesn't contain all epistemological goodness after all.\n\n*(Eliezer, whispering:  Ashley has a point, by the way.  See [ naturalistic reflection].)*\n\nBlaise:  Maybe it'd be better to say that there's a dualism between good ways of computing predictions and being in actual worlds where that kind of predicting works well?  Like, you could also see Laplace's Rule as implementing the rules for a world with randomness where the original billiard ball ends up in a random place, so that the first thing you see is equally likely to be 1 or 0, and then to get what probably happens on round 2, we tell the world what happened on round 1 so that it can update what the background random events were.\n\nAshley:  Mmmaybe.\n\nBlaise:  If you go with the version where Solomonoff induction is over programs that just spit out a determined string of ones and zeroes, we could see those programs as corresponding to particular environments - ways the world *could be* that would produce our sensory input, the sequence.  Though in that case, I think we'd need to jump ahead and consider the more sophisticated decision-problem that appears in [11v]: an environment is a program that takes your motor outputs as its input, and then outputs your sensory inputs as its output.  But personally, I think I prefer the version of Solomonoff induction where the elementary hypotheses are computer programs that reason about the world a particular way.  Because I think there are valid lessons we can derive from that.  We should, in fact, prefer simple meta-level rules of reasoning that have done best in predicting the future given the past, not just prefer simple object-level views of the world.  Like, jumping to science from pre-science and so on.  Or going from \"explain the world using heroic mythology\" to \"explain the world using differential equations\".\n\nAshley:  Did you say something earlier about the deterministic and probabilistic versions of Solomonoff induction giving the same answers?  Like, is it a distinction without a difference whether we ask about simple programs that reproduce the observed data versus simple programs that assign a lot of probability to the data?\n\nBlaise:  I'm *told* the answers are the same but I confess I can't quite see why that's true, unless there's some added assumption I'm missing.  So let's talk about programs that assign probabilities for now, because I think that case is clearer.  By the way, I note that we've started talking about *simple* programs that assign high probability to our observations so far.\n\nAshley:  Yes, well, it seems like an obvious step, especially considering that you were already talking about \"simple programs\" and Occam's Razor a while back.  Solomonoff induction is part of the Bayesian program of inference, right?\n\nBlaise:  Indeed.  Very much so.\n\nAshley:  Okay, so let's talk about the program, or hypothesis, for \"This barrel has an unknown frequency of white and green balls\", versus the hypothesis \"This barrel has 10 white and 10 green balls\", versus the hypothesis, \"This barrel always puts out a green ball after a white ball and vice versa.\"  Let's say we see a green ball, then a white ball, the sequence **GW**.  The first hypothesis assigns this probability 1/2 * 1/3 = 1/6, the second hypothesis assigns this probability 10/20 * 9/19 or roughly 1/4, and the third hypothesis assigns probability 1/2 * 1.  Now it seems to me that there's some important sense in which, even though Laplace's Rule assigned a lower probability to the data, it's significantly simpler than the second and third hypotheses and is the wiser answer.  Does Solomonoff induction agree?\n\nBlaise:  I think you might be taking into account some prior knowledge that isn't in the sequence itself, there.  Like, things that alternate either **101010...** or **010101...** are *objectively* simple in the sense that a short computer program simulates them or assigns probabilities to them.  It's just unlikely to be true about *a barrel of red and green balls*.  If **10** is literally the first sense data that you ever see, when you are just a tiny little infant superintelligence with two bits of binary data, then \"Maybe the universe consists of alternating bits\" is no less reasonable than \"Maybe the universe produces bits with an unknown random frequency anywhere between 0 and 1.\"\n\nAshley:  Conceded.  But as I was going to say, we have three hypotheses that assigned 1/6, ~1/4, and 1/2 to the observed data; but to know the posterior probabilities of these hypotheses we need to actually say how relatively likely they were a priori, so we can multiply by the odds ratio.  Like, if the prior odds were 3:2:1, the posterior odds would be 3:2:1 * (2/12 : 3/12 : 6/12) = 3:2:1 * 2:3:6 = 6:6:6 = 1:1:1.  Now, how would Solomonoff induction assign prior probabilities to those computer programs?  Because I remember you saying, way back when, that you thought Solomonoff was the answer to \"How should Bayesians assign priors?\"\n\nBlaise:  Well, how would you do it?\n\nAshley:  I mean... yes, the simpler rules should be favored, but it seems to me that there's some deep questions as to the exact quantitative relative 'simplicity' of the rules (M + 1) / (M + N + 2), or the rule (10 - M) / (20 - M - N), or the rule \"alternate the bits\"...\n\nBlaise:  Eh, just come up with a rule.\n\nAshley:  Okay, if I just say the rule I think you're looking for, the rule would be, \"The complexity of a computer program is the number of bits needed to specify it to some arbitrary but reasonable choice of compiler or Universal Turing Machine, and since there's 16 possible programs made of 4 bits, the prior probability is 1/2 to the power of the number of bits.\"  So if it takes 16 bits to specify Laplace's Rule of Succession, which seems a tad optimistic, then the prior probability would be 1/65536, which seems a tad pessimistic.\n\nBlaise:  Now just apply that rule to the infinity of possible computer programs that assign probabilities to the observed data, update their posterior probabilities based on the probability they've assigned to the evidence so far, sum over all of them to get your next prediction, and we're done.  And yes, that requires a [ hypercomputer] that can solve the [halting problem](https://en.wikipedia.org/wiki/Halting_problem), but we're talking ideals here.\n\n[todo: write out equation, once almost done or editor is no longer running LaTeX each time.]\n\nAshley:  Uh.\n\nBlaine:  Yes?\n\nAshley:  Um...\n\nBlaine:  What is it?\n\nAshley:  You invoked a countably infinite set, so I'm trying to figure out if my predicted probability for the next bit must necessarily converge to a limit as I consider increasingly large finite subsets in any order.\n\nBlaine *(sighs)*:  Of course you are.\n\nAshley:  I think you might have left out some important caveats.  Like, if I take the rule literally, then the program \"**0**\" has probability 1/2, the program \"**1** has probability 1/2, the program **01** has probability 1/4 and now the total probability is 1.25 which is *too much,* and I can't actually normalize it because the series sums to infinity.  Now, this just means we need to, say, decide that the probability of a program having length 1 is 1/2, the probability of it having length 2 is 1/4, and so on out to infinity, but it's an added postulate.\n\nBlaine:  The conventional method is to require a [prefix-free code](https://en.wikipedia.org/wiki/Prefix_code).  If \"**0111** is a valid program then **01110** cannot be a valid program.  With that constraint, assigning \"1/2 to the power of the length of the code\", to all valid codes, will sum to less than 1; and we can normalize their relative probabilities to get the actual prior.\n\nAshley:  Okay.  And you're sure that it doesn't matter in what order we consider more and more programs as we approach the limit, because... no, never mind.  Every program has positive probability mass, with the total set summing to 1, and Bayesian updating doesn't change that.  So as I consider more and more programs, in any order, there's only so many large contributions that can be made from the mix - only so often that the final probability can change.  Like, let's say there are at most 99 programs with probability 1% that assign probability 0 to the next bit being a 1; that's only 99 times the final answer can go down by as much as 0.01, as the limit is approached.\n\nBlaine:  This idea generalizes, and is important.  List all possible computer programs, in any order you like.  Use any definition of *simplicity* that you like, so long as for any given amount of simplicity, there are only a finite number of computer programs that simple.  As you go on carving off chunks of prior probability mass and assigning them to programs, it *must* be the case that as programs get more and complicated, their prior probability approaches zero! - though it's still positive for every finite program, because of [Cromwell's Rule](https://en.wikipedia.org/wiki/Cromwell%27s_rule).  If you consider, say, 1%, then you can't have more than 99 programs assigned 1% probability and still obey Cromwell's Rule, which means there must be some *most complex* program that is assigned 1% probability, which means every more complicated program must have less than 1% probability out to the end of the infinite list.  Since there's only a finite number of programs simpler than some level epsilon, there must be some *least* probable program of that kind, and there must be a level of complication delta such that every program more complicated than delta is less probable than every program as simple as epsilon.  And so it goes.\n\nAshley:  Huh.  I don't think I've ever heard that justification for Occam's Razor before.  I think I like it.  I mean, I've heard a lot of appeals to the empirical simplicity of the world, and so on, but this is the first time I've seen a *logical* proof that, in the limit, more complicated hypotheses *must* be less likely than simple ones.\n\nBlaine:  Behold the awesomeness that is Solomonoff Induction!\n\nAshley:  Uh, but you didn't actually use the notion of *computational* simplicity to get that conclusion, you just required that the supply of probability mass is finite and the supply of potential complications is infinite.\n\nBlaine:  Well, maybe.  But it so happens that Yudkowsky did invent or reinvent that argument after pondering Solomonoff induction, and if it predates him and Solomonoff then Yudkowsky doesn't know the source.  Concrete inspiration for simplifiable arguments is also a credit to a theory, especially if the simplifiable argument didn't exist before that.\n\nAshley:  Fair enough.  My next question is about the choice of Universal Turing Machine - the choice of compiler for our program codes.  There's an infinite number of possibilities there, and in principle, the right choice of compiler can make our probability for the next thing we'll see be anything we like.   At least I'd expect this to be the case, based on how the \"[problem of induction](https://en.wikipedia.org/wiki/Problem_of_induction)\" usually goes.  So with the right choice of Universal Turing Machine, our online crackpot can still make it be the case that Solomonoff induction predicts Canada invading the USA.\n\nBlaine:  One way of looking at the problem of good epistemology, I'd say, is that the job of a good epistemology is not to make it *impossible* to err, you can still blow off your foot if you really insist on pointing the shotgun at your foot and pulling the trigger.  The job of good epistemology is to make it *more obvious* when you're about to blow your own foot off with a shotgun.  On this dimension, Solomonoff Induction excels.  If you claim that we ought to pick an enormously complicated compiler to encode our hypotheses, in order to make the 'simplest hypothesis that fits the evidence' be one that predicts Canada invading the USA, then it should be obvious to everyone except you, and indeed, obvious to you if you're not completely delusional, that you are in the process of screwing up.  \n\nAshley:  Ah, but of course they'll say that their code is just the simple and natural choice of Universal Turing Machine, because they'll exhibit a meta-UTM which outputs that UTM given only a short code.  And if you say the meta-UTM is complicated -\n\nBlaine:  Flon's Law says, \"There is not now, nor has there ever been, nor will there ever be, any programming language in which it is the least bit difficult to write bad code.\"  You can't make it impossible for people to screw up, but you can make it *more obvious.*  And Solomonoff induction would make it even more obvious than might at first be obvious, because -\n\nAshley:  Your Honor, I move to have the previous sentence taken out and shot.\n\nBlaine:  Solomonoff induction, if we could actually run the hypercomputer, would be incredibly good at highlighting stupidity, even more than might at first be obvious.  Let's say that the whole of your sensory information is the string **10101010...**  Consider the stupid hypothesis, \"This program has a 99% probability of producing a **1** on every turn\", which you jumped to after seeing the first bit.  What would you need to claim your priors were like - what Universal Turing Machine would you need to endorse - in order to maintain blind faith in that hypothesis in the face of ever-mounting evidence?\n\nAshley:  You'd need a Universal Turing Machine **blind-utm** that assigned a very high probability to the **blind** program \"def ProbNextElementIsOne(previous_sequence): return 0.99\".  Like, if **blind-utm** sees the code **0**, it executes the **blind** program \"return 0.99\".  And to defend yourself against charges that your UTM **blind-utm** was not itself simple, you'd need a meta-UTM, **blind-meta** which, when it sees the code **10**, executes **blind-utm**.  And to really wrap it up, you'd need to take a fixed point through all towers of meta and use diagonalization to create the UTM **blind-diag** that, when it sees the program code **0**, executes \"return 0.99\", and when it sees the program code **10**, executes **blind-diag**.  I guess I can see some sense in which, even if that doesn't resolve Hume's problem of induction, anyone *actually advocating that* would be committing blatant shenanigans on a commonsense level, arguably more blatant than it would have been if we hadn't actually made them present the UTM.\n\nBlaine:  Actually, the shenanigans have to be much worse than that in order to fool Solomonoff induction.  Like, Solomonoff induction using your **blind-diag** isn't fooled for a minute, even taking **blind-diag** entirely on its own terms.\n\nAshley:  Really?\n\nBlaise:  Assuming 60 sequence items per second?  Yes, absolutely, Solomonoff induction shrugs off the delusion in the first minute, unless there's further and even more blatant shenanigans.  We did require that your **blind-diag** be a *Universal* Turing Machine, meaning that it can reproduce every computable probability distribution over sequences, given some particular code to compile.  Let's say there's a 200-bit code **laplace** for Laplace's Rule of Succession, \"lambda sequence: return (sequence.count('1') + 1) / (len(sequence) + 2)\", so that its prior probability relative to the 1-bit code for **blind** is 2^-200.  Let's say that the sense data is around 50/50 1s and 0s.  Every time we see a 1, **blind** gains a factor of 2 over **laplace** (99% vs. 50% probability), and every time we see a 0, **blind** loses a factor of 50 over **laplace** (1% vs. 50% probability).  On average, every 2 bits of the sequence, **blind** is losing a factor of 25 or, say, a bit more than 4 bits, i.e., on average **blind** is losing two bits of probability per element of the sequence observed.  So it's only going to take 100 bits, or a little less than two seconds, for **laplace** to win out over **blind**.\n\nAshley:  I see.  I was focusing on a UTM that assigned lots of prior probability to **blind**, but what I really needed was a compiler that, *while still being universal* and encoding every possibility somewhere, still assigned a really tiny probability to **laplace**, **faircoin** that encodes \"return 0.5\", and every other hypothesis that does better, round by round, than **blind**.  So what I really need to carry off the delusion is **obstinate-diag** that is universal, assigns high probability to **blind**, requires billions of bits to specify **laplace**, and also requires billions of bits to specify any UTM that can execute **laplace** as a shorter code than billions of bits.  Because otherwise the skeptic will say, \"Ah, but given the evidence, this other UTM was better instead.\"  I agree that those are even more blatant shenanigans than I thought.\n\nBlaise:  Yes.  And even *then*, even if your UTM takes two billion bits to specify **faircoin**, Solomonoff induction will lose its faith in **blind** after seeing a billion bits.  Which will actually happen before the first year is out, if we're getting 60 bits per second.  And if you turn around and say, \"Oh, well, I didn't mean *that* was my UTM, I really meant *this* was my UTM, this thing over here where it takes a *trillion* bits to encode **faircoin**\", then that's probability-theory-violating shenanigans, you'd have to be backpedaling on your already-ridiculous prior.\n\nAshley:  That's actually a very interesting point - that what's needed for Bayesian to maintain a delusion in the face of mounting evidence is not so much a blind faith in the delusory hypothesis, as a blind skepticism of all its alternatives.  But what if their UTM requires a googol bits to specify **faircoin**?  What if **blind** and **blind-diag**, or programs pretty much isomorphic to them, are the only programs that can be specified in less than a googol bits?\n\nBlaise:  Then your desire to shoot your own foot off has been made very, very apparent to anyone who understands Solomonoff induction.  We're not going to get absolutely objective prior probabilities as a matter of logical deduction, not without principles that are unknown to me and beyond the scope of Solomonoff induction.  But we can make it really obvious and force you to construct a downright embarrassing Universal Turing Machine in order to be stupid.  We can give our pragmatic answer to the problem of induction, not by appealing that \"a universe that can be predicted better than chance by simple computation\" is something that's likely a priori, but by appealing that it would violate [Cromwell's Rule](https://en.wikipedia.org/wiki/Cromwell%27s_rule) and be exceedingly special pleading to assign the possibility of a computationally learnable universe a probability of less than 2 to the negative millionth power *a priori*.\n\nAshley:  I don't know that good *pragmatic* answers to the problem of induction were ever in short supply.  Still, on the margins, it's a more forceful pragmatic answer than the last one I remember hearing.\n\nBlaise:  Yay!  *Now* isn't Solomonoff induction wonderful?\n\nAshley:  Maybe?  You didn't really use the principle of *computational* simplicity to derive that lesson.  You just used that *some inductive principle* ought to have a prior probability of more than 2^(-1,000,000).\n\nBlaise:  ...\n\nAshley:  Can you give me an example of a problem where the *computational* definition of simplicity matters and can't be factored back out?\n\nBlaise:  Okay, fine, as it happens, yes I can.  I can give you *three* examples of how it matters.\n\nAshley:  Vun... two... three!  Three examples!  Ha-ah-ah!\n\nBlaise:  Do you *have* to do that every - oh, never mind.  Example one is that diffraction is a simpler explanation of rainbows than divine intervention, example two is that galaxies and quantum mechanics are not so improbable that no one could ever believe in them, and example three is outperforming Terrence Tao.\n\nAshley:  These statements are all so obvious that no further explanation of any of them is required.\n\n(in progress)",
      "metaText": "",
      "isTextLoaded": true,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 5,
      "maintainerCount": 1,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": 0,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 54,
      "redLinkCount": 0,
      "lockedBy": "2",
      "lockedUntil": "2017-12-25 00:28:56",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": {
        "edit": {
          "has": false,
          "reason": "You don't have domain permission to edit this page"
        },
        "proposeEdit": {
          "has": true,
          "reason": ""
        },
        "delete": {
          "has": false,
          "reason": "You don't have domain permission to delete this page"
        },
        "comment": {
          "has": false,
          "reason": "You can't comment in this domain because you are not a member"
        },
        "proposeComment": {
          "has": true,
          "reason": ""
        }
      },
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [
        "11w"
      ],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [
        "4y7"
      ],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [
        {
          "id": "2247",
          "parentId": "1r6",
          "childId": "1hh",
          "type": "requirement",
          "creatorId": "1",
          "createdAt": "2016-06-17 21:58:56",
          "level": 2,
          "isStrong": true,
          "everPublished": true
        },
        {
          "id": "2256",
          "parentId": "21c",
          "childId": "1hh",
          "type": "requirement",
          "creatorId": "1",
          "createdAt": "2016-06-17 21:58:56",
          "level": 2,
          "isStrong": false,
          "everPublished": true
        }
      ],
      "subjects": [
        {
          "id": "2246",
          "parentId": "11w",
          "childId": "1hh",
          "type": "subject",
          "creatorId": "1",
          "createdAt": "2016-06-17 21:58:56",
          "level": 2,
          "isStrong": false,
          "everPublished": true
        },
        {
          "id": "5824",
          "parentId": "1hh",
          "childId": "1hh",
          "type": "subject",
          "creatorId": "1",
          "createdAt": "2016-08-02 16:54:25",
          "level": 2,
          "isStrong": true,
          "everPublished": true
        }
      ],
      "lenses": [],
      "lensParentId": "11w",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "22944",
          "pageId": "1hh",
          "userId": "2",
          "edit": 54,
          "type": "newEdit",
          "createdAt": "2017-12-25 00:28:56",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "22943",
          "pageId": "1hh",
          "userId": "2",
          "edit": 53,
          "type": "newEdit",
          "createdAt": "2017-12-24 22:36:36",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "22799",
          "pageId": "1hh",
          "userId": "2cl",
          "edit": 52,
          "type": "newEdit",
          "createdAt": "2017-10-07 21:33:55",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "22795",
          "pageId": "1hh",
          "userId": "2bj",
          "edit": 51,
          "type": "newEdit",
          "createdAt": "2017-10-05 09:39:15",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "22782",
          "pageId": "1hh",
          "userId": "8nz",
          "edit": 50,
          "type": "newEdit",
          "createdAt": "2017-09-21 00:47:43",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": "Fix consistency of example ball colors"
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "22781",
          "pageId": "1hh",
          "userId": "8nz",
          "edit": 49,
          "type": "newEditProposal",
          "createdAt": "2017-09-21 00:46:52",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "22761",
          "pageId": "1hh",
          "userId": "8p0",
          "edit": 48,
          "type": "newEdit",
          "createdAt": "2017-09-20 00:25:38",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "22760",
          "pageId": "1hh",
          "userId": "8nz",
          "edit": 47,
          "type": "newEdit",
          "createdAt": "2017-09-19 22:12:53",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": "Fix typo"
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "21114",
          "pageId": "1hh",
          "userId": "2",
          "edit": 46,
          "type": "newEdit",
          "createdAt": "2016-12-23 22:29:30",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "18333",
          "pageId": "1hh",
          "userId": "1yq",
          "edit": 0,
          "type": "newTag",
          "createdAt": "2016-08-04 13:43:50",
          "auxPageId": "4y7",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "18086",
          "pageId": "1hh",
          "userId": "1",
          "edit": 0,
          "type": "newTeacher",
          "createdAt": "2016-08-02 16:54:26",
          "auxPageId": "1hh",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "18087",
          "pageId": "1hh",
          "userId": "1",
          "edit": 0,
          "type": "newSubject",
          "createdAt": "2016-08-02 16:54:26",
          "auxPageId": "1hh",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "17552",
          "pageId": "1hh",
          "userId": "1",
          "edit": 44,
          "type": "newEdit",
          "createdAt": "2016-07-26 17:22:10",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "8152",
          "pageId": "1hh",
          "userId": "32",
          "edit": 43,
          "type": "newEdit",
          "createdAt": "2016-03-03 18:03:17",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "8117",
          "pageId": "1hh",
          "userId": "32",
          "edit": 42,
          "type": "newEdit",
          "createdAt": "2016-03-03 04:41:39",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "7363",
          "pageId": "1hh",
          "userId": "2",
          "edit": 41,
          "type": "newRequirement",
          "createdAt": "2016-02-17 22:08:52",
          "auxPageId": "21c",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "7347",
          "pageId": "1hh",
          "userId": "2",
          "edit": 41,
          "type": "newEdit",
          "createdAt": "2016-02-17 21:28:01",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "7331",
          "pageId": "1hh",
          "userId": "2",
          "edit": 40,
          "type": "newRequirement",
          "createdAt": "2016-02-17 20:52:58",
          "auxPageId": "1r6",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "7329",
          "pageId": "1hh",
          "userId": "2",
          "edit": 40,
          "type": "newSubject",
          "createdAt": "2016-02-17 20:52:53",
          "auxPageId": "11w",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "6448",
          "pageId": "1hh",
          "userId": "1",
          "edit": 40,
          "type": "newEdit",
          "createdAt": "2016-02-05 01:49:27",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "5258",
          "pageId": "1hh",
          "userId": "2",
          "edit": 39,
          "type": "newEdit",
          "createdAt": "2016-01-15 04:06:11",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "5212",
          "pageId": "1hh",
          "userId": "2",
          "edit": 38,
          "type": "newEdit",
          "createdAt": "2016-01-12 02:35:30",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "5211",
          "pageId": "1hh",
          "userId": "2",
          "edit": 37,
          "type": "newEdit",
          "createdAt": "2016-01-12 02:35:04",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "5210",
          "pageId": "1hh",
          "userId": "2",
          "edit": 36,
          "type": "newEdit",
          "createdAt": "2016-01-12 02:34:15",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "5209",
          "pageId": "1hh",
          "userId": "2",
          "edit": 35,
          "type": "newEdit",
          "createdAt": "2016-01-12 02:33:13",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "5208",
          "pageId": "1hh",
          "userId": "2",
          "edit": 34,
          "type": "newEdit",
          "createdAt": "2016-01-12 02:32:15",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "5207",
          "pageId": "1hh",
          "userId": "2",
          "edit": 33,
          "type": "newEdit",
          "createdAt": "2016-01-12 02:26:11",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "5206",
          "pageId": "1hh",
          "userId": "2",
          "edit": 32,
          "type": "newEdit",
          "createdAt": "2016-01-12 02:25:46",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "5205",
          "pageId": "1hh",
          "userId": "2",
          "edit": 31,
          "type": "newEdit",
          "createdAt": "2016-01-12 02:23:25",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "5204",
          "pageId": "1hh",
          "userId": "2",
          "edit": 30,
          "type": "newEdit",
          "createdAt": "2016-01-12 01:52:45",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "5181",
          "pageId": "1hh",
          "userId": "32",
          "edit": 29,
          "type": "newEdit",
          "createdAt": "2016-01-11 06:25:36",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "5180",
          "pageId": "1hh",
          "userId": "32",
          "edit": 28,
          "type": "newEdit",
          "createdAt": "2016-01-11 05:56:39",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "5152",
          "pageId": "1hh",
          "userId": "2",
          "edit": 27,
          "type": "newEdit",
          "createdAt": "2016-01-10 03:34:33",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "5151",
          "pageId": "1hh",
          "userId": "2",
          "edit": 26,
          "type": "newEdit",
          "createdAt": "2016-01-10 03:33:28",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "5150",
          "pageId": "1hh",
          "userId": "2",
          "edit": 25,
          "type": "newEdit",
          "createdAt": "2016-01-10 03:31:36",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "5149",
          "pageId": "1hh",
          "userId": "2",
          "edit": 24,
          "type": "newEdit",
          "createdAt": "2016-01-10 03:25:49",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "5148",
          "pageId": "1hh",
          "userId": "2",
          "edit": 23,
          "type": "newEdit",
          "createdAt": "2016-01-10 03:24:39",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "5147",
          "pageId": "1hh",
          "userId": "2",
          "edit": 22,
          "type": "newEdit",
          "createdAt": "2016-01-10 01:31:22",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "5146",
          "pageId": "1hh",
          "userId": "2",
          "edit": 21,
          "type": "newEdit",
          "createdAt": "2016-01-10 01:30:08",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "5145",
          "pageId": "1hh",
          "userId": "2",
          "edit": 20,
          "type": "newEdit",
          "createdAt": "2016-01-10 01:28:53",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "5144",
          "pageId": "1hh",
          "userId": "2",
          "edit": 19,
          "type": "newEdit",
          "createdAt": "2016-01-10 01:23:09",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "5143",
          "pageId": "1hh",
          "userId": "2",
          "edit": 18,
          "type": "newEdit",
          "createdAt": "2016-01-10 01:14:54",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "5142",
          "pageId": "1hh",
          "userId": "2",
          "edit": 17,
          "type": "newEdit",
          "createdAt": "2016-01-10 00:04:26",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "5141",
          "pageId": "1hh",
          "userId": "2",
          "edit": 16,
          "type": "newEdit",
          "createdAt": "2016-01-09 23:25:29",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "5140",
          "pageId": "1hh",
          "userId": "2",
          "edit": 15,
          "type": "newEdit",
          "createdAt": "2016-01-09 23:23:49",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "5139",
          "pageId": "1hh",
          "userId": "2",
          "edit": 14,
          "type": "newEdit",
          "createdAt": "2016-01-09 23:20:17",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "5136",
          "pageId": "1hh",
          "userId": "2",
          "edit": 13,
          "type": "newEdit",
          "createdAt": "2016-01-09 23:18:52",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "5135",
          "pageId": "1hh",
          "userId": "2",
          "edit": 12,
          "type": "newEdit",
          "createdAt": "2016-01-09 23:18:14",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "5134",
          "pageId": "1hh",
          "userId": "2",
          "edit": 11,
          "type": "newEdit",
          "createdAt": "2016-01-09 23:17:35",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "5058",
          "pageId": "1hh",
          "userId": "2",
          "edit": 10,
          "type": "newEdit",
          "createdAt": "2016-01-07 04:22:09",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "5056",
          "pageId": "1hh",
          "userId": "2",
          "edit": 9,
          "type": "newEdit",
          "createdAt": "2016-01-07 00:55:52",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "5055",
          "pageId": "1hh",
          "userId": "2",
          "edit": 8,
          "type": "newEdit",
          "createdAt": "2016-01-07 00:55:12",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "5054",
          "pageId": "1hh",
          "userId": "2",
          "edit": 7,
          "type": "newEdit",
          "createdAt": "2016-01-07 00:41:35",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "5046",
          "pageId": "1hh",
          "userId": "2",
          "edit": 6,
          "type": "newEdit",
          "createdAt": "2016-01-06 21:27:00",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "4807",
          "pageId": "1hh",
          "userId": "2",
          "edit": 5,
          "type": "newEdit",
          "createdAt": "2015-12-30 04:03:51",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "4806",
          "pageId": "1hh",
          "userId": "2",
          "edit": 4,
          "type": "newEdit",
          "createdAt": "2015-12-30 03:58:25",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "4805",
          "pageId": "1hh",
          "userId": "2",
          "edit": 3,
          "type": "newEdit",
          "createdAt": "2015-12-30 03:57:37",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "4804",
          "pageId": "1hh",
          "userId": "2",
          "edit": 2,
          "type": "newEdit",
          "createdAt": "2015-12-30 03:56:41",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "4803",
          "pageId": "1hh",
          "userId": "2",
          "edit": 1,
          "type": "newEdit",
          "createdAt": "2015-12-30 03:55:19",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "4777",
          "pageId": "1hh",
          "userId": "2",
          "edit": 0,
          "type": "newParent",
          "createdAt": "2015-12-30 00:54:18",
          "auxPageId": "11w",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        }
      ],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": true,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "1ln": {
      "likeableId": "553",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "1ln",
      "edit": 6,
      "editSummary": "alias. note to self: come back and explain new requisites system.",
      "prevEdit": 5,
      "currentEdit": 6,
      "wasPublished": true,
      "type": "wiki",
      "title": "Arbital requisites",
      "clickbait": "To understand a thing you often need to understand some other things.",
      "textLength": 1210,
      "alias": "arbital_requisite",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "1yq",
      "editCreatedAt": "2016-07-19 23:24:15",
      "pageCreatorId": "1",
      "pageCreatedAt": "2016-01-11 17:09:53",
      "seeDomainId": "0",
      "editDomainId": "3",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 0,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 316,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": 0,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "1lw": {
      "likeableId": "559",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "1lw",
      "edit": 5,
      "editSummary": "added links",
      "prevEdit": 4,
      "currentEdit": 5,
      "wasPublished": true,
      "type": "wiki",
      "title": "Mathematics",
      "clickbait": "Mathematics is the study of numbers and other ideal objects that can be described by axioms.",
      "textLength": 745,
      "alias": "math",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "1yq",
      "editCreatedAt": "2016-06-22 17:49:03",
      "pageCreatorId": "2",
      "pageCreatedAt": "2016-01-15 03:02:51",
      "seeDomainId": "0",
      "editDomainId": "1",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 0,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 2288,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": 0,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "1lz": {
      "likeableId": "562",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "1lz",
      "edit": 54,
      "editSummary": "",
      "prevEdit": 53,
      "currentEdit": 54,
      "wasPublished": true,
      "type": "wiki",
      "title": "Bayes' rule",
      "clickbait": "Bayes' rule is the core theorem of probability theory saying how to revise our beliefs when we make a new observation.",
      "textLength": 6132,
      "alias": "bayes_rule",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "2",
      "editCreatedAt": "2017-02-21 23:15:37",
      "pageCreatorId": "2",
      "pageCreatedAt": "2016-01-15 04:12:00",
      "seeDomainId": "0",
      "editDomainId": "1",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": true,
      "todoCount": 1,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 131484,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0
      ],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": -2,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "1r6": {
      "likeableId": "692",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 4,
      "dislikeCount": 0,
      "likeScore": 4,
      "individualLikes": [],
      "pageId": "1r6",
      "edit": 2,
      "editSummary": "",
      "prevEdit": 1,
      "currentEdit": 2,
      "wasPublished": true,
      "type": "wiki",
      "title": "Math 2",
      "clickbait": "Do you work with math on a fairly routine basis?  Do you have little trouble grasping abstract structures and ideas?",
      "textLength": 405,
      "alias": "math2",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "2",
      "editCreatedAt": "2016-01-26 02:57:46",
      "pageCreatorId": "2",
      "pageCreatedAt": "2016-01-26 02:35:13",
      "seeDomainId": "0",
      "editDomainId": "1",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": true,
      "todoCount": 0,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 824,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": 0,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "1rt": {
      "likeableId": "711",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "1rt",
      "edit": 3,
      "editSummary": "",
      "prevEdit": 2,
      "currentEdit": 3,
      "wasPublished": true,
      "type": "wiki",
      "title": "Arbital path",
      "clickbait": "Arbital path is a linear sequence of pages tailored specifically to teach a given concept to a user.",
      "textLength": 2327,
      "alias": "Arbital_path",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "1",
      "editCreatedAt": "2016-05-11 20:53:18",
      "pageCreatorId": "1",
      "pageCreatedAt": "2016-01-27 16:33:23",
      "seeDomainId": "0",
      "editDomainId": "3",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 0,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 214,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": 0,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "21c": {
      "likeableId": "977",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "21c",
      "edit": 7,
      "editSummary": "",
      "prevEdit": 6,
      "currentEdit": 7,
      "wasPublished": true,
      "type": "wiki",
      "title": "Laplace's Rule of Succession",
      "clickbait": "Suppose you flip a coin with an unknown bias 30 times, and see 4 heads and 26 tails.  The Rule of Succession says the next flip has a 5/32 chance of showing heads.",
      "textLength": 5685,
      "alias": "laplace_rule_of_succession",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "8b",
      "editCreatedAt": "2016-10-31 12:29:12",
      "pageCreatorId": "2",
      "pageCreatedAt": "2016-02-17 21:28:43",
      "seeDomainId": "0",
      "editDomainId": "1",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 0,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 554,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": 0,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "2c": {
      "likeableId": "1282",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "2c",
      "edit": 40,
      "editSummary": "",
      "prevEdit": 39,
      "currentEdit": 40,
      "wasPublished": true,
      "type": "wiki",
      "title": "Advanced agent properties",
      "clickbait": "How smart does a machine intelligence need to be, for its niceness to become an issue?  \"Advanced\" is a broad term to cover cognitive abilities such that we'd need to start considering AI alignment.",
      "textLength": 21699,
      "alias": "advanced_agent",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "2",
      "editCreatedAt": "2017-03-25 05:59:44",
      "pageCreatorId": "2",
      "pageCreatedAt": "2015-03-24 01:31:50",
      "seeDomainId": "0",
      "editDomainId": "2",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 3,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 943,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0
      ],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": -2,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "2v": {
      "likeableId": "1760",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "2v",
      "edit": 27,
      "editSummary": "",
      "prevEdit": 26,
      "currentEdit": 27,
      "wasPublished": true,
      "type": "wiki",
      "title": "AI alignment",
      "clickbait": "The great civilizational problem of creating artificially intelligent computer systems such that running them is a good idea.",
      "textLength": 5071,
      "alias": "ai_alignment",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "2",
      "editCreatedAt": "2017-01-27 20:32:06",
      "pageCreatorId": "2",
      "pageCreatedAt": "2015-03-26 23:12:18",
      "seeDomainId": "0",
      "editDomainId": "2",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 3,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 3822,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": 0,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "35z": {
      "likeableId": "0",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "35z",
      "edit": 0,
      "editSummary": "",
      "prevEdit": 0,
      "currentEdit": 0,
      "wasPublished": false,
      "type": "",
      "title": "",
      "clickbait": "",
      "textLength": 0,
      "alias": "",
      "externalUrl": "",
      "sortChildrenBy": "",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "",
      "editCreatedAt": "",
      "pageCreatorId": "",
      "pageCreatedAt": "",
      "seeDomainId": "",
      "editDomainId": "",
      "submitToDomainId": "",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": false,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 0,
      "isEditorComment": false,
      "isApprovedComment": false,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 0,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": 0,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "3d": {
      "likeableId": "2273",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "3d",
      "edit": 33,
      "editSummary": "",
      "prevEdit": 32,
      "currentEdit": 33,
      "wasPublished": true,
      "type": "wiki",
      "title": "Arbital",
      "clickbait": "Arbital is the place for crowdsourced, intuitive math explanations.",
      "textLength": 5201,
      "alias": "Arbital",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "1",
      "editCreatedAt": "2016-08-08 16:07:52",
      "pageCreatorId": "1",
      "pageCreatedAt": "2015-03-30 22:19:47",
      "seeDomainId": "0",
      "editDomainId": "8",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 0,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 2635,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": 0,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "3hs": {
      "likeableId": "2499",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "3hs",
      "edit": 19,
      "editSummary": "added link to exemplar pages",
      "prevEdit": 18,
      "currentEdit": 19,
      "wasPublished": true,
      "type": "wiki",
      "title": "Author's guide to Arbital",
      "clickbait": "How to write intuitive, flexible content on Arbital.",
      "textLength": 4420,
      "alias": "author_guide_to_arbital",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "1yq",
      "editCreatedAt": "2016-08-08 14:32:40",
      "pageCreatorId": "1",
      "pageCreatedAt": "2016-05-10 17:55:35",
      "seeDomainId": "0",
      "editDomainId": "3",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 0,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 433,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": 0,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "3n": {
      "likeableId": "2281",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "3n",
      "edit": 7,
      "editSummary": "",
      "prevEdit": 6,
      "currentEdit": 7,
      "wasPublished": true,
      "type": "wiki",
      "title": "Arbital \"parent\" relationship",
      "clickbait": "Parent-child relationship between pages implies a strong, inseparable connection.",
      "textLength": 2510,
      "alias": "Arbital_parent_child",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "8pb",
      "editCreatedAt": "2017-09-20 13:30:49",
      "pageCreatorId": "1",
      "pageCreatedAt": "2015-04-01 19:51:44",
      "seeDomainId": "0",
      "editDomainId": "3",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 0,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 199,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": 0,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "4y7": {
      "likeableId": "3096",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 1,
      "dislikeCount": 0,
      "likeScore": 1,
      "individualLikes": [],
      "pageId": "4y7",
      "edit": 8,
      "editSummary": "applied ER's change to the clickbait",
      "prevEdit": 7,
      "currentEdit": 8,
      "wasPublished": true,
      "type": "wiki",
      "title": "C-Class",
      "clickbait": "This page has substantial content, but may not thoroughly cover the topic, may not meet style and prose standards, or may not explain the concept in a way the target audience will reliably understand.",
      "textLength": 709,
      "alias": "c_class_meta_tag",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "1yq",
      "editCreatedAt": "2016-08-27 17:22:59",
      "pageCreatorId": "1yq",
      "pageCreatedAt": "2016-06-30 02:11:33",
      "seeDomainId": "0",
      "editDomainId": "3",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 0,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 136,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": 0,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "5v": {
      "likeableId": "2350",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "5v",
      "edit": 6,
      "editSummary": "",
      "prevEdit": 5,
      "currentEdit": 6,
      "wasPublished": true,
      "type": "wiki",
      "title": "Algorithmic complexity",
      "clickbait": "When you compress the information, what you are left with determines the complexity.",
      "textLength": 4035,
      "alias": "Kolmogorov_complexity",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "2vh",
      "editCreatedAt": "2016-06-14 22:52:34",
      "pageCreatorId": "2",
      "pageCreatedAt": "2015-05-26 22:10:25",
      "seeDomainId": "0",
      "editDomainId": "1",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 0,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 366,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0
      ],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": -2,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "6s": {
      "likeableId": "2379",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "6s",
      "edit": 16,
      "editSummary": "",
      "prevEdit": 15,
      "currentEdit": 16,
      "wasPublished": true,
      "type": "wiki",
      "title": "Epistemic and instrumental efficiency",
      "clickbait": "An efficient agent never makes a mistake you can predict.  You can never successfully predict a directional bias in its estimates.",
      "textLength": 9533,
      "alias": "efficiency",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "2",
      "editCreatedAt": "2016-06-16 20:21:25",
      "pageCreatorId": "2",
      "pageCreatedAt": "2015-06-09 20:26:31",
      "seeDomainId": "0",
      "editDomainId": "2",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 2,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 1714,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0
      ],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": -2,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    }
  },
  "edits": {
    "1hh": {
      "likeableId": "457",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 10,
      "dislikeCount": 0,
      "likeScore": 10,
      "individualLikes": [],
      "pageId": "1hh",
      "edit": 10,
      "editSummary": "",
      "prevEdit": 9,
      "currentEdit": 54,
      "wasPublished": true,
      "type": "wiki",
      "title": "Semitechnical intro dialogue",
      "clickbait": "An introduction to Solomonoff induction for the unfamiliar reader who isn't bad at math",
      "textLength": 60990,
      "alias": "1hh",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "2",
      "editCreatedAt": "2016-01-07 04:22:09",
      "pageCreatorId": "2",
      "pageCreatedAt": "2015-12-30 03:55:19",
      "seeDomainId": "0",
      "editDomainId": "1",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": false,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 4,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 1032,
      "text": "Ashley:  So, you're going to tell me about... Solomon's induction?\n\nBlaine:  Solomonoff induction.  Goes back to the 1960s and the mathematician Ray Solomonoff.  The key idea in Solomonoff induction is to do sequence prediction by having a probability distribution over -\n\nAshley:  Wait.  Before you launch right into an explanation of what Solomonoff induction *is*, I'd like you to try to tell me what it *does*, or why people study it in the first place.  I find that helps me organize my listening.\n\nBlaine:  Um... okay.  Let me think for a second...\n\nAshley:  Also, while I can imagine things that \"sequence prediction\" might mean, I haven't actually encountered it in a technical context, so you'd better go a bit further back and start more at the beginning.  I do know what a probability distribution is.\n\nBlaine:  Okay.  So... one way of framing the usual reason why people study this sort of thing in the first place, is that sometimes, by studying what it would be possible to do with unlimited computing power, we can gain valuable intuitions about epistemology - that's, uh, the field that studies how to reason about factual questions, how to build a map of reality that reflects the territory...\n\nAshley:  I have some idea what 'epistemology' is, yes.  But I think you might need to start even further back, maybe with some sort of concrete example or something.\n\nBlaine:  Okay.  Um.  So one anecdote that I sometimes use to frame the value of computer science to the study of epistemology, is Edgar Allen Poe's argument in 1833 that chess was uncomputable.\n\nAshley:  That doesn't sound like a thing that actually happened.\n\nBlaine:  I know, but it totally *did* happen and not in a metaphorical sense either!  Edgar Allen Poe wrote an essay explaining why no automaton would ever be able to play chess, and he specifically mentioned \"Mr. Babbage's computing engine\" as an example.  You see, in the nineteenth century, there was for a time this sensation known as the Mechanical Turk - supposedly a machine, an automaton, that could play chess.  At the grandmaster level, no less.  Now today, when we're accustomed to the idea that a computer chip needs to be doing billions of operations per second to play chess, you or I know *immediately* that the Mechanical Turk must have been a fraud and that there must have been a concealed operator inside - a midget, as it turned out - especially when, if you reached out to upset one of the Turk's pieces, the Turk would reach back out and set it back up again, and if you kept on doing it, the Turk would refuse to play, and so on.  *Today* we know that this sort of thing is *hard* to build into a machine.  But in the 19th century, even that much wasn't known.  So when Edgar Allen Poe, who besides being an author was also an accomplished magician, set out to write an essay about the Mechanical Turk, he spent the *second* half of the essay dissecting what was known about the Turk's appearance to (correctly) figure out where the human operator was hiding.  But Poe spent the *first* half of the essay arguing that no automaton - nothing like Mr. Babbage's computing engine - could possibly play chess, which was how he knew *a priori* that the Turk had a concealed human operator.\n\nAshley:  And what was Poe's argument?\n\nBlaine:  Poe observed that in an algebraical problem, each step followed from the previous step of necessity, which was why the steps in solving an algebraical problem could be represented by the deterministic motions of gears in something like Mr. Babbage's computing engine.  But in a chess problem, Poe said, there are many possible chess moves, and no move follows with necessity from the position of the board; and even if you did select one move, the opponent's move would not follow with necessity, so you couldn't represent it with the determined motion of automatic gears.  Therefore, Poe said, whatever was operating the Mechanical Turk must have the nature of Cartesian mind, rather than the nature of deterministic matter, and this was knowable *a priori*.  And then he started figuring out where the required operator was hiding.\n\nAshley:  That's... actually really impressive reasoning, and completely wrong at the same time.\n\nBlaine:  I know!  Isn't it great?\n\nAshley:  I mean, that sounds like Poe correctly identified the *hard* part of playing computer chess - the branching factor of moves and countermoves - the reason why no *simple* machine could do it, and he just didn't realize that a deterministic machine could deterministically check many possible moves in order to figure out the game tree.  So close, and yet so far.\n\nBlaine:  More than a century later, in 1950, Claude Shannon published the first paper ever written on computer chess... and in passing, Shannon gave the formula for playing perfect chess if you had unlimited computing power, the algorithm you'd use to extrapolate the entire game tree.  We could say that Shannon gave a short program that would solve chess if you ran it on a hypercomputer, where a hypercomputer is an ideal computer that can run any finite computation immediately.  And *then* Shannon passed on to talking about the problem of locally guessing how good a board position was, so that you could play chess using only a *small* search.  I say all this to make a point about the value of knowing how to solve problems using hypercomputers, even though hypercomputers don't exist.  Yes, there's often a *huge* gap between the unbounded solution and the practical solution.  It wasn't until 1997, forty-seven years after Shannon's paper giving the unbounded solution, that Deep Blue actually won the world chess championship -\n\nAshley:  And that wasn't just a question of faster computing hardware running Shannon's ideal search algorithm.  There were a lot of new insights along the way, most notably the alpha-beta pruning algorithm and a lot of improvements in positional evaluation.\n\nBlaine:  Right!  But I think some people overreact to that forty-seven year gap, and act like it's *worthless* to have an unbounded understanding of a computer program, just because you might still be forty-seven years away from a practical solution.  But if you don't even have a solution that would run on a hyercomputer, you're Poe in 1833, not Shannon in 1950.  The reason I tell the anecdote about Poe is to illustrate that Poe was *confused* about computer chess in a way that Shannon was not.  When we don't know how to solve a problem even given infinite computing power, the very work we are trying to do is in some sense murky to us.  When we can state code that would solve the problem given a hypercomputer, we have become *less* confused.  Once we have the unbounded solution we understand, in some basic sense, *the kind of work we are trying to perform,* and then we can try to figure out how to do it *efficiently* -\n\nAshley:  Which may well require new insights into the structure of the problem, or even a conceptual revolution in how we imagine the work we're trying to do.\n\nBlaine:  Yes, but the point is that *you can't even get started on that* if you're arguing about how playing chess has the nature of Cartesian mind rather than matter, because deterministic gear motions can't represent non-determined chess moves.  At that point you're not 50 years away from winning the chess championship, you're 150 years away, because it took an extra 100 years to move humanity's understanding to the point where you could easily see how to play chess using a hypercomputer.  I'm not trying to exalt the unbounded solution by denigrating the work required to get a bounded solution, I'm not saying that when we have an unbounded solution we're practically there and the rest is a matter of mere lowly efficiency, I'm trying to compare having the unbounded solution to the horrific confusion of *not knowing what kind of work we're trying to do.*\n\nAshley:  Okay.  I think I understand why, on your view, it's important to know how to solve problems using infinitely fast computers, or hypercomputers as you call them.  When we can say how to answer a question using infinite computing power, that means we crisply understand the question itself, in some sense, while if we can't figure out how to solve a problem using unbounded computing power, that means we're confused about the problem in some sense.  I can see that, yes.  I mean, if you've ever tried to teach the more doomed sort of undergraduate to write computer programs, you know what it means to be confused about what it takes to compute something.  So what does this have to do with \"Solomonoff induction\"?\n\nBlaine:  Ah!  Well, suppose I asked you how to do epistemology using infinite computing power?\n\nAshley:  My good fellow, I would at once reply, \"Beep.  Whirr.  Problem 'do epistemology' not crisply specified.\"  At this stage of affairs, I do not think this reply indicates any fundamental confusion on my part; rather I think it is you who must be clearer.  And don't tell me that 'epistemology' means 'how we should reason in order to construct an accurate map of reality,' because that's still not crisply specified.\n\nBlaine:  Well, perhaps.  But even there I would suggest that it's a mark of intellectual progress to be able to take vague and underspecified ideas like 'do good epistemology' and turn them *into* crisply specified problems.  Imagine that I went up to my friend Cecil, and said, \"How would you do good epistemology given unlimited computing power and a short Python program?\" and Cecil at once came back with an answer, and it was actually a pretty reasonable and good answer once explained.  Cecil would probably know something quite interesting that you do not presently know - for one thing, the knowledge of *a good way to formalize* the problem of 'doing good epistemology' in the first place.\n\nAshley:  I confess to being rather skeptical of this hypothetical.  But if that actually happened - if I agreed, to my own satisfaction, that someone had stated a short Python program that would 'do good epistemology' if run on an unboundedly fast computer - then I agree that I'd probably have learned something *quite interesting* about epistemology.\n\nBlaine:  What Cecil knows about, in this hypothetical, is Solomonoff induction.  In the same way that Claude Shannon answered \"Given infinite computing power, how would you play perfect chess?\", Ray Solomonoff answered \"Given infinite computing power, how would you perfectly find the best hypothesis that fits the facts?\"\n\nAshley:  Suddenly, I find myself strongly suspicious of whatever you are about to say to me.\n\nBlaine:  That's understandable.\n\nAshley:  In particular, I'll ask at once whether \"Solomonoff induction\" assumes that our *hypotheses* are being given to us on a silver platter by the programmers along with the exact data we're supposed to explain, or whether the algorithm is organizing its own data from a big messy situation and *inventing* good hypotheses from scratch -\n\nBlaine:  Great question!  It's the second one.\n\nAshley:  Really?  Okay, now I have to ask whether Solomonoff induction is a recognized concept in good standing in the field of academic computer science, because that does not sound like something modern-day computer science knows how to do.\n\nBlaine:  I wouldn't say it's a widely known concept, but it's one that's in good academic standing.  The method isn't used in modern machine learning because it requires an infinitely fast computer and isn't easily approximated the way that chess is.\n\nAshley:  This really sounds *very* suspicious.  Last time I checked, we hadn't *begun* to formalize the creation of good new hypotheses from scratch.  I've heard about claims to have 'automated' the work that, say, Newton did in inventing classical mechanics, and I've found them all to be incredibly dubious.  Which is to say, they were rigged demos and lies.\n\nBlaine:  I know, but -\n\nAshley:  And then I'm even more suspicious of a claim that someone's algorithm would solve this problem if only they had infinite computing power.  Having some researcher claim that their Good-Old-Fashioned AI semantic network *would* be intelligent if run on a computer so large that, conveniently, nobody can ever test their theory, is not going to persuade me.\n\nBlaine:  ...Do I really strike you as that much of a charlatan?  What have I ever done to you, that you would expect me to try pulling a scam like that?\n\nAshley:  That's fair.  I shouldn't accuse you of planning that scam when I haven't seen you say it.  But I'm pretty sure the problem of \"coming up with good new hypotheses in a world full of messy data\" is basically AI-complete, like, solving this problem means you have strong AI.  And even Mentif-\n\nBlaine:  Do not say the name, or he will appear!\n\nAshley:  Sorry.  Even the legendary first and greatest of all AI crackpots, He-Who-Googles-His-Name, could assert that his algorithms would be all-powerful on a computer large enough to make his claim unfalsifiable.  So what?\n\nBlaine:  That's a very sensible reply given your current background knowledge and this, again, is *exactly* the kind of mental state that reflects a problem that is *confusing* rather than just *hard to implement.*  It's the sort of confusion Poe might feel in 1833, or close to it.  In other words, it's just the sort of conceptual issue we *would* have solved at the point where we could state a short program that could run on a hypercomputer.  Which Ray Solomonoff did in 1964.\n\nAshley:  Okay, let's hear about this supposed general solution to epistemology.\n\nBlaine:  First, try to solve the following puzzle.  1, 3, 4, 7, 11, 18, 29...?\n\nAshley:  Let me look at those for a moment... 47.\n\nBlaine:  Congratulations on engaging in, as we snooty types would call it, 'sequence prediction'.\n\nAshley:  I'm following you so far.\n\nBlaine:  The smarter you are, the more easily you can find the hidden patterns in sequences and predict them successfully. You had to notice the resemblance to the Fibonacci rule to guess the next number. Someone who didn't already know about Fibonacci, or who was worse at mathematical thinking, would have taken longer to understand the sequence or maybe never learned to predict it at all.\n\nAshley: Still with you.\n\nBlaine: It's not a sequence of *numbers* per se... but can you see how the question, \"The sun has risen on the last million days. What is the probability that it rises tomorrow?\" could be viewed as a kind of sequence prediction problem?\n\nAshley: Only if some programmer neatly parses up the world into a series of \"Did the Sun rise on day X starting in 4.5 billion BCE, 0 means no and 1 means yes?  1, 1, 1, 1, 1...\" and so on.  Which is exactly the sort of shenanigan that I see as cheating.\n\nBlaine:  I agree that's cheating.  But suppose I have a robot running around with a webcam showing it a 1920x1080 pixel field that refreshes 60 times a second with 32-bit colors.  I could view that as a giant sequence and ask the robot to predict what it will see happen when it rolls out to watch a sunrise the next day.\n\nAshley: I can't help but notice that the 'sequence' of webcam frames is absolutely enormous, like, the sequence is made up of 66-megabit 'numbers' appearing 3600 times per minute... oh, right, computers much bigger than the universe. And now you're smiling evilly, so I guess that's the point. I also notice that the sequence is no longer deterministically predictable, that it is no longer a purely mathematical object, and that the sequence of webcam frames observed will depend on the robot's choices. This makes me feel a bit shaky about the analogy to predicting the mathematical sequence 1, 1, 2, 3, 5.\n\nBlaine: I'll try to address those points in order. First, Solomonoff induction is about assigning *probabilities* to the next item in the sequence. I mean, if I showed you a box that said 1, 1, 2, 3, 5, 8 you would not be absolutely certain that the next item would be 13. There could be some more complicated rule that just looked Fibonacci-ish but then diverged. You might guess with 90% probability but not 100% probability, or something like that.\n\nAshley: This has stopped feeling to me like math.\n\nBlaine: There is a *large* branch of math, to say nothing of computer science, that deals in probabilities and statistical prediction.  We are going to be describing absolutely lawful and deterministic ways of assigning probabilities after seeing 1, 3, 4, 7, 11, 18.\n\nAshley:  Okay, but if you're later going to tell me that this lawful probabilistic prediction rule underlies a generally intelligent epistemological reasoner, I'm already skeptical.  No matter how large a computer it's run on, I find it hard to imagine that some simple set of rules for assigning probabilities is going to encompass truly and generally intelligent answers about sequence prediction, like Terence Tao would give after looking at the sequence for a while.  We just have no idea how Terence Tao works, so we can't duplicate his abilities in a formal rule, no matter how much computing power that rule gets... you're smiling evilly again.  I'll be *quite* interested if that evil smile turns out to be justified.\n\nBlaine:  Indeed.\n\nAshley: I also find it hard to imagine that this deterministic mathematical rule for assigning probabilities would notice if a box was outputting an encoded version of \"To be or not to be\" from Shakespeare by mapping A to Z onto 1 to 26, which I would notice eventually though not immediately on seeing 20, 15, 2, 5, 15, 18... and you're *still* smiling evilly.\n\nBlaine:  Indeed.  That is *exactly* what Solomonoff induction does.  Furthermore, we have theorems establishing that Solomonoff induction can do it better than you or Terence Tao.\n\nAshley:  A *theorem* proves this.  As in a necessary mathematical truth.  Even though we have no idea how Terence Tao works empirically... and there's evil smile number four.  Okay.  I am *very* skeptical, but willing to be convinced.\n\nBlaine:  So if you actually did have a hypercomputer, you could *cheat*, right?  And Solomonoff induction is the most ridiculously cheating cheat in the history of cheating.\n\nAshley:  Go on.\n\nBlaine:  We just run all possible computer programs to see which are the simplest computer programs that best predict the data seen so far, and use those programs to predict what comes next.  This mixture contains, among other things, an exact copy of Terence Tao, thereby allowing us to prove theorems about their relative performance.\n\nAshley:  What.\n\nBlaine:  That's what the equation for Solomonoff induction says.\n\nAshley:  Is this an actual reputable math thing?  I mean really?\n\nBlaine:  I'll get to formalizing the math shortly, but you did ask me to first state the point of it all.  The point of Solomonoff induction is that it gives us a gold-standard ideal for sequence prediction, and this gold-standard prediction only errs by a bounded amount, over infinite time, relative to the best computable sequence predictor.  We can also see it as formalizing the intuitive idea that was expressed by William Ockham a few centuries earlier that simpler theories are more likely to be correct, and as telling us that 'simplicity' should be measured in algorithmic complexity, which is the size of a computer program required to output a hypothesis's predictions.\n\nAshley:  I think I would have to read more on this subject to actually follow that.  What I'm hearing is that Solomonoff induction is a reputable idea that is important because it gives us a kind of ideal for sequence prediction.  This ideal also has something to do with Occam's Razor and the idea that the simplest theory is the one that can be represented by the shortest computer program.  You identify this with \"doing good epistemology\".\n\nBlaine:  Yes, those are legitimate takeaways.  Another way of looking at it is that Solomonoff induction is an ideal but uncomputable answer to the question \"What should our priors be?\", which is left open by understanding [ Bayesian updating].\n\nAshley:  Can you say how Solomonoff induction answers the question of, say, the prior probability that Canada is planning to invade the United States?  To name one example of a thing I've seen being estimated ludicrously high on a crackpot website that actually tried to invoke Bayesian probability about it.  Does Solomonoff induction let me tell him that he's making a math error, instead of just calling him silly in an admittedly informal fashion?\n\nBlaine:  If you're expecting to sit down with Leibniz and say, \"Gentlemen, let us calculate\" then you're setting your expectations too high.  Solomonoff gives us an idea of how we *should* compute that quantity given *unlimited* computing power.  It doesn't give us a firm procedural recipe for how we can best approximate that ideal in real life using bounded computing power or human brains.  Knowing the ideal, we *can* extract some intuitive advice that might help our online crackpot if only he'd actually listen to it.\n\nAshley:  But according to you, Solomonoff induction does say in principle what is the prior probability that Canada will invade the United States.\n\nBlaine:  Yes, up to a choice of universal Turing machine.\n\nAshley *(looking highly skeptical):*  So I plug a universal Turing machine into the formalism, and in principle, I get out a uniquely determined probability that Canada invades the US.\n\nBlaine:  Exactly!\n\nAshley:  Uh huh.  Well, go on.\n\nBlaine:  So, first, we have to transform this into a sequence prediction problem.\n\nAshley:  Like a sequence of years in which Canada has and hasn't invaded the US, mostly zero except around 1812 -\n\nBlaine:  *No!*  To get a good prediction about Canada we need much more data than that, and I don't mean a dump of Canadian GDP either.  Imagine a sequence that contains all the sensory data you have ever received over your lifetime - not just the hospital room that you saw when you opened your eyes right after your birth, but the darkness your brain received as input while you were still in your mother's womb.  Every word you've ever heard.  Every letter you've ever seen on a computer screen, not as ASCII letters but as the raw pattern of neural impulses that gets sent down from your retina.\n\nAshley:  That seems like a lot of data and some of it is redundant, there'll be lots of similar pixels for blue sky -\n\nBlaine:  That data is what *you* got as an agent.  If we want to translate the question of the prediction problem Ashley faces into theoretical terms, we should give the sequence predictor *all* the data that you had available, including all those repeating blue pixels of the sky.  Who knows?  Maybe there was a Canadian warplane in there that you didn't notice.\n\nAshley:  But it's impossible for my brain to remember all that data.  If we neglect for the moment how the retina actually works and suppose that I'm seeing the same 1920 x 1080 picture the robot would, that's far more data than I can *realistically* take into account.\n\nBlaine:  So then Solomonoff induction can do better than you can, using its unlimited computing power and memory.  That's fine.  When we're talking about the sequence prediction problem in principle, we want to start by talking about *all* the inputs and considering later which inputs might be forgotten or neglected; in the limit of unlimited computing power, there'd be no reason not to take into account all of it.\n\nAshley:  But what if you can do better by forgetting more?\n\nBlaine:  If you have limited computing power, that makes sense.  With unlimited computing power, that really shouldn't happen and that indeed is one of the lessons of Solomonoff induction.  An unbounded Bayesian never expects to do worse by updating on another item of evidence - for one thing, you can always just do the same thing you would have done if you hadn't seen the item.  That kind of lesson *is* one of the things that might not be intuitively obvious, but which you can start to feel deeply by walking through the math of probability theory that provides a kind of conceptual underpinning and clarity for epistemology.  With *unlimited* computing power, nothing goes wrong as a result of trying to process 4 gigabits per second; every extra bit just produces a better expected future prediction.\n\nAshley:  Okay, so we start with literally all the data I have available, 4 gigabits per second of it if we imagine 1920 by 1080 frames of 32-bit pixels at 60Hz - though I remember hearing 100 megabits would be a better estimate of what the retina sends out, and that it's pared down to 1 megabit per second very quickly.\n\nBlaine:  We start with all the data you *had* available since the day you were born.\n\nAshley:  In that case, your sequence of sensory inputs seems incomplete.  There are some things I know not just by observing the outside world.  Chimpanzees learn to be afraid of skulls and snakes much faster than they learn to be afraid of other arbitrary shapes.  I was probably better at learning to walk in Earth gravity than I would have been at navigating in zero G.  There are heuristics I'm born with, based on how my brain was wired, which ultimately stems from my DNA specifying the way that proteins should fold to form neurons -\n\nBlaine:  So, for purposes of following along with the argument, let's say that your DNA is analogous to the code of the computer program that makes predictions.  What you're observing here is that humans have 750 megabytes of DNA, and even if most of that is junk and not all of what's left is specifying brain behavior, it still leaves a pretty large computer program that could have a lot of prior information programmed into it.  Let's say that your brain, or rather, your infant pre-brain wiring algorithm, was effectively a 7.5 megabyte program - if it's actually 75 megabytes, that makes little difference.  By exposing that 7.5 megabyte program to all the information coming in from your eyes, ears, nose, proprioceptive sensors telling you where your limbs were, stomach pains, and so on, your brain updated itself into forming the modern Ashley, whose hundred trillion synapses might be encoded by, say, one petabyte of information.\n\nAshley:  A key part of that process was my brain learning to *control* the environment, not just passively observing it.  Like, I'm *certain* it mattered to my brain wiring that my brain saw the room shift in a certain way when it sent out signals telling my eyes to move, and so on.\n\nBlaine:  Yes, that's surely true.  But talking about the sequential *control* problem is more complicated math.  [11v] is the agent that uses Solomonoff induction as its epistemology and expected reward as its decision theory.  That introduces extra complexity, so it makes sense to talk about just Solomonoff induction first.  Imagine for the moment that we were *just* looking at your sensory data, and trying to predict what would come next in that.\n\nAshley:  Wouldn't it make more sense to look at the brain's inputs and outputs, if we wanted to predict the next input, and not just the previous input?\n\nBlaine:  It'd make the problem easier for a Solomonoff inductor to solve, sure; but it also makes the problem more complicated.  Let's talk instead about what would happen if you took the complete sensory record of your life, gave it to an ideally smart agent, and asked the agent to predict what you would see next.  Maybe the agent could do an even better job of prediction if we also told it about your brain's outputs, but I don't think it would be helpless to see patterns in the inputs.\n\nAshley:  It sounds like a pretty hard problem to me.\n\nBlaine:    In terms of what can be predicted *in principle* given this kind of data, what facts are *actually reflected in it* that Solomonoff induction might uncover, we shouldn't imagine a human trying to analyze the data, we should imagine [an entire advanced civilization pondering it for years](http://lesswrong.com/lw/qk/that_alien_message/).  Like, if the Ashley had already read Shakespeare's Hamlet - if the image of those pages had already crossed the sensory stream - and then the Ashley saw a mysterious box outputting 20, 15, 2, 5, 15, 18, I think somebody eavesdropping on that sensory data would be equally able to guess that this was encoding 'tobeor' and guess that the next thing the Ashley saw would be the box outputting 14.  You wouldn't even need an entire alien civilization of superintelligent cryptographers to guess that.\n\nAshley:  But the next item in the Ashley-sequence wouldn't actually be 14.  It would be this huge 1920 x 1080 visual field that showed the box flashing a little picture of '14'.\n\nBlaine:  Sure.  Otherwise it would be a rigged demo, as you say.\n\nAshley:  And what with all the dust specks in my visual field, and maybe my deciding to tilt my head or saccade my eyes using motor instructions that aren't in the sequence, there's no way any possible agent could exactly predict the 66-megabit integer representing the next visual frame.\n\nBlaine:  Indeed, there'd be some element of thermodynamic and quantum randomness preventing that exact prediction even in principle.  So instead of predicting an *exact* next frame, we put a probability distribution on it.\n\nAshley:  A probability distribution over possible 66-megabit frames?  Like, a table with 2 to the 2 to the 66 million entries, summing to 1?\n\nBlaine:  Sure.  2^(32^(1920 x 1080)) isn't a large number when you have unlimited computing power.  As Martin Gardner once observed, \"most finite numbers are very much larger\".  Like I said, Solomonoff induction is an epistemic ideal that requires an unreasonably large amount of computing power.\n\nAshley:  I don't deny that big computations can sometimes help us understand little ones.  But at the point when we're talking about probability distributions that large, I have some trouble holding onto what the probability distribution is supposed to *mean*.\n\nBlaine:  Really?  Just imagine a probability distribution over N possibilities, then let N go to 2 to the 2 to the 66 million.  If we were talking about a letter ranging from A to Z, then putting 100 times as much probability mass on (X, Y, Z) as on the rest of the alphabet, would say that, although you didn't know *exactly* what letter would happen, you expected in a general sense that it would be toward the end of the alphabet.  You would have used 26 probabilities, summing to 1, to very precisely state that vague prediction.  In Solomonoff induction, since we have unlimited computing power, we express our uncertainty about a 1920 x 1080 video frame the same way.  All the various pixel fields you could see if your eye jumped to a plausible place, saw a plausible number of dust specks, and saw the box flash something that visually encoded '14', would have high probability.  Pixel fields where the box vanished and was replaced with a glow-in-the-dark unicorn would have very low, though not zero, probability.\n\nAshley:  Is that really reasonable?\n\nBlaine:  If we could not make identifications like these *in principle*, there would be no principled way in which we could say that you had ever *expected to see something happen* - no way to say that one visual field your eyes saw, had higher probability than any other sensory experience.  We couldn't justify science; we couldn't say that, having performed Galileo's experiment by rolling an inclined cylinder down a plane, Galileo's theory was thereby to some degree supported by having assigned *a high relative probability* to the only actual observations our eyes ever report.\n\nAshley:  I feel a little unsure of that jump, but I suppose I can go along with that for now.  Then the question of \"What probability does Solomonoff induction assign to Canada invading?\" is to be identified, in principle, with the question \"Given my past life experiences and all the visual information that's entered my eyes, what is the relative probability of seeing visual information that encodes Google News with the headline 'CANADA INVADES USA' at some point during the next 300 million seconds?\"\n\nBlaine:  Right!\n\nAshley:  And Solomonoff induction has an in-principle way of assigning this a relatively low probability, which our online silly person could do well to learn from as a matter of principle, even if we couldn't carry out the exact calculations.\n\nBlaine:  Precisely!\n\nAshley:  Fairness requires that I congratulate you on having come further in formalizing 'do good epistemology' as a sequence prediction problem than I previously thought you might.  I mean, you haven't satisfied me yet,  but I wasn't expecting you to get even this far.  So what exactly *is* the ideal calculation?\n\nBlaine:  We're not quite ready to launch right into that.  Next we consider how to represent a *hypothesis* inside this formalism.\n\nAshley:  Hmm.  You said something earlier about updating on a probabilistic mixture of computer programs, which leads me to suspect that in this formalism, a hypothesis or *way the world can be* is a computer program that outputs a sequence of integers.\n\nBlaine:  There's indeed a version of Solomonoff induction that works like that.  But I prefer the version where a hypothesis assigns *probabilities* to sequences of integers.  Like, if the hypothesis is that the world is a fair coin, then we shouldn't try to make that hypothesis predict \"heads - tails - tails - tails - heads\" but should let it just assign a 1/32 prior probability to the sequence HTTTH.\n\nAshley:  I can see that for coins, but I feel a bit iffier on what this means for *the real world*.\n\nBlaine:  A single hypothesis inside the Solomonoff mixture would be a computer program that took in a series of video frames, and assigned a probability to each possible next video frame.  Or for greater simplicity and elegance, imagine a program that took in a sequence of bits, ones and zeroes, and output a rational number for the probability of the next bit being '1'.  We can readily go back and forth between a program like that, and a probability distribution over sequences.  Like, if you can answer all of the questions, \"What's the probability that the coin comes up heads on the first flip?\", \"What's the probability of the coin coming up heads on the second flip, if it came up heads on the first flip?\", and \"What's the probability that the coin comes up heads on the second flip, if it came up tails on the first flip?\" then we can turn that into a probability distribution over sequences of two coinflips.  Analogously, if we have a program that outputs the probability of the next bit, conditioned on a finite number of previous bits taken as input, that program corresponds to a probability distribution over all infinite sequences of bits.\n\nAshley:  I think I followed along with that in theory, though it's not a type of math I'm used to (yet).  So then in what sense is reality a program like that?  In what sense is a program like that a way the world could be - a hypothesis about the world?\n\nBlaine:  Well, I mean, for one thing, we can see the infant Ashley as a program with 7.5 megabytes of information about how to wire up its brain in response to sense data, that sees a bunch of sense data and then has some degree of relative surprise that the Ashley would express about possible additional sense data.  Like in the baby-looking-paradigm experiments where you show a baby an object disappearing behind a screen, and the baby looks longer at those cases, and so we suspect that babies have a concept of object permanence.\n\nAshley:  That sounds like a program that's a way Ashley could be, not a program that's a way the world could be.\n\nBlaine:  Those indeed are dual perspectives on the meaning of Solmonoff induction.  Maybe we can shed some light on this by considering a simpler induction rule, Laplace's Rule of Succession, so-called because it was the original problem and solution considered by Thomas Bayes in the 1750s, and Pierre-Simon Laplace is the one who invented Bayesian inference.\n\nAshley:  Wait, what?\n\nBlaine:  Suppose you have a biased coin with an unknown bias, and every possible bias between 0 and 1 is equally probable.\n\nAshley:  Sounds like a very unlikely situation to arise in the real world.  In the real world, it's quite likely that an unknown frequency is exactly 0, 1, or 1/2, while if you assign equal probability density to every part of the real number field between 0 and 1, the probability of 1 is 0.  Indeed, the probability of all rational numbers put together is zero.\n\nBlaine:  The original problem considered by Thomas Bayes was about an ideal billiard ball bouncing back and forth on an ideal billiard table many times and eventually slowing to a halt; and then bouncing other billiards to see if they halted to the left or the right of the first billiard.  You can see why, in first considering the simplest form of this problem without any complications, we might consider every position of the first billiard to be equally probable.\n\nAshley:  Oh, fine, but if the billiard was really an ideal rolling sphere and the walls were perfectly reflective, it'd never halt in the first place.\n\nBlaine:  Suppose we're told that, after rolling 5 more billiard balls, one billiard ball was to the right, or R, and the other four were to the left, or Ls.  What is the probability that the next billiard ball rolled will be on the left?\n\nAshley:  Five sevenths.\n\nBlaine:  Ah, you've heard this problem before?\n\nAshley:  No, but it's obvious.\n\nBlaine:  Uh... really?\n\nAshley:   Combinatorics.  Consider just the orderings of the balls, instead of their exact positions.  Designate the original ball with the symbol **|**, the next five balls as **LLLLR**, and the next ball to be rolled as **+**.  Given that the current ordering is **LLLL|R** and that all positions and spacings of the underlying balls are equally likely, after rolling the **+**, there will be seven equally likely orderings **+LLLL|R**, **L+LLL|R**, **LL+LL|R**, and so on up to **LLLL|L+R** and **LLLL|R+**.  In five of those seven orderings, the **+** is on the left of the **|**.  In general, if we see M of **L** and N of **R**, the probability of the next item being an **L** is (M + 1) / (M + N + 2).\n\nBlaise:  Gosh...  Well, there's also a more complicated proof devised by Thomas Bayes that starts by considering every position of the original ball to be equally likely a priori, the additional balls as providing evidence about that position, and then integrating over the posterior probabilities of the original ball's possible positions to arrive at the probability that the next ball lands on the left or right.\n\nAshley:  Heh.  And is all that extra work useful if you also happen to know a little combinatorics?\n\nBlaise:  Wel, it tells me exactly how my beliefs about the original ball change with each new piece of evidence.  Suppose I instead asked you something along the lines of, \"Given 4 **L** and 1 **R**, where do you think the original ball **+** is most likely to be on the number line?  How likely is it to be within 0.1 distance of there?\"  Thomas Bayes said how to calculate the exact posterior density function for the ball's probable position after seeing the evidence.\n\nAshley:  That's fair; I don't see a simple combinatoric answer for the later part.\n\nBlaise:  Anyway, let's just take at face value that Laplace's Rule of Succession says that, after observing M 1s and N 0s, the probability of getting a 1 next is (M + 1) / (M + N + 2).\n\nAshley:  But of course.\n\nBlaise:  We can consider Laplace's Rule as a short Python program that takes in a sequence of 1s and 0s, and spits out the probability that the next bit in the sequence will be 1.  We can also consider it as a probability distribution over infinite sequences, like this:\n\n- **1** :  1/2\n- **0** :  1/2\n- **10** : 1/2 * 1/3 = 1/6\n- **11** : 1/2 * 2/3 = 1/3\n- **01** : 1/6\n\nBlaise:  ...and so on.  Now, we can view this as a rule someone might espouse for *predicting* coinflips, but also view it as corresponding to a particular class of possible worlds containing randomness.  I mean, Laplace's Rule isn't the only rule you could use.  Suppose I had a barrel containing ten white balls and ten green balls.  If you already knew this about the barrel, then after seeing M white balls and N green balls, you'd predict the next ball being white with probability (10 - M) / (20 - M - N).  If you use Laplace's Rule, that's like believing the world was like a billiards table with an original ball rolling to a stop at a random point and new balls ending up on the left or right.  If you use (10 - M) / (20 - M - N), that's like the hypothesis that there's ten red balls and ten white balls in a barrel.  There isn't really a sharp border between rules we can use to predict the world, and rules for how the world behaves -\n\nAshley:  Well, that sounds just plain wrong.  The map is not the territory, don'cha know?  If Solomonoff induction can't tell the difference between maps and territories, maybe it doesn't contain all epistemological goodness after all.\n\n*(Eliezer, whispering:  Ashley has a point, by the way.  See [ naturalistic reflection].)*\n\nBlaise:  Maybe it'd be better to say that there's a dualism between good ways of computing predictions and being in actual worlds where that kind of predicting works well?  Like, you could also see Laplace's Rule as implementing the rules for a world with randomness where the original billiard ball ends up in a random place, so that the first thing you see is equally likely to be 1 or 0, and then to get what probably happens on round 2, we tell the world what happened on round 1 so that it can update what the background random events were.\n\nAshley:  Mmmaybe.\n\nBlaise:  If you go with the version where Solomonoff induction is over programs that just spit out a determined string of ones and zeroes, we could see those programs as corresponding to particular environments - ways the world *could be* that would produce our sensory input, the sequence.  Though in that case, I think we'd need to jump ahead and consider the more sophisticated decision-problem that appears in [11v]: an environment is a program that takes your motor outputs as its input, and then outputs your sensory inputs as its output.  But personally, I think I prefer the version of Solomonoff induction where the elementary hypotheses are computer programs that reason about the world a particular way.  Because I think there are valid lessons we can derive from that.  We should, in fact, prefer simple meta-level rules of reasoning that have done best in predicting the future given the past, not just prefer simple object-level views of the world.  Like, jumping to science from pre-science and so on.  Or going from \"explain the world using heroic mythology\" to \"explain the world using differential equations\".\n\nAshley:  Did you say something earlier about the deterministic and probabilistic versions of Solomonoff induction giving the same answers?  Like, is it a distinction without a difference whether we ask about simple programs that reproduce the observed data versus simple programs that assign a lot of probability to the data?\n\nBlaise:  I'm *told* the answers are the same but I confess I can't quite see why that's true, unless there's some added assumption I'm missing.  So let's talk about programs that assign probabilities for now, because I think that case is clearer.  By the way, I note that we've started talking about *simple* programs that assign high probability to our observations so far.\n\nAshley:  Yes, well, it seems like an obvious step, especially considering that you were already talking about \"simple programs\" and Occam's Razor a while back.  Solomonoff induction is part of the Bayesian program of inference, right?\n\nBlaise:  Indeed.  Very much so.\n\nAshley:  Okay, so let's talk about the program, or hypothesis, for \"This barrel has an unknown frequency of white and green balls\", versus the hypothesis \"This barrel has 10 white and 10 green balls\", versus the hypothesis, \"This barrel always puts out a green ball after a white ball and vice versa.\"  Let's say we see a green ball, then a white ball, the sequence **GW**.  The first hypothesis assigns this probability 1/2 * 1/3 = 1/6, the second hypothesis assigns this probability 10/20 * 9/19 or roughly 1/4, and the third hypothesis assigns probability 1/2 * 1.  Now it seems to me that there's some important sense in which, even though Laplace's Rule assigned a lower probability to the data, it's significantly simpler than the second and third hypotheses and is the wiser answer.  Does Solomonoff induction agree?\n\nBlaise:  I think you might be taking into account some prior knowledge that isn't in the sequence itself, there.  Like, things that alternate either **101010...** or **010101...** are *objectively* simple in the sense that a short computer program simulates them or assigns probabilities to them.  It's just unlikely to be true about *a barrel of red and green balls*.  If **10** is literally the first sense data that you ever see, when you are just a tiny little infant superintelligence with two bits of binary data, then \"Maybe the universe consists of alternating bits\" is no less reasonable than \"Maybe the universe produces bits with an unknown random frequency anywhere between 0 and 1.\"\n\nAshley:  Conceded.  But as I was going to say, we have three hypotheses that assigned 1/6, ~1/4, and 1/2 to the observed data; but to know the posterior probabilities of these hypotheses we need to actually say how relatively likely they were a priori, so we can multiply by the odds ratio.  Like, if the prior odds were 3:2:1, the posterior odds would be 3:2:1 * (2/12 : 3/12 : 6/12) = 3:2:1 * 2:3:6 = 6:6:6 = 1:1:1.  Now, how would Solomonoff induction assign prior probabilities to those computer programs?  Because I remember you saying, way back when, that you thought Solomonoff was the answer to \"How should Bayesians assign priors?\"\n\nBlaise:  Well, how would you do it?\n\nAshley:  I mean... yes, the simpler rules should be favored, but it seems to me that there's some deep questions as to the exact quantitative relative 'simplicity' of the rules (M + 1) / (M + N + 2), or the rule (10 - M) / (20 - M - N), or the rule \"alternate the bits\"...\n\nBlaise:  Eh, just come up with a rule.\n\nAshley:  Okay, if I just say the rule I think you're looking for, the rule would be, \"The complexity of a computer program is the number of bits needed to specify it to some arbitrary but reasonable choice of compiler or Universal Turing Machine, and since there's 16 possible programs made of 4 bits, the prior probability is 1/2 to the power of the number of bits.\"  So if it takes 16 bits to specify Laplace's Rule of Succession, which seems a tad optimistic, then the prior probability would be 1/65536, which seems a tad pessimistic.\n\nBlaise:  Now just apply that rule to the infinity of possible computer programs that assign probabilities to the observed data, update their posterior probabilities based on the probability they've assigned to the evidence so far, sum over all of them to get your next prediction, and we're done.  And yes, that requires a [ hypercomputer] that can solve the [halting problem](https://en.wikipedia.org/wiki/Halting_problem), but we're talking ideals here.\n\n[todo: write out equation, once almost done or editor is no longer running LaTeX each time.]\n\nAshley:  Uh.\n\nBlaine:  Yes?\n\nAshley:  Um...\n\nBlaine:  What is it?\n\nAshley:  You invoked a countably infinite set, so I'm trying to figure out if my predicted probability for the next bit must necessarily converge to a limit as I consider increasingly large finite subsets in any order.\n\nBlaine *(sighs)*:  Of course you are.\n\nAshley:  I think you might have left out some important caveats.  Like, if I take the rule literally, then the program \"**0**\" has probability 1/2, the program \"**1** has probability 1/2, the program **01** has probability 1/4 and now the total probability is 1.25 which is *too much,* and I can't actually normalize it because the series sums to infinity.  Now, this just means we need to, say, decide that the probability of a program having length 1 is 1/2, the probability of it having length 2 is 1/4, and so on out to infinity, but it's an added postulate.\n\nBlaine:  The conventional method is to require a [prefix-free code](https://en.wikipedia.org/wiki/Prefix_code).  If \"**0111** is a valid program then **01110** cannot be a valid program.  With that constraint, assigning \"1/2 to the power of the length of the code\", to all valid codes, will sum to less than 1; and we can normalize their relative probabilities to get the actual prior.\n\nAshley:  Okay.  And you're sure that it doesn't matter in what order we consider more and more programs as we approach the limit, because... no, never mind.  Every program has positive probability mass, with the total set summing to 1, and Bayesian updating doesn't change that.  So as I consider more and more programs, in any order, there's only so many large contributions that can be made from the mix - only so often that the final probability can change.  Like, let's say there are at most 99 programs with probability 1% that assign probability 0 to the next bit being a 1; that's only 99 times the final answer can go down by as much as 0.01, as the limit is approached.\n\nBlaine:  This idea generalizes, and is important.  List all possible computer programs, in any order you like.  Use any definition of *simplicity* that you like, so long as for any given amount of simplicity, there are only a finite number of computer programs that simple.  As you go on carving off chunks of prior probability mass and assigning them to programs, it *must* be the case that as programs get more and complicated, their prior probability approaches zero! - though it's still positive for every finite program, because of [Cromwell's Rule](https://en.wikipedia.org/wiki/Cromwell%27s_rule).  If you consider, say, 1%, then you can't have more than 99 programs assigned 1% probability and still obey Cromwell's Rule, which means there must be some *most complex* program that is assigned 1% probability, which means every more complicated program must have less than 1% probability out to the end of the infinite list.  Since there's only a finite number of programs simpler than some level epsilon, there must be some *least* probable program of that kind, and there must be a level of complication delta such that every program more complicated than delta is less probable than every program as simple as epsilon.  And so it goes.\n\nAshley:  Huh.  I don't think I've ever heard that justification for Occam's Razor before.  I think I like it.  I mean, I've heard a lot of appeals to the empirical simplicity of the world, and so on, but this is the first time I've seen a *logical* proof that, in the limit, more complicated hypotheses *must* be less likely than simple ones.\n\nBlaine:  Behold the awesomeness that is Solomonoff Induction!\n\nAshley:  Uh, but you didn't actually use the notion of *computational* simplicity to get that conclusion, you just required that the supply of probability mass is finite and the supply of potential complications is infinite.\n\nBlaine:  Well, maybe.  But it so happens that Yudkowsky did invent or reinvent that argument after pondering Solomonoff induction, and if it predates him and Solomonoff then Yudkowsky doesn't know the source.  Concrete inspiration for simplifiable arguments is also a credit to a theory, especially if the simplifiable argument didn't exist before that.\n\nAshley:  Fair enough.  My next question is about the choice of Universal Turing Machine - the choice of compiler for our program codes.  There's an infinite number of possibilities there, and in principle, the right choice of compiler can make our probability for the next thing we'll see be anything we like.   At least I'd expect this to be the case, based on how the \"[problem of induction](https://en.wikipedia.org/wiki/Problem_of_induction)\" usually goes.  So with the right choice of Universal Turing Machine, our online crackpot can still make it be the case that Solomonoff induction predicts Canada invading the USA.\n\nBlaine:  One way of looking at the problem of good epistemology, I'd say, is that the job of a good epistemology is not to make it *impossible* to err, you can still blow off your foot if you really insist on pointing the shotgun at your foot and pulling the trigger.  The job of good epistemology is to make it *more obvious* when you're about to blow your own foot off with a shotgun.  On this dimension, Solomonoff Induction excels.  If you claim that we ought to pick an enormously complicated compiler to encode our hypotheses, in order to make the 'simplest hypothesis that fits the evidence' be one that predicts Canada invading the USA, then it should be obvious to everyone except you, and indeed, obvious to you if you're not completely delusional, that you are in the process of screwing up.  \n\nAshley:  Ah, but of course they'll say that their code is just the simple and natural choice of Universal Turing Machine, because they'll exhibit a meta-UTM which outputs that UTM given only a short code.  And if you say the meta-UTM is complicated -\n\nBlaine:  Flon's Law says, \"There is not now, nor has there ever been, nor will there ever be, any programming language in which it is the least bit difficult to write bad code.\"  You can't make it impossible for people to screw up, but you can make it *more obvious.*  And Solomonoff induction would make it even more obvious than might at first be obvious, because -\n\nAshley:  Your Honor, I move to have the previous sentence taken out and shot.\n\nBlaine:  Solomonoff induction, if we could actually run the hypercomputer, would be incredibly good at highlighting stupidity, even more than might at first be obvious.  Let's say that the whole of your sensory information is the string **10101010...**  Consider the stupid hypothesis, \"This program has a 99% probability of producing a **1** on every turn\", which you jumped to after seeing the first bit.  What would you need to claim your priors were like - what Universal Turing Machine would you need to endorse - in order to maintain blind faith in that hypothesis in the face of ever-mounting evidence?\n\nAshley:  You'd need a Universal Turing Machine **blind-utm** that assigned a very high probability to the **blind** program \"def ProbNextElementIsOne(previous_sequence): return 0.99\".  Like, if **blind-utm** sees the code **0**, it executes the **blind** program \"return 0.99\".  And to defend yourself against charges that your UTM **blind-utm** was not itself simple, you'd need a meta-UTM, **blind-meta** which, when it sees the code **10**, executes **blind-utm**.  And to really wrap it up, you'd need to take a fixed point through all towers of meta and use diagonalization to create the UTM **blind-diag** that, when it sees the program code **0**, executes \"return 0.99\", and when it sees the program code **10**, executes **blind-diag**.  I guess I can see some sense in which, even if that doesn't resolve Hume's problem of induction, anyone *actually advocating that* would be committing blatant shenanigans on a commonsense level, arguably more blatant than it would have been if we hadn't actually made them present the UTM.\n\nBlaine:  Actually, the shenanigans have to be much worse than that in order to fool Solomonoff induction.  Like, Solomonoff induction using your **blind-diag** isn't fooled for a minute, even taking **blind-diag** entirely on its own terms.\n\nAshley:  Really?\n\nBlaise:  Assuming 60 sequence items per second?  Yes, absolutely, Solomonoff induction shrugs off the delusion in the first minute, unless there's further and even more blatant shenanigans.  We did require that your **blind-diag** be a *Universal* Turing Machine, meaning that it can reproduce every computable probability distribution over sequences, given some particular code to compile.  Let's say there's a 200-bit code **laplace** for Laplace's Rule of Succession, \"lambda sequence: return (sequence.count('1') + 1) / (len(sequence) + 2)\", so that its prior probability relative to the 1-bit code for **blind** is 2^-200.  Let's say that the sense data is around 50/50 1s and 0s.  Every time we see a 1, **blind** gains a factor of 2 over **laplace** (99% vs. 50% probability), and every time we see a 0, **blind** loses a factor of 50 over **laplace** (1% vs. 50% probability).  On average, every 2 bits of the sequence, **blind** is losing a factor of 25 or, say, a bit more than 4 bits, i.e., on average **blind** is losing two bits of probability per element of the sequence observed.  So it's only going to take 100 bits, or a little less than two seconds, for **laplace** to win out over **blind**.\n\nAshley:  I see.  I was focusing on a UTM that assigned lots of prior probability to **blind**, but what I really needed was a compiler that, *while still being universal* and encoding every possibility somewhere, still assigned a really tiny probability to **laplace**, **faircoin** that encodes \"return 0.5\", and every other hypothesis that does better, round by round, than **blind**.  So what I really need to carry off the delusion is **obstinate-diag** that is universal, assigns high probability to **blind**, requires billions of bits to specify **laplace**, and also requires billions of bits to specify any UTM that can execute **laplace** as a shorter code than billions of bits.  Because otherwise the skeptic will say, \"Ah, but given the evidence, this other UTM was better instead.\"  I agree that those are even more blatant shenanigans than I thought.\n\nBlaise:  Yes.  And even *then*, even if your UTM takes two billion bits to specify **faircoin**, Solomonoff induction will lose its faith in **blind** after seeing a billion bits.  Which will actually happen before the first year is out, if we're getting 60 bits per second.  And if you turn around and say, \"Oh, well, I didn't mean *that* was my UTM, I really meant *this* was my UTM, this thing over here where it takes a *trillion* bits to encode **faircoin**\", then that's probability-theory-violating shenanigans, you'd have to be backpedaling on your already-ridiculous prior.\n\nAshley:  That's actually a very interesting point - that what's needed for Bayesian to maintain a delusion in the face of mounting evidence is not so much a blind faith in the delusory hypothesis, as a blind skepticism of all its alternatives.  But what if their UTM requires a googol bits to specify **faircoin**?  What if **blind** and **blind-diag**, or programs pretty much isomorphic to them, are the only programs that can be specified in less than a googol bits?\n\nBlaise:  Then your desire to shoot your own foot off has been made very, very apparent to anyone who understands Solomonoff induction.  We're not going to get absolutely objective prior probabilities as a matter of logical deduction, not without principles that are unknown to me and beyond the scope of Solomonoff induction.  But we can make it really obvious and force you to construct a downright embarrassing Universal Turing Machine in order to be stupid.  We can give our pragmatic answer to the problem of induction, not by appealing that \"a universe that can be predicted better than chance by simple computation\" is something that's likely a priori, but by appealing that it would violate [Cromwell's Rule](https://en.wikipedia.org/wiki/Cromwell%27s_rule) and be exceedingly special pleading to assign the possibility of a computationally learnable universe a probability of less than 2 to the negative millionth power *a priori*.\n\nAshley:  I don't know that good *pragmatic* answers to the problem of induction were ever in short supply.  Still, on the margins, it's a more forceful pragmatic answer than the last one I remember hearing.\n\nBlaise:  Yay!  *Now* isn't Solomonoff induction wonderful?\n\nAshley:  Maybe?  You didn't really use the principle of *computational* simplicity to derive that lesson.  You just used that *some inductive principle* ought to have a prior probability of more than 2^(-1,000,000).\n\nBlaise:  ...\n\nAshley:  Can you give me an example of a problem where the *computational* definition of simplicity matters and can't be factored back out?\n\nBlaise:  Okay, fine, as it happens, yes I can.  I can give you *three* examples of how it matters.\n\nAshley:  Vun... two... three!  Three examples!  Ha-ah-ah!\n\nBlaise:  Do you *have* to do that every - oh, never mind.  Example one is that diffraction is a simpler explanation of rainbows than divine intervention, example two is that galaxies and quantum mechanics are not so improbable that no one could ever believe in them, and example three is outperforming Terrence Tao.\n\nAshley:  These statements are all so obvious that no further explanation of any of them is required.\n\n(in progress)",
      "metaText": "",
      "isTextLoaded": true,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 5,
      "maintainerCount": 1,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": 0,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 54,
      "redLinkCount": 0,
      "lockedBy": "2",
      "lockedUntil": "2017-12-25 00:28:56",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": {
        "edit": {
          "has": false,
          "reason": "You don't have domain permission to edit this page"
        },
        "proposeEdit": {
          "has": true,
          "reason": ""
        },
        "delete": {
          "has": false,
          "reason": "You don't have domain permission to delete this page"
        },
        "comment": {
          "has": false,
          "reason": "You can't comment in this domain because you are not a member"
        },
        "proposeComment": {
          "has": true,
          "reason": ""
        }
      },
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [
        "11w"
      ],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [
        "4y7"
      ],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [
        {
          "id": "2247",
          "parentId": "1r6",
          "childId": "1hh",
          "type": "requirement",
          "creatorId": "1",
          "createdAt": "2016-06-17 21:58:56",
          "level": 2,
          "isStrong": true,
          "everPublished": true
        },
        {
          "id": "2256",
          "parentId": "21c",
          "childId": "1hh",
          "type": "requirement",
          "creatorId": "1",
          "createdAt": "2016-06-17 21:58:56",
          "level": 2,
          "isStrong": false,
          "everPublished": true
        }
      ],
      "subjects": [
        {
          "id": "2246",
          "parentId": "11w",
          "childId": "1hh",
          "type": "subject",
          "creatorId": "1",
          "createdAt": "2016-06-17 21:58:56",
          "level": 2,
          "isStrong": false,
          "everPublished": true
        },
        {
          "id": "5824",
          "parentId": "1hh",
          "childId": "1hh",
          "type": "subject",
          "creatorId": "1",
          "createdAt": "2016-08-02 16:54:25",
          "level": 2,
          "isStrong": true,
          "everPublished": true
        }
      ],
      "lenses": [],
      "lensParentId": "11w",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "22944",
          "pageId": "1hh",
          "userId": "2",
          "edit": 54,
          "type": "newEdit",
          "createdAt": "2017-12-25 00:28:56",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "22943",
          "pageId": "1hh",
          "userId": "2",
          "edit": 53,
          "type": "newEdit",
          "createdAt": "2017-12-24 22:36:36",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "22799",
          "pageId": "1hh",
          "userId": "2cl",
          "edit": 52,
          "type": "newEdit",
          "createdAt": "2017-10-07 21:33:55",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "22795",
          "pageId": "1hh",
          "userId": "2bj",
          "edit": 51,
          "type": "newEdit",
          "createdAt": "2017-10-05 09:39:15",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "22782",
          "pageId": "1hh",
          "userId": "8nz",
          "edit": 50,
          "type": "newEdit",
          "createdAt": "2017-09-21 00:47:43",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": "Fix consistency of example ball colors"
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "22781",
          "pageId": "1hh",
          "userId": "8nz",
          "edit": 49,
          "type": "newEditProposal",
          "createdAt": "2017-09-21 00:46:52",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "22761",
          "pageId": "1hh",
          "userId": "8p0",
          "edit": 48,
          "type": "newEdit",
          "createdAt": "2017-09-20 00:25:38",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "22760",
          "pageId": "1hh",
          "userId": "8nz",
          "edit": 47,
          "type": "newEdit",
          "createdAt": "2017-09-19 22:12:53",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": "Fix typo"
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "21114",
          "pageId": "1hh",
          "userId": "2",
          "edit": 46,
          "type": "newEdit",
          "createdAt": "2016-12-23 22:29:30",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "18333",
          "pageId": "1hh",
          "userId": "1yq",
          "edit": 0,
          "type": "newTag",
          "createdAt": "2016-08-04 13:43:50",
          "auxPageId": "4y7",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "18086",
          "pageId": "1hh",
          "userId": "1",
          "edit": 0,
          "type": "newTeacher",
          "createdAt": "2016-08-02 16:54:26",
          "auxPageId": "1hh",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "18087",
          "pageId": "1hh",
          "userId": "1",
          "edit": 0,
          "type": "newSubject",
          "createdAt": "2016-08-02 16:54:26",
          "auxPageId": "1hh",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "17552",
          "pageId": "1hh",
          "userId": "1",
          "edit": 44,
          "type": "newEdit",
          "createdAt": "2016-07-26 17:22:10",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "8152",
          "pageId": "1hh",
          "userId": "32",
          "edit": 43,
          "type": "newEdit",
          "createdAt": "2016-03-03 18:03:17",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "8117",
          "pageId": "1hh",
          "userId": "32",
          "edit": 42,
          "type": "newEdit",
          "createdAt": "2016-03-03 04:41:39",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "7363",
          "pageId": "1hh",
          "userId": "2",
          "edit": 41,
          "type": "newRequirement",
          "createdAt": "2016-02-17 22:08:52",
          "auxPageId": "21c",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "7347",
          "pageId": "1hh",
          "userId": "2",
          "edit": 41,
          "type": "newEdit",
          "createdAt": "2016-02-17 21:28:01",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "7331",
          "pageId": "1hh",
          "userId": "2",
          "edit": 40,
          "type": "newRequirement",
          "createdAt": "2016-02-17 20:52:58",
          "auxPageId": "1r6",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "7329",
          "pageId": "1hh",
          "userId": "2",
          "edit": 40,
          "type": "newSubject",
          "createdAt": "2016-02-17 20:52:53",
          "auxPageId": "11w",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "6448",
          "pageId": "1hh",
          "userId": "1",
          "edit": 40,
          "type": "newEdit",
          "createdAt": "2016-02-05 01:49:27",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "5258",
          "pageId": "1hh",
          "userId": "2",
          "edit": 39,
          "type": "newEdit",
          "createdAt": "2016-01-15 04:06:11",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "5212",
          "pageId": "1hh",
          "userId": "2",
          "edit": 38,
          "type": "newEdit",
          "createdAt": "2016-01-12 02:35:30",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "5211",
          "pageId": "1hh",
          "userId": "2",
          "edit": 37,
          "type": "newEdit",
          "createdAt": "2016-01-12 02:35:04",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "5210",
          "pageId": "1hh",
          "userId": "2",
          "edit": 36,
          "type": "newEdit",
          "createdAt": "2016-01-12 02:34:15",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "5209",
          "pageId": "1hh",
          "userId": "2",
          "edit": 35,
          "type": "newEdit",
          "createdAt": "2016-01-12 02:33:13",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "5208",
          "pageId": "1hh",
          "userId": "2",
          "edit": 34,
          "type": "newEdit",
          "createdAt": "2016-01-12 02:32:15",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "5207",
          "pageId": "1hh",
          "userId": "2",
          "edit": 33,
          "type": "newEdit",
          "createdAt": "2016-01-12 02:26:11",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "5206",
          "pageId": "1hh",
          "userId": "2",
          "edit": 32,
          "type": "newEdit",
          "createdAt": "2016-01-12 02:25:46",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "5205",
          "pageId": "1hh",
          "userId": "2",
          "edit": 31,
          "type": "newEdit",
          "createdAt": "2016-01-12 02:23:25",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "5204",
          "pageId": "1hh",
          "userId": "2",
          "edit": 30,
          "type": "newEdit",
          "createdAt": "2016-01-12 01:52:45",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "5181",
          "pageId": "1hh",
          "userId": "32",
          "edit": 29,
          "type": "newEdit",
          "createdAt": "2016-01-11 06:25:36",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "5180",
          "pageId": "1hh",
          "userId": "32",
          "edit": 28,
          "type": "newEdit",
          "createdAt": "2016-01-11 05:56:39",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "5152",
          "pageId": "1hh",
          "userId": "2",
          "edit": 27,
          "type": "newEdit",
          "createdAt": "2016-01-10 03:34:33",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "5151",
          "pageId": "1hh",
          "userId": "2",
          "edit": 26,
          "type": "newEdit",
          "createdAt": "2016-01-10 03:33:28",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "5150",
          "pageId": "1hh",
          "userId": "2",
          "edit": 25,
          "type": "newEdit",
          "createdAt": "2016-01-10 03:31:36",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "5149",
          "pageId": "1hh",
          "userId": "2",
          "edit": 24,
          "type": "newEdit",
          "createdAt": "2016-01-10 03:25:49",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "5148",
          "pageId": "1hh",
          "userId": "2",
          "edit": 23,
          "type": "newEdit",
          "createdAt": "2016-01-10 03:24:39",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "5147",
          "pageId": "1hh",
          "userId": "2",
          "edit": 22,
          "type": "newEdit",
          "createdAt": "2016-01-10 01:31:22",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "5146",
          "pageId": "1hh",
          "userId": "2",
          "edit": 21,
          "type": "newEdit",
          "createdAt": "2016-01-10 01:30:08",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "5145",
          "pageId": "1hh",
          "userId": "2",
          "edit": 20,
          "type": "newEdit",
          "createdAt": "2016-01-10 01:28:53",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "5144",
          "pageId": "1hh",
          "userId": "2",
          "edit": 19,
          "type": "newEdit",
          "createdAt": "2016-01-10 01:23:09",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "5143",
          "pageId": "1hh",
          "userId": "2",
          "edit": 18,
          "type": "newEdit",
          "createdAt": "2016-01-10 01:14:54",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "5142",
          "pageId": "1hh",
          "userId": "2",
          "edit": 17,
          "type": "newEdit",
          "createdAt": "2016-01-10 00:04:26",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "5141",
          "pageId": "1hh",
          "userId": "2",
          "edit": 16,
          "type": "newEdit",
          "createdAt": "2016-01-09 23:25:29",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "5140",
          "pageId": "1hh",
          "userId": "2",
          "edit": 15,
          "type": "newEdit",
          "createdAt": "2016-01-09 23:23:49",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "5139",
          "pageId": "1hh",
          "userId": "2",
          "edit": 14,
          "type": "newEdit",
          "createdAt": "2016-01-09 23:20:17",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "5136",
          "pageId": "1hh",
          "userId": "2",
          "edit": 13,
          "type": "newEdit",
          "createdAt": "2016-01-09 23:18:52",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "5135",
          "pageId": "1hh",
          "userId": "2",
          "edit": 12,
          "type": "newEdit",
          "createdAt": "2016-01-09 23:18:14",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "5134",
          "pageId": "1hh",
          "userId": "2",
          "edit": 11,
          "type": "newEdit",
          "createdAt": "2016-01-09 23:17:35",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "5058",
          "pageId": "1hh",
          "userId": "2",
          "edit": 10,
          "type": "newEdit",
          "createdAt": "2016-01-07 04:22:09",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "5056",
          "pageId": "1hh",
          "userId": "2",
          "edit": 9,
          "type": "newEdit",
          "createdAt": "2016-01-07 00:55:52",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "5055",
          "pageId": "1hh",
          "userId": "2",
          "edit": 8,
          "type": "newEdit",
          "createdAt": "2016-01-07 00:55:12",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "5054",
          "pageId": "1hh",
          "userId": "2",
          "edit": 7,
          "type": "newEdit",
          "createdAt": "2016-01-07 00:41:35",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "5046",
          "pageId": "1hh",
          "userId": "2",
          "edit": 6,
          "type": "newEdit",
          "createdAt": "2016-01-06 21:27:00",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "4807",
          "pageId": "1hh",
          "userId": "2",
          "edit": 5,
          "type": "newEdit",
          "createdAt": "2015-12-30 04:03:51",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "4806",
          "pageId": "1hh",
          "userId": "2",
          "edit": 4,
          "type": "newEdit",
          "createdAt": "2015-12-30 03:58:25",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "4805",
          "pageId": "1hh",
          "userId": "2",
          "edit": 3,
          "type": "newEdit",
          "createdAt": "2015-12-30 03:57:37",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "4804",
          "pageId": "1hh",
          "userId": "2",
          "edit": 2,
          "type": "newEdit",
          "createdAt": "2015-12-30 03:56:41",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "4803",
          "pageId": "1hh",
          "userId": "2",
          "edit": 1,
          "type": "newEdit",
          "createdAt": "2015-12-30 03:55:19",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "4777",
          "pageId": "1hh",
          "userId": "2",
          "edit": 0,
          "type": "newParent",
          "createdAt": "2015-12-30 00:54:18",
          "auxPageId": "11w",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        }
      ],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": true,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    }
  },
  "users": {
    "1": {
      "id": "1",
      "firstName": "Alexei",
      "lastName": "Andreev",
      "lastWebsiteVisit": "2018-02-18 09:35:21",
      "isSubscribed": false,
      "domainMembershipMap": {}
    },
    "2": {
      "id": "2",
      "firstName": "Eliezer",
      "lastName": "Yudkowsky",
      "lastWebsiteVisit": "2019-12-21 03:34:41",
      "isSubscribed": false,
      "domainMembershipMap": {}
    },
    "5": {
      "id": "5",
      "firstName": "Eric",
      "lastName": "Rogstad",
      "lastWebsiteVisit": "2019-08-23 01:44:10",
      "isSubscribed": false,
      "domainMembershipMap": {}
    },
    "32": {
      "id": "32",
      "firstName": "Nate",
      "lastName": "Soares",
      "lastWebsiteVisit": "2017-05-10 13:41:18",
      "isSubscribed": false,
      "domainMembershipMap": {}
    },
    "1yq": {
      "id": "1yq",
      "firstName": "Eric",
      "lastName": "Bruylant",
      "lastWebsiteVisit": "2017-04-14 18:00:22",
      "isSubscribed": false,
      "domainMembershipMap": {}
    },
    "2bj": {
      "id": "2bj",
      "firstName": "Robert",
      "lastName": "Bell",
      "lastWebsiteVisit": "2017-12-10 09:52:27",
      "isSubscribed": false,
      "domainMembershipMap": {}
    },
    "2cl": {
      "id": "2cl",
      "firstName": "Travis",
      "lastName": "Rivera",
      "lastWebsiteVisit": "2017-10-08 22:46:28",
      "isSubscribed": false,
      "domainMembershipMap": {}
    },
    "2vh": {
      "id": "2vh",
      "firstName": "Jaime",
      "lastName": "Sevilla Molina",
      "lastWebsiteVisit": "2018-12-06 12:14:41",
      "isSubscribed": false,
      "domainMembershipMap": {}
    },
    "8b": {
      "id": "8b",
      "firstName": "Owen",
      "lastName": "Cotton-Barratt",
      "lastWebsiteVisit": "2017-01-26 20:23:21",
      "isSubscribed": false,
      "domainMembershipMap": {}
    },
    "8nz": {
      "id": "8nz",
      "firstName": "Michael",
      "lastName": "Keenan",
      "lastWebsiteVisit": "2018-04-25 19:27:28",
      "isSubscribed": false,
      "domainMembershipMap": {}
    },
    "8p0": {
      "id": "8p0",
      "firstName": "Gurkenglas",
      "lastName": "Gurkenglas",
      "lastWebsiteVisit": "2017-10-22 02:00:49",
      "isSubscribed": false,
      "domainMembershipMap": {}
    },
    "8pb": {
      "id": "8pb",
      "firstName": "Jacob",
      "lastName": "van Eeden",
      "lastWebsiteVisit": "2017-10-02 19:17:29",
      "isSubscribed": false,
      "domainMembershipMap": {}
    }
  },
  "domains": {
    "1": {
      "id": "1",
      "pageId": "1lw",
      "createdAt": "2016-01-15 03:02:51",
      "alias": "math",
      "canUsersComment": false,
      "canUsersProposeComment": true,
      "canUsersProposeEdits": true,
      "friendDomainIds": []
    },
    "2": {
      "id": "2",
      "pageId": "2v",
      "createdAt": "2015-03-26 23:12:18",
      "alias": "value_alignment",
      "canUsersComment": false,
      "canUsersProposeComment": true,
      "canUsersProposeEdits": true,
      "friendDomainIds": []
    },
    "3": {
      "id": "3",
      "pageId": "3d",
      "createdAt": "2015-03-30 22:19:47",
      "alias": "Arbital",
      "canUsersComment": false,
      "canUsersProposeComment": true,
      "canUsersProposeEdits": true,
      "friendDomainIds": []
    },
    "8": {
      "id": "8",
      "pageId": "198",
      "createdAt": "2015-12-13 23:14:48",
      "alias": "TeamArbital",
      "canUsersComment": false,
      "canUsersProposeComment": true,
      "canUsersProposeEdits": true,
      "friendDomainIds": []
    },
    "21": {
      "id": "21",
      "pageId": "1",
      "createdAt": "2015-02-10 17:12:19",
      "alias": "AlexeiAndreev",
      "canUsersComment": false,
      "canUsersProposeComment": true,
      "canUsersProposeEdits": true,
      "friendDomainIds": [
        "2069"
      ]
    }
  },
  "masteries": {
    "11w": {
      "pageId": "11w",
      "has": false,
      "wants": false,
      "level": 0,
      "updatedAt": ""
    },
    "1hh": {
      "pageId": "1hh",
      "has": false,
      "wants": false,
      "level": 0,
      "updatedAt": ""
    },
    "1r6": {
      "pageId": "1r6",
      "has": false,
      "wants": false,
      "level": 0,
      "updatedAt": ""
    },
    "21c": {
      "pageId": "21c",
      "has": false,
      "wants": false,
      "level": 0,
      "updatedAt": ""
    }
  },
  "marks": {},
  "pageObjects": {},
  "result": {},
  "globalData": null
}
