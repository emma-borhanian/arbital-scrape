{
  "resetEverything": false,
  "user": {
    "id": "",
    "firstName": "",
    "lastName": "",
    "lastWebsiteVisit": "",
    "isSubscribed": false,
    "domainMembershipMap": {},
    "fbUserId": "",
    "email": "",
    "isAdmin": false,
    "emailFrequency": "",
    "emailThreshold": 0,
    "ignoreMathjax": false,
    "showAdvancedEditorMode": false,
    "isSlackMember": false,
    "analyticsId": "aid:M97yvNYOCe01nQUnCidxWlGK+O/QIlDxNQS0FMsPG90",
    "hasReceivedMaintenanceUpdates": false,
    "hasReceivedNotifications": false,
    "newNotificationCount": 0,
    "newAchievementCount": 0,
    "maintenanceUpdateCount": 0,
    "invitesClaimed": [],
    "mailchimpInterests": {},
    "continueBayesPath": null,
    "continueLogPath": null
  },
  "pages": {
    "1": {
      "likeableId": "1",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "1",
      "edit": 5,
      "editSummary": "",
      "prevEdit": 4,
      "currentEdit": 5,
      "wasPublished": true,
      "type": "group",
      "title": "Alexei Andreev",
      "clickbait": "There is no spoon",
      "textLength": 304,
      "alias": "AlexeiAndreev",
      "externalUrl": "",
      "sortChildrenBy": "alphabetical",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "1",
      "editCreatedAt": "2016-12-13 02:34:00",
      "pageCreatorId": "1",
      "pageCreatedAt": "2015-09-04 16:14:58",
      "seeDomainId": "0",
      "editDomainId": "21",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 0,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 741,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": 0,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "2": {
      "likeableId": "938",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "2",
      "edit": 4,
      "editSummary": "",
      "prevEdit": 3,
      "currentEdit": 4,
      "wasPublished": true,
      "type": "group",
      "title": "Eliezer Yudkowsky",
      "clickbait": "Cofounder, with Nick Bostrom, of the field of value alignment theory.",
      "textLength": 512,
      "alias": "EliezerYudkowsky",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "2",
      "editCreatedAt": "2015-12-19 01:46:45",
      "pageCreatorId": "2",
      "pageCreatedAt": "2015-09-04 16:14:58",
      "seeDomainId": "0",
      "editDomainId": "2",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 5,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 2016,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": 0,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "15": {
      "likeableId": "140",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "15",
      "edit": 2,
      "editSummary": "",
      "prevEdit": 1,
      "currentEdit": 2,
      "wasPublished": true,
      "type": "wiki",
      "title": "Serum 25-Hydroxyvitamin D and Risks of Colon and Rectal Cancer in Finnish Men",
      "clickbait": "",
      "textLength": 323,
      "alias": "15",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "1",
      "editCreatedAt": "2015-03-26 22:21:26",
      "pageCreatorId": "1",
      "pageCreatedAt": "2015-02-27 22:44:15",
      "seeDomainId": "0",
      "editDomainId": "21",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 0,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 26,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": 0,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "178": {
      "likeableId": "202",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "178",
      "edit": 6,
      "editSummary": "",
      "prevEdit": 5,
      "currentEdit": 6,
      "wasPublished": true,
      "type": "wiki",
      "title": "Arbital \"tag\" relationship",
      "clickbait": "Tags are a way to connect pages that share a common topic.",
      "textLength": 2689,
      "alias": "Arbital_tag",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "1",
      "editCreatedAt": "2016-05-11 15:44:58",
      "pageCreatorId": "1",
      "pageCreatedAt": "2015-11-15 15:31:40",
      "seeDomainId": "0",
      "editDomainId": "3",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 0,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 96,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": 0,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "185": {
      "likeableId": "0",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "185",
      "edit": 0,
      "editSummary": "",
      "prevEdit": 0,
      "currentEdit": 0,
      "wasPublished": false,
      "type": "",
      "title": "",
      "clickbait": "",
      "textLength": 0,
      "alias": "",
      "externalUrl": "",
      "sortChildrenBy": "",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "",
      "editCreatedAt": "",
      "pageCreatorId": "",
      "pageCreatedAt": "",
      "seeDomainId": "",
      "editDomainId": "",
      "submitToDomainId": "",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": false,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 0,
      "isEditorComment": false,
      "isApprovedComment": false,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 0,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": 0,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "187": {
      "likeableId": "0",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "187",
      "edit": 0,
      "editSummary": "",
      "prevEdit": 0,
      "currentEdit": 0,
      "wasPublished": false,
      "type": "",
      "title": "",
      "clickbait": "",
      "textLength": 0,
      "alias": "",
      "externalUrl": "",
      "sortChildrenBy": "",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "",
      "editCreatedAt": "",
      "pageCreatorId": "",
      "pageCreatedAt": "",
      "seeDomainId": "",
      "editDomainId": "",
      "submitToDomainId": "",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": false,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 0,
      "isEditorComment": false,
      "isApprovedComment": false,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 0,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": 0,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "198": {
      "likeableId": "266",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "198",
      "edit": 4,
      "editSummary": "",
      "prevEdit": 3,
      "currentEdit": 4,
      "wasPublished": true,
      "type": "wiki",
      "title": "Team Arbital",
      "clickbait": "The people behind Arbital",
      "textLength": 184,
      "alias": "TeamArbital",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "1",
      "editCreatedAt": "2016-06-17 16:55:46",
      "pageCreatorId": "1",
      "pageCreatedAt": "2015-12-13 23:14:48",
      "seeDomainId": "0",
      "editDomainId": "8",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 0,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 1185,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": 0,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "370": {
      "likeableId": "2144",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "370",
      "edit": 4,
      "editSummary": "reflecting the fact that we only have one type of mark now.",
      "prevEdit": 3,
      "currentEdit": 4,
      "wasPublished": true,
      "type": "wiki",
      "title": "Arbital mark",
      "clickbait": "What is a mark on Arbital? When is it created? Why is it important?",
      "textLength": 1724,
      "alias": "arbital_mark",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "1yq",
      "editCreatedAt": "2016-07-22 00:08:32",
      "pageCreatorId": "1",
      "pageCreatedAt": "2016-04-14 23:12:16",
      "seeDomainId": "0",
      "editDomainId": "3",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 0,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 58,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": 0,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "595": {
      "likeableId": "0",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "595",
      "edit": 3,
      "editSummary": "",
      "prevEdit": 2,
      "currentEdit": 3,
      "wasPublished": true,
      "type": "wiki",
      "title": "Arbital page alias",
      "clickbait": "",
      "textLength": 1215,
      "alias": "arbital_alias",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "1yq",
      "editCreatedAt": "2016-07-21 23:06:57",
      "pageCreatorId": "5",
      "pageCreatedAt": "2016-07-10 00:52:28",
      "seeDomainId": "0",
      "editDomainId": "3",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 0,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 46,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": 0,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "596": {
      "likeableId": "0",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "596",
      "edit": 1,
      "editSummary": "",
      "prevEdit": 0,
      "currentEdit": 1,
      "wasPublished": true,
      "type": "wiki",
      "title": "Arbital page title",
      "clickbait": "",
      "textLength": 738,
      "alias": "Arbital_title",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "5",
      "editCreatedAt": "2016-07-10 01:18:37",
      "pageCreatorId": "5",
      "pageCreatedAt": "2016-07-10 01:18:37",
      "seeDomainId": "0",
      "editDomainId": "3",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 0,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 31,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": 0,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "597": {
      "likeableId": "3067",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "597",
      "edit": 2,
      "editSummary": "added clickbait",
      "prevEdit": 1,
      "currentEdit": 2,
      "wasPublished": true,
      "type": "wiki",
      "title": "Arbital page clickbait",
      "clickbait": "The text you are reading right now is clickbait.",
      "textLength": 1128,
      "alias": "Arbital_clickbait",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "1yq",
      "editCreatedAt": "2016-08-05 17:48:01",
      "pageCreatorId": "5",
      "pageCreatedAt": "2016-07-10 01:24:23",
      "seeDomainId": "0",
      "editDomainId": "3",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 0,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 43,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": 0,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "12y": {
      "likeableId": "88",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "12y",
      "edit": 2,
      "editSummary": "",
      "prevEdit": 1,
      "currentEdit": 2,
      "wasPublished": true,
      "type": "group",
      "title": "Patrick LaVictoire",
      "clickbait": "",
      "textLength": 199,
      "alias": "PatrickLaVictoir",
      "externalUrl": "",
      "sortChildrenBy": "alphabetical",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "12y",
      "editCreatedAt": "2016-01-25 22:25:07",
      "pageCreatorId": "12y",
      "pageCreatedAt": "2015-09-04 16:14:58",
      "seeDomainId": "0",
      "editDomainId": "32",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 0,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 64,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": 0,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "17b": {
      "likeableId": "204",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "17b",
      "edit": 16,
      "editSummary": "",
      "prevEdit": 15,
      "currentEdit": 16,
      "wasPublished": true,
      "type": "wiki",
      "title": "Arbital lens",
      "clickbait": "A lens is a page that presents another page's content from a different angle.",
      "textLength": 7216,
      "alias": "Arbital_lens",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "1yq",
      "editCreatedAt": "2016-12-05 13:10:54",
      "pageCreatorId": "1",
      "pageCreatedAt": "2015-11-15 18:01:48",
      "seeDomainId": "0",
      "editDomainId": "3",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 0,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 687,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": 0,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "18v": {
      "likeableId": "252",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "18v",
      "edit": 4,
      "editSummary": "",
      "prevEdit": 2,
      "currentEdit": 4,
      "wasPublished": true,
      "type": "wiki",
      "title": "Expected utility formalism",
      "clickbait": "Expected utility is the central idea in the quantitative implementation of consequentialism",
      "textLength": 390,
      "alias": "expected_utility_formalism",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "2",
      "editCreatedAt": "2017-02-16 19:21:50",
      "pageCreatorId": "2",
      "pageCreatedAt": "2015-12-02 23:59:07",
      "seeDomainId": "0",
      "editDomainId": "15",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 1,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 1982,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0
      ],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": -2,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "1fx": {
      "likeableId": "406",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "1fx",
      "edit": 7,
      "editSummary": "",
      "prevEdit": 6,
      "currentEdit": 7,
      "wasPublished": true,
      "type": "wiki",
      "title": "Reflective stability",
      "clickbait": "Wanting to think the way you currently think, building other agents and self-modifications that think the same way.",
      "textLength": 1681,
      "alias": "reflective_stability",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "2",
      "editCreatedAt": "2016-05-21 12:50:56",
      "pageCreatorId": "2",
      "pageCreatedAt": "2015-12-28 20:34:16",
      "seeDomainId": "0",
      "editDomainId": "2",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 2,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 461,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0
      ],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": -2,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "1ln": {
      "likeableId": "553",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "1ln",
      "edit": 6,
      "editSummary": "alias. note to self: come back and explain new requisites system.",
      "prevEdit": 5,
      "currentEdit": 6,
      "wasPublished": true,
      "type": "wiki",
      "title": "Arbital requisites",
      "clickbait": "To understand a thing you often need to understand some other things.",
      "textLength": 1210,
      "alias": "arbital_requisite",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "1yq",
      "editCreatedAt": "2016-07-19 23:24:15",
      "pageCreatorId": "1",
      "pageCreatedAt": "2016-01-11 17:09:53",
      "seeDomainId": "0",
      "editDomainId": "3",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 0,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 316,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": 0,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "1lw": {
      "likeableId": "559",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "1lw",
      "edit": 5,
      "editSummary": "added links",
      "prevEdit": 4,
      "currentEdit": 5,
      "wasPublished": true,
      "type": "wiki",
      "title": "Mathematics",
      "clickbait": "Mathematics is the study of numbers and other ideal objects that can be described by axioms.",
      "textLength": 745,
      "alias": "math",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "1yq",
      "editCreatedAt": "2016-06-22 17:49:03",
      "pageCreatorId": "2",
      "pageCreatedAt": "2016-01-15 03:02:51",
      "seeDomainId": "0",
      "editDomainId": "1",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 0,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 2288,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": 0,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "1ly": {
      "likeableId": "561",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "1ly",
      "edit": 26,
      "editSummary": "",
      "prevEdit": 25,
      "currentEdit": 26,
      "wasPublished": true,
      "type": "wiki",
      "title": "Bayesian update",
      "clickbait": "Bayesian updating: the ideal way to change probabilistic beliefs based on evidence.",
      "textLength": 1520,
      "alias": "bayes_update",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "2",
      "editCreatedAt": "2017-02-08 18:36:41",
      "pageCreatorId": "2",
      "pageCreatedAt": "2016-01-15 03:45:14",
      "seeDomainId": "0",
      "editDomainId": "1",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 0,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 994,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0
      ],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": -2,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "1rj": {
      "likeableId": "702",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "1rj",
      "edit": 25,
      "editSummary": "",
      "prevEdit": 24,
      "currentEdit": 25,
      "wasPublished": true,
      "type": "wiki",
      "title": "Conditional probability",
      "clickbait": "The notation for writing \"The probability that someone has green eyes, if we know that they have red hair.\"",
      "textLength": 6468,
      "alias": "conditional_probability",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "2",
      "editCreatedAt": "2016-10-08 02:05:05",
      "pageCreatorId": "2",
      "pageCreatedAt": "2016-01-26 23:06:38",
      "seeDomainId": "0",
      "editDomainId": "1",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 0,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 5397,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0
      ],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": -2,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "1rt": {
      "likeableId": "711",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "1rt",
      "edit": 3,
      "editSummary": "",
      "prevEdit": 2,
      "currentEdit": 3,
      "wasPublished": true,
      "type": "wiki",
      "title": "Arbital path",
      "clickbait": "Arbital path is a linear sequence of pages tailored specifically to teach a given concept to a user.",
      "textLength": 2327,
      "alias": "Arbital_path",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "1",
      "editCreatedAt": "2016-05-11 20:53:18",
      "pageCreatorId": "1",
      "pageCreatedAt": "2016-01-27 16:33:23",
      "seeDomainId": "0",
      "editDomainId": "3",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 0,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 214,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": 0,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "1y6": {
      "likeableId": "881",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "1y6",
      "edit": 21,
      "editSummary": "",
      "prevEdit": 20,
      "currentEdit": 21,
      "wasPublished": true,
      "type": "wiki",
      "title": "Belief revision as probability elimination",
      "clickbait": "Update your beliefs by throwing away large chunks of probability mass.",
      "textLength": 5701,
      "alias": "bayes_rule_elimination",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "2",
      "editCreatedAt": "2016-10-08 18:59:55",
      "pageCreatorId": "2",
      "pageCreatedAt": "2016-02-10 05:11:56",
      "seeDomainId": "0",
      "editDomainId": "1",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 0,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 7476,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0
      ],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": -2,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "2rb": {
      "likeableId": "1675",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "2rb",
      "edit": 1,
      "editSummary": "",
      "prevEdit": 0,
      "currentEdit": 1,
      "wasPublished": true,
      "type": "wiki",
      "title": "Reflective consistency",
      "clickbait": "A decision system is reflectively consistent if it can approve of itself, or approve the construction of similar decision systems (as well as perhaps approving other decision systems too).",
      "textLength": 1298,
      "alias": "reflective_consistency",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "2",
      "editCreatedAt": "2016-03-22 01:25:48",
      "pageCreatorId": "2",
      "pageCreatedAt": "2016-03-22 01:25:48",
      "seeDomainId": "0",
      "editDomainId": "2",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 0,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 276,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0
      ],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": -2,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "2v": {
      "likeableId": "1760",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "2v",
      "edit": 27,
      "editSummary": "",
      "prevEdit": 26,
      "currentEdit": 27,
      "wasPublished": true,
      "type": "wiki",
      "title": "AI alignment",
      "clickbait": "The great civilizational problem of creating artificially intelligent computer systems such that running them is a good idea.",
      "textLength": 5071,
      "alias": "ai_alignment",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "2",
      "editCreatedAt": "2017-01-27 20:32:06",
      "pageCreatorId": "2",
      "pageCreatedAt": "2015-03-26 23:12:18",
      "seeDomainId": "0",
      "editDomainId": "2",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 3,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 3822,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": 0,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "35z": {
      "likeableId": "0",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "35z",
      "edit": 0,
      "editSummary": "",
      "prevEdit": 0,
      "currentEdit": 0,
      "wasPublished": false,
      "type": "",
      "title": "",
      "clickbait": "",
      "textLength": 0,
      "alias": "",
      "externalUrl": "",
      "sortChildrenBy": "",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "",
      "editCreatedAt": "",
      "pageCreatorId": "",
      "pageCreatedAt": "",
      "seeDomainId": "",
      "editDomainId": "",
      "submitToDomainId": "",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": false,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 0,
      "isEditorComment": false,
      "isApprovedComment": false,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 0,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": 0,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "3d": {
      "likeableId": "2273",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "3d",
      "edit": 33,
      "editSummary": "",
      "prevEdit": 32,
      "currentEdit": 33,
      "wasPublished": true,
      "type": "wiki",
      "title": "Arbital",
      "clickbait": "Arbital is the place for crowdsourced, intuitive math explanations.",
      "textLength": 5201,
      "alias": "Arbital",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "1",
      "editCreatedAt": "2016-08-08 16:07:52",
      "pageCreatorId": "1",
      "pageCreatedAt": "2015-03-30 22:19:47",
      "seeDomainId": "0",
      "editDomainId": "8",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 0,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 2635,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": 0,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "3ft": {
      "likeableId": "0",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "3ft",
      "edit": 2,
      "editSummary": "",
      "prevEdit": 1,
      "currentEdit": 2,
      "wasPublished": true,
      "type": "wiki",
      "title": "Peano Arithmetic",
      "clickbait": "A system for proving theorems about arithmetic, which is strong enough to include self-reference.",
      "textLength": 1126,
      "alias": "peano_arithmetic",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "12y",
      "editCreatedAt": "2016-05-06 18:17:18",
      "pageCreatorId": "12y",
      "pageCreatedAt": "2016-05-06 17:57:22",
      "seeDomainId": "0",
      "editDomainId": "32",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 0,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 67,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0
      ],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": -2,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "3hs": {
      "likeableId": "2499",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "3hs",
      "edit": 19,
      "editSummary": "added link to exemplar pages",
      "prevEdit": 18,
      "currentEdit": 19,
      "wasPublished": true,
      "type": "wiki",
      "title": "Author's guide to Arbital",
      "clickbait": "How to write intuitive, flexible content on Arbital.",
      "textLength": 4420,
      "alias": "author_guide_to_arbital",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "1yq",
      "editCreatedAt": "2016-08-08 14:32:40",
      "pageCreatorId": "1",
      "pageCreatedAt": "2016-05-10 17:55:35",
      "seeDomainId": "0",
      "editDomainId": "3",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 0,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 433,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": 0,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "3n": {
      "likeableId": "2281",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "3n",
      "edit": 7,
      "editSummary": "",
      "prevEdit": 6,
      "currentEdit": 7,
      "wasPublished": true,
      "type": "wiki",
      "title": "Arbital \"parent\" relationship",
      "clickbait": "Parent-child relationship between pages implies a strong, inseparable connection.",
      "textLength": 2510,
      "alias": "Arbital_parent_child",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "8pb",
      "editCreatedAt": "2017-09-20 13:30:49",
      "pageCreatorId": "1",
      "pageCreatedAt": "2015-04-01 19:51:44",
      "seeDomainId": "0",
      "editDomainId": "3",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 0,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 199,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": 0,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "3rk": {
      "likeableId": "2859",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 1,
      "dislikeCount": 0,
      "likeScore": 1,
      "individualLikes": [],
      "pageId": "3rk",
      "edit": 9,
      "editSummary": "",
      "prevEdit": 8,
      "currentEdit": 9,
      "wasPublished": true,
      "type": "wiki",
      "title": "Start",
      "clickbait": "This page gives a basic overview of the topic, but may be missing important information or have stylistic issues. If you're able to, please help expand or improve it!",
      "textLength": 890,
      "alias": "start_meta_tag",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "2",
      "editCreatedAt": "2016-08-03 22:19:29",
      "pageCreatorId": "2",
      "pageCreatedAt": "2016-05-22 15:13:16",
      "seeDomainId": "0",
      "editDomainId": "8",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 0,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 136,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": 0,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "4v": {
      "likeableId": "2318",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 2,
      "dislikeCount": 0,
      "likeScore": 2,
      "individualLikes": [],
      "pageId": "4v",
      "edit": 6,
      "editSummary": "",
      "prevEdit": 5,
      "currentEdit": 6,
      "wasPublished": true,
      "type": "wiki",
      "title": "Work in progress",
      "clickbait": "This page is being actively worked on by an editor. Check with them before making major changes.",
      "textLength": 131,
      "alias": "work_in_progress_meta_tag",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "1yq",
      "editCreatedAt": "2016-07-05 22:48:12",
      "pageCreatorId": "2",
      "pageCreatedAt": "2015-04-17 01:27:41",
      "seeDomainId": "0",
      "editDomainId": "8",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 1,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 98,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": 0,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "4v4": {
      "likeableId": "2858",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 1,
      "dislikeCount": 0,
      "likeScore": 1,
      "individualLikes": [],
      "pageId": "4v4",
      "edit": 1,
      "editSummary": "",
      "prevEdit": 0,
      "currentEdit": 1,
      "wasPublished": true,
      "type": "wiki",
      "title": "Still needs work",
      "clickbait": "The next step up from \"Work in Progress\".  The page can be read as complete, but is a draft that needs further review and fine-tuning.",
      "textLength": 155,
      "alias": "still_needs_work",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "2",
      "editCreatedAt": "2016-06-27 01:42:54",
      "pageCreatorId": "2",
      "pageCreatedAt": "2016-06-27 01:42:54",
      "seeDomainId": "0",
      "editDomainId": "3",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": false,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 0,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": true,
      "viewCount": 19,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": 0,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "55w": {
      "likeableId": "3113",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "55w",
      "edit": 7,
      "editSummary": "",
      "prevEdit": 6,
      "currentEdit": 7,
      "wasPublished": true,
      "type": "wiki",
      "title": "Löb's theorem",
      "clickbait": "Löb's theorem ",
      "textLength": 1644,
      "alias": "lobs_theorem",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "5",
      "editCreatedAt": "2016-07-30 04:03:46",
      "pageCreatorId": "2vh",
      "pageCreatedAt": "2016-07-06 21:10:36",
      "seeDomainId": "0",
      "editDomainId": "1",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 2,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 692,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0
      ],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": -2,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "58b": {
      "likeableId": "3654",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 2,
      "dislikeCount": 0,
      "likeScore": 2,
      "individualLikes": [],
      "pageId": "58b",
      "edit": 12,
      "editSummary": "",
      "prevEdit": 11,
      "currentEdit": 12,
      "wasPublished": true,
      "type": "wiki",
      "title": "Logical decision theories",
      "clickbait": "Root page for topics on logical decision theory, with multiple intros for different audiences.",
      "textLength": 404,
      "alias": "logical_dt",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "2",
      "editCreatedAt": "2018-06-01 18:56:49",
      "pageCreatorId": "2",
      "pageCreatedAt": "2016-07-08 18:06:14",
      "seeDomainId": "0",
      "editDomainId": "15",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 0,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 3916,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": 0,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "58c": {
      "likeableId": "0",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "58c",
      "edit": 1,
      "editSummary": "",
      "prevEdit": 0,
      "currentEdit": 1,
      "wasPublished": true,
      "type": "wiki",
      "title": "Decision Theory",
      "clickbait": "",
      "textLength": 161,
      "alias": "DecisionTheory",
      "externalUrl": "",
      "sortChildrenBy": "alphabetical",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "1",
      "editCreatedAt": "2016-07-08 18:23:14",
      "pageCreatorId": "1",
      "pageCreatedAt": "2016-07-08 18:23:14",
      "seeDomainId": "0",
      "editDomainId": "15",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 0,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 178,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": 0,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "5b2": {
      "likeableId": "0",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "5b2",
      "edit": 2,
      "editSummary": "",
      "prevEdit": 1,
      "currentEdit": 2,
      "wasPublished": true,
      "type": "wiki",
      "title": "Omega (alien philosopher-troll)",
      "clickbait": "The entity that sets up all those trolley problems.  An alien philosopher/troll imbued with unlimited powers, excellent predictive ability, and very odd motives.",
      "textLength": 3250,
      "alias": "omega_troll",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "2",
      "editCreatedAt": "2016-07-10 22:16:14",
      "pageCreatorId": "2",
      "pageCreatedAt": "2016-07-10 22:14:21",
      "seeDomainId": "0",
      "editDomainId": "123",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 0,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 117,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0
      ],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": -2,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "5gc": {
      "likeableId": "3172",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "5gc",
      "edit": 2,
      "editSummary": "",
      "prevEdit": 1,
      "currentEdit": 18,
      "wasPublished": true,
      "type": "wiki",
      "title": "Introduction to Logical Decision Theory for Analytic Philosophers",
      "clickbait": "Why 'choose as if controlling the logical output of your decision algorithm' is the most appealing candidate for the principle of rational choice.",
      "textLength": 50515,
      "alias": "ldt_intro_phil",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "2",
      "editCreatedAt": "2016-07-21 22:01:35",
      "pageCreatorId": "2",
      "pageCreatedAt": "2016-07-21 01:36:22",
      "seeDomainId": "0",
      "editDomainId": "15",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": false,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 16,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 560,
      "text": "(This introduction is aimed at people entering from a general analytic-philosophy viewpoint.  If you're already familiar with causal decision theory, evidential decision theory, and Newcomblike problems, please see the [ldt_intro_dt LDT intro for decision theorists].)\n\nIs it rational to vote in elections?\n\nSuppose that roughly a hundred thousand people are voting in an election.  Then surely the chance of the election coming down to any one vote is tiny.  Say there are 50,220 votes for Kang and 50,833 votes for Kodos.  This is a close election as such things go.  But if you do vote for Kang, that just means 50,221 votes for Kang vs. 50,833 votes for Kodos.  If we're to select actions on the basis of their probable consequences, it seems that with overwhelming probability, the consequences of 'voting' and 'not voting' are nearly identical.\n\nAn economist who argues that voting is 'irrational' on this basis is deploying a formal answer from the most commonly accepted version of decision theory in contemporary philosophy, namely an answer from [-causal_decision_theory] (CDT).  To answer \"What would have happened if I'd voted?\", CDT says to imagine a world in which everything that happened before your vote stays constant; and the physical, causal consequences downstream of your vote are recomputed.  (The rules for imagining this are further formalized within the theory of [ causal models].)  When the dust settles, CDT's formally computed consequences for \"voting\" vs. \"not voting\" are 50,221 vs. 50,220 votes for Kang.  The only exception is if your physical act of voting *caused* someone else to vote (in which case the consequence is 50,222 votes for Kang).\n\nAn odd consequence of this view is that if the election *is* settled by one vote, say 8,001 for Kang vs. 8,000 for Kodos, then all 8,001 Kang voters should each view themselves as having individually swung the whole election - since if counterfactually they had voted for Kodos, the election would have gone to Kodos.  Conversely, in an election decided by 8,002 to 8,000 votes, nobody's vote changed anything.\n\nDilemmas such as these are part of a larger class of scenarios known as [newcomblike Newcomblike decision problems] where the world contains other agents that are similar to you or predicting you with significant accuracy.  This problem class also includes the [prisoners_dilemma Prisoner's Dilemma]; whether to turn down a lowball offer when [ultimatum_game bargaining]; and thought experiments involving [5b2 powerful aliens] who are excellent predictors of human behavior.\n\n*Logical decision theories* are a recently invented family of decision theories claiming to pose a more attractive alternative to causal decision theory.  Logical decision theory asserts that the principle of rational choice is \"Decide as though you are choosing the *logical output* of your *decision algorithm*.\"  Allegedly, there are major and significant reasons why (a) this is a decision theory that economists and computer scientists should prefer to use; and (b) this rule and its formalizations are more appealing candidates for the principle of rational choice.\n\nThis introduction will:\n\n- Overview Newcomblike problems and the contemporary view of them in analytic philosophy;\n- Introduce logical decision theories and how they differ formally from other well-known decision theories;\n- Reconsider well-known Newcomblike problems in the light of logical decision theory;\n- Make the case for logical decision theory as the principle of rational choice;\n- Overview some of the more interesting results in logical decision theory;\n- And point to further reading.\n\n# Newcomblike problems\n\nRoughly, Newcomblike problems can be seen as those where somebody similar to you, or trying to predict you, exists in the environment.  In this case your decision can *correlate* with events outside you, without your action *physically causing* those events.\n\n## Newcomb's Problem\n\nThe original [newcombs_problem Newcomb's Problem] was as follows:\n\nAn alien named [5b2 Omega] presents you with two boxes, a transparent box A containing \\$1,000, and an opaque Box B.  Omega then flies away, leaving you with the choice of whether to take only Box B ('one-box') or to take both Box A plus Box B ('two-box').\n\nOmega has put $1,000,000 in Box B if and only if Omega predicted that you would take only one box.  Otherwise Box B is empty. %note: The original formulation of Newcomb's Problem also specified that if Omega predicts you will decide to try to flip a coin, Omega leaves Box B empty.%\n\nOmega has already departed, so Box B is already empty or already full.\n\nOmega is an excellent predictor of human behavior and has never been observed to be mistaken. %note: We can suppose Omega has run this experiment 73 times previously and predicted correctly each time.  Since people do seem to form strongly held views about what they'd do in Newcomb's Problem, it is plausible that Omega could get this level of predictive accuracy by e.g. looking at your brain a few hours previously.%\n\nDo you take both boxes, or only Box B?\n\n- Argument 1:  People who take only Box B tend to walk away rich.  People who two-box tend to walk away poor.  It is better to be rich than poor.\n- Argument 2:  Omega has already made its prediction.  Box B is already empty or already full.  It would be [dt_rational irrational] to leave behind Box A for no reason.  It's true that Omega has chosen to reward people with irrational [dt_disposition dispositions] in this setup, but Box B is now *already empty*, and irrationally leaving Box A behind would just [causal_counterfactual counterfactually] result in your getting \\$0 instead of \\$1,000.\n\nThis setup went on to generate an incredible amount of debate.  Conventionally, Newcomb's Problem is seen as exhibiting a split between [evidential_dt evidential decision theory] and [causal_dt causal decision theory].\n\n## Evidential and causal decision theory\n\nAlmost everyone in present and historical debate on decision theory has agreed that [dt_rational rational] agents choose by calculating the [18v expected utility] *conditional on* each possible decision.  The central question of decision theory turns out to be, \"How exactly do we condition our probabilities on our possible decisions?\"\n\nMost times the expected utility formula is mentioned outside of decision theory, it is shown as follows:\n\n$$\\mathbb E[\\mathcal U|a_x] = \\sum_{o_i \\in \\mathcal O} \\mathcal U(o_i) \\cdot \\mathbb P(o_i|a_x)$$\n\nwhere\n\n- $\\mathbb E[\\mathcal U|a_x]$ is our average expectation of utility, if action $a_x$ is chosen;\n- $\\mathcal O$ is the set of possible outcomes;\n- $\\mathcal U$ is our utility function, mapping outcomes onto real numbers;\n- $\\mathbb P(o_i|a_x)$ is the [1rj conditional probability] of outcome $o_i$ if $a_x$ is chosen.\n\nThis formula is widely agreed to be wrong.\n\nThe problem is the use of standard evidential conditioning in $\\mathbb P(o_i|a_x).$  On this formula we are behaving as if we're asking, \"What would be my [1y6 revised] probability for $\\mathbb P(o_i),$ if I was *told the news* or *observed the evidence* that my action had been $a_x$?\"\n\nCausal decision theory says we should instead use the *counterfactual conditional* $\\ \\mathbb P(a_x \\ \\square \\! \\! \\rightarrow o_i).$\n\nThe difference between evidential and counterfactual conditioning is standardly contrasted by these two sentences:\n\n- If Lee Harvey Oswald didn't shoot John F. Kennedy, somebody else did.\n- If Lee Harvey Oswald hadn't shot John F. Kennedy, somebody else would have.\n\nIn the first sentence, we're being told as news that Oswald didn't shoot Kennedy, and [1ly updating our beliefs] to integrate this with the rest of our observations.\n\nIn the second world, we're imagining how a counterfactual world would have played out if Oswald had acted differently.  That is, to visualize the causal counterfactual:\n\n- We imagine everything in the world being the same up until the point where Oswald decides to shoot Kennedy.\n- We surgically intervene on our imagined world to change Oswald's decision to not-shooting, without changing any other facts about the past.\n- We rerun our model of the world's mechanisms forward from the point of change, to determine what *would have* happened.\n\nIf $K$ denotes the proposition that somebody else shot Kennedy and $O$ denotes the proposition that Oswald shot him, then the first sentence and second sentence are respectively talking about:\n\n- $\\mathbb P(K| \\neg O)$\n- $\\mathbb P(\\neg O \\ \\square \\!\\! \\rightarrow K)$\n\n(Further formalizations of how to [causal_counterfactuals compute causal counterfactuals] are given by Judea Pearl et. al.'s theory of [ causal models].)\n\nSo according to causal decision theory, the expected utility formula should read:\n\n$$\\mathbb E[\\mathcal U|a_x] = \\sum_{o_i \\in \\mathcal O} \\mathcal U(o_i) \\cdot \\mathbb P(a_x \\ \\square \\!\\! \\rightarrow o_i)$$\n\n## Newcomblike problems allegedly prying apart evidential and causal decisions\n\nThe current majority view of Newcomblike problems is that their distinctive feature is prying apart the verdicts of evidential and causal decision theory.\n\nFor example, on the conventional analysis of the original [newcombs_problem Newcomb's Problem], taking only Box B is *good news* about whether Box B contains \\$1,000,000, but cannot *cause* Box B to contain \\$1,000,000.\n\nFor a converse example in which the verdict of evidential decision theory seems much less reasonable, consider the following dilemma:\n\nSuppose that toxoplasmosis, a parasitic infection carried by cats, can cause toxoplasmosis-infected humans to become fonder of cats. %note:  \"Toxoplasmosis makes humans like cats\" was formerly thought to actually be true.  More recently, this result may have failed to replicate, which is unfortunate because we liked the really crisp example.%  You are now faced with a cute cat that has been checked by a veterinarian who says this cat definitely does *not* have toxoplasmosis.\n\nIf you decide to pet the cat, an impartial observer watching you will conclude that you are 10% more likely to have toxoplasmosis, which can be a fairly detrimental infection.  If you don't pet the cat, you'll miss out on the hedonic enjoyment of petting it.  Do you pet the cat?\n\nIn this case, evidential decision theory says not to pet the cat, since a 10% increase in the probability that you have toxoplasmosis is a significantly larger downside than the enjoyment of petting this one cat is an upside.\n\nCausal decision theory says that petting the cat can't *cause* you to contract toxoplasmosis.  After observing your own action, you may realize that you had toxoplasmosis all along, which isn't good; but in the counterfactual case where you didn't pet the cat, you would *still* have toxoplasmosis.\n\n(In the standard philosophical literature this dilemma is usually presented under the heading of [ Solomon's Problem], in which King Solomon must decide whether to be *the kind of person* who commits adultery which makes his rule more likely to be overthrown.  It may also be presented as the [ Smoking Lesion] dilemma, in which the impulse to smoke stems from a gene which also causes lung cancer, but lung cancer does not actually cause smoking.  Our own discussion uses the [toxoplasmosis_dilemma] because this seems less liable to cause confusion, especially in non-analytic-philosophers.)\n\nFor this reason, evidential decision theory is widely regarded as leading to 'an irrational policy of managing the news'.\n\nOn the majority view within contemporary decision theory, this is the reply to the \"If you're so rational, why aincha rich?\" argument in favor of one-boxing on Newcomb's Problem.  Somebody who actually takes only Box B is merely 'managing the news' about Box B, not actually acting to maximize the causal impacts of their actions.  Omega choosing to reward people who only take Box B is akin to happening to already have toxoplasmosis at the start of the decision problem, or Omega deciding to reward only evidential decision theorists.  Evidential agents only seem to win in 'Why aincha rich?' scenarios because they're managing the news in a way that an artificial problem setup declares to be news about wealth.\n\nSince many philosophers continue to find two-boxing on Newcomb's Problem to be an exceptionally unappealing decision, multiple attempts have been made to 'hybridize' evidential and causal decision theory,[todo: put citations here, or a greenlink to a page on hybridization attempts] to construct a theory which behaves 'evidentially' in some cases (like Newcomb's Problem) and 'causally' in other cases (like the [ Toxoplasmosis Dilemma] / Solomon's Problem).  Robert Nozick suggested at one point that evidentially computed and causally computed expected utilities be averaged together, so that an evidential gain of \\$1,000,000 could outweigh a causal loss of \\$1,000 on Newcomb's Problem.\n\n## When wealth doesn't follow evidential reasoning\n\nLogical decision theorists deny that decision theory ought to be analyzed as a conflict between evidential and causal utilities; and observe that it is easy to possible to pry apart *both* theories from the behavior that corresponds to being the richest agent at the end of the problem.  Consider e.g. Parfit's Hitchhiker:\n\n### Parfit's Hitchhiker\n\n> You are lost in the desert, your water bottle almost exhausted, when somebody drives up in a lorry.  The driver of this lorry is (a) entirely selfish, and (b) very good at detecting lies. %note: Maybe the driver went through Paul Ekman's training for reading facial microexpressions.%\n> \n> The driver says that they will drive you into town, but only if you promise to give them \\$1,000 on arrival. %note: We assume that relative to your situation and the local laws, there's no way that this contract would be enforceable apart from your goodwill.%\n\nIf you value your life at \\$1,000,000 and are otherwise motivated only by self-interest (e.g. you attach no utility to keeping promises as such), then this problem seems isomorphic to Gary Drescher's [ transparent Newcomb's Problem]: in which Box B is transparent, and Omega has already put \\$1,000,000 into Box B iff Omega predicts that you will one-box when faced with a visibly full Box B.\n\nBoth evidential decision theory and causal decision theory say to two-box in this case.  In particular, the evidential agent that has already updated on observing a full Box B will not update to an empty Box B after observing themselves two-box; they will instead conclude that Omega sometimes makes mistakes.  Similarly, an evidential agent who has already reached the safety of the city will conclude that the driver has made an error of reading faces, if they observe themselves refuse to pay the \\$1,000.\n\nThus the behavioral disposition that corresponds to ending up rich (the disposition to pay when you reach town, or one-box after seeing a full Box B) has been pried apart from both causal and evidential decision theories.\n\nWe might also observe that the driver in Parfit's Hitchhiker is not behaving as an arbitrary alien philosopher-troll like Omega.  The driver's reasoning seems entirely understandable in terms of self-interest.\n\n### The Termites Dilemma\n\nSuppose I have a strong reputation for being truthful, and also a strong reputation for being able to predict other people's behavior (especially the behavior of people who have publicly shown themselves to be evidential decision theorists).  You are the owner of a large apartment complex.  I send you the following letter:\n\n> Dear You:  I might or might not have seen signs of termites in your apartment complex.  If there is in fact termite damage, it will probably cost you around \\$1,000,000 to fix.  By the way, I'd also like to ask you to wire \\$1,000 to my bank account.  I now assure you that the following proposition is true:\n> *((Your apartment has termites) XOR (I predicted you would send me \\$1,000.))*\n\nAn evidential decision agent then reasons as follows:  \"If I choose not to send \\$1,000, then with high probability my apartment building has termites, which will cost me \\$1,000,000.  But if I do send \\$1,000, then my apartment building does not have termites.\"  The evidential decision agent therefore wires the \\$1,000.\n\nAgain, *ending up rich* has been pried apart from the advice of evidential decision theory; evidential agents will wander the world being exploited, while causal decision agents won't be targeted to begin with.\n\n# Logical decision theory\n\nA logical decision theorist defends as the principle of rational choice:  \"Choose as if controlling the *logical output* of your *decision algorithm.*\"\n\nWe'll see shortly that this corresponds to:\n\n- One-boxing on Newcomb's Problem\n- Paying the driver in Parfit's Hitchhiker\n- Petting the cat in the Toxoplasmosis Dilemma\n- Refusing to pay in the Termites Dilemma\n\n(The fact that these agents seem to 'end up rich' in a certain sense is not the only possible defense that can be given of logical decision theory; there are also more basic defenses of the allegedly superior internal coherence of this rule, counterarguments against arguments that it is inherently irrational to try to choose according to logical consequences, etcetera.)\n\nThis principle, as phrased, is informal.  \"Logical decision theories\" are really a family of recently proposed decision theories, none of which stands out as being clearly ahead of the others in all regards, but which are allegedly all better than causal decision theory.\n\n[ Functional decision theory] takes the form of an expected utility formula, written as follows:\n\n$$\\mathsf Q(s) = \\big ( \\underset{\\pi_x \\in \\Pi}{argmax} \\ \\sum_{o_i \\in \\mathcal O} \\mathcal U(o_i) \\cdot \\mathbb P(\\ulcorner \\mathsf Q = \\pi_x \\urcorner \\triangleright o_i) \\big ) (s)$$\n\nWhere:\n\n- $\\mathsf Q$ is the agent's current decision algorithm - that is, the whole calculation presently running.\n- $s$ is the agent's sense data.\n- $\\pi_x \\in \\Pi$ is the output of $\\mathsf Q$, a *policy* that maps sense data to actions.\n- $\\ulcorner \\mathsf Q = \\pi_x \\urcorner$ is the proposition that, as a logical fact, the output of algorithm $\\mathsf Q$ is $\\pi_x.$\n- $\\mathbb P(X \\triangleright o_i)$ is the probability of $o_i$ conditioned on the logical fact $X.$\n\nThis is not fully formalized because work is still in progress on laying down an exact algorithm for the logical-conditioning or [logical_counterfactual logical counterfactual] operator $X \\triangleright Y$.  But it's worth noting that many discussions of causal decision theory have treated causal counterfactuals as *self-evident* or as a heaven-sent conditional distribution $\\mathbb P(\\bullet \\ || \\ \\bullet).$  Functional decision theory is at least no *more* informal than causal decision theory thus treated.  If we feel that it is intuitively obvious how the universe 'would have looked' if our logical algorithm had yielded an output of two-boxing, then functional decision theory yields a clear output relative to this intuition.\n\nTwo special-case ways of calculating $X \\triangleright Y$ yield non-general but useful logical decision theories:\n\n- [timeless_dt] takes in a standard causal model that includes some nodes intended to represent logical propositions, and then computes standard counterfactuals inside this causal model.\n- [proof_based_dt] treats $X$ as premise introduced into a standard proof algorithm, and works out further logical implications.\n\n[timeless_dt] suffices to formalize all the dilemmas, thought experiments, and economic scenarios as well as they were ever formalized in causal decision theory.\n\n[proof_based_dt] allows us to deploy running code that simulates [modal_agents] deciding what to do in multiplayer dilemmas, e.g. the [prisoners_dilemma].\n\nBut we know these two formalizations aren't complete because:\n\n- The proof formalism we use for modal agents has [ weird edge cases] indicating that it only correctly formalizes the intuitive notion of logical conditioning some of the time.\n- We don't have a general algorithm for *building* causal models that include logical facts, and it's not clear that the [timeless_dt TDT] representation can model any setup more complicated than \"run this exact algorithm in two different places\".\n\nAn important feature quietly introduced in the above formula for $\\mathsf Q,$ is that $\\mathsf Q$ chooses *policies* (mappings from sensory observations $s$ to actions) rather than outputting actions directly.  This makes functional decision theory [updateless_dt updateless], a feature with deep ramifications and justifications that pretty much require digging into the longer articles.  A *very* rough example is that this feature is useful for, e.g., the [absentminded_driver Absent-Minded Driver problem] where the *correlation* between our current observations, and the background variables that determine good policy, is itself dependent on our policy.\n\n## LDT behavior on some Newcomblike problems\n\n### Newcomb's Problem\n\nWe can set this up in TDT using this augmented logical graph:\n\n[todo: make actual graph]\n\n- (Your mind at 7am) -> ( | $\\mathsf Q$ --> Omega's prediction of your decision) -> ($\\mathsf Q$ --> Your decision at 8am  | Box B) -> (Your payoff)\n\nOn CDT, we compute the counterfactual \"What if I one-box / two-box?\" by supervening on the node \"Your decision at 8am\"; on TDT we supervene on the output of $\\mathsf Q.$  Aside from that, both systems obey standard rules about changing only that one point and running all the other rules of the model forward.\n\nThus, TDT one-boxes.\n\n### Parfit's Hitchhiker\n\nWe can set up in TDT as follows:\n\n[todo: make actual graph]\n\n- (Your mind at 7am) -> ( | $\\mathsf Q(city)$ --> Driver's prediction of what you will do if you see yourself in the city) -> ( | Driver's choice to take you to city) -> ($\\mathsf Q(city)$ --> Your decision whether to pay when you're in the city) | ($\\mathsf Q(desert)$ --> Your decision about how to die in the desert) -> (Your payoff)\n\nGenerically, it's easiest to set up this general class of problem in [updateless_dt updateless] form so we can just choose policies (mappings from observations to actions) - in other words, at every point where $\\mathsf Q$ appears, we act as if optimizing over the whole $\\mathsf Q,$ and only then take into account our observations.\n\nHowever, in this particular case, just optimizing over $\\mathsf Q(city)$ is enough to give us the answer.  Even once we're already in the city, when we compute the *counterfactual* starting by setting $\\mathsf Q(city)$ while changing no prior causes and then running the rules forward, we will get the result, \"If I hadn't chosen to pay \\$1,000, then I would have died in the desert.\"\n\n### Toxoplasmosis Dilemma\n\nThere are complications in setting this up formally, since we need a background mechanic that succeeds in creating a correlation between \"pet the cat\" and \"has toxoplasmosis\"--if everyone is an LDT agent and everyone decides to pet the cat, then there won't be any correlation in the first place.\n\nIn [functional_dt functional decision theory], we assume that $\\mathsf Q$ knows its own formula $\\ulcorner \\mathsf Q \\urcorner$ (e.g. via [godelian_diagonalization]).  So if we say that different agents have slightly different utility functions correlated with toxoplasmosis, then in functional decision theory, each agent ought to already know its own algorithm and to have [tickle_defense already updated about toxoplasmosis].  (FDT is not always a good descriptive theory of human behavior!) %note: It's an open problem to formulate a more realistic LDT that may not have full knowledge about its own quoted algorithm.%\n\nTo hack our way to a roughly similar setup, we can suppose that there's some mix of EDT agents and LDT agents encountering the problem; and that [5b2 Omega] has told us, \"Through no fault or virtue of their own, it just so happens that in this particular random sample, agent types that don't pet the cat after being given this information already have toxoplasmosis with 10% frequency, and agent types who do pet the cat already have toxoplasmosis with 20% frequency.\"\n\nThen our graph might look something like this:\n\n- ($\\mathsf Q(warning)$ | toxoplasmosis frequency) -> (Omega warning?), (pet cat?) -> (payoff)\n\nIn this setup, our decision to pet the cat and toxoplasmosis both affect Omega's warning, our decision to pet the cat affects whether we get cat hedons, and cat hedons and toxoplasmosis both affect our payoff.\n\nWe compute, \"If-counterfactually people like me *didn't* pet the cat, then (a) I wouldn't have received cat-petting hedons, (b) I'd still have toxoplasmosis with the same probability, and (c) Omega would've given us a different statistical summary.\"\n\nOn the more regular toxoplasmosis problem, this might analogously work out to thinking, \"If-counterfactually people like me didn't pet cats in this situation, then there wouldn't be any correlation between toxoplasmosis and petting in the first place; but actual toxoplasmosis wouldn't be reduced in any way.\"\n\n### Termites Dilemma\n\n- ($\\mathsf Q(message)$) -> (agent's decision to try blackmail, Termites -> agent's message, whether we pay) -> ( | | Termites -> payoff)\n\nSince an LDT agent doesn't pay in the Termites dilemma, nobody sends us a message in the first place.\n\nBut if we did get the message, we'd compute $\\mathsf Q$ by noticing that the policy (message -> pay) results in our getting a message and our paying if there are no termites, while if there are termites, the agent wouldn't send us a message.  Regardless of the prior probability of termites, this does more poorly than the policy of not paying.\n\n# LDT as the principle of rational choice\n\nIt should now be clear that the family of logical decision theories gives *different* answers in Newcomblike problems compared to some widely-analyzed previous theories.  Are these *better* answers?  Are they *more rational* answers?\n\nThe argument for considering \"Choose as if controlling the logical output of your decision algorithm\" as the principle of *rational* choice--rather than being 'useful irrationality' or some such--rests on three main pillars:\n\n- The argument that CDT counterfactuals are not inherently any more sensible than LDT counterfactuals, since it's not like there are actual counterfactual worlds floating out there or a previously God-given rule that we must decide based on a particular kind of counterfactual;\n- The argument that 'Why aincha rich?' ought to have considerable force here, since Newcomblike problems are not especially unfair or unrealistic (e.g. voting in elections), and we *can* make our decision algorithm's output be anything we want, to just the same degree as we can control our actions.\n- The argument from greater internal coherence and simplicity:  CDT agents wistfully wish they were more LDT-ish agents.  LDT agents prefer to be LDT, have no need for precommitments to dispute control of their future choices with their future selves, and don't predictably reverse their preferences between different times.\n\n## Freedom of counterfactual imagination\n\nThe standard case for causal decision theory rests *primarily* on the assertion that it is prima facie irrational to act as if, e.g., one-boxing in Newcomb's Problem can cause box B to be full.\n\nIs it not in some sense *true,* after Parfit's driver has conveyed the LDT agent to the city, that in the counterfactual world where the LDT agent does not choose to pay at the time, the LDT agent remains in the city and does not vanish away into the desert?  In this sense, must not the LDT agent be deluded about some question of fact, or be acting as if so deluded?\n\nThe logical decision theorist's response has two major subthrusts:\n\n- Since there are no actual counterfactual worlds floating out there in the void where I performed a different action, describing the world where I acted differently is just an act of imagination.  It isn't *false* if I have some lawful, simple, coherent rule for imagining the conditional results of my actions that isn't a classical causal counterfactual, and this rule makes the rest of my decision theory work well.  \"Counterfactuals were made for humanity, not humanity for counterfactuals.\"\n- I don't one-box on Newcomb's Problem *because* I think it physically causes Box B to be full.  I one-box on Newcomb's Problem because I have computed this output in an entirely different way.  It [petitio_principii begs the question] to assume a rational agent must make its decision by carrying out a particular ritual of cognition about which things physically cause other things, and then criticize me for \"acting as if\" I falsely believe that my choice physically causes Box B to be full.\n\nThe first element of the response says that there are not *actually* alternate Earths floating alongside our planet and clearly visible from here, letting us see with our naked eyes what our action-conditionals should be.  Critiquing an action-conditional on the grounds, \"That counterfactual is *false*,\" is not as straightforward as saying, e.g., \"Your assertion that most humans on Earth have eight legs is *false* under a [ correspondence theory of truth], because we can look around the Earth and see that most people don't have eight legs.\"\n\nThis might be a jarring step in the argument, from the standpoint of a philosophical tradition that's accustomed to, e.g., considering statements about modal necessity to have a truth-value that is evaluated to some heaven-sent set of possible worlds.  But again, on the [ Standard Model of physics], there are not actually any counterfactual worlds floating out there. %note: Even the many-worlds interpretation of quantum mechanics doesn't modify this.  There is no rule saying that there must be a world floating out there for each kind of possible decision we could take, where nothing else has changed except that decision. And from an LDT agent's standpoint, we are asking about the decision-algorithm Q, and its alternate outputs are logical impossibilities; see below.%\n\nWe can fix some logical rule for evaluating a particular kind of $\\operatorname{counterfactual}_1$, such as [ Pearl's intervention] $\\operatorname {do}().$  It can then be a [logical_validity valid] deduction given the [correspondence_truth true] history of our Earth that \"If-$\\operatorname{counterfactual}_1$ Lee Harvey Oswald had not shot John F. Kennedy, nobody else would have.\"  If we understand the fixed logical sense of \"If... hadn't\" in terms of $\\operatorname{counterfactual}_1$, then it can be informative about the history of the actual world to be told, \"If Oswald hadn't shot Kennedy, nobody else would've.\"\n\nThe LDT agent is thinking about a different rule, $\\operatorname{counterfactual}_2$ (which happens to yield the same answer in the case of Kennedy and Oswald).  The logical decision theorist observes both \"There are no actual counterfactual worlds floating out there, at least not where we can see them, so critiquing my output isn't as simple as pointing to an actual-world statement being false\" and \"The point we're debating is exactly whether a rational agent ought to use $\\operatorname{counterfactual}_1$ or $\\operatorname{counterfactual}_2,$ so you can't point to $\\operatorname{counterfactual}_2$'s outputs and declare them 'false' or 'irrational' by comparing them with $\\operatorname{counterfactual}_1.$\"\n\nIn fact, the logical decision theorist can turn around this argument and deliver a critique of classical causal decision theory:  Any expected utility agent does calculate one conditional where a correspondence theory of truth directly applies to the answer, namely the conditional on the action it *actually takes.*  CDT's calculation of this counterfactual conditional on Newcomblike problems is often wrong compared to the actual world.\n\nFor example, in Newcomb's Problem, suppose that the base rate of people one-boxing is 2/3.  Suppose we start with a CDT agent not yet knowing its own decision, %note: If the CDT agent does already know its own decision, why would it still be trying to compute it?% that uses the standard $\\operatorname {do}()$ rules for [counterfactual_do counterfactual surgery].  This agent will calculate that its expected value is (2/3 * \\$1M + \\$1K) if it takes both boxes and (2/3 * \\$1M) otherwise.  This yields the classic CDT answer of 'take both boxes', but it does so by calculating a conditional expected utility premised on 'take both boxes', which yields the quantitatively wrong answer.  Even if afterwards the CDT agent realizes that box B is empty, it will still have calculated an objectively false conditional in order to make its decision.\n\n(As an obvious patch, causal decision theorists have suggested a patched CDT that can observe its own suspected action, update, and then recalculate expected utilities to choose again.  But this patched algorithm is known to go into infinite loops on some Newcomblike problems!  A twice-patched algorithm can prevent infinite loops by randomizing its actions in some cases, in which case a stable solution is guaranteed to exist.  But then the expected utility, calculated conditional on that mixed strategy, is again wrong for the actual world!  See the analysis of [death_in_damascus].)\n\nTaking a step back and looking at the issue from inside the perspective of LDT, some decision algorithm $\\mathsf Q$ is asking about worlds conditional on various actions $a$ or policies $\\pi.$  All but one of these worlds are logically impossible - it is no more possible for $\\mathsf Q$ to have some different output than it actually has, than for 2 + 2 to equal 5.  Usually, while we are deciding, we will not know *which* of our seemingly potential choices are logically impossible; but all except one of them are. %note: If you already know your decision, why are you still trying to decide?  If you know you definitely won't do some action, why bother spending the computing power to evaluate its expected utility?  Some Newcomblike dilemmas can pose [ apparent exceptions to this rule], but that's a longer story.%\n\nSince we are asking about worlds that are mostly logically impossible in any case, we are free to visualize the logically impossible ones in a way that is conducive to ending up rich (see the next subsection) and that has good coherence properties (see the subsection after that).\n\nBut even if we're asking about the 'reasonableness' of the visualizations qua visualizations, a logical decision theorist might say at least the following:\n\n- Our visualization of the conditional that *is* logically possible, and matches actual reality in that regard, ought to match the rest of actual reality (which CDT does not).\n- If I'm similar to another 900 people deciding whether to vote using sufficiently similar algorithms to $\\mathsf Q,$ then it is more 'reasonable' to visualize a world where *all* the outputs of $\\mathsf Q$ move in lockstep, then to visualize only one output varying.\n\nThat is:  If you must imagine a world where 91 is a prime number, at least have it be prime *all* the time, not prime on some occasions and composite on others.  To imagine \"91 is sometimes prime and sometimes composite\" is wrong in an immediately visible way, much faster than we can think of the prime factors 7 and 13.  Supposing \"Maybe $\\mathsf Q$ decides not to vote after all?\" is imagining an 'opaque' impossibility that we haven't yet realized to be impossible.  Supposing \"Maybe my $\\mathsf Q$ outputs 'don't vote' but all the other instances of $\\mathsf Q$ output 'do vote'?\" is transparently impossible.\n\n(Of course this is all a viewpoint from within LDT.  A causal decision theorist could reply that they are just imagining a physical variable changing, and not thinking of any logical algorithms at all.)\n\n%%comment:\n%todo: Move this part to a longer discussion of 'reasonable' counterfactuals.  It has caveats about trying to drive down the probability of worlds you're already inside, in order to resist blackmail and so on.%\n\nAlthough this point is still a bit controversial among logical decision theorists, some logical decision theorists would assert that *on any particular reasoning step,* there's no reason for a rational algorithm to visualize a world that algorithm already knows to be impossible.\n\nE.g., even if Parfit's driver has already conveyed you into the city, you are bothering to imagine 'What if I don't pay?' in order to *verify* that you can't get an even better outcome where you don't pay and are still in the city.  If you're bothering to calculate your actions at all, then your algorithm $\\mathsf Q$ doesn't already know this.\n\nA CDT agent imagines worlds where it two-boxes but still gets \\$1,001,000; this is a decision rule that reasons about *transparently* impossible worlds on its intermediate steps. \n\n%todo:\nMore points to talk about in a section on reasonable counterfactuals:  A CDT agent could reply that they're just imagining some local exception to the laws of physics; but then they're not exactly visualizing OMG MAGIC in those worlds, so they are trying to visualize the logical impossibility after all.\n%\n%%\n\n## Newcomblike dilemmas are a fair problem class\n\nThe classic objection to causal decision theory has always been, \"If you're so rational, why aincha rich?\"  The classic reply is summarized in e.g. \"Foundations of Causal Decision Theory\" by James Joyce: %note: No, not that James Joyce.%\n\n> Rachel has a perfectly good answer to the \"Why ain't you rich?\" question. \"I am not rich,\" she will say, \"because I am not the kind of person the psychologist thinks will refuse the money. I'm just not like you, Irene. Given that I know that I am the type who takes the money, and given that the psychologist knows that I am this type, it was reasonable of me to think that the \\$1,000,000 was not in my account. The \\$1,000 was the most I was going to get no matter what I did. So the only reasonable thing for me to do was to take it.\"\n> \n> Irene may want to press the point here by asking, “But don’t you wish you were like me, Rachel? Don’t you wish that you were the refusing type?” There is a tendency to think that Rachel, a committed causal decision theorist, must answer this question in the negative, which seems obviously wrong (given that being like Irene would have made her rich). This is not the case. Rachel can and should admit that she does wish she were more like Irene. “It would have been better for me,” she might concede, “had I been the refusing type.” At this point Irene will exclaim, “You’ve admitted it! It wasn’t so smart to take the money after all.” Unfortunately for Irene, her conclusion does not follow from Rachel’s premise. Rachel will patiently explain that wishing to be a refuser in a Newcomb problem is not inconsistent with thinking that one should take the $1,000 whatever type one is. When Rachel wishes she was Irene’s type she is wishing for Irene’s options, not sanctioning her choice.\n\nThis accuses Omega of simply being prejudiced against rational agents--by the time the experiment starts, rational agents have already been disadvantaged.\n\nBut Omega is not *per se* disadvantaging rational agents, or causal decision agents in particular.  We can imagine that Omicron goes about whacking CDT agents in the head with a sledgehammer, *regardless of what they choose,* and this would indeed seems 'unfair', in the sense that it seems we can't deduce anything about the fitness of CDT agents from their doing worse in Omicron's dilemma.  We can imagine that Upsilon puts a million dollars in Box B only if you are a sort of agent that chooses between 'one-boxing' and 'two-boxing' *by choosing the first option in alphabetical order,* and if you one-box for any other reason, Upsilon empties the box.\n\nOmicron and Upsilon are indeed privileging particular algorithms; their rules make explicit mention of \"CDT\" or \"alphabetical ordering\"; they care *why* you behave a certain way and not just that you do so.  But Omega does not care *why* you one-box. %note: In the original formulation of Newcomb's Problem, it was said that Omega leaves Box B empty if you try to choose by flipping a coin.  This is one reason to prefer the formulation where Omega can predict coinflips.% You can one-box because of LDT, or EDT, or because you're the sort of agent that always prefers the option highest in alphabetical order; Omega will fill Box B just the same.\n%%comment:  %note:  On some views of the nature of rationality, it's exactly *because* Omega only cares what we do, and not why we do it, that Newcomb's Problem is a test of rationality; 'rationality' *is* exactly what we do when we only care about the results and not the reasoning we use to get them.% %%\n\nAccording to a logical decision theorist, when a problem depends just on our *behavior* or \"the type of decisions that we make, being the people that we are\" and not on any other properties of our algorithm apart from that, then that seems like a sufficient condition to designate the problem as 'fair'.\n\nIndeed, we can see decision theories as *corresponding to* a class of problems that they think are [fair_problem_class fair]:\n\n- CDT thinks a problem is 'fair' if your results depend only on your physical act, and not on anyone else's predictions about your physical act or any other logical correlations that don't stem from the physical act.  On problems in this class, CDT agents always end up as rich as any other agent encountering the problem.\n- [functional_dt Functional decision theory] thinks it's 'fair' for a problem to depend at any point, on any logical or physical consequence, of any disposition you have to behave in any way, in any situation; so long as the problem depends *only* on this behavioral disposition and not on any other aspect of your code or algorithm apart from that.\n- In [proofbased_dt] and [modal_agents], your payoff may depend on whether other agents can *prove* that you behave a certain way, not just whether you actually behave that way.  For example, other agents may cooperate with you only if they can *prove* that you cooperate with them.\n\nA causal decision theorist might argue that on an *ideal* version of [parfits_hitchhiker], the driver is really making their decision by looking at our face; and that if we were ideal CDT agents, we'd be able to control this behavior and optimize the real, physical channel by which we are influencing the driver.\n\nA logical decision theorist replies, \"Okay, but maybe I don't have perfect control of my face, and therefore the output of my algorithm $\\mathsf Q(city)$ is affecting both what I buy in the city and my current prediction of what I'll buy in the city, which in turn affects my facial expression.  In real life, I'm not good enough at deception or self-deception to break this logical correlation.  So the logical correlation is actually there and we need to use a decision theory that can handle the wider problem class.  Why is that so terribly unfair?\"\n\nOr similarly in the case of voting in elections: maybe we just are in fact logically entangled with a cohort of people thinking similarly to ourselves, and nothing in particular is going to move us into the problem class where this correlation doesn't exist.  Why is that unfair?\n\n(The LDT [fair_problem_class class of 'fair' problems] comes very close to *dominating* the CDT class, that is, it is very nearly true that LDT agents think a strictly larger class of problems are fair and do well in a strictly wider set of situations.  But since (on the LDT view) CDT agents are blind to some LDT-relevant correlations, it is possible to construct LDT-unfair dilemmas that CDT agents think are fair.  For example, suppose Omega would have bombed an orphanage two days ago if the LDT algorithm in particular yielded the output of picking up a certain \\$5 bill in the street, but Omega doesn't similarly discriminate against the CDT algorithm.  The CDT agent cheerfully walks over and picks up the \\$5 bill, *and* believes that the superstitious LDT agent would have received just the same payoff for this same physical action, making the problem CDT-fair.  For an *arguably* structually similar, but much more natural-seeming problem, see [100ldt_1cdt_pd_tournament here].)\n\nAnother way of seeing decision theories as corresponding to problem classes is by looking at the considerations that the decision theory allows itself to take into account; decision theories generally think that considerations they are not allowed to take into account are unfair.\n\nIn the passage from before on Rachel and Irene, James Joyce continues:\n\n> Rational Rachel recognizes that, whether she is the type that was predicted (on Friday) to take the money or the type that was predicted to refuse it, **there is nothing she can do now to alter her type.**  She thus has no reason to pass up the extra \\$1,000. \\[emphasis added.\\]\n\nFrom an intuitive standpoint, an LDT agent sees Rachel dooming herself to two-box *only* because she believes herself to be powerless; if she believed herself to be in control of her type, she could [logical_control control] her type.\n\nA more formal view would be that an LDT agent becomes the one-boxing type because the logical algorithm $\\mathsf Q$ computes its logical output--the 'type' that everything dependent on the logical output of $\\mathsf Q$ depends on--by taking into account *all* the consequences of that 'type'.  As for any disempowering thoughts about it being \"too late\" to control consequences of $\\mathsf Q$ that occur before some particular time, $\\mathsf Q$ isn't built to specially exclude those consequences from its calculation.  If someone objects to the term 'control', it can at least be said that $\\mathsf Q$ has a symmetry in that everything affected by its 'type' is being modeled in the calculation that *determines* the type.\n\nIf sometimes logical correlations do in fact exist--as in the case of voting; or as in the case of not being able to perfectly control our facial muscles or beliefs when facing the driver in [parfits_hitchhiker]; or as in the case of a machine agent e.g. running on Ethereum or whose code has become known by some other channel--then in what sense is it rational for an algorithm to wantonly exclude some consequences of the algorithm's 'type', from being weighed into the calculation that determines the algorithm's 'type'?\n\n%%comment:\n\n%todo: move this to a separate section on control, maybe in the defense of counterfactuals. % \n\nRachel's algorithm $\\mathsf R$ is computing its answer right that very moment, deciding right then what type of agent to be.  While $\\mathsf R$ is fated in some sense to two-box, it 'could' have been a one-boxing algorithm to just the same extent that any of us 'could' perform some action that is the better for us.  If we can regard ourselves as [logical_control controlling] our physical acts, we might as well regard ourselves as controlling 'the type of decision we make'.  \n\nAs for the charge that the key proposition lies in the physical past, Gary Drescher observes that we can by raising our hand today, [logical_control control] a certain proposition about the state of the universe a billion years earlier: namely, 'The state of the universe today is such that its development under physical law will lead to (your name here) raising their hand a billion years later.'  Similarly, we can today, by an act of will control 'the sort of decision we will make a day later, being the person that we are' as it was true about our physical state yesterday. %%\n\n## Consistency and elegance\n\nA CDT agent, given the chance to make preparations before Omega's arrival, might pay a \\$100 fee (or \\$100,000 fee) to have an assistant stand nearby and threaten to shoot them if they don't leave behind Box A.\n\nThen, given the chance later by surprise, the same CDT agent would pay \\$100 to make the gun-toting assistant go away--even believing that Omega has accurately predicted this, and that Box B is therefore empty.\n\nThis is an example of what an economic psychologist or behavioral economist would call a [ dynamic inconsistency].  If Omega visits at 7:30am, then the CDT agent at 7am and the same CDT agent at 8am have different preferences about what they respectively want the CDT agent to do at 8am.  The CDT agent at 7am will pay precommitment costs to try to wrench control away from the CDT agent at 8am; the CDT agent at 8am will pay costs to wrench control back.\n\nHistorically speaking, this property of CDT was especially important from the perspective of the people with a computer-science orientation who helped to develop the family of logical decision theories.  If CDT is dynamically inconsistent then it is of necessity [2rb reflectively inconsistent]; that is, the CDT algorithm does not want its future self to use the CDT algorithm.  If one were to suppose a self-modifying AI that started out using CDT, it would immediately modify itself to use a [son_of_CDT different theory] instead.\n\n\n\nFrom the standpoint of an LDT agent, this is no more excusable than an agent exhibiting non-Bayesian behavior on the [allais_paradox Allais Paradox], paying \\$1 to throw a switch and then paying another \\$1 to throw it back.  On the paradigm of a logical decision theorist, a rational agent would not exhibit such incoherence and drive itself around in circles; the true principle of rational choice ought not to be so inconsistent.  We sometimes learn new information that changes our instrumental preferences, but a rational agent with a constant utility function should not be able to *predict* preference reversals any more than a rational agent should be able to predict a net directional change in its probability estimates.\n\n\n\n\n%todo:\nfreedom of counterfactual;\nobjection from logical impossibility;\nwhy aincha rich? fairness of Newcomblike problems and dominance;\nargument from coherence:\n vs. Son of CDT;\n%\n\n%comment:\n(actually move this to conclusion, after presenting more interesting results)\nThe thesis is that when all these pillars are considered, we end up in a situation where, in an intuitive or pretheoretic sense, LDT seems to have it all over CDT.  That is, CDT may still seem more 'rational' (if not richer!) from within the standpoint of a pure CDT agent that can't consider any other view; but if LDT and CDT were being presented side-by-side to someone who had not as yet considered either, CDT would have little or no appeal.\n%\n\n\n",
      "metaText": "",
      "isTextLoaded": true,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 1,
      "maintainerCount": 1,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": 0,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 18,
      "redLinkCount": 0,
      "lockedBy": "2",
      "lockedUntil": "2018-06-01 18:51:52",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": {
        "edit": {
          "has": false,
          "reason": "You don't have domain permission to edit this page"
        },
        "proposeEdit": {
          "has": true,
          "reason": ""
        },
        "delete": {
          "has": false,
          "reason": "You don't have domain permission to delete this page"
        },
        "comment": {
          "has": false,
          "reason": "You can't comment in this domain because you are not a member"
        },
        "proposeComment": {
          "has": true,
          "reason": ""
        }
      },
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [
        "58b"
      ],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [
        "3rk"
      ],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [
        {
          "id": "5381",
          "parentId": "58b",
          "childId": "5gc",
          "type": "subject",
          "creatorId": "2",
          "createdAt": "2016-07-17 23:55:28",
          "level": 2,
          "isStrong": true,
          "everPublished": true
        }
      ],
      "lenses": [],
      "lensParentId": "58b",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "23016",
          "pageId": "5gc",
          "userId": "2",
          "edit": 18,
          "type": "newEdit",
          "createdAt": "2018-06-01 18:51:52",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "23015",
          "pageId": "5gc",
          "userId": "2",
          "edit": 17,
          "type": "newEdit",
          "createdAt": "2018-06-01 18:47:24",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "22244",
          "pageId": "5gc",
          "userId": "2",
          "edit": 16,
          "type": "newEdit",
          "createdAt": "2017-03-03 18:52:41",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "22243",
          "pageId": "5gc",
          "userId": "2",
          "edit": 0,
          "type": "newEditGroup",
          "createdAt": "2017-03-03 18:52:39",
          "auxPageId": "15",
          "oldSettingsValue": "123",
          "newSettingsValue": "15"
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "20164",
          "pageId": "5gc",
          "userId": "2",
          "edit": 15,
          "type": "newEdit",
          "createdAt": "2016-10-16 05:07:13",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "20162",
          "pageId": "5gc",
          "userId": "2",
          "edit": 14,
          "type": "newEdit",
          "createdAt": "2016-10-16 04:34:50",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "20161",
          "pageId": "5gc",
          "userId": "2",
          "edit": 13,
          "type": "newEdit",
          "createdAt": "2016-10-16 04:34:07",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "20160",
          "pageId": "5gc",
          "userId": "2",
          "edit": 12,
          "type": "newEdit",
          "createdAt": "2016-10-16 04:26:17",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "20159",
          "pageId": "5gc",
          "userId": "2",
          "edit": 11,
          "type": "newEdit",
          "createdAt": "2016-10-16 04:25:45",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "20154",
          "pageId": "5gc",
          "userId": "2",
          "edit": 10,
          "type": "newEdit",
          "createdAt": "2016-10-16 03:59:36",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "18301",
          "pageId": "5gc",
          "userId": "2",
          "edit": 0,
          "type": "newTag",
          "createdAt": "2016-08-03 22:39:21",
          "auxPageId": "3rk",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "18269",
          "pageId": "5gc",
          "userId": "2",
          "edit": 9,
          "type": "newEdit",
          "createdAt": "2016-08-03 20:03:17",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "17473",
          "pageId": "5gc",
          "userId": "2",
          "edit": 8,
          "type": "newEdit",
          "createdAt": "2016-07-25 00:50:45",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "17472",
          "pageId": "5gc",
          "userId": "2",
          "edit": 7,
          "type": "newEdit",
          "createdAt": "2016-07-25 00:46:42",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "17470",
          "pageId": "5gc",
          "userId": "2",
          "edit": 0,
          "type": "newAlias",
          "createdAt": "2016-07-25 00:45:33",
          "auxPageId": "",
          "oldSettingsValue": "5gc",
          "newSettingsValue": "ldt_intro_phil"
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "17471",
          "pageId": "5gc",
          "userId": "2",
          "edit": 6,
          "type": "newEdit",
          "createdAt": "2016-07-25 00:45:33",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "17468",
          "pageId": "5gc",
          "userId": "2",
          "edit": 0,
          "type": "newTag",
          "createdAt": "2016-07-25 00:42:39",
          "auxPageId": "4v4",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "17467",
          "pageId": "5gc",
          "userId": "2",
          "edit": 0,
          "type": "deleteTag",
          "createdAt": "2016-07-25 00:42:26",
          "auxPageId": "4v",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "17465",
          "pageId": "5gc",
          "userId": "2",
          "edit": 5,
          "type": "newEdit",
          "createdAt": "2016-07-25 00:42:15",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "17422",
          "pageId": "5gc",
          "userId": "2",
          "edit": 4,
          "type": "newEdit",
          "createdAt": "2016-07-23 18:48:45",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "17374",
          "pageId": "5gc",
          "userId": "2",
          "edit": 3,
          "type": "newEdit",
          "createdAt": "2016-07-23 01:56:29",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "17258",
          "pageId": "5gc",
          "userId": "2",
          "edit": 2,
          "type": "newEdit",
          "createdAt": "2016-07-21 22:01:35",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "17207",
          "pageId": "5gc",
          "userId": "2",
          "edit": 0,
          "type": "newTag",
          "createdAt": "2016-07-21 01:36:24",
          "auxPageId": "4v",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "17209",
          "pageId": "5gc",
          "userId": "2",
          "edit": 0,
          "type": "newSubject",
          "createdAt": "2016-07-21 01:36:24",
          "auxPageId": "58b",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "17206",
          "pageId": "5gc",
          "userId": "2",
          "edit": 0,
          "type": "newParent",
          "createdAt": "2016-07-21 01:36:23",
          "auxPageId": "58b",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "17204",
          "pageId": "5gc",
          "userId": "2",
          "edit": 1,
          "type": "newEdit",
          "createdAt": "2016-07-21 01:36:22",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        }
      ],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": true,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "5hp": {
      "likeableId": "3220",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "5hp",
      "edit": 5,
      "editSummary": "",
      "prevEdit": 4,
      "currentEdit": 5,
      "wasPublished": true,
      "type": "wiki",
      "title": "99LDT x 1CDT oneshot PD tournament as arguable counterexample to LDT doing better than CDT",
      "clickbait": "Arguendo, if 99 LDT agents and 1 CDT agent are facing off in a one-shot Prisoner's Dilemma tournament, the CDT agent does better on a problem that CDT considers 'fair'.",
      "textLength": 5283,
      "alias": "pd_tournament_99ldt_1cdt",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "2",
      "editCreatedAt": "2016-07-23 22:29:45",
      "pageCreatorId": "2",
      "pageCreatedAt": "2016-07-21 01:10:27",
      "seeDomainId": "0",
      "editDomainId": "123",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 1,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 160,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0
      ],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": -2,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "5n9": {
      "likeableId": "0",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "5n9",
      "edit": 4,
      "editSummary": "",
      "prevEdit": 3,
      "currentEdit": 4,
      "wasPublished": true,
      "type": "wiki",
      "title": "Causal decision theories",
      "clickbait": "On CDT, to choose rationally, you should imagine the world where your physical act changes, then imagine running that world forward in time.  (Therefore, it's irrational to vote in elections.)",
      "textLength": 17408,
      "alias": "causal_dt",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "2",
      "editCreatedAt": "2016-08-02 00:36:46",
      "pageCreatorId": "2",
      "pageCreatedAt": "2016-07-29 22:15:27",
      "seeDomainId": "0",
      "editDomainId": "123",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 2,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 186,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0
      ],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": -2,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "5pt": {
      "likeableId": "3322",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "5pt",
      "edit": 2,
      "editSummary": "",
      "prevEdit": 1,
      "currentEdit": 2,
      "wasPublished": true,
      "type": "wiki",
      "title": "Newcomblike decision problems",
      "clickbait": "Decision problems in which your choice correlates with something other than its physical consequences (say, because somebody has predicted you very well) can do weird things to some decision theories.",
      "textLength": 745,
      "alias": "newcomblike",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "2",
      "editCreatedAt": "2016-08-03 20:04:59",
      "pageCreatorId": "2",
      "pageCreatedAt": "2016-07-31 22:18:17",
      "seeDomainId": "0",
      "editDomainId": "123",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 0,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 120,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0
      ],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": -2,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "5pv": {
      "likeableId": "3610",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "5pv",
      "edit": 6,
      "editSummary": "formatting fixes",
      "prevEdit": 4,
      "currentEdit": 6,
      "wasPublished": true,
      "type": "wiki",
      "title": "Newcomb's Problem",
      "clickbait": "There are two boxes in front of you, Box A and Box B.  You can take both boxes, or only Box B.  Box A contains $1000.  Box B contains $1,000,000 if and only if Omega predicted you'd take only Box B.",
      "textLength": 7843,
      "alias": "newcombs_problem",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "1yq",
      "editCreatedAt": "2016-10-13 16:53:43",
      "pageCreatorId": "2",
      "pageCreatedAt": "2016-08-01 00:09:09",
      "seeDomainId": "0",
      "editDomainId": "123",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 0,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 114,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0
      ],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": -2,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "5px": {
      "likeableId": "0",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "5px",
      "edit": 1,
      "editSummary": "",
      "prevEdit": 0,
      "currentEdit": 1,
      "wasPublished": true,
      "type": "wiki",
      "title": "Evidential decision theories",
      "clickbait": "Theories which hold that the principle of rational choice is \"Choose the act that would be the best news, if somebody told you that you'd chosen that act.\"",
      "textLength": 1011,
      "alias": "evidential_dt",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "2",
      "editCreatedAt": "2016-08-01 00:22:14",
      "pageCreatorId": "2",
      "pageCreatedAt": "2016-08-01 00:22:14",
      "seeDomainId": "0",
      "editDomainId": "123",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 0,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 106,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0
      ],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": -2,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "5py": {
      "likeableId": "0",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "5py",
      "edit": 1,
      "editSummary": "",
      "prevEdit": 0,
      "currentEdit": 1,
      "wasPublished": true,
      "type": "wiki",
      "title": "Prisoner's Dilemma",
      "clickbait": "You and an accomplice have been arrested.  Both of you must decide, in isolation, whether to testify against the other prisoner--which subtracts one year from your sentence, and adds two to theirs.",
      "textLength": 7237,
      "alias": "prisoners_dilemma",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "2",
      "editCreatedAt": "2016-08-01 02:05:58",
      "pageCreatorId": "2",
      "pageCreatedAt": "2016-08-01 02:05:58",
      "seeDomainId": "0",
      "editDomainId": "123",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 0,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 65,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0
      ],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": -2,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "5qh": {
      "likeableId": "3491",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "5qh",
      "edit": 9,
      "editSummary": "",
      "prevEdit": 8,
      "currentEdit": 9,
      "wasPublished": true,
      "type": "wiki",
      "title": "Absent-Minded Driver dilemma",
      "clickbait": "A road contains two identical intersections.  An absent-minded driver wants to turn right at the second intersection.  \"With what probability should the driver turn right?\" argue decision theorists.",
      "textLength": 8990,
      "alias": "absentminded_driver",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "2",
      "editCreatedAt": "2016-08-02 00:26:32",
      "pageCreatorId": "2",
      "pageCreatedAt": "2016-08-01 19:48:01",
      "seeDomainId": "0",
      "editDomainId": "123",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 0,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 145,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0
      ],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": -2,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "5qn": {
      "likeableId": "3343",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "5qn",
      "edit": 9,
      "editSummary": "",
      "prevEdit": 8,
      "currentEdit": 9,
      "wasPublished": true,
      "type": "wiki",
      "title": "Death in Damascus",
      "clickbait": "Death tells you that It is coming for you tomorrow.  You can stay in Damascus or flee to Aleppo.  Whichever decision you actually make is the wrong one.  This gives some decision theories trouble.",
      "textLength": 12821,
      "alias": "death_in_damascus",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "5yw",
      "editCreatedAt": "2017-03-21 12:39:52",
      "pageCreatorId": "2",
      "pageCreatedAt": "2016-08-02 04:05:38",
      "seeDomainId": "0",
      "editDomainId": "123",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 0,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 502,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0
      ],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": -2,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "5rf": {
      "likeableId": "0",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "5rf",
      "edit": 3,
      "editSummary": "",
      "prevEdit": 2,
      "currentEdit": 3,
      "wasPublished": true,
      "type": "wiki",
      "title": "Toxoplasmosis dilemma",
      "clickbait": "A parasitic infection, carried by cats, may make humans enjoy petting cats more.  A kitten, now in front of you, isn't infected.  But if you *want* to pet it, you may already be infected.  Do you?",
      "textLength": 10760,
      "alias": "toxoplasmosis_dilemma",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "2",
      "editCreatedAt": "2016-08-05 01:43:23",
      "pageCreatorId": "2",
      "pageCreatedAt": "2016-08-04 00:10:56",
      "seeDomainId": "0",
      "editDomainId": "123",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 2,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 92,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0
      ],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": -2,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "5rz": {
      "likeableId": "3692",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "5rz",
      "edit": 1,
      "editSummary": "",
      "prevEdit": 0,
      "currentEdit": 1,
      "wasPublished": true,
      "type": "wiki",
      "title": "Updateless decision theories",
      "clickbait": "Decision theories that maximize their policies (mappings from sense inputs to actions), rather than using their sense inputs to update their beliefs and then selecting actions.",
      "textLength": 900,
      "alias": "updateless_dt",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "2",
      "editCreatedAt": "2016-08-04 23:29:04",
      "pageCreatorId": "2",
      "pageCreatedAt": "2016-08-04 23:29:04",
      "seeDomainId": "0",
      "editDomainId": "123",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 0,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 377,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0
      ],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": -2,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "5s0": {
      "likeableId": "3334",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "5s0",
      "edit": 3,
      "editSummary": "",
      "prevEdit": 2,
      "currentEdit": 3,
      "wasPublished": true,
      "type": "wiki",
      "title": "Parfit's Hitchhiker",
      "clickbait": "You are dying in the desert.  A truck-driver who is very good at reading faces finds you, and offers to drive you into the city if you promise to pay $1,000 on arrival.  You are a selfish rationalist.",
      "textLength": 3861,
      "alias": "parfits_hitchhiker",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "2",
      "editCreatedAt": "2016-08-05 00:11:59",
      "pageCreatorId": "2",
      "pageCreatedAt": "2016-08-05 00:07:43",
      "seeDomainId": "0",
      "editDomainId": "123",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 0,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 548,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0
      ],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": -2,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "5tp": {
      "likeableId": "0",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "5tp",
      "edit": 6,
      "editSummary": "",
      "prevEdit": 5,
      "currentEdit": 6,
      "wasPublished": true,
      "type": "wiki",
      "title": "Ultimatum Game",
      "clickbait": "A Proposer decides how to split $10 between themselves and the Responder.  The Responder can take what is offered, or refuse, in which case both parties get nothing.",
      "textLength": 16790,
      "alias": "ultimatum_game",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "2",
      "editCreatedAt": "2016-08-10 06:53:20",
      "pageCreatorId": "2",
      "pageCreatedAt": "2016-08-10 03:28:27",
      "seeDomainId": "0",
      "editDomainId": "123",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 2,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 129,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0
      ],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": -2,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    },
    "7d6": {
      "likeableId": "0",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "7d6",
      "edit": 1,
      "editSummary": "",
      "prevEdit": 0,
      "currentEdit": 1,
      "wasPublished": true,
      "type": "wiki",
      "title": "Fair problem class",
      "clickbait": "A problem is 'fair' (according to logical decision theory) when only the results matter and not how we get there.",
      "textLength": 825,
      "alias": "fair_problem_class",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "2",
      "editCreatedAt": "2017-01-12 06:02:28",
      "pageCreatorId": "2",
      "pageCreatedAt": "2017-01-12 06:02:28",
      "seeDomainId": "0",
      "editDomainId": "15",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": true,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 0,
      "isEditorComment": false,
      "isApprovedComment": false,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 209,
      "text": "",
      "metaText": "",
      "isTextLoaded": false,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 0,
      "maintainerCount": 0,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0
      ],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": -2,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 0,
      "redLinkCount": 0,
      "lockedBy": "",
      "lockedUntil": "",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": null,
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [],
      "lenses": [],
      "lensParentId": "",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": false,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    }
  },
  "edits": {
    "5gc": {
      "likeableId": "3172",
      "likeableType": "page",
      "myLikeValue": 0,
      "likeCount": 0,
      "dislikeCount": 0,
      "likeScore": 0,
      "individualLikes": [],
      "pageId": "5gc",
      "edit": 2,
      "editSummary": "",
      "prevEdit": 1,
      "currentEdit": 18,
      "wasPublished": true,
      "type": "wiki",
      "title": "Introduction to Logical Decision Theory for Analytic Philosophers",
      "clickbait": "Why 'choose as if controlling the logical output of your decision algorithm' is the most appealing candidate for the principle of rational choice.",
      "textLength": 50515,
      "alias": "ldt_intro_phil",
      "externalUrl": "",
      "sortChildrenBy": "likes",
      "hasVote": false,
      "voteType": "",
      "votesAnonymous": false,
      "editCreatorId": "2",
      "editCreatedAt": "2016-07-21 22:01:35",
      "pageCreatorId": "2",
      "pageCreatedAt": "2016-07-21 01:36:22",
      "seeDomainId": "0",
      "editDomainId": "15",
      "submitToDomainId": "0",
      "isAutosave": false,
      "isSnapshot": false,
      "isLiveEdit": false,
      "isMinorEdit": false,
      "indirectTeacher": false,
      "todoCount": 16,
      "isEditorComment": false,
      "isApprovedComment": true,
      "isResolved": false,
      "snapshotText": "",
      "anchorContext": "",
      "anchorText": "",
      "anchorOffset": 0,
      "mergedInto": "",
      "isDeleted": false,
      "viewCount": 560,
      "text": "(This introduction is aimed at people entering from a general analytic-philosophy viewpoint.  If you're already familiar with causal decision theory, evidential decision theory, and Newcomblike problems, please see the [ldt_intro_dt LDT intro for decision theorists].)\n\nIs it rational to vote in elections?\n\nSuppose that roughly a hundred thousand people are voting in an election.  Then surely the chance of the election coming down to any one vote is tiny.  Say there are 50,220 votes for Kang and 50,833 votes for Kodos.  This is a close election as such things go.  But if you do vote for Kang, that just means 50,221 votes for Kang vs. 50,833 votes for Kodos.  If we're to select actions on the basis of their probable consequences, it seems that with overwhelming probability, the consequences of 'voting' and 'not voting' are nearly identical.\n\nAn economist who argues that voting is 'irrational' on this basis is deploying a formal answer from the most commonly accepted version of decision theory in contemporary philosophy, namely an answer from [-causal_decision_theory] (CDT).  To answer \"What would have happened if I'd voted?\", CDT says to imagine a world in which everything that happened before your vote stays constant; and the physical, causal consequences downstream of your vote are recomputed.  (The rules for imagining this are further formalized within the theory of [ causal models].)  When the dust settles, CDT's formally computed consequences for \"voting\" vs. \"not voting\" are 50,221 vs. 50,220 votes for Kang.  The only exception is if your physical act of voting *caused* someone else to vote (in which case the consequence is 50,222 votes for Kang).\n\nAn odd consequence of this view is that if the election *is* settled by one vote, say 8,001 for Kang vs. 8,000 for Kodos, then all 8,001 Kang voters should each view themselves as having individually swung the whole election - since if counterfactually they had voted for Kodos, the election would have gone to Kodos.  Conversely, in an election decided by 8,002 to 8,000 votes, nobody's vote changed anything.\n\nDilemmas such as these are part of a larger class of scenarios known as [newcomblike Newcomblike decision problems] where the world contains other agents that are similar to you or predicting you with significant accuracy.  This problem class also includes the [prisoners_dilemma Prisoner's Dilemma]; whether to turn down a lowball offer when [ultimatum_game bargaining]; and thought experiments involving [5b2 powerful aliens] who are excellent predictors of human behavior.\n\n*Logical decision theories* are a recently invented family of decision theories claiming to pose a more attractive alternative to causal decision theory.  Logical decision theory asserts that the principle of rational choice is \"Decide as though you are choosing the *logical output* of your *decision algorithm*.\"  Allegedly, there are major and significant reasons why (a) this is a decision theory that economists and computer scientists should prefer to use; and (b) this rule and its formalizations are more appealing candidates for the principle of rational choice.\n\nThis introduction will:\n\n- Overview Newcomblike problems and the contemporary view of them in analytic philosophy;\n- Introduce logical decision theories and how they differ formally from other well-known decision theories;\n- Reconsider well-known Newcomblike problems in the light of logical decision theory;\n- Make the case for logical decision theory as the principle of rational choice;\n- Overview some of the more interesting results in logical decision theory;\n- And point to further reading.\n\n# Newcomblike problems\n\nRoughly, Newcomblike problems can be seen as those where somebody similar to you, or trying to predict you, exists in the environment.  In this case your decision can *correlate* with events outside you, without your action *physically causing* those events.\n\n## Newcomb's Problem\n\nThe original [newcombs_problem Newcomb's Problem] was as follows:\n\nAn alien named [5b2 Omega] presents you with two boxes, a transparent box A containing \\$1,000, and an opaque Box B.  Omega then flies away, leaving you with the choice of whether to take only Box B ('one-box') or to take both Box A plus Box B ('two-box').\n\nOmega has put $1,000,000 in Box B if and only if Omega predicted that you would take only one box.  Otherwise Box B is empty. %note: The original formulation of Newcomb's Problem also specified that if Omega predicts you will decide to try to flip a coin, Omega leaves Box B empty.%\n\nOmega has already departed, so Box B is already empty or already full.\n\nOmega is an excellent predictor of human behavior and has never been observed to be mistaken. %note: We can suppose Omega has run this experiment 73 times previously and predicted correctly each time.  Since people do seem to form strongly held views about what they'd do in Newcomb's Problem, it is plausible that Omega could get this level of predictive accuracy by e.g. looking at your brain a few hours previously.%\n\nDo you take both boxes, or only Box B?\n\n- Argument 1:  People who take only Box B tend to walk away rich.  People who two-box tend to walk away poor.  It is better to be rich than poor.\n- Argument 2:  Omega has already made its prediction.  Box B is already empty or already full.  It would be [dt_rational irrational] to leave behind Box A for no reason.  It's true that Omega has chosen to reward people with irrational [dt_disposition dispositions] in this setup, but Box B is now *already empty*, and irrationally leaving Box A behind would just [causal_counterfactual counterfactually] result in your getting \\$0 instead of \\$1,000.\n\nThis setup went on to generate an incredible amount of debate.  Conventionally, Newcomb's Problem is seen as exhibiting a split between [evidential_dt evidential decision theory] and [causal_dt causal decision theory].\n\n## Evidential and causal decision theory\n\nAlmost everyone in present and historical debate on decision theory has agreed that [dt_rational rational] agents choose by calculating the [18v expected utility] *conditional on* each possible decision.  The central question of decision theory turns out to be, \"How exactly do we condition our probabilities on our possible decisions?\"\n\nMost times the expected utility formula is mentioned outside of decision theory, it is shown as follows:\n\n$$\\mathbb E[\\mathcal U|a_x] = \\sum_{o_i \\in \\mathcal O} \\mathcal U(o_i) \\cdot \\mathbb P(o_i|a_x)$$\n\nwhere\n\n- $\\mathbb E[\\mathcal U|a_x]$ is our average expectation of utility, if action $a_x$ is chosen;\n- $\\mathcal O$ is the set of possible outcomes;\n- $\\mathcal U$ is our utility function, mapping outcomes onto real numbers;\n- $\\mathbb P(o_i|a_x)$ is the [1rj conditional probability] of outcome $o_i$ if $a_x$ is chosen.\n\nThis formula is widely agreed to be wrong.\n\nThe problem is the use of standard evidential conditioning in $\\mathbb P(o_i|a_x).$  On this formula we are behaving as if we're asking, \"What would be my [1y6 revised] probability for $\\mathbb P(o_i),$ if I was *told the news* or *observed the evidence* that my action had been $a_x$?\"\n\nCausal decision theory says we should instead use the *counterfactual conditional* $\\ \\mathbb P(a_x \\ \\square \\! \\! \\rightarrow o_i).$\n\nThe difference between evidential and counterfactual conditioning is standardly contrasted by these two sentences:\n\n- If Lee Harvey Oswald didn't shoot John F. Kennedy, somebody else did.\n- If Lee Harvey Oswald hadn't shot John F. Kennedy, somebody else would have.\n\nIn the first sentence, we're being told as news that Oswald didn't shoot Kennedy, and [1ly updating our beliefs] to integrate this with the rest of our observations.\n\nIn the second world, we're imagining how a counterfactual world would have played out if Oswald had acted differently.  That is, to visualize the causal counterfactual:\n\n- We imagine everything in the world being the same up until the point where Oswald decides to shoot Kennedy.\n- We surgically intervene on our imagined world to change Oswald's decision to not-shooting, without changing any other facts about the past.\n- We rerun our model of the world's mechanisms forward from the point of change, to determine what *would have* happened.\n\nIf $K$ denotes the proposition that somebody else shot Kennedy and $O$ denotes the proposition that Oswald shot him, then the first sentence and second sentence are respectively talking about:\n\n- $\\mathbb P(K| \\neg O)$\n- $\\mathbb P(\\neg O \\ \\square \\!\\! \\rightarrow K)$\n\n(Further formalizations of how to [causal_counterfactuals compute causal counterfactuals] are given by Judea Pearl et. al.'s theory of [ causal models].)\n\nSo according to causal decision theory, the expected utility formula should read:\n\n$$\\mathbb E[\\mathcal U|a_x] = \\sum_{o_i \\in \\mathcal O} \\mathcal U(o_i) \\cdot \\mathbb P(a_x \\ \\square \\!\\! \\rightarrow o_i)$$\n\n## Newcomblike problems allegedly prying apart evidential and causal decisions\n\nThe current majority view of Newcomblike problems is that their distinctive feature is prying apart the verdicts of evidential and causal decision theory.\n\nFor example, on the conventional analysis of the original [newcombs_problem Newcomb's Problem], taking only Box B is *good news* about whether Box B contains \\$1,000,000, but cannot *cause* Box B to contain \\$1,000,000.\n\nFor a converse example in which the verdict of evidential decision theory seems much less reasonable, consider the following dilemma:\n\nSuppose that toxoplasmosis, a parasitic infection carried by cats, can cause toxoplasmosis-infected humans to become fonder of cats. %note:  \"Toxoplasmosis makes humans like cats\" was formerly thought to actually be true.  More recently, this result may have failed to replicate, which is unfortunate because we liked the really crisp example.%  You are now faced with a cute cat that has been checked by a veterinarian who says this cat definitely does *not* have toxoplasmosis.\n\nIf you decide to pet the cat, an impartial observer watching you will conclude that you are 10% more likely to have toxoplasmosis, which can be a fairly detrimental infection.  If you don't pet the cat, you'll miss out on the hedonic enjoyment of petting it.  Do you pet the cat?\n\nIn this case, evidential decision theory says not to pet the cat, since a 10% increase in the probability that you have toxoplasmosis is a significantly larger downside than the enjoyment of petting this one cat is an upside.\n\nCausal decision theory says that petting the cat can't *cause* you to contract toxoplasmosis.  After observing your own action, you may realize that you had toxoplasmosis all along, which isn't good; but in the counterfactual case where you didn't pet the cat, you would *still* have toxoplasmosis.\n\n(In the standard philosophical literature this dilemma is usually presented under the heading of [ Solomon's Problem], in which King Solomon must decide whether to be *the kind of person* who commits adultery which makes his rule more likely to be overthrown.  It may also be presented as the [ Smoking Lesion] dilemma, in which the impulse to smoke stems from a gene which also causes lung cancer, but lung cancer does not actually cause smoking.  Our own discussion uses the [toxoplasmosis_dilemma] because this seems less liable to cause confusion, especially in non-analytic-philosophers.)\n\nFor this reason, evidential decision theory is widely regarded as leading to 'an irrational policy of managing the news'.\n\nOn the majority view within contemporary decision theory, this is the reply to the \"If you're so rational, why aincha rich?\" argument in favor of one-boxing on Newcomb's Problem.  Somebody who actually takes only Box B is merely 'managing the news' about Box B, not actually acting to maximize the causal impacts of their actions.  Omega choosing to reward people who only take Box B is akin to happening to already have toxoplasmosis at the start of the decision problem, or Omega deciding to reward only evidential decision theorists.  Evidential agents only seem to win in 'Why aincha rich?' scenarios because they're managing the news in a way that an artificial problem setup declares to be news about wealth.\n\nSince many philosophers continue to find two-boxing on Newcomb's Problem to be an exceptionally unappealing decision, multiple attempts have been made to 'hybridize' evidential and causal decision theory,[todo: put citations here, or a greenlink to a page on hybridization attempts] to construct a theory which behaves 'evidentially' in some cases (like Newcomb's Problem) and 'causally' in other cases (like the [ Toxoplasmosis Dilemma] / Solomon's Problem).  Robert Nozick suggested at one point that evidentially computed and causally computed expected utilities be averaged together, so that an evidential gain of \\$1,000,000 could outweigh a causal loss of \\$1,000 on Newcomb's Problem.\n\n## When wealth doesn't follow evidential reasoning\n\nLogical decision theorists deny that decision theory ought to be analyzed as a conflict between evidential and causal utilities; and observe that it is easy to possible to pry apart *both* theories from the behavior that corresponds to being the richest agent at the end of the problem.  Consider e.g. Parfit's Hitchhiker:\n\n### Parfit's Hitchhiker\n\n> You are lost in the desert, your water bottle almost exhausted, when somebody drives up in a lorry.  The driver of this lorry is (a) entirely selfish, and (b) very good at detecting lies. %note: Maybe the driver went through Paul Ekman's training for reading facial microexpressions.%\n> \n> The driver says that they will drive you into town, but only if you promise to give them \\$1,000 on arrival. %note: We assume that relative to your situation and the local laws, there's no way that this contract would be enforceable apart from your goodwill.%\n\nIf you value your life at \\$1,000,000 and are otherwise motivated only by self-interest (e.g. you attach no utility to keeping promises as such), then this problem seems isomorphic to Gary Drescher's [ transparent Newcomb's Problem]: in which Box B is transparent, and Omega has already put \\$1,000,000 into Box B iff Omega predicts that you will one-box when faced with a visibly full Box B.\n\nBoth evidential decision theory and causal decision theory say to two-box in this case.  In particular, the evidential agent that has already updated on observing a full Box B will not update to an empty Box B after observing themselves two-box; they will instead conclude that Omega sometimes makes mistakes.  Similarly, an evidential agent who has already reached the safety of the city will conclude that the driver has made an error of reading faces, if they observe themselves refuse to pay the \\$1,000.\n\nThus the behavioral disposition that corresponds to ending up rich (the disposition to pay when you reach town, or one-box after seeing a full Box B) has been pried apart from both causal and evidential decision theories.\n\nWe might also observe that the driver in Parfit's Hitchhiker is not behaving as an arbitrary alien philosopher-troll like Omega.  The driver's reasoning seems entirely understandable in terms of self-interest.\n\n### The Termites Dilemma\n\nSuppose I have a strong reputation for being truthful, and also a strong reputation for being able to predict other people's behavior (especially the behavior of people who have publicly shown themselves to be evidential decision theorists).  You are the owner of a large apartment complex.  I send you the following letter:\n\n> Dear You:  I might or might not have seen signs of termites in your apartment complex.  If there is in fact termite damage, it will probably cost you around \\$1,000,000 to fix.  By the way, I'd also like to ask you to wire \\$1,000 to my bank account.  I now assure you that the following proposition is true:\n> *((Your apartment has termites) XOR (I predicted you would send me \\$1,000.))*\n\nAn evidential decision agent then reasons as follows:  \"If I choose not to send \\$1,000, then with high probability my apartment building has termites, which will cost me \\$1,000,000.  But if I do send \\$1,000, then my apartment building does not have termites.\"  The evidential decision agent therefore wires the \\$1,000.\n\nAgain, *ending up rich* has been pried apart from the advice of evidential decision theory; evidential agents will wander the world being exploited, while causal decision agents won't be targeted to begin with.\n\n# Logical decision theory\n\nA logical decision theorist defends as the principle of rational choice:  \"Choose as if controlling the *logical output* of your *decision algorithm.*\"\n\nWe'll see shortly that this corresponds to:\n\n- One-boxing on Newcomb's Problem\n- Paying the driver in Parfit's Hitchhiker\n- Petting the cat in the Toxoplasmosis Dilemma\n- Refusing to pay in the Termites Dilemma\n\n(The fact that these agents seem to 'end up rich' in a certain sense is not the only possible defense that can be given of logical decision theory; there are also more basic defenses of the allegedly superior internal coherence of this rule, counterarguments against arguments that it is inherently irrational to try to choose according to logical consequences, etcetera.)\n\nThis principle, as phrased, is informal.  \"Logical decision theories\" are really a family of recently proposed decision theories, none of which stands out as being clearly ahead of the others in all regards, but which are allegedly all better than causal decision theory.\n\n[ Functional decision theory] takes the form of an expected utility formula, written as follows:\n\n$$\\mathsf Q(s) = \\big ( \\underset{\\pi_x \\in \\Pi}{argmax} \\ \\sum_{o_i \\in \\mathcal O} \\mathcal U(o_i) \\cdot \\mathbb P(\\ulcorner \\mathsf Q = \\pi_x \\urcorner \\triangleright o_i) \\big ) (s)$$\n\nWhere:\n\n- $\\mathsf Q$ is the agent's current decision algorithm - that is, the whole calculation presently running.\n- $s$ is the agent's sense data.\n- $\\pi_x \\in \\Pi$ is the output of $\\mathsf Q$, a *policy* that maps sense data to actions.\n- $\\ulcorner \\mathsf Q = \\pi_x \\urcorner$ is the proposition that, as a logical fact, the output of algorithm $\\mathsf Q$ is $\\pi_x.$\n- $\\mathbb P(X \\triangleright o_i)$ is the probability of $o_i$ conditioned on the logical fact $X.$\n\nThis is not fully formalized because work is still in progress on laying down an exact algorithm for the logical-conditioning or [logical_counterfactual logical counterfactual] operator $X \\triangleright Y$.  But it's worth noting that many discussions of causal decision theory have treated causal counterfactuals as *self-evident* or as a heaven-sent conditional distribution $\\mathbb P(\\bullet \\ || \\ \\bullet).$  Functional decision theory is at least no *more* informal than causal decision theory thus treated.  If we feel that it is intuitively obvious how the universe 'would have looked' if our logical algorithm had yielded an output of two-boxing, then functional decision theory yields a clear output relative to this intuition.\n\nTwo special-case ways of calculating $X \\triangleright Y$ yield non-general but useful logical decision theories:\n\n- [timeless_dt] takes in a standard causal model that includes some nodes intended to represent logical propositions, and then computes standard counterfactuals inside this causal model.\n- [proof_based_dt] treats $X$ as premise introduced into a standard proof algorithm, and works out further logical implications.\n\n[timeless_dt] suffices to formalize all the dilemmas, thought experiments, and economic scenarios as well as they were ever formalized in causal decision theory.\n\n[proof_based_dt] allows us to deploy running code that simulates [modal_agents] deciding what to do in multiplayer dilemmas, e.g. the [prisoners_dilemma].\n\nBut we know these two formalizations aren't complete because:\n\n- The proof formalism we use for modal agents has [ weird edge cases] indicating that it only correctly formalizes the intuitive notion of logical conditioning some of the time.\n- We don't have a general algorithm for *building* causal models that include logical facts, and it's not clear that the [timeless_dt TDT] representation can model any setup more complicated than \"run this exact algorithm in two different places\".\n\nAn important feature quietly introduced in the above formula for $\\mathsf Q,$ is that $\\mathsf Q$ chooses *policies* (mappings from sensory observations $s$ to actions) rather than outputting actions directly.  This makes functional decision theory [updateless_dt updateless], a feature with deep ramifications and justifications that pretty much require digging into the longer articles.  A *very* rough example is that this feature is useful for, e.g., the [absentminded_driver Absent-Minded Driver problem] where the *correlation* between our current observations, and the background variables that determine good policy, is itself dependent on our policy.\n\n## LDT behavior on some Newcomblike problems\n\n### Newcomb's Problem\n\nWe can set this up in TDT using this augmented logical graph:\n\n[todo: make actual graph]\n\n- (Your mind at 7am) -> ( | $\\mathsf Q$ --> Omega's prediction of your decision) -> ($\\mathsf Q$ --> Your decision at 8am  | Box B) -> (Your payoff)\n\nOn CDT, we compute the counterfactual \"What if I one-box / two-box?\" by supervening on the node \"Your decision at 8am\"; on TDT we supervene on the output of $\\mathsf Q.$  Aside from that, both systems obey standard rules about changing only that one point and running all the other rules of the model forward.\n\nThus, TDT one-boxes.\n\n### Parfit's Hitchhiker\n\nWe can set up in TDT as follows:\n\n[todo: make actual graph]\n\n- (Your mind at 7am) -> ( | $\\mathsf Q(city)$ --> Driver's prediction of what you will do if you see yourself in the city) -> ( | Driver's choice to take you to city) -> ($\\mathsf Q(city)$ --> Your decision whether to pay when you're in the city) | ($\\mathsf Q(desert)$ --> Your decision about how to die in the desert) -> (Your payoff)\n\nGenerically, it's easiest to set up this general class of problem in [updateless_dt updateless] form so we can just choose policies (mappings from observations to actions) - in other words, at every point where $\\mathsf Q$ appears, we act as if optimizing over the whole $\\mathsf Q,$ and only then take into account our observations.\n\nHowever, in this particular case, just optimizing over $\\mathsf Q(city)$ is enough to give us the answer.  Even once we're already in the city, when we compute the *counterfactual* starting by setting $\\mathsf Q(city)$ while changing no prior causes and then running the rules forward, we will get the result, \"If I hadn't chosen to pay \\$1,000, then I would have died in the desert.\"\n\n### Toxoplasmosis Dilemma\n\nThere are complications in setting this up formally, since we need a background mechanic that succeeds in creating a correlation between \"pet the cat\" and \"has toxoplasmosis\"--if everyone is an LDT agent and everyone decides to pet the cat, then there won't be any correlation in the first place.\n\nIn [functional_dt functional decision theory], we assume that $\\mathsf Q$ knows its own formula $\\ulcorner \\mathsf Q \\urcorner$ (e.g. via [godelian_diagonalization]).  So if we say that different agents have slightly different utility functions correlated with toxoplasmosis, then in functional decision theory, each agent ought to already know its own algorithm and to have [tickle_defense already updated about toxoplasmosis].  (FDT is not always a good descriptive theory of human behavior!) %note: It's an open problem to formulate a more realistic LDT that may not have full knowledge about its own quoted algorithm.%\n\nTo hack our way to a roughly similar setup, we can suppose that there's some mix of EDT agents and LDT agents encountering the problem; and that [5b2 Omega] has told us, \"Through no fault or virtue of their own, it just so happens that in this particular random sample, agent types that don't pet the cat after being given this information already have toxoplasmosis with 10% frequency, and agent types who do pet the cat already have toxoplasmosis with 20% frequency.\"\n\nThen our graph might look something like this:\n\n- ($\\mathsf Q(warning)$ | toxoplasmosis frequency) -> (Omega warning?), (pet cat?) -> (payoff)\n\nIn this setup, our decision to pet the cat and toxoplasmosis both affect Omega's warning, our decision to pet the cat affects whether we get cat hedons, and cat hedons and toxoplasmosis both affect our payoff.\n\nWe compute, \"If-counterfactually people like me *didn't* pet the cat, then (a) I wouldn't have received cat-petting hedons, (b) I'd still have toxoplasmosis with the same probability, and (c) Omega would've given us a different statistical summary.\"\n\nOn the more regular toxoplasmosis problem, this might analogously work out to thinking, \"If-counterfactually people like me didn't pet cats in this situation, then there wouldn't be any correlation between toxoplasmosis and petting in the first place; but actual toxoplasmosis wouldn't be reduced in any way.\"\n\n### Termites Dilemma\n\n- ($\\mathsf Q(message)$) -> (agent's decision to try blackmail, Termites -> agent's message, whether we pay) -> ( | | Termites -> payoff)\n\nSince an LDT agent doesn't pay in the Termites dilemma, nobody sends us a message in the first place.\n\nBut if we did get the message, we'd compute $\\mathsf Q$ by noticing that the policy (message -> pay) results in our getting a message and our paying if there are no termites, while if there are termites, the agent wouldn't send us a message.  Regardless of the prior probability of termites, this does more poorly than the policy of not paying.\n\n# LDT as the principle of rational choice\n\nIt should now be clear that the family of logical decision theories gives *different* answers in Newcomblike problems compared to some widely-analyzed previous theories.  Are these *better* answers?  Are they *more rational* answers?\n\nThe argument for considering \"Choose as if controlling the logical output of your decision algorithm\" as the principle of *rational* choice--rather than being 'useful irrationality' or some such--rests on three main pillars:\n\n- The argument that CDT counterfactuals are not inherently any more sensible than LDT counterfactuals, since it's not like there are actual counterfactual worlds floating out there or a previously God-given rule that we must decide based on a particular kind of counterfactual;\n- The argument that 'Why aincha rich?' ought to have considerable force here, since Newcomblike problems are not especially unfair or unrealistic (e.g. voting in elections), and we *can* make our decision algorithm's output be anything we want, to just the same degree as we can control our actions.\n- The argument from greater internal coherence and simplicity:  CDT agents wistfully wish they were more LDT-ish agents.  LDT agents prefer to be LDT, have no need for precommitments to dispute control of their future choices with their future selves, and don't predictably reverse their preferences between different times.\n\n## Freedom of counterfactual imagination\n\nThe standard case for causal decision theory rests *primarily* on the assertion that it is prima facie irrational to act as if, e.g., one-boxing in Newcomb's Problem can cause box B to be full.\n\nIs it not in some sense *true,* after Parfit's driver has conveyed the LDT agent to the city, that in the counterfactual world where the LDT agent does not choose to pay at the time, the LDT agent remains in the city and does not vanish away into the desert?  In this sense, must not the LDT agent be deluded about some question of fact, or be acting as if so deluded?\n\nThe logical decision theorist's response has two major subthrusts:\n\n- Since there are no actual counterfactual worlds floating out there in the void where I performed a different action, describing the world where I acted differently is just an act of imagination.  It isn't *false* if I have some lawful, simple, coherent rule for imagining the conditional results of my actions that isn't a classical causal counterfactual, and this rule makes the rest of my decision theory work well.  \"Counterfactuals were made for humanity, not humanity for counterfactuals.\"\n- I don't one-box on Newcomb's Problem *because* I think it physically causes Box B to be full.  I one-box on Newcomb's Problem because I have computed this output in an entirely different way.  It [petitio_principii begs the question] to assume a rational agent must make its decision by carrying out a particular ritual of cognition about which things physically cause other things, and then criticize me for \"acting as if\" I falsely believe that my choice physically causes Box B to be full.\n\nThe first element of the response says that there are not *actually* alternate Earths floating alongside our planet and clearly visible from here, letting us see with our naked eyes what our action-conditionals should be.  Critiquing an action-conditional on the grounds, \"That counterfactual is *false*,\" is not as straightforward as saying, e.g., \"Your assertion that most humans on Earth have eight legs is *false* under a [ correspondence theory of truth], because we can look around the Earth and see that most people don't have eight legs.\"\n\nThis might be a jarring step in the argument, from the standpoint of a philosophical tradition that's accustomed to, e.g., considering statements about modal necessity to have a truth-value that is evaluated to some heaven-sent set of possible worlds.  But again, on the [ Standard Model of physics], there are not actually any counterfactual worlds floating out there. %note: Even the many-worlds interpretation of quantum mechanics doesn't modify this.  There is no rule saying that there must be a world floating out there for each kind of possible decision we could take, where nothing else has changed except that decision. And from an LDT agent's standpoint, we are asking about the decision-algorithm Q, and its alternate outputs are logical impossibilities; see below.%\n\nWe can fix some logical rule for evaluating a particular kind of $\\operatorname{counterfactual}_1$, such as [ Pearl's intervention] $\\operatorname {do}().$  It can then be a [logical_validity valid] deduction given the [correspondence_truth true] history of our Earth that \"If-$\\operatorname{counterfactual}_1$ Lee Harvey Oswald had not shot John F. Kennedy, nobody else would have.\"  If we understand the fixed logical sense of \"If... hadn't\" in terms of $\\operatorname{counterfactual}_1$, then it can be informative about the history of the actual world to be told, \"If Oswald hadn't shot Kennedy, nobody else would've.\"\n\nThe LDT agent is thinking about a different rule, $\\operatorname{counterfactual}_2$ (which happens to yield the same answer in the case of Kennedy and Oswald).  The logical decision theorist observes both \"There are no actual counterfactual worlds floating out there, at least not where we can see them, so critiquing my output isn't as simple as pointing to an actual-world statement being false\" and \"The point we're debating is exactly whether a rational agent ought to use $\\operatorname{counterfactual}_1$ or $\\operatorname{counterfactual}_2,$ so you can't point to $\\operatorname{counterfactual}_2$'s outputs and declare them 'false' or 'irrational' by comparing them with $\\operatorname{counterfactual}_1.$\"\n\nIn fact, the logical decision theorist can turn around this argument and deliver a critique of classical causal decision theory:  Any expected utility agent does calculate one conditional where a correspondence theory of truth directly applies to the answer, namely the conditional on the action it *actually takes.*  CDT's calculation of this counterfactual conditional on Newcomblike problems is often wrong compared to the actual world.\n\nFor example, in Newcomb's Problem, suppose that the base rate of people one-boxing is 2/3.  Suppose we start with a CDT agent not yet knowing its own decision, %note: If the CDT agent does already know its own decision, why would it still be trying to compute it?% that uses the standard $\\operatorname {do}()$ rules for [counterfactual_do counterfactual surgery].  This agent will calculate that its expected value is (2/3 * \\$1M + \\$1K) if it takes both boxes and (2/3 * \\$1M) otherwise.  This yields the classic CDT answer of 'take both boxes', but it does so by calculating a conditional expected utility premised on 'take both boxes', which yields the quantitatively wrong answer.  Even if afterwards the CDT agent realizes that box B is empty, it will still have calculated an objectively false conditional in order to make its decision.\n\n(As an obvious patch, causal decision theorists have suggested a patched CDT that can observe its own suspected action, update, and then recalculate expected utilities to choose again.  But this patched algorithm is known to go into infinite loops on some Newcomblike problems!  A twice-patched algorithm can prevent infinite loops by randomizing its actions in some cases, in which case a stable solution is guaranteed to exist.  But then the expected utility, calculated conditional on that mixed strategy, is again wrong for the actual world!  See the analysis of [death_in_damascus].)\n\nTaking a step back and looking at the issue from inside the perspective of LDT, some decision algorithm $\\mathsf Q$ is asking about worlds conditional on various actions $a$ or policies $\\pi.$  All but one of these worlds are logically impossible - it is no more possible for $\\mathsf Q$ to have some different output than it actually has, than for 2 + 2 to equal 5.  Usually, while we are deciding, we will not know *which* of our seemingly potential choices are logically impossible; but all except one of them are. %note: If you already know your decision, why are you still trying to decide?  If you know you definitely won't do some action, why bother spending the computing power to evaluate its expected utility?  Some Newcomblike dilemmas can pose [ apparent exceptions to this rule], but that's a longer story.%\n\nSince we are asking about worlds that are mostly logically impossible in any case, we are free to visualize the logically impossible ones in a way that is conducive to ending up rich (see the next subsection) and that has good coherence properties (see the subsection after that).\n\nBut even if we're asking about the 'reasonableness' of the visualizations qua visualizations, a logical decision theorist might say at least the following:\n\n- Our visualization of the conditional that *is* logically possible, and matches actual reality in that regard, ought to match the rest of actual reality (which CDT does not).\n- If I'm similar to another 900 people deciding whether to vote using sufficiently similar algorithms to $\\mathsf Q,$ then it is more 'reasonable' to visualize a world where *all* the outputs of $\\mathsf Q$ move in lockstep, then to visualize only one output varying.\n\nThat is:  If you must imagine a world where 91 is a prime number, at least have it be prime *all* the time, not prime on some occasions and composite on others.  To imagine \"91 is sometimes prime and sometimes composite\" is wrong in an immediately visible way, much faster than we can think of the prime factors 7 and 13.  Supposing \"Maybe $\\mathsf Q$ decides not to vote after all?\" is imagining an 'opaque' impossibility that we haven't yet realized to be impossible.  Supposing \"Maybe my $\\mathsf Q$ outputs 'don't vote' but all the other instances of $\\mathsf Q$ output 'do vote'?\" is transparently impossible.\n\n(Of course this is all a viewpoint from within LDT.  A causal decision theorist could reply that they are just imagining a physical variable changing, and not thinking of any logical algorithms at all.)\n\n%%comment:\n%todo: Move this part to a longer discussion of 'reasonable' counterfactuals.  It has caveats about trying to drive down the probability of worlds you're already inside, in order to resist blackmail and so on.%\n\nAlthough this point is still a bit controversial among logical decision theorists, some logical decision theorists would assert that *on any particular reasoning step,* there's no reason for a rational algorithm to visualize a world that algorithm already knows to be impossible.\n\nE.g., even if Parfit's driver has already conveyed you into the city, you are bothering to imagine 'What if I don't pay?' in order to *verify* that you can't get an even better outcome where you don't pay and are still in the city.  If you're bothering to calculate your actions at all, then your algorithm $\\mathsf Q$ doesn't already know this.\n\nA CDT agent imagines worlds where it two-boxes but still gets \\$1,001,000; this is a decision rule that reasons about *transparently* impossible worlds on its intermediate steps. \n\n%todo:\nMore points to talk about in a section on reasonable counterfactuals:  A CDT agent could reply that they're just imagining some local exception to the laws of physics; but then they're not exactly visualizing OMG MAGIC in those worlds, so they are trying to visualize the logical impossibility after all.\n%\n%%\n\n## Newcomblike dilemmas are a fair problem class\n\nThe classic objection to causal decision theory has always been, \"If you're so rational, why aincha rich?\"  The classic reply is summarized in e.g. \"Foundations of Causal Decision Theory\" by James Joyce: %note: No, not that James Joyce.%\n\n> Rachel has a perfectly good answer to the \"Why ain't you rich?\" question. \"I am not rich,\" she will say, \"because I am not the kind of person the psychologist thinks will refuse the money. I'm just not like you, Irene. Given that I know that I am the type who takes the money, and given that the psychologist knows that I am this type, it was reasonable of me to think that the \\$1,000,000 was not in my account. The \\$1,000 was the most I was going to get no matter what I did. So the only reasonable thing for me to do was to take it.\"\n> \n> Irene may want to press the point here by asking, “But don’t you wish you were like me, Rachel? Don’t you wish that you were the refusing type?” There is a tendency to think that Rachel, a committed causal decision theorist, must answer this question in the negative, which seems obviously wrong (given that being like Irene would have made her rich). This is not the case. Rachel can and should admit that she does wish she were more like Irene. “It would have been better for me,” she might concede, “had I been the refusing type.” At this point Irene will exclaim, “You’ve admitted it! It wasn’t so smart to take the money after all.” Unfortunately for Irene, her conclusion does not follow from Rachel’s premise. Rachel will patiently explain that wishing to be a refuser in a Newcomb problem is not inconsistent with thinking that one should take the $1,000 whatever type one is. When Rachel wishes she was Irene’s type she is wishing for Irene’s options, not sanctioning her choice.\n\nThis accuses Omega of simply being prejudiced against rational agents--by the time the experiment starts, rational agents have already been disadvantaged.\n\nBut Omega is not *per se* disadvantaging rational agents, or causal decision agents in particular.  We can imagine that Omicron goes about whacking CDT agents in the head with a sledgehammer, *regardless of what they choose,* and this would indeed seems 'unfair', in the sense that it seems we can't deduce anything about the fitness of CDT agents from their doing worse in Omicron's dilemma.  We can imagine that Upsilon puts a million dollars in Box B only if you are a sort of agent that chooses between 'one-boxing' and 'two-boxing' *by choosing the first option in alphabetical order,* and if you one-box for any other reason, Upsilon empties the box.\n\nOmicron and Upsilon are indeed privileging particular algorithms; their rules make explicit mention of \"CDT\" or \"alphabetical ordering\"; they care *why* you behave a certain way and not just that you do so.  But Omega does not care *why* you one-box. %note: In the original formulation of Newcomb's Problem, it was said that Omega leaves Box B empty if you try to choose by flipping a coin.  This is one reason to prefer the formulation where Omega can predict coinflips.% You can one-box because of LDT, or EDT, or because you're the sort of agent that always prefers the option highest in alphabetical order; Omega will fill Box B just the same.\n%%comment:  %note:  On some views of the nature of rationality, it's exactly *because* Omega only cares what we do, and not why we do it, that Newcomb's Problem is a test of rationality; 'rationality' *is* exactly what we do when we only care about the results and not the reasoning we use to get them.% %%\n\nAccording to a logical decision theorist, when a problem depends just on our *behavior* or \"the type of decisions that we make, being the people that we are\" and not on any other properties of our algorithm apart from that, then that seems like a sufficient condition to designate the problem as 'fair'.\n\nIndeed, we can see decision theories as *corresponding to* a class of problems that they think are [fair_problem_class fair]:\n\n- CDT thinks a problem is 'fair' if your results depend only on your physical act, and not on anyone else's predictions about your physical act or any other logical correlations that don't stem from the physical act.  On problems in this class, CDT agents always end up as rich as any other agent encountering the problem.\n- [functional_dt Functional decision theory] thinks it's 'fair' for a problem to depend at any point, on any logical or physical consequence, of any disposition you have to behave in any way, in any situation; so long as the problem depends *only* on this behavioral disposition and not on any other aspect of your code or algorithm apart from that.\n- In [proofbased_dt] and [modal_agents], your payoff may depend on whether other agents can *prove* that you behave a certain way, not just whether you actually behave that way.  For example, other agents may cooperate with you only if they can *prove* that you cooperate with them.\n\nA causal decision theorist might argue that on an *ideal* version of [parfits_hitchhiker], the driver is really making their decision by looking at our face; and that if we were ideal CDT agents, we'd be able to control this behavior and optimize the real, physical channel by which we are influencing the driver.\n\nA logical decision theorist replies, \"Okay, but maybe I don't have perfect control of my face, and therefore the output of my algorithm $\\mathsf Q(city)$ is affecting both what I buy in the city and my current prediction of what I'll buy in the city, which in turn affects my facial expression.  In real life, I'm not good enough at deception or self-deception to break this logical correlation.  So the logical correlation is actually there and we need to use a decision theory that can handle the wider problem class.  Why is that so terribly unfair?\"\n\nOr similarly in the case of voting in elections: maybe we just are in fact logically entangled with a cohort of people thinking similarly to ourselves, and nothing in particular is going to move us into the problem class where this correlation doesn't exist.  Why is that unfair?\n\n(The LDT [fair_problem_class class of 'fair' problems] comes very close to *dominating* the CDT class, that is, it is very nearly true that LDT agents think a strictly larger class of problems are fair and do well in a strictly wider set of situations.  But since (on the LDT view) CDT agents are blind to some LDT-relevant correlations, it is possible to construct LDT-unfair dilemmas that CDT agents think are fair.  For example, suppose Omega would have bombed an orphanage two days ago if the LDT algorithm in particular yielded the output of picking up a certain \\$5 bill in the street, but Omega doesn't similarly discriminate against the CDT algorithm.  The CDT agent cheerfully walks over and picks up the \\$5 bill, *and* believes that the superstitious LDT agent would have received just the same payoff for this same physical action, making the problem CDT-fair.  For an *arguably* structually similar, but much more natural-seeming problem, see [100ldt_1cdt_pd_tournament here].)\n\nAnother way of seeing decision theories as corresponding to problem classes is by looking at the considerations that the decision theory allows itself to take into account; decision theories generally think that considerations they are not allowed to take into account are unfair.\n\nIn the passage from before on Rachel and Irene, James Joyce continues:\n\n> Rational Rachel recognizes that, whether she is the type that was predicted (on Friday) to take the money or the type that was predicted to refuse it, **there is nothing she can do now to alter her type.**  She thus has no reason to pass up the extra \\$1,000. \\[emphasis added.\\]\n\nFrom an intuitive standpoint, an LDT agent sees Rachel dooming herself to two-box *only* because she believes herself to be powerless; if she believed herself to be in control of her type, she could [logical_control control] her type.\n\nA more formal view would be that an LDT agent becomes the one-boxing type because the logical algorithm $\\mathsf Q$ computes its logical output--the 'type' that everything dependent on the logical output of $\\mathsf Q$ depends on--by taking into account *all* the consequences of that 'type'.  As for any disempowering thoughts about it being \"too late\" to control consequences of $\\mathsf Q$ that occur before some particular time, $\\mathsf Q$ isn't built to specially exclude those consequences from its calculation.  If someone objects to the term 'control', it can at least be said that $\\mathsf Q$ has a symmetry in that everything affected by its 'type' is being modeled in the calculation that *determines* the type.\n\nIf sometimes logical correlations do in fact exist--as in the case of voting; or as in the case of not being able to perfectly control our facial muscles or beliefs when facing the driver in [parfits_hitchhiker]; or as in the case of a machine agent e.g. running on Ethereum or whose code has become known by some other channel--then in what sense is it rational for an algorithm to wantonly exclude some consequences of the algorithm's 'type', from being weighed into the calculation that determines the algorithm's 'type'?\n\n%%comment:\n\n%todo: move this to a separate section on control, maybe in the defense of counterfactuals. % \n\nRachel's algorithm $\\mathsf R$ is computing its answer right that very moment, deciding right then what type of agent to be.  While $\\mathsf R$ is fated in some sense to two-box, it 'could' have been a one-boxing algorithm to just the same extent that any of us 'could' perform some action that is the better for us.  If we can regard ourselves as [logical_control controlling] our physical acts, we might as well regard ourselves as controlling 'the type of decision we make'.  \n\nAs for the charge that the key proposition lies in the physical past, Gary Drescher observes that we can by raising our hand today, [logical_control control] a certain proposition about the state of the universe a billion years earlier: namely, 'The state of the universe today is such that its development under physical law will lead to (your name here) raising their hand a billion years later.'  Similarly, we can today, by an act of will control 'the sort of decision we will make a day later, being the person that we are' as it was true about our physical state yesterday. %%\n\n## Consistency and elegance\n\nA CDT agent, given the chance to make preparations before Omega's arrival, might pay a \\$100 fee (or \\$100,000 fee) to have an assistant stand nearby and threaten to shoot them if they don't leave behind Box A.\n\nThen, given the chance later by surprise, the same CDT agent would pay \\$100 to make the gun-toting assistant go away--even believing that Omega has accurately predicted this, and that Box B is therefore empty.\n\nThis is an example of what an economic psychologist or behavioral economist would call a [ dynamic inconsistency].  If Omega visits at 7:30am, then the CDT agent at 7am and the same CDT agent at 8am have different preferences about what they respectively want the CDT agent to do at 8am.  The CDT agent at 7am will pay precommitment costs to try to wrench control away from the CDT agent at 8am; the CDT agent at 8am will pay costs to wrench control back.\n\nHistorically speaking, this property of CDT was especially important from the perspective of the people with a computer-science orientation who helped to develop the family of logical decision theories.  If CDT is dynamically inconsistent then it is of necessity [2rb reflectively inconsistent]; that is, the CDT algorithm does not want its future self to use the CDT algorithm.  If one were to suppose a self-modifying AI that started out using CDT, it would immediately modify itself to use a [son_of_CDT different theory] instead.\n\n\n\nFrom the standpoint of an LDT agent, this is no more excusable than an agent exhibiting non-Bayesian behavior on the [allais_paradox Allais Paradox], paying \\$1 to throw a switch and then paying another \\$1 to throw it back.  On the paradigm of a logical decision theorist, a rational agent would not exhibit such incoherence and drive itself around in circles; the true principle of rational choice ought not to be so inconsistent.  We sometimes learn new information that changes our instrumental preferences, but a rational agent with a constant utility function should not be able to *predict* preference reversals any more than a rational agent should be able to predict a net directional change in its probability estimates.\n\n\n\n\n%todo:\nfreedom of counterfactual;\nobjection from logical impossibility;\nwhy aincha rich? fairness of Newcomblike problems and dominance;\nargument from coherence:\n vs. Son of CDT;\n%\n\n%comment:\n(actually move this to conclusion, after presenting more interesting results)\nThe thesis is that when all these pillars are considered, we end up in a situation where, in an intuitive or pretheoretic sense, LDT seems to have it all over CDT.  That is, CDT may still seem more 'rational' (if not richer!) from within the standpoint of a pure CDT agent that can't consider any other view; but if LDT and CDT were being presented side-by-side to someone who had not as yet considered either, CDT would have little or no appeal.\n%\n\n\n",
      "metaText": "",
      "isTextLoaded": true,
      "isSubscribedToDiscussion": false,
      "isSubscribedToUser": false,
      "isSubscribedAsMaintainer": false,
      "discussionSubscriberCount": 1,
      "maintainerCount": 1,
      "userSubscriberCount": 0,
      "lastVisit": "",
      "hasDraft": false,
      "votes": [],
      "voteSummary": [],
      "muVoteSummary": 0,
      "voteScaling": 0,
      "currentUserVote": 0,
      "voteCount": 0,
      "lockedVoteType": "",
      "maxEditEver": 18,
      "redLinkCount": 0,
      "lockedBy": "2",
      "lockedUntil": "2018-06-01 18:51:52",
      "nextPageId": "",
      "prevPageId": "",
      "usedAsMastery": false,
      "proposalEditNum": 0,
      "permissions": {
        "edit": {
          "has": false,
          "reason": "You don't have domain permission to edit this page"
        },
        "proposeEdit": {
          "has": true,
          "reason": ""
        },
        "delete": {
          "has": false,
          "reason": "You don't have domain permission to delete this page"
        },
        "comment": {
          "has": false,
          "reason": "You can't comment in this domain because you are not a member"
        },
        "proposeComment": {
          "has": true,
          "reason": ""
        }
      },
      "summaries": {},
      "creatorIds": [],
      "childIds": [],
      "parentIds": [
        "58b"
      ],
      "commentIds": [],
      "questionIds": [],
      "tagIds": [
        "3rk"
      ],
      "relatedIds": [],
      "markIds": [],
      "explanations": [],
      "learnMore": [],
      "requirements": [],
      "subjects": [
        {
          "id": "5381",
          "parentId": "58b",
          "childId": "5gc",
          "type": "subject",
          "creatorId": "2",
          "createdAt": "2016-07-17 23:55:28",
          "level": 2,
          "isStrong": true,
          "everPublished": true
        }
      ],
      "lenses": [],
      "lensParentId": "58b",
      "pathPages": [],
      "learnMoreTaughtMap": {},
      "learnMoreCoveredMap": {},
      "learnMoreRequiredMap": {},
      "editHistory": {},
      "domainSubmissions": {},
      "answers": [],
      "answerCount": 0,
      "commentCount": 0,
      "newCommentCount": 0,
      "linkedMarkCount": 0,
      "changeLogs": [
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "23016",
          "pageId": "5gc",
          "userId": "2",
          "edit": 18,
          "type": "newEdit",
          "createdAt": "2018-06-01 18:51:52",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "23015",
          "pageId": "5gc",
          "userId": "2",
          "edit": 17,
          "type": "newEdit",
          "createdAt": "2018-06-01 18:47:24",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "22244",
          "pageId": "5gc",
          "userId": "2",
          "edit": 16,
          "type": "newEdit",
          "createdAt": "2017-03-03 18:52:41",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "22243",
          "pageId": "5gc",
          "userId": "2",
          "edit": 0,
          "type": "newEditGroup",
          "createdAt": "2017-03-03 18:52:39",
          "auxPageId": "15",
          "oldSettingsValue": "123",
          "newSettingsValue": "15"
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "20164",
          "pageId": "5gc",
          "userId": "2",
          "edit": 15,
          "type": "newEdit",
          "createdAt": "2016-10-16 05:07:13",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "20162",
          "pageId": "5gc",
          "userId": "2",
          "edit": 14,
          "type": "newEdit",
          "createdAt": "2016-10-16 04:34:50",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "20161",
          "pageId": "5gc",
          "userId": "2",
          "edit": 13,
          "type": "newEdit",
          "createdAt": "2016-10-16 04:34:07",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "20160",
          "pageId": "5gc",
          "userId": "2",
          "edit": 12,
          "type": "newEdit",
          "createdAt": "2016-10-16 04:26:17",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "20159",
          "pageId": "5gc",
          "userId": "2",
          "edit": 11,
          "type": "newEdit",
          "createdAt": "2016-10-16 04:25:45",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "20154",
          "pageId": "5gc",
          "userId": "2",
          "edit": 10,
          "type": "newEdit",
          "createdAt": "2016-10-16 03:59:36",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "18301",
          "pageId": "5gc",
          "userId": "2",
          "edit": 0,
          "type": "newTag",
          "createdAt": "2016-08-03 22:39:21",
          "auxPageId": "3rk",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "18269",
          "pageId": "5gc",
          "userId": "2",
          "edit": 9,
          "type": "newEdit",
          "createdAt": "2016-08-03 20:03:17",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "17473",
          "pageId": "5gc",
          "userId": "2",
          "edit": 8,
          "type": "newEdit",
          "createdAt": "2016-07-25 00:50:45",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "17472",
          "pageId": "5gc",
          "userId": "2",
          "edit": 7,
          "type": "newEdit",
          "createdAt": "2016-07-25 00:46:42",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "17470",
          "pageId": "5gc",
          "userId": "2",
          "edit": 0,
          "type": "newAlias",
          "createdAt": "2016-07-25 00:45:33",
          "auxPageId": "",
          "oldSettingsValue": "5gc",
          "newSettingsValue": "ldt_intro_phil"
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "17471",
          "pageId": "5gc",
          "userId": "2",
          "edit": 6,
          "type": "newEdit",
          "createdAt": "2016-07-25 00:45:33",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "17468",
          "pageId": "5gc",
          "userId": "2",
          "edit": 0,
          "type": "newTag",
          "createdAt": "2016-07-25 00:42:39",
          "auxPageId": "4v4",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "17467",
          "pageId": "5gc",
          "userId": "2",
          "edit": 0,
          "type": "deleteTag",
          "createdAt": "2016-07-25 00:42:26",
          "auxPageId": "4v",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "17465",
          "pageId": "5gc",
          "userId": "2",
          "edit": 5,
          "type": "newEdit",
          "createdAt": "2016-07-25 00:42:15",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "17422",
          "pageId": "5gc",
          "userId": "2",
          "edit": 4,
          "type": "newEdit",
          "createdAt": "2016-07-23 18:48:45",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "17374",
          "pageId": "5gc",
          "userId": "2",
          "edit": 3,
          "type": "newEdit",
          "createdAt": "2016-07-23 01:56:29",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "17258",
          "pageId": "5gc",
          "userId": "2",
          "edit": 2,
          "type": "newEdit",
          "createdAt": "2016-07-21 22:01:35",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "17207",
          "pageId": "5gc",
          "userId": "2",
          "edit": 0,
          "type": "newTag",
          "createdAt": "2016-07-21 01:36:24",
          "auxPageId": "4v",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "17209",
          "pageId": "5gc",
          "userId": "2",
          "edit": 0,
          "type": "newSubject",
          "createdAt": "2016-07-21 01:36:24",
          "auxPageId": "58b",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "17206",
          "pageId": "5gc",
          "userId": "2",
          "edit": 0,
          "type": "newParent",
          "createdAt": "2016-07-21 01:36:23",
          "auxPageId": "58b",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        },
        {
          "likeableId": "0",
          "likeableType": "changeLog",
          "myLikeValue": 0,
          "likeCount": 0,
          "dislikeCount": 0,
          "likeScore": 0,
          "individualLikes": [],
          "id": "17204",
          "pageId": "5gc",
          "userId": "2",
          "edit": 1,
          "type": "newEdit",
          "createdAt": "2016-07-21 01:36:22",
          "auxPageId": "",
          "oldSettingsValue": "",
          "newSettingsValue": ""
        }
      ],
      "feedSubmissions": [],
      "searchStrings": {},
      "hasChildren": false,
      "hasParents": true,
      "redAliases": {},
      "improvementTagIds": [],
      "nonMetaTagIds": [],
      "todos": [],
      "slowDownMap": null,
      "speedUpMap": null,
      "arcPageIds": null,
      "contentRequests": {}
    }
  },
  "users": {
    "1": {
      "id": "1",
      "firstName": "Alexei",
      "lastName": "Andreev",
      "lastWebsiteVisit": "2018-02-18 09:35:21",
      "isSubscribed": false,
      "domainMembershipMap": {}
    },
    "2": {
      "id": "2",
      "firstName": "Eliezer",
      "lastName": "Yudkowsky",
      "lastWebsiteVisit": "2019-12-21 03:34:41",
      "isSubscribed": false,
      "domainMembershipMap": {}
    },
    "5": {
      "id": "5",
      "firstName": "Eric",
      "lastName": "Rogstad",
      "lastWebsiteVisit": "2019-08-23 01:44:10",
      "isSubscribed": false,
      "domainMembershipMap": {}
    },
    "12y": {
      "id": "12y",
      "firstName": "Patrick",
      "lastName": "LaVictoire",
      "lastWebsiteVisit": "2019-04-18 00:41:25",
      "isSubscribed": false,
      "domainMembershipMap": {}
    },
    "1yq": {
      "id": "1yq",
      "firstName": "Eric",
      "lastName": "Bruylant",
      "lastWebsiteVisit": "2017-04-14 18:00:22",
      "isSubscribed": false,
      "domainMembershipMap": {}
    },
    "2vh": {
      "id": "2vh",
      "firstName": "Jaime",
      "lastName": "Sevilla Molina",
      "lastWebsiteVisit": "2018-12-06 12:14:41",
      "isSubscribed": false,
      "domainMembershipMap": {}
    },
    "5yw": {
      "id": "5yw",
      "firstName": "Duncan",
      "lastName": "Wilson",
      "lastWebsiteVisit": "2017-06-25 01:22:13",
      "isSubscribed": false,
      "domainMembershipMap": {}
    },
    "8pb": {
      "id": "8pb",
      "firstName": "Jacob",
      "lastName": "van Eeden",
      "lastWebsiteVisit": "2017-10-02 19:17:29",
      "isSubscribed": false,
      "domainMembershipMap": {}
    }
  },
  "domains": {
    "1": {
      "id": "1",
      "pageId": "1lw",
      "createdAt": "2016-01-15 03:02:51",
      "alias": "math",
      "canUsersComment": false,
      "canUsersProposeComment": true,
      "canUsersProposeEdits": true,
      "friendDomainIds": []
    },
    "2": {
      "id": "2",
      "pageId": "2v",
      "createdAt": "2015-03-26 23:12:18",
      "alias": "value_alignment",
      "canUsersComment": false,
      "canUsersProposeComment": true,
      "canUsersProposeEdits": true,
      "friendDomainIds": []
    },
    "3": {
      "id": "3",
      "pageId": "3d",
      "createdAt": "2015-03-30 22:19:47",
      "alias": "Arbital",
      "canUsersComment": false,
      "canUsersProposeComment": true,
      "canUsersProposeEdits": true,
      "friendDomainIds": []
    },
    "8": {
      "id": "8",
      "pageId": "198",
      "createdAt": "2015-12-13 23:14:48",
      "alias": "TeamArbital",
      "canUsersComment": false,
      "canUsersProposeComment": true,
      "canUsersProposeEdits": true,
      "friendDomainIds": []
    },
    "15": {
      "id": "15",
      "pageId": "58c",
      "createdAt": "2016-07-08 18:23:14",
      "alias": "DecisionTheory",
      "canUsersComment": false,
      "canUsersProposeComment": true,
      "canUsersProposeEdits": true,
      "friendDomainIds": []
    },
    "21": {
      "id": "21",
      "pageId": "1",
      "createdAt": "2015-02-10 17:12:19",
      "alias": "AlexeiAndreev",
      "canUsersComment": false,
      "canUsersProposeComment": true,
      "canUsersProposeEdits": true,
      "friendDomainIds": [
        "2069"
      ]
    },
    "32": {
      "id": "32",
      "pageId": "12y",
      "createdAt": "2015-09-02 21:53:02",
      "alias": "PatrickLaVictoir",
      "canUsersComment": false,
      "canUsersProposeComment": true,
      "canUsersProposeEdits": true,
      "friendDomainIds": []
    },
    "123": {
      "id": "123",
      "pageId": "2",
      "createdAt": "2015-03-05 18:45:34",
      "alias": "EliezerYudkowsky",
      "canUsersComment": false,
      "canUsersProposeComment": true,
      "canUsersProposeEdits": true,
      "friendDomainIds": []
    }
  },
  "masteries": {
    "58b": {
      "pageId": "58b",
      "has": false,
      "wants": false,
      "level": 0,
      "updatedAt": ""
    }
  },
  "marks": {},
  "pageObjects": {},
  "result": {},
  "globalData": null
}
