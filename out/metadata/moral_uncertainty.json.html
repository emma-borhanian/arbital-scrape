<!DOCTYPE html><html><head><meta charset="utf-8"><title>Moral uncertainty</title><link rel="stylesheet" type="text/css" href="../common.css"></head><body><pre style="white-space:pre-wrap">{
  localUrl: '<a href="../page/moral_uncertainty.html">../page/moral_uncertainty.html</a>',
  arbitalUrl: '<a href="https://arbital.com/p/moral_uncertainty">https://arbital.com/p/moral_uncertainty</a>',
  rawJsonUrl: '<a href="../raw/7s2.json">../raw/7s2.json</a>',
  likeableId: '<a href="0.json.html">0</a>',
  likeableType: 'page',
  myLikeValue: '0',
  likeCount: '0',
  dislikeCount: '0',
  likeScore: '0',
  individualLikes: [],
  pageId: '<a href="moral_uncertainty.json.html">moral_uncertainty</a>',
  edit: '1',
  editSummary: '',
  prevEdit: '0',
  currentEdit: '1',
  wasPublished: 'true',
  type: 'wiki',
  title: 'Moral uncertainty',
  clickbait: 'A meta-utility function in which the utility function as usually considered, takes on different values in different possible worlds, potentially distinguishable by evidence.',
  textLength: '2104',
  alias: 'moral_uncertainty',
  externalUrl: '',
  sortChildrenBy: 'likes',
  hasVote: 'false',
  voteType: '',
  votesAnonymous: 'false',
  editCreatorId: '<a href="EliezerYudkowsky.json.html">EliezerYudkowsky</a>',
  editCreatedAt: '2017-02-08 04:40:43',
  pageCreatorId: '<a href="EliezerYudkowsky.json.html">EliezerYudkowsky</a>',
  pageCreatedAt: '2017-02-08 04:40:43',
  seeDomainId: '<a href="0.json.html">0</a>',
  editDomainId: '<a href="EliezerYudkowsky.json.html">EliezerYudkowsky</a>',
  submitToDomainId: '<a href="0.json.html">0</a>',
  isAutosave: 'false',
  isSnapshot: 'false',
  isLiveEdit: 'true',
  isMinorEdit: 'false',
  indirectTeacher: 'false',
  todoCount: '0',
  isEditorComment: 'false',
  isApprovedComment: 'false',
  isResolved: 'false',
  snapshotText: '',
  anchorContext: '',
  anchorText: '',
  anchorOffset: '0',
  mergedInto: '',
  isDeleted: 'false',
  viewCount: '94',
  text: '[summary: &quot;Moral uncertainty&quot; in the context of AI refers to an agent with an &quot;uncertain utility function&quot;. That is, we can view the agent as pursuing a [1fw utility function] that takes on different values in different subsets of possible worlds.\n\nFor example, an agent might have a [meta_utility meta-utility function] saying that eating cake has a utility of &amp;euro;8 in worlds where Lee Harvey Oswald shot John F. Kennedy and that eating cake has a utility of &amp;euro;10 in worlds where it was the other way around.  This agent will be motivated to inquire into political history to find out which utility function is probably the &#39;correct&#39; one (relative to this meta-utility function).]\n\n&quot;Moral uncertainty&quot; in the context of AI refers to an agent with an &quot;uncertain utility function&quot;.  That is, we can view the agent as pursuing a [1fw utility function] that takes on different values in different subsets of possible worlds.\n\nFor example, an agent might have a [meta_utility meta-utility function] saying that eating cake has a utility of &amp;euro;8 in worlds where Lee Harvey Oswald shot John F. Kennedy and that eating cake has a utility of &amp;euro;10 in worlds where it was the other way around.  This agent will be motivated to inquire into political history to find out which utility function is probably the &#39;correct&#39; one (relative to this meta-utility function), though it will never be [4mq absolutely sure].\n\nMoral uncertainty must be resolvable by some conceivable observation in order to function as uncertainty.  Suppose for example that an agent&#39;s probability distribution $\\Delta U$ over the &#39;true&#39; utility function $U$ asserts a dependency on a fair quantum coin that was flipped inside a sealed box then destroyed by explosives: the utility function is $U_1$ over outcomes in the worlds where the coin came up heads, and if the coin came up tails the utility function is $U_2.$  If the agent thinks it has no way of ever figuring out what happened inside the box, it will thereafter behave as if it had a single, constant, certain utility function equal to $0.5 \\cdot U_1 + 0.5 \\cdot U_2.$',
  metaText: '',
  isTextLoaded: 'true',
  isSubscribedToDiscussion: 'false',
  isSubscribedToUser: 'false',
  isSubscribedAsMaintainer: 'false',
  discussionSubscriberCount: '1',
  maintainerCount: '1',
  userSubscriberCount: '0',
  lastVisit: '',
  hasDraft: 'false',
  votes: [],
  voteSummary: 'null',
  muVoteSummary: '0',
  voteScaling: '0',
  currentUserVote: '-2',
  voteCount: '0',
  lockedVoteType: '',
  maxEditEver: '0',
  redLinkCount: '0',
  lockedBy: '',
  lockedUntil: '',
  nextPageId: '',
  prevPageId: '',
  usedAsMastery: 'false',
  proposalEditNum: '0',
  permissions: {
    edit: {
      has: 'false',
      reason: 'You don&#39;t have domain permission to edit this page'
    },
    proposeEdit: {
      has: 'true',
      reason: ''
    },
    delete: {
      has: 'false',
      reason: 'You don&#39;t have domain permission to delete this page'
    },
    comment: {
      has: 'false',
      reason: 'You can&#39;t comment in this domain because you are not a member'
    },
    proposeComment: {
      has: 'true',
      reason: ''
    }
  },
  summaries: {},
  creatorIds: [
    '<a href="EliezerYudkowsky.json.html">EliezerYudkowsky</a>'
  ],
  childIds: [
    '<a href="ideal_target.json.html">ideal_target</a>'
  ],
  parentIds: [
    '<a href="preference_framework.json.html">preference_framework</a>'
  ],
  commentIds: [],
  questionIds: [],
  tagIds: [
    '<a href="start_meta_tag.json.html">start_meta_tag</a>'
  ],
  relatedIds: [],
  markIds: [],
  explanations: [],
  learnMore: [],
  requirements: [],
  subjects: [],
  lenses: [],
  lensParentId: '',
  pathPages: [],
  learnMoreTaughtMap: {},
  learnMoreCoveredMap: {},
  learnMoreRequiredMap: {},
  editHistory: {},
  domainSubmissions: {},
  answers: [],
  answerCount: '0',
  commentCount: '0',
  newCommentCount: '0',
  linkedMarkCount: '0',
  changeLogs: [
    {
      likeableId: '<a href="0.json.html">0</a>',
      likeableType: 'changeLog',
      myLikeValue: '0',
      likeCount: '0',
      dislikeCount: '0',
      likeScore: '0',
      individualLikes: [],
      id: '21967',
      pageId: '<a href="moral_uncertainty.json.html">moral_uncertainty</a>',
      userId: '<a href="EliezerYudkowsky.json.html">EliezerYudkowsky</a>',
      edit: '0',
      type: 'newChild',
      createdAt: '2017-02-08 04:56:28',
      auxPageId: '<a href="ideal_target.json.html">ideal_target</a>',
      oldSettingsValue: '',
      newSettingsValue: ''
    },
    {
      likeableId: '<a href="0.json.html">0</a>',
      likeableType: 'changeLog',
      myLikeValue: '0',
      likeCount: '0',
      dislikeCount: '0',
      likeScore: '0',
      individualLikes: [],
      id: '21964',
      pageId: '<a href="moral_uncertainty.json.html">moral_uncertainty</a>',
      userId: '<a href="EliezerYudkowsky.json.html">EliezerYudkowsky</a>',
      edit: '0',
      type: 'newParent',
      createdAt: '2017-02-08 04:40:53',
      auxPageId: '<a href="preference_framework.json.html">preference_framework</a>',
      oldSettingsValue: '',
      newSettingsValue: ''
    },
    {
      likeableId: '<a href="0.json.html">0</a>',
      likeableType: 'changeLog',
      myLikeValue: '0',
      likeCount: '0',
      dislikeCount: '0',
      likeScore: '0',
      individualLikes: [],
      id: '21965',
      pageId: '<a href="moral_uncertainty.json.html">moral_uncertainty</a>',
      userId: '<a href="EliezerYudkowsky.json.html">EliezerYudkowsky</a>',
      edit: '0',
      type: 'newTag',
      createdAt: '2017-02-08 04:40:53',
      auxPageId: '<a href="start_meta_tag.json.html">start_meta_tag</a>',
      oldSettingsValue: '',
      newSettingsValue: ''
    },
    {
      likeableId: '<a href="0.json.html">0</a>',
      likeableType: 'changeLog',
      myLikeValue: '0',
      likeCount: '0',
      dislikeCount: '0',
      likeScore: '0',
      individualLikes: [],
      id: '21962',
      pageId: '<a href="moral_uncertainty.json.html">moral_uncertainty</a>',
      userId: '<a href="EliezerYudkowsky.json.html">EliezerYudkowsky</a>',
      edit: '1',
      type: 'newEdit',
      createdAt: '2017-02-08 04:40:43',
      auxPageId: '',
      oldSettingsValue: '',
      newSettingsValue: ''
    }
  ],
  feedSubmissions: [],
  searchStrings: {},
  hasChildren: 'true',
  hasParents: 'true',
  redAliases: {},
  improvementTagIds: [],
  nonMetaTagIds: [],
  todos: [],
  slowDownMap: 'null',
  speedUpMap: 'null',
  arcPageIds: 'null',
  contentRequests: {}
}</pre></body></html>